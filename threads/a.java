
	              Do you remember the first time you used a computer? Was it as an adult, in the workplace, struggling to adjust to a new technology? Or did you grow up surrounded by technology, and start using new electronics as soon as they became available? Is your technology story a bit of both, or neither?
Your Instructor has posted to the Discussion Board, sharing a personal history of computing. This includes the types of devices used, and how the Instructor’s experience of computers has changed over the years. Now it is your turn: respond to your Instructor’s post with your own history of computing.
To prepare for this Discussion

Review the Learning Resources for this Week, especially the Lecture Notes and ‘Sample online discussion interaction’.
Read your Instructor’s personal history of computing, posted in the Discussion Board.
Reflect on your own history of experience with computers and computing.

To complete this DiscussionPost:&nbsp;Create an initial post in which you share your own personal history with computers and computing. Address the following:

Your first experiences using a computer.
Changes in the types of computer devices you have used over time (e.g. desktop, notebook, PDA, smart phone, tablet, portable music players).
How common these types of devices are in your workplace or community.
Changes in tasks for which you have used computers and electronic devices.
Changes in your comfort level with computers over time.

Respond:&nbsp;Respond to your colleagues. Address the following:

Compare and contrast the differences between your experiences with those of your colleagues, particularly differences in the availability of certain kinds of devices in your communities, and relative levels of comfort using these devices.
Critically reflect on the implications of these differences for implementing technology in a workplace in each of these communities.

How to Respond to DiscussionsThe participation component will assess the students’ contribution to the class discussions in the seminar. In particular it will assess the extent to which students:

Contribute original views.
Build creatively on the views of others.
Show intelligent critique of theory or practice.
Add an international or cultural aspect to the class.
Show originality in the application of theory.
Clarify the theory being studied.
Encourage the effective contribution of other members of the class.

The ‘Participation’ component has two elements, attendance and contribution:

Attendance is evidenced by making a substantial posting to a Discussion thread on a minimum of 3 days in the week, not including initial DQ responses. ‘Substantial’ postings will normally be in the order of 2–3 paragraphs and 3–5 postings per DQ, not including the initial DQ responses, or those responses required by other assignments.
Contribution is evidenced by value of the postings to the learning of the class.

Instructors will evaluate participation based on both of these criteria (attendance and contribution). Note that, for minimum passing grade, participation posts must be balanced during the week and not posted primarily in the last day or two of the seminar.Remember to cite and reference examples from the Learning Resources and journal articles for this week.
For all Discussions&nbsp;(unless stated otherwise):

Create a single document with your initial post. Your document should be 350–500 words, though you will be marked based on the quality of your writing, not on the number of words.
By Sunday, post the text of your document to the Discussion Board for this Week, and upload the document using the Turnitin submission link for this Discussion.
By Wednesday, make 3–5 substantial follow-up responses to your colleagues. These can include responses to your colleagues’ initial posts, as well as responses to colleagues who responded to your own initial post. Your total Discussion Board participation must occur on at least 3 individual days during each week. Follow-up responses should be significant contributions to the Discussion. Do not submit your follow-up responses to Turnitin.
In general, online discussion is best when you:


Ask insightful questions.
Extend the discussion into new but relevant areas.
Model or promote critical reflection.
Support your arguments with citations and references from the assigned Learning Resources and other literature, using Harvard Liverpool Referencing Style.



Ensure that you spread your discussion posts across at least three separate days of each week. This will help maximise the value of your discussion with colleagues and serve to meet the learning objectives for each activity.
Click on the&nbsp;Reply&nbsp;button below to reveal the textbox for entering your message. Then click on the&nbsp;Submit&nbsp;button to post your message.

	              
	              Interesting geeky story. I don't have the answer to your questions since I wasn't born yet.
Kidding:-) Let me try by googling.
1."England versus the West Indies at the Port of Spain Oval in Trinidad" : year 1968.
2. I'm not quite sure about the definition of mainframe computers and "pro-mainframe computers". It seems UNIVAC I wa the first computer people call it mainframe, which is made by a company named&nbsp;Eckert–Mauchly Computer Corporation. With UNIVAC I the addtion takes 525 microseconds.&nbsp;My Samsung Galaxy 5 has a 2.5GH quad-core CPU. Assuming the addition in my phone CPU take one cycle and it can fully utilize the mulitple core capability, the difference would be:&nbsp;
(525*10^-6) / (1/(2.5 * 10^9)/4) = 5, 250, 000 times faster
3. I assume the question asks for electronic calculator. I can only guess it's a Japanese company.
4.&nbsp;The first really widespread high-level language was Fortran, but the very first one is&nbsp;&nbsp;Plankalkül.

	              
	              Hi ,

Loved the story especially the ending.....

Before writing my own out I thought I'd have a go at the quiz and without looking up any of the answers I thought i'd just have a gutsy shot off the top of my head. I'll probably end up embarrasing myself but no harm in having a bit of fun to start.

1) not sure the year exactly but punch cards have been around and used for ages and I would guess still used to start people off in learning computers for some time after.. I'd take a shot at mid 1970's around 1975.. reason being the song. David Cassisdy I'm sure was a 70's singer
2) First commercial mainframe I'd guess IBM, no clue as to the spec without looking it up but i'd imaging they would be some 1000 times slower than a modern day iphone or the likes.
3) I would take a guess at Casio .. This might be my youth taking over from the truth as I was only born in 77 myself and despite many forms of calculaters over the years i'd take a punt at that being the first commercially.
4) Fortran has been used for a long time.. To be honest I have no clue which was the first.. I'd probably guess at Cobol or womething like that.. The first languages I learned was Delphi and Assembler at college...&nbsp; Having read Terry's reply previous to this I've never heard of Plankalkul so I'm assuming he's right...


	              
	              
Interesting story….let me try

1)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "Daydreamer" was Cassidy's second and final number one single in the UK Singles Chart, spending three weeks at the top of the chart in October 1973. So I'd guess early 1970's

2)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; First commercial mainframe I'd guess IBM, Average speed rating of computers was based on calculations for a mix of instructions with the results given in kilo Instructions per Second (kIPS). The most famous was the Gibson Mix, produced by Jack Clark Gibson of IBM for scientific applications. The KIPS are calculated as 1000 / weighted average instructions time in microseconds. This is then multiplied by a factor of between 1.0 and 2.0 to adjust for different architecture, mainly 1.0 where instructions relate to a single address and an accumulator and 1.25 with 1 address and multiple registers.

3)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; A Japanese calculator company named Busicom

4)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Fortran wasn’t the very first high-level programming language. The first high-level programming language designed for computers was Plankalkül, created by Konrad Zuse.

	              
	              TL; DR;
It wasn't the most politically stable period around the end of the 80s in China. Yet I was still having a lot of fun being the leader boy of a children's drama group in a kids activity centre of a small city. I thought I would become an actor in the future. Seriously, not a movie star, just an actor. I did play some tiny parts in a few movies. I haven't had a line of my own, ever. Well, as they said, there's no small parts, only small actors.
Despite my dream of being an actor, I was constantly distracted by the computer interest group next to our drama practicing room. The Apple II computers all looked too fancy to me. It was hard to move away my eyes once I put them on the computers every time. But in the late 80s in a small city in China, the distance between a computer and a boy who did stage performance should be measured in light-years.
A life changing event then happened to me, my drama teacher was assigned a new job as a teacher in an art school (old Socialism system still), so the drama group had to be dismissed. Considering the children’s contribution to the centre during the years, we were allowed to choose to join any group in the centre, for free. So I picked the computer group. I was 12.
Although I didn’t even know there are 24 letters in English, I quickly fell in love with computers. My dream changed to becoming a software developer. Seriously, not an architect, just a developer. And that hasn’t changed any more for the following 26 years.
My first computer was an Apple II and then a Chinese copy of Apple II, and then I started to program my neighbour’s Nintendo box (Chinese copy) when I realised that it has the same 6502 CPU which I know the assembly language. One of the pains of the early days was that I only have cassettes or floppy disks pretty late, so I had to create the same thing again and again.
I got some limited access to PCs later but high schools in China didn’t really promote any hobbies. My parent bought me a 386 computer when I attend the university, studying, well of course, computer science. I used VAX in the university labs. After that I experienced almost all sort of computers because of my profession, from the chips in a bulb (yes, a bulb, for light) to the distributed wireless access network.
Regarding the comfort level with computers over time, I would like to share something that’s probably different than most of the ideas:-)
I’m speaking as a software developer, I felt that working with computers has been getting harder and harder. Of course, computers and computer software are getting more and more powerful and more and more friendly. But creating software is another story. People wanted to make software tools more and more powerful and easy to use all the time. As David Wheeler said: "All problems in computer science can be solved by another level of indirection”. So people kept adding higher level “indirections” (also called abstractions). Clearly these abstractions have a lot of wisdom in them, but they are also “leaky”. As a professional you cannot just use the abstraction without caring about all the lower level details. Therefore, eventually, you have to know everything. That’s why I think it’s getting harder and harder to be a professional software developer. If you love software development from heart, this is like ever-lasting fun. If you don’t… We’ll, I’d say you’d better do. You see, there are 2 facts: software is getting increasingly more important in any industry, and making software is getting increasingly harder. Therefore the ones who know how to make software will be the kings of the future.
Let me give one example. I translated a book that teaches kids programming with Python (from English to Simplified Chinese). During the translation I became very angry. The original author tried to teach the kids Object-Oriented design. When I first learned programming in BASIC, I didn’t even know there are 25 letters (yes, I became a little bit smarter when I was 13), but I was able to create my own game with simple procedure programming. But now the poor kids need to learn object, class, members and polymorphism.
By the way, I don’t thing one will be one of the kings of the future if he only expects to learn some high level philosophies (including management theories) that covers everything and even the future. Abstractions do not work that well here when it comes to making software. All the really useful things are in the detail. Luckily, that’s also where all the fun is.
	              
	              Hi.
Interesting account and experience. For sure you can personally vouch for the tremendous improvements in technology if you actually worked with those punched cards. I've only had the limited benefit of reading about them.&nbsp;
I'll give the quiz a try. Just given me the sense of how rusty my knowledge is but here goes

For question 1 I’d say 1974 coz from Googling David Cassidy, the song was an October 1973 release and a hit. The fact that it caught your attention meant that it was trending at the time and since you mention that the month was February it follows that the time period may have been after its release in 1973
Ist commercial mainframe computer was probably IBM. Instructions per second not sure. In comparison to todays average intelligent mobile phone they were probably 100 times slower
Must have been the Japanese – Sony Corporation
I beleive FORTRAN was the first high level programming language as far as I recall. COBOL may came later

Thanks&nbsp;
Joseph Warero

	              
	              It was a chilly February afternoon, as I made my way down cobbled side roads for my weekly tutorial session, amidst the dreaming spires of Oxford.&nbsp; Minutes later, I sat expectantly in my College tutor’s office.&nbsp; A David Cassidy hit song – “Daydreamer” played softly in the background.&nbsp; How very apt, I thought. “Have a glass of sherry, old boy!” bellowed my tutor.&nbsp; He glanced momentarily at the slide rule and book of log tables I had brought in for the tutorial session, got up from watching Goeff Boycott on TV hit another test cricket boundary (England versus the West Indies at the Port of Spain Oval in Trinidad, I think), and announced – “I have some good news to tell you – your group will be having its first computing course next week.&nbsp; I’m sure it will be an interesting experience”.
&nbsp;
His last sentence re-echoed in my mind as we traipsed into the large auditorium at the university computing centre a week later.&nbsp; At precisely nine o’clock a silver-haired man appeared, dressed in a tweed jacket, with horn-rimmed glasses.&nbsp; He sternly proclaimed himself to be our instructor for the three-day “Introduction to computing” course.&nbsp; The instructor then proceeded to deliver a two-hour lecture on Fortran programming.&nbsp; I recall that an inordinately large portion of the lecture was devoted to representing negative binary numbers, and the use of Hollerith characters for reading and formatting the text output.&nbsp; It was utterly soporific and I remember dozing off at least twice.
&nbsp;
Eventually we got to the fun part – we were going to write our first Fortran program, to compute the surface area of a given shape.&nbsp; After about two hours tinkering with Fortran statements, I was ready.&nbsp; I had sketched out about a dozen lines of codes, and remember how tedious that had been.&nbsp; The course tutor looked at the program code, nodded his head and said gruffly – “Yes, that looks okay.&nbsp; Take it to Nigel next door – he will help generate your punched cards”.&nbsp;
&nbsp;
Now, this was really fancy stuff – using a special terminal, Nigel produced a stack of coloured cards with side perforations.&nbsp; He handed the cards to me, and warned rather blithely to avoid dropping the cards – “If you do so, who knows what results you’ll get”.&nbsp; I took the deck of program cards to the mainframe operator’s room round the corner.&nbsp; The mainframe suite was most impressive!&nbsp; The entire floor was populated by gleaming rows of cabinet-sized equipment, with a huge card reader mounted outside, next to an equally big line-printer.&nbsp; “Come back tomorrow, your output will be stacked in the shelf out here” said the ginger-haired operator, as he took the cards from me.
&nbsp;
I got back the next day, eager to see whether my twelve-line program masterpiece had actually worked.&nbsp; I trembled with some excitement as I went through the huge wad of printed output, with my name emblazoned across the first page.&nbsp; Unfortunately, my excitement did not last very long.&nbsp; After six pages of routine output, the printout indicated that an error had occurred – I had inadvertently typed ‘x’ instead of ‘*’ on the eighth line of the program, when multiplying two variables.&nbsp; “Oh no, I will have to do this all over again, and wait for another day for the output” I exclaimed (with some other choice expletives, which cannot be reproduced here, added). I turned round to a fellow student and remarked – “These infernal machines are too frustrating, I don’t see them ever catching on!”&nbsp;
&nbsp;
How wrong I was!!
&nbsp;
&nbsp;
FUN QUIZ:
&nbsp;

&nbsp;Can anyone guess exactly what year this took place? (look for clues in the story, and no, it's not before world war one! :-) )
Which company produced the first commercial mainframe computers?&nbsp; How fast were these (e.g. how many instructions per second)?&nbsp; How does this compare with the microprocessor(s) in today’s average intelligent mobile phone (speed-wise)?
Electronic calculators were sold commercially some years after this story – does anyone know which company sold the first commercial calculators?
Was Fortran the very first high-level programming language?&nbsp; If not, what was?

&nbsp;
P.S. Sorry, no prizes for the right answers – just virtual brownie points and bragging rights :-)&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;

	              
	              My first experiences using a computer.
&nbsp;It’s December 1990 and I’ve just completed high school. An uncle of mine had recently returned from the United States having resided there for over 20 years and now wanted to settle down back home. We travelled to the coastal town of Mombasa town, in his unique red&nbsp;GMC Jimmy&nbsp;SUV to receive his container of goods from the port. He had come back with many large boxes and had later invited me over to his house to help him unpack and put things together.
That was my first time to come in contact with a computer. I remember him sharing stories about the “486” computers which were the ones trending at the time; about a Bill Gates who had developed an operating system that was now loaded on every personal computer and was one of the richest men in the world.
I was intrigued, fascinated about those computers; spent hours looking at him typing out speedily on the keyboard, his hands gliding effortlessly and his eyes fixed on the screen. “I must learn how to do that”, I promised myself. He introduced me to Mavis Beacon, a typing tutor program to help me get past “hunting and pecking” for keys on the keyboard. What better way for a student in his gap year to spend his time !
&nbsp;I loved to play the games that were installed on the computer, my favourite ones being&nbsp;Prince, Space Invaders and Pacman. I later developed interest in software packages and spent many hours typing out documents on Word Perfect and learning Mitch Kapor’s elaborate Lotus 123 program which I simply found amazing. The manner in which I could create macros to perform routine tasks (/Worksheet/Column/Set width and the like) created my foundation and interest in computer programming.
By the time I was going to college for my undergraduate course, my passion for computers was entrenched, and my mind already made up on what I wanted to do with my career. During my vacation days over the four years of my undergraduate course, I would return and teach computer packages at the Elgon View Commercial College that my uncle Steve had since founded.
&nbsp;Changes in the types of computer devices I have used over time (e.g. desktop, notebook, PDA, smart phone, tablet, portable music players).
When I graduated from college, I purchased a Pentium computer and spent long nights writing computer &nbsp;programs in Clipper, Delphi and later Visual Basic 5. A couple of years later and now in formal employment, I bought my first laptop; a Toshiba Tecara A90 that I loved to bits. It wasn’t until the year 2000 that I bought my first mobile phone, a Sony Ericsson 1018 from the 1st&nbsp;mobile phone provider in Kenya – Kencell Kenya. It was a monster of a gadget by todays miniaturised mobile phone standards. Later on I transitioned to a Nokia 3310 that was smaller, cuter and felt nicer to hold and had a fancy tune to boot. As I started feeling more accomplished and sophisticated, the company I worked for bought me a Blackberry to improve office correspondence. Suddenly I could send and receive email on my phone and make the appearance of a busy and successful IT professional. The advent of the touch screens saw me purchase a lower end Samsung Duos handset and later on the Samsung Galaxy Note 10.1.
&nbsp;How common these types of devices are in my workplace or community.
Nowadays in Kenya, although mobile phone penetration is still not yet at its peak, pretty much everywhere you go, everyone has a mobile phone or is planning to get one. It’s become more than a lifestyle or accessory that you carry along for most people. From the phone book to friends and acquaintances on the internet, the mobile phone in the work place or as a social tool can hardly be divorced from its owners.
Changes in tasks for which I have used computers and electronic devices.
The ability of these devices to access the internet led to the demise of many cyber café’s that blossomed earlier on as most people could now check their emails and interact with friends on Facebook on the go. In my experience electronic devices have totally transformed the way in which I conduct my social and business interactions. The ability to surf the net wherever I was and the viewing experience and ability to write notes on my tablet was quite a remarkable experience. On the social side there was huge transformation in my social life; coming from the pre-mobile phone era where I had to plan a date weeks in advance; agree on precise place and time to meet and in the event that my date was a no-show, had no recourse to contact them.
Changes in comfort level with computers over time.
Nowadays I am completely at ease with computers and hardly a day passes without me looking up something on it. The ease with which data and programs can be shared and synchronised between my mobile phone, Tablet computer, laptop and desktop computer has made it possible to move around with ease without necessarily having to tag my laptop along with me all the time. From being a tool for games, it’s now a productivity and social tool for me.
&nbsp;
&nbsp;

	              
	              I'm going to try the quiz first;
I would have took a guess at early 70's with the reference to Boycott, and the David Cassidy song?
I always thought IBM made the first commercial mainframe, which would have been post the Bletchley code crackers.
Surely it was Casio for the calculator?
I am clueless on the high level language - was it Scouse? :-)
	              
	              My first experiences using a computer
I am the youngest child of 4 and I can remember way back to when my older siblings were sat around a strange device in the living room in front of the tv and I was not allowed to touch under any circumstance. Bizaar noises would repeatedly ping back and forth with the odd groan from my brothers and sister.
This was the early 1980's and I was roughly between the ages of 5 and 7 later I was to understand that this device was in fact called a Binatone TV System and the infamous built in tennis, squash and football games which oddly looked all the same to me with it's single block graphics and two controllers which just had a simple round wheel on it.
Despite not being allowed to play on it, this device caught my attention drove me to search out my own version that I could play myself and that I could ask my parents for on future christmas and birthdays.
Some time had passed and finally one christmas I had the pleasure of my very own first computer. It was a Commadore 16. With what felt like an eternity to set up and even longer to load a game on the cassette deck I finally had the joys of "Ye Ar Kung-Fu" and "Paperboy" to fall in love with.&nbsp; By this time it would have been around 1986/7 and I was 9 or 10 years old. I can still remember what sounded like randomly pitched noises that the cassette tapes used to make as they loaded up the games.
I have happy memories of those days because by that time my siblings were all late teenagers and my oldest brother had just left home to join the army. The following year because of my vast usage of the C16 my parents got me the C64. I can remember sitting up late at night with my other brother asking him to help me type this strange set of code that I found at the back of one of the instruction manuals, thinking something would happen only to get either a dreaded syntax error or for a brief moment and set of line pixels would draw on the screen and quickly stop or dissapear. Little did I know at the time but these were simple programs written in BASIC and so began my computer journey.

Amiga 1200
Sega MegaDrive
Super Nintendo
Gameboy
Atary Lynx
Tiny PC (Intel P 133 with MMX)&nbsp; - This is what i'd say was my first proper pc.
PS2
PS3
PS4
Macbook Air
iPad
And many more devices including more current windows pc's


Computers and computer technology has vastly changed during my life at an incredable pace. From the above examples I went from very basic simple computers and consoles right through to the modern day devices we all know and love in the space of 20 years. From individually playing games to playing online with stangers from accross the world while simultaneously checking my emails and posting on Facebook&nbsp; what i've ate for my tea. (yes I am one of those guys).
Computers in the modern day are a vital tool not only in the workplace but socially also. As a child in school we never had mobile phones and certainly didn't have anything like the Internet but today they are seen as the norm and an extension of yourself, if your not on facebook or have a smartphone your looked at as being strange. Ask a child today to play outside or go and "SEEK" on your friends and they won't have a clue what you mean or they might respond by flipping out their smartphone and begin to skype each other.
The irony of this thread is that we are all doing it now... We are all learning online with people we would never have met under normal situations. Fantastic global connections over numerous countries all here together talking about our own unique experiences yet strangely similar ones in many ways.
The technological world we live in today is a million miles from where it humbly started, and there is more to come.
Where will we end up in another 20 years?
Who knows but it will be an interesting ride and I for one am glad I was born in this era to see it happen.



	              
	              Good answers overall, Terry.
Nice calculation in (2) :-). &nbsp;This skill will come in handy later in the course when you will do similar computations.
By the way, the year in (1) is a little close but wrong.
Anthony
	              
	              Hi Craig,
I quite like the last answer :-). &nbsp;Liverpudlian "scouse" would make an interesting natural language, for computing - it is quite expressive.
Regards,
Tony
	              
	              “First, Second and Third”&nbsp;
My first (and second) experience of computers was a real disappointment. It was my third experience that brought it all to life.
At the time, I felt the only positive thing to come from my first experience was getting a new portable TV. In colour!
I cannot recall the exact year but it was early 1980’s. My Parents and Grandparents bought me a new BBC microcomputer! I remember thinking it was so ugly and it didn’t even look clean. It was a horrible looking cream colour. It was a 2 MHz CPU. Russell (1981). What am I going to do with that I thought?
Anyway, over the next few weeks (felt like months!) and with the tireless and unwavering support of my Grandad (I had chosen him to assist me, simply because he was an Electrical Engineer, and therefore he would just know how it works!) we set to it, writing a programme in BASIC to display the launch of a rocket. What was the purpose? To what end, and to what outcome I had no clue.
My second experience was equally disappointing. This time it was through my time at secondary school. I studied in class with all of my friends, and began the process of familiarising myself with concept and use of computers. However, I continued to struggle to understand the need for these types of computers. What was the benefit? What value do they serve for people like me? I wasn’t really interested in computer games – I had not really developed my skills using my BBC microcomputer – and I could do calculations for school on my scientific calculator (or my Dad’s abacus, which wasn’t much slower than the BBC!&nbsp;J). It is fair to say, I was not convinced and was hugely skeptical.
It was my third experience that brought it all to life. Crucially, this was very different, as it was in a work environment. You may wonder, given my first and second experiences, why on earth would I take my first job, out in the big real world, in a computer department. My thoughts exactly!&nbsp;
Well, I now realise that the decision earlier in my life to exploit my Grandads Electrical Engineering skills was not really down to me – he had actually engineered the opportunity for me to think I was choosing him, when in fact he was choosing to help me whether I liked it or not! It was my Grandad that had also opened the door for my first job, and I was on my way.
I started work aged sixteen, and joined a small computer department with an enormous computer room which was full of huge computers. I was responsible for printing the production reports for a large electronics manufacturing company. It was here that I changed my perspective on computers, and began to understand the value and benefit of these machines in the context of business. The reports were really important, providing detailed information on creation of electronic components including production cycles, warehouse stock and distribution. These reports were central to enabling the production control staff to manage the business end to end. We had Digital PDP and VAX systems, with huge removable disks for storing data, and printers that filled the room. The operating system for the VAX was VMS and I seem to recall it being MUMPS for the PDP (we even had old punch card machines for finance reporting programs, and probably not a Hz in sight!).
I could finally see the value and benefit of computers. It felt like the business would grind to a halt if I weren’t able to get my reports out on time!
Since then (and somewhat falling into a computer career) I have been lucky to work across almost all of the IT enterprise and experienced the incredible pace of computer evolution.
I love the way that computers and many forms of technology are now so intuitive. I have a son who is about the same age I was when I got the BBC microcomputer. His childhood and teenage years will be somewhat different to mine, as technology is now very much a commonplace in human behaviour and across society and the often cry of “Dad, have you seen the iPad charger? I need to get online!”
The iPad mini has a dual core A5 chip. Apple Inc. (2014). The CPU is 1 GHz. This is 1 billion cycles per second, compared with the BBC microcomputer that was 2 million cycles per second, 2 MHz. Russell (1981). How times have changed, both with power and with size!
Is it really evolution, or is it more like transformation? Is the future prospect of computers unfathomable?
References
Russell, R.T., ed. 1981. BBC Microcomputer System Technical Specification Issue 3 [online]. British Broadcasting Corporation. Available from: http://www.computinghistory.org.uk/pdf/acorn/Acorn-BBC-Micro-Specification-Issue-3.pdf [Accessed 5 September 2014]
Apple Inc., 2014. iPad mini first generation specification [online]. Apple Inc. Available from: https://www.apple.com/uk/ipad-mini-first-generation/specs [Accessed 5 September 2014]

	              
	              Anthony&nbsp;
If Terry kindly offers to do all computations from now on and allows us to re use them, is that against the policy on plagiarism. I am joking here, but&nbsp;I am seriously dreading anything that needs a calculation anything like that one!
Best Wishes
Craig




	              
	              Hi Terry,
I'm hoping to call on you for help with computational calculations! :-)
Interesting to hear you worked on VAX computers in the Labs. Assuming that was VMS operating system?
I spent the early part of my career working on VAX platforms, in a datacentre for a large UK telecommuncations company. I was told that we were the first organisation to recieve the VAX 9000 platform into the UK, either 1990 or 1991, and that it cost almost £1m. We had many VAX platforms, clustered also.&nbsp;
Looking forward to working with you
Regards, Craig
	              
	              Hi Craig &nbsp;Wasn't expecting someone to Know the BBC computer! I got introduced with computers in the 80's on the BBC computers and then Commodore 64 and Sinclair Spectrum !&nbsp;I used BASIC to code also playede&nbsp;simple games. Robin
	              
	              Craig, I'm good at counting:-) Looking forward to working with you too!
I didn't got much experience with VAX during my time in the labs, so I don't remember the OS back at school.
But (luckily?) I got to work on it again when I worked in the Nokia Networks. Then the OS was VMS:-) I worked on the 2G/3G radio network. We could only do cross compiling using those old VAX machines. And there's no new VAX machines, when any of them down there's no backup. I think now they have virtual machines and the new technoloy doesn't rely on VAX system any more. So this should not be a business secret:P
	              
	              Thanks Anthony,
In my early years of programming, I use for loops instead of sleep() to keep the timing. So I felt the CPUs getting faster:-)
I sensed that question(1) is tricky:-) I guess now it's fair to say that I cannot answer because I wasn't born yet. And I grew up in a different cultural environment and surrounded by different medias:P
Let me adjust my answer to between 1969~1972.
br, Terry
	              
	              Hi Robin
Yes, a very long time ago...
A few of my friends had the ZX spectrums and the Commodores (...not the Band!), I seem to recall they were quite good for games.&nbsp;
All the best, Craig
	              
	              No, don't worry about this Craig - any maths you have to do for this module is usually straightforward, and will not happen too often.
Anthony
	              
	              Yes, I understand - it was quite a different era, but I have to say (with understandable bias) that it was a really good period to be a student.
By the way, your answer to Q1 is still a little out, but getting closer :-)
Anthony
	              
	              Good answers, although not all correct, Chris. &nbsp;The answers will be revealed on Monday.
Anthony
	              
	              Generally good, and in some cases quite interesting answers, Babatunde. &nbsp;The answers will be revealed on Monday.
Anthony

	              
	              " My first (and second) experience of computers was a real disappointment. It was my third experience that brought it all to life." - this statement resonates with me, Craig. &nbsp;
As you read in my earlier story, the first experience was not very good, and I can tell you that my second experience, during my Master's program, was only marginally better. &nbsp;My interest in computing only sparked the third time around, whilst completing my Doctorate, strangely enough still using Fortran, but with better interfaces and more responsive systems.
Anthony
	              
	              Hi Anthony
Great story. Well, today I’ll start with answering the questions, and tomorrow I will send in my story.

My first guess was 1973 because you mention the “hit song”, so i googled “David Cassidy” + “hit song” + Daydreamer, and most results mentioned are 1973. Then I went for another clue, the cricket match Played at Queen's Park Oval, Port of Spain, Trinidad. And I found that they played to test matches at that location, both in the year 1968 (espn.co.uk).
The video “From the Pencil to the Keypad”, approx. 1m 30sek into the video, stated that the first commercially sold mainframe was the UNIVAC. This is also supported by the article “We can code it” (Raja, Tasneem. 2014) with the text “Once the UNIVAC was unveiled, ….began clamoring for mainframes of their own”, hence indicating the UNIVAC was the first commercial mainframe. But also from this week’s lecture notes, specifically the sentence “They began selling…”, I conclude that it was the UNIVAC.
I first guess was Texas. I then tried to google, and as far as I can see, the answer is Texas. (http://www.vintagecalculators.com/html/texas_instruments.html)
“The first higher level language to be widely used”(Bergan, Thomas J. (Tim), p.3 and 5)

Ad 1) ESPN Cricinfo, http://www.espncricinfo.com/ci/engine/match/63017.html (accessed 6th September 2014)
Ad 2) The History Channel (n.d.) ‘From the pencil to the keypad’, Modern Marvels [Online]. Available from: http://www.history.com/shows/modern-marvels/videos/who-invented-the-computer, (Accessed: 6 September 2014).
Raja, Tasneem. “WE CAN CODE IT”, Mother Jones, 03628841, Jul/Aug2014, Vol. 39, Issue 4 (viewed 9 September 2014). http://eds.b.ebscohost.com.ezproxy.liv.ac.uk/eds/detail/detail?vid=2&amp;sid=958b46f7-8c7c-4396-a366-18c97f7e701f%40sessionmgr114&amp;hid=115&amp;bdata=JnNpdGU9ZWRzLWxpdmUmc2NvcGU9c2l0ZQ%3d%3d#db=a9h&amp;AN=96532762
Laureate online education..“Computer Structures – Lecture Notes, Week 1: Participating in the global classroom / Part I of the history”
Ad 3) http://www.vintagecalculators.com/html/texas_instruments.html (accessed 6th september 2014)
Ad 4) Sammet, Jean E. July 1972, Volume 15, Number 7. Chapter 2.4 “Major languages”, and Chapter 3.1.1 “Earliest Yesars: 1952-1956” http://eds.b.ebscohost.com.ezproxy.liv.ac.uk/eds/pdfviewer/pdfviewer?sid=7911db62-1dd4-47cc-8e56-ab64f5813940%40sessionmgr111&amp;vid=23&amp;hid=115
Kind regards
Bo W. Mogensen
	              
	              Anthony
I sense you have been like a child in a toy store over the last 10 years, being part of the incredibly fast paced advancement in computing, particularly referencing the responsiveness but&nbsp;especially the&nbsp;user interface capabilities!
I believe this has been one of the key elements to the consumerisation of computing technology and the social impact on a global level. The fact that my Mum has an iPhone and is constantly surfing the internet to learn, and then using facetime with her Grandchildren, is a testament to that.
For my mind, it feels that technology has moved so much quicker over this recent period, than the 30-50 years previous. Sure, it moved, however I am not convinced at any rate near the last decade.
I have a question for you. As a headline, and in addition to the obvious evolution of component engineering (such as CPU, Memory etc), would you suggest that this immense progress is down to a handful of key life events - initially with the creation of the internet, then with availability of amazing search engines and data acquisition, then with mobile, and then ultimately with social media?... Computers are very much a bedrock for global society, and no longer just for big businesses!?&nbsp;
Cheers, Craig
	              
	              Hi Joseph,
I like your systematic, Sherlock Holmes style, approach to answering Q1 - very logical :-).
As Sherlock would say - "Elementary my dear Watson, quite elementary"
Anthony
	              
	              Hi Craig,
Whilst there were undoubtedly some key/seminal developments, I think the progress made with computing has also been dependent on many small, seemingly innocuous, developments and mini-breakthroughs. &nbsp;I suspect that the synergy from all those big and small developments has hastened the pace of progress with computing over the past decade. &nbsp;
In addition, the fact that we now have demonstrable convergence between computing, communications, entertainment and social interaction makes the progress made strikingly obvious. &nbsp;In fact, I think it is going to get even more interesting over the coming 10 – 20 years!
I wonder whether other class members have similar or alternate views on this?
Anthony

	              
	              Computer, this term is reasoning in my ears since I was little, I always knew what a computer is and how it’s used. But I had access to a computer only much later, just before starting my university journey.
•&nbsp;&nbsp;Machine Selection
Personally, I have not faced this problem since it was a gift. But at that time we had computers commonly called: Pentium I, Pentium II, III and IV, which was referring to Intel processors used in these PCs. There was also MAC computers but was a little more expensive.
And I can remember that at that time it was not given to everyone to have a computer (PC) in my country. I got my first PC from my dad, which was in fact a Pentium II.
P6 (Pentium II) have met much success, especially in my country, mainly due to its low cost. So there were a lot of these machines on the Congolese market.
In short I received a new PC, so no big worries on the unpacking side; packing was Nickel, just finding the right place to install it.
•&nbsp;&nbsp;Software, Hardware and first experiences
My PC was a Windows 98 operating system, an Intel Pentium II Processor 300 MHz, and 128 Mb of RAM and 20 GB hard disk, if I have a good memory.
So I was freaking out a bit there, because I didn't know anything about handling a PC. But I began to learn step by step with the help of my big brother.
I was immediately dazzled and fascinated by this magnificent machine that could process a lot of information and which was able to do almost everything (treated text, play video, play music, play games, etc.)
So, I got a taste of the computer science and decided to study computer science in the university, while initially I wanted to study chemistry.
I'll never forget the first software I've had to use, Microsoft Word. It was really fascinating. But what I was most marked was the mouse, yes the mouse!!
It was magical, how a little thing which looks like a real mouse, could move objects that I see on my screen?
•&nbsp;&nbsp;Computing
My earliest experiences with programming, was in the computer laboratory of the Faculty of Science, University of Kinshasa. After having accumulated theories about the programming language C, I was finally in front of a PC to write and compile my first computer program.
I remember that everyone had to choose a question from a list and do an exercise; my question was about writing a program that would solve an equation of second degree. And it was so fascinating to see that I could give instructions to the machine and the machine executed these instructions line by line. I became a kind of creator...
That’s in few lines a summary of my personal history with computer and computing.
Since there I’m always interested to the computer, internet and programming world. During my university journey I’ve moved from one computer to another. Among all types of computer that I have to used, the most interest one is “laptop”, oh yes! Move a computer from a place to another or put it into your bag. This was really fantastic.
The other revolution is about mobile phones, the evolution of mobile phones and the technology used is very fast, I can remember that my first mobile phone was a Siemens A36, then a Motorola V3, then Motorola L7, Nokia N93, Nokia N95. Then we went to smart phones, Blackberry Curve, Blackberry Bold 9900, Blackberry Q10, Samsung S3, Samsung Note 3 and now Samsung S5.
This world is very fast, things are changing every time and most of time people are lost. But the good thing is that these smart phones are real computers, we can do a lot of things that can be done with a computer and they are running very powerful operating systems like android or windows.
Computer, internet, programming and smart phones (tablet, portable music player etc) changed my life and I’m currently unable to live without them. Computers are my studies, my job, ma passion and my life.
In computer we trust.
In web we trust.

	              
	              Let me try to answer the questions.

1974
IBM
Sharp
Fortran


	              
	              let me try aloso.
1. 1970
2. IBM,&nbsp;
Mainframe : 1MIPS
Smart phone (S5) : 14.000 MIPS
3. Sharp
4. Yes, Fortran is the first high-level programming language
	              
	              Hi Craig &amp; Anthony,

I agree Anthony, while I feel Craig is correct to say there have been some definable and noticable what you could call 'landmark' moments in the development and growth of tech I also do share the view that some very small and not generally recognised events along with these large steps have significantly helped along the way much like a catalyst.
Simply things like touch screen technology catapulted the mobile market overnight where nearly every phone on the market nowadays is a smartphone... This in turn drove the billion dollar industry around 'phone apps'. It also completely change the pc market with the development of the first tablets and the OS systems behind them.
I'm sure there are many more examples of these changes when matched with larger steps paved the way to some great leaps.

Like you said it's going to be very interesting over the next 10-20 years.


	              
	              Hi Terry,

I enjoyed reading your introduction. What caught me the most was actually your shift in passions from acting to IT.
Reason being is that I had a similar shift. I was convinced from a young age despite liking computers I was sure I was going to be a football player. But changes to first passion in playing football (soccer) due to injuries etc I then changed course to do my degree in IT and I havent looked back since.
One other thing I noticed which I agree with you is that computers and software has become far harder as they grow. I feel sometimes just because tech has advanced a lot of software houses then feel the need to fill that space rather than just making what they have better and more efficient. It's like the make things more difficult just because they can rather than asking the question why.



	              
	              Well I heard about computers from my cousin when I was fifteen years old in late 90s. He was studying computer science in college but I got my first chance to see a computer in real in 1999 at my teacher’s place who was an electrical engineer and with the help of his friend trying to fix some problem with the OS and it was for sure Microsoft Windows J. At time we heard stories that with the help of computers we can do anything. I became very curious and wanted to explore the truth behind these fairy tales. 
As I was very clear in my mind to study about computer in college so I took admission in short course to get familiar with computers during break after final school exams. Our instructor gave us few lectures on DOS commands and then in second week I finally got change to use a 386 computer and test the commands to create modifies and deletes directories. Later we also used paint brush and fox pro programming language on Windows 98 but I was not happy because of slow processing speed specially while working on paint brush application. 
When I started college in year 2000, my father bought me a computer with AMD 856 MHz Processor 64 MB RAM and 80 GB HDD. It was like a super computer with compare to what a used beforeJ. Later with new OS and resource hungry application I kept upgrading the specification as per the study requirements in college and then in university. I also remember my first 32 MB flash drive which was a big relive from floppy disks which never worked for me. I was always comfortable using computers and new technologies but the only concern for me was the speed and battery time for portable devices. 
Other than using computers for study it was a big source of entertainment for me and I have to change my CD ROM every now ant than because of playing movies and music CDs. Now computer and smart phone are a necessity in every aspect of my life and I even can’t function without these. Before I used to remember the contact numbers of my family and friends but now I event don’t bother to remember my own. I have no dictionary at home and the online search engines are my first preference for any information. Some time I realize that now I am not as good with pen as I was before because now I use keyboard all the time. There is no doubt that we are noticeably progressing using these gadgets and technology but what we don’t realize is the kind of prize we of paying for it
	              
	              Thanks Anthony
I look forward to hearing the views from our other colleagues in class.
Personally, I am looking forward to teleportation over the next 10 - 20 years, the roads are so busy in the UK... &nbsp;:-)
Regards Craig

	              
	              
	                Attachment: &nbsp;Your history of computing.docx (13.489 KB)
	              
	              Dr. Anthony,
Thank you for sharing your story, it has set the context for me to write my own. I would first like to attempt to answer the question before completing and uploading my story.&nbsp;
So here goes...
Question 1: From the opening paragraph you mentioned that it was February, you also mention that your tutor was watching Geoffrey Boycott on TV. England was playing against the West Indies in a test&nbsp; match, which my research informs, was played on February 2, 19674.( http://www.howstat.com.au/cricket/statistics/Series/SeriesStats.asp?SeriesCode=0187). Based on those facts I would have to say the year 1974.
Question 2: &nbsp;The Remington Rand Company produced the first commercial mainframe computer. " Its reported processing speed was 0.525 milliseconds for arithmetic functions, 2.15 milliseconds for multiplication and 3.9 Milliseconds for division"( http://www.computermuseum.li/Testpage/UNIVAC-FAQ.htm, 2001)
Compared to today's microprocessors in mobile devices boasting clock rates of up to 1.6Ghz and beyond. These devices can produce extensively far more input-output operations per second.
Question 3: The first electronic calculator was sold by Bell Punch Co. Ltd and Sumlock-Comptometer Ltd. in 1963
Question 4: According to The FORTRAN Programming Language (http://groups.engin.umd.umich.edu/CIS/course.des/cis400/fortran/fortran.html, 1999). It was the first high-level language using the first compiler ever, developed by a team of programmers at IBM led by John Backus and was first published in 1957.
&nbsp;
References
1973-1974 West Indies Vs England&nbsp; - Test Series [online]. Available from: http://www.howstat.com.au/cricket/statistics/Series/SeriesStats.asp?SeriesCode=0187 (Accessed: 6 September 2014).
&nbsp;
The UNIVAC FAQ [online]. Available from: http://www.computermuseum.li/Testpage/UNIVAC-FAQ.htm (Accessed: 6 September 2014).

&nbsp;The History Channel (n.d.) ‘From the pencil to the keypad’, Modern Marvels [Online]. Available from: http://www.history.com/shows/modern-marvels/videos/who-invented-the-computer&nbsp;(Accessed: 6 September 2014)

Specifications for Samsung I9500 Galaxy S4 [online]. Available from: http://www.gsmarena.com/samsung_i9500_galaxy_s4-5125.php (Accessed: 6 September 2014).
&nbsp;
Vintage Calculators Web Museum [online]. Available from: http://www.vintagecalculators.com/html/history_of_electronic_calculat.html (Accessed: 6 September 2014).
&nbsp;
The FORTRAN Programming Language [online]. Available from:
http://groups.engin.umd.umich.edu/CIS/course.des/cis400/fortran/fortran.html (Accessed: 6 September 2014).

	              
	              Hi christopher,
Thanks for the feedback. It feels really good when I realise someone can understand my poor written English:-)
Let's be the kind of computer professionals that don't make things complicated for the others:-)
br, Terry
	              
	              Hi Anthony,
Yes, I have a similar view. I also think the converged domains are expending. Communications and enterainment has been traditional close with computing, so they are the first ones to be converged. Social interaction was obvious after the invention of network, so they are converged. Nowadays, finance is probably more than 90% computing, cargo delivery is probably 80% computing, hospital 80%? And computing is growing in every domain.
So eventually all the companies will become software companies. Hooray!
Really looking forward to seeing that happen:-)
br, Terry
	              
	              From single use to multi-use
The Commodore 64 was the first computer in my life. An epic machine which me and my friends mainly used for playing games. It was 1984 and you had to load the programmes using a normal audio cassette tape which took many minutes while producing beeping noises as the ones and the zeros were loaded at an audible level. There were broadcasts over the radio of those beeping noises which you could record and use as programme. Sometimes we would try to code in Basic with commands like '10 print text, 20 repeat 10' to fill the screen with strange words.
It was in the 1990's that I started using more sophisticated computers with interchangeable parts like the Intel 80368DX. Programmes were dedicated for certain clock speeds in that time. When upgraded to a Intel 80468SX a lot of the games could not be used anymore as they were going to fast. But it was in that period I started to compile my computers from parts I got, found or bought.
The Mac and Windows operating system made command line interaction redundant. It gave a boost to special purpose programmes running on that general hardware. In 1998 I started with others a grassroots broadcast organisation on the Amsterdam cable network. By building an edit machine we did not have to rely on linear editing using tapes and therefore do not have to take the copy degrading during the editing process into account any more. Unfortunately it did not take less time to edit video. Rendering would take about 2 hours for 5 minutes of content. When a mistake was made or the system would crash you had to wait again. &nbsp;
Around the Millennium I started working as a journalist in the computer domain using a cell phone. I reported about the business developments and the retail channel, both as web editor and copywriter for paper magazines. And I did not have to be in an office to make the necessary calls. &nbsp;This was my big introduction to all the available computer and software for the B2B and B2C market. Acronyms like CRM and CMS entered my vocabulary and I would learn more about IT-infrastructures using middle-ware and databases. At the same time I got introduced to web publishing.
During the following years I saw all possible consumer and semi-professional computer products pass by and had the chance to review many of them. The laptop became slowly a machine you could actually bring along. The first Windows based tablets called Origami arrived in 2006 which you had to control with mainly with your thumbs. The Origami was clearly a step too far for the consumer who was just getting accustomed to the laptop. The beginning of the decline in traditional PC was set as it always stayed a machine that was located either in the office or attic. It was not the media server that bridged the gap to the living room but it was the laptop. &nbsp;A first second screen entered the living room.
My first blackberry really changed my relation with work. Having access to mail and news wherever you were was enlightening and time consuming. My career moved me to a management position of a team responsible for video productions, computer games and interactive media. DVD-authoring and Macromedia/Adobe Director were used a lot besides HTML coding. With a technology partner we published games for the mobile phone. At that stage there were 60 totally different models of mainly Nokia phones you had to support to reach an audience. A sheer contrast to current times where you have 60 companies making all the same phone. &nbsp;Slowly all started to move to the internet and I got more and more involved in developing User Generated services for photo/video magazines and also for the travel business. 
With my specialisation in the field of internet, new media and interactivity I was hired as senior policy advisor research and development at the Dutch public service broadcast organisation NPO. I had to developed strategies on a conceptual level and their practical implementations on topics like second screens, connected TV, network neutrality, open source, adaptive streaming, p2p, visual radio, device specific video distribution, etc.. &nbsp;The changing role of media and the devices used to access those became my major study.
Of course the laptop, tablet and Smartphone are important as new media interaction models but as interesting is the development of the TV becoming also a computer empowered with HTML overlaying and an IP-return channel. Audiences are nowadays connected with each other (digital couch), can participate directly with the programme and go prolonge accessing media with their own interest whenever wherever they want.
Working at a national broadcaster and later for the European Broadcasting Union brought me close to the big professional IT (decentralised) infrastructures. AT the NPO I had to create also new workflows in conglomerates of interlinked companies from production to distribution, opening massive archives to the public and breaking in cooperation with ISPs new internet records with regard to amounts of concurrent viewers during World Cups or Olympic Games. Within the EBU the exchange of technical knowledge between industries is important which can lead to new standards that can change behaviour in the future, be it the streaming format MPEG DASH or Hybrid Broadband Broadcast TV that merges linear and on demand experiences. 
Covering my life with computer related products I notice I am still typing this on a laptop. Looking around me I see an iPads, a Kindle, a Playstation, a Connected TV and a drawer full of (old) smart phones. All being used at different moments at different times for different uses. Compared to the PC that was used to do all in one room this is a major change.

	              
	              Hi Terry.
Good to hear that you still haven't yet "hung your boots" and lost your love for programming &nbsp;26 years on. It's great that you are generously willing to delve deeper into learning and application of emerging technologies in software coding constructs. My experience with many of the colleagues I was with in college is that as they moved higher up the command chain, they tendended to slowly shed off core programming work in favour of management positions where they oversee developers and attend strategy meetings.
There's often the argument that individuals are often promoted to higher levels of incompetence; hence one can be an excellent programmer but struggle as a manager. Organization often make the mistake of promoting such a skilled resource to senior management position that has little to do with programming and they end up doing more of resource planning, management and attending the many many organizational meetings. This may of course kill off the passion that one had in programming
Joseph


	              
	              My first computer experience
When I was very little I dissected an Atari 2600 to try to figure out how the games would come out of that thing.
By my first, real, computer experience was the year when volcano Nevado del Ruiz erupted wiping out from the face of earth a small town called Armero. The first time I saw a computer was after the middle year vacation, so it must have been some time during or right after July (school year ends in Colombia in November). I was lucky enough to be in a British bi-lingual school in Bogota, that my mom was the head of the multimedia department and that I was good friends with the grandson of the owner. As a result I knew most of the professors and was almost free to roam around the school after school hours. That year, for the first time ever, a computer lab was setup in the school for the undergraduates (I was in primary school), the lab was right beside the multimedia labs and since my mom often worked until late, I was allowed to enter the computer lab while I was waiting for her. That is how I saw for the first time a PC-AT running PC DOS. For me it was like magic: I was fascinating at the monochrome screen with green text on it and how programs could perform things like calculations or word processing (Lotus 123 and Word Perfect) and at the little 5 ¼ (back then) floppy disks. How could all that marvel fit in there?
By the time Nevado del Ruiz actually erupted I was familiar with all PC manuals, and I have spent so much hours in that PC lab that under graduates would come to me and ask things when they had a problem with their computer.
In 1991 I moved to Switzerland with my mom. And again I was lucky as her husband was a software engineer for a bank and had at home one of the very first IBM PS/2 with Windows 2.0, which I have never seen before. I had restricted access to this PC but I had access to a lot of handbooks of something called Turbo Pascal. It was only then that I truly started to understand how that machine was able to produce things in the screen. A year after playing with Turbo Pascal, I started to program a “text based graphics” game (lines created out of dots, dots clustered together to form shapes), in which you had to be fast enough with the arrow keys of the keyboard to escape a volcano explosion following a path with random obstacles (circles, squares or rectangles). I suppose the idea came from that volcano experience that shocked not only me, but an entire country. By the time I completed it, a game named Doom was released and I realized the world of computers was far bigger and faster evolving than I ever imagined.
In retrospective I think that is the most important thing I learned back then: the computer world was big, growing rapidly and that it would never stop. And it never did. From desktop to servers, from cell phone as big as a brick to the iPhone, from CompuServe email to twitter, I have used many things that became obsolete and the only constant I have even see is change. Today at work and home things like internet connected devices are very common, but when I first entered that computer lab, there was only 7 stand-alone computers and I bet the school paid a great deal of money for them. Today, and IPad can do way more things that all those 7 computers put together.
Since the very beginning I felt at ease with compute devices overall, not because I had some innate talent to understand them (after all, I am no Steve Jobs), but because I had the curiosity in me and because I was in the right place at the right moment to have the opportunity to understand them. That is what gave me the passion for computers and I am lucky to have a work that I love and that always challenge my curiosity.

	              
	              My first experiences started with games stations where we put in cartridges. Then came the Commodore 64 where I moved from games to word processing and light programming (got the code from a magazine) and saving my work on my music cassette tapes. In school, 16 years old, I tried some programming (Fortran?). 
My use of computer devices has changed a lot. The biggest change, and comport level increase, was in the bank, especially the IT-department, where I worked with IBM mainframe terminal systems called 3270 and 4700, AT/XT machines, PC’s and laptops. Regarding local area networks, I first shortly experienced Novell with the multi-player game called Sniper, then moving on to Token Ring and Ethernet. And not to forget the “retro” sounds from the modem at home when I first experienced the internet – fantastic. Looking at OS and LAN, I’ve primarily worked with DOS, Windows, OS/2 and back to Microsoft. Regarding LAN, it started shortly with Novell and Token Ring moving on to Ethernet.
My IT fascination started with my Commodore 64 (C64), but really took off after starting in the IT-department where I even more realised the huge potential. And I must admit that I primarily have a professional view on IT compared to the private or social aspect. 
My tasks have changed a lot. I started professionally as a supporter, and had my hands in hardware, light spreadsheet macro programming and installations. Later my career has “naturally” moved towards a more general and managerial/coordinating role. I often address myself as:

“I am a generalist”. I’m am very much focused on how to put IT (ICT?) into work, ensuring a stable and up to date environment, and ensuring that we have the correct tools for the respective tasks, continuously improving processes, collect and share information and knowledge. Hence, even though I started as a supporter, my career has “naturally” moved towards a more general and leadership/coordinating role.
“I am a translator”. Because of my “small time” programming experiences (e.g. Visual Basic), diploma course in programming languages, and my many years working with developers, I can interpret and translate the end-users and the developer. I have been in many situations where the, much technical-wiser, technician/developer has come to a dead-end, and by asking the correct (open minded) questions, and sometimes just looking at the logic in the code, I’ve been able to help.

A trigger for me staying in the IT business is, as I often have told people “I thrive in the IT environment because it is so dynamic, that in principle, we don’t know what happens next, and that’s exciting”. I normally don’t get frustrated by incidents/problems (after all, the IT also has to put up with the human variable J ). But one thing that can frustrate me is still having bad user interfaces in the year 2014, e.g. we have a service desk system which has some word processing objects, and they are really old fashioned!
IT has changed a lot of things, and gives a lot of possibilities. At first, I was bound to a piece of hardware, and then came the internet, and now we have all these portable devices. So now, I always have my smartphone and keep my laptop close to me, and this, combined with internet and wireless networks, I can be almost anywhere doing whatever I like, e.g. participating in this MSc programme. Also my wife (who one’s threatened to bomb my computerJ ) can’t do without her smartphone or iPad. And looking at my youngest son (12 years), I’m a bit envious (compared to what I was used to when I was a child), when he is playing internet games on his laptop while at the same time watching live stream from others playing the game and another iPad Skyping with his gamin friends. I honestly believe we need this development. It has moved very fast these last 20 years, and I expect it to go even faster. IT as a whole is very common in my country, everybody has a computer/laptop, smartphone and internet. We have several fibre connections to other countries. Still, we are as a country 1-2 years behind e.g. Denmark when it comes to prices and speed.
Finally, I’d like to hear your thoughts on this issue: I have a friend, who works as a journalist, and he asks me: “Processes seem faster, but we spend enormous efforts to keep it running and solving incidents and problems. Could we have done things differently, more effectively, have our efforts really been worth it?”
Best regards
Bo W. Mogensen
	              
	              Hi Joseph,
I will probably never "hung my boots":-)
Yes, I've also seen the tendency you've mentioned. And I've seen it killed giant companies.
I would say "multiple learning" is very important in this industry. 2 Reasons, the communication cost is too high; you will never know the most important decisions are in which abstraction layer.
br, Terry
	              
	              Some good and interesting answers, Ricardo :-).
Anthony
	              
	              IntroductionToday, the computer science is ameliorated at a breathless pace, no matter the hardware or software; even in nowadays of rapid technological diversification, the abruptness of this change is astonishing. Recollected to my computer history, it was unfamiliar for me until left high school. This is because computer was not such popular during 20 years ago in Hong Kong. Of course, supposing according to Brookshear (2011)'s opinion: “One of the earlier computing devices was the abacus.” then as a Chinese, my first computing devices could be retrospect to my primary school life.Intel 80486 with windows 3.1However, my real first computer is Intel 80486 with windows 3.1 during 1993. I remember that I used the PC for paper work without network connection. My first time to visit Internet was around one year later where used Robotics modem as well as the first accessed website was "yahoo". The Intel 80486, also known as 486, was announced during 1989, it was the first x86 chip to adopt over than a million transistors as an integrated floating-point unit and a large on-chip cache, it was also the first tightly pipelined x86 design. This implies 486 was a 4th generation of binary compatible CPUs. During those years, 486 was a high-performance CPU that on average, a 50 MHz 486 CPU can implement 40 million instructions every second and can achieve 50 MIPS peak capability.Windows 3.1 is established by Microsoft, and is a 16-bit OS (operation system) to use on PCs. The first sold of Windows 3.1 was in April 1992 and the subsequent editions produced between 1992 and 1994 until end of 1994 which was substituted by Windows 95. Windows 3.1 is a MS-DOS-based platform and it produced many enhancement packs such as ameliorated system stability, TrueType fonts, workgroup networking and expanded support for multimedia.IBM I-Serials (AS400)I am currently working at IBM and mainly responsible for AS400 (IBM I-Series), thus AS400 has a extraordinary significance for me. The AS400 adopts the PowerPC microprocessor and RISC (reduced instruction set computer) technology. The OS of AS400 is known as OS400. OS400 has closely tied a Java virtual memory and multi-terabytes of disk storage that make AS400 being a type of versatile all-purpose server. AS400 is able to supplant Web servers or PC servers in the global businesses environment, contending with both UNIX and Wintel servers. The AS400 is diffusely set up in government agencies, in large corporations at the department level, in small companies, and in almost every industry segment. However, a according to IBM, these are certain significant new applications for it as following:1. Web and e-commerce serving2. Data warehousing3. Corporate groupware services4. Java application developmentSmartphone and AndroidNowadays, I personally believe that the computer device that deepest impact our daily life is the Smartphone that I can use the Smartphone to handle the personal or business mail, I also can use its real communication apps, such as facebook, whatsapp or google+ and etc to contact my colleagues or friends, even I can use it to watch TV programs, video clips or play games. Most of our daily computer processing can be applied in the Smartphone. In fact, it is amazing that the Smartphone today is more powerful than The Apollo Guidance Computer. I am current using sonny xperia with android 4.4 kitkat.Android is a Linux-based open source OS for mobile devices, mainly for smart phones and tablet computers; it is produced by the Google Open Handset Alliance (OHA, open handhelds Union). And Android 4.4 kitkat is easily the best edition of the platform to date that leading the aspect by developing the update beyond its Nexus line to the Moto G also neatly demonstrates the move to better the Android experience on low-end, affordable hardware.Referencesandroid.com (2014); KitKat 4.4 Smart, simple, and truly yours; Available on: (http://www.android.com/intl/en/versions/kit-kat-4-4/) (Accessed on: 7-Sep-2014)Beñat Bilbao-Osorio, Soumitra Dutta, and Bruno Lanvin (2014); The Global InformationTechnology Report 2014; Available on: (http://www3.weforum.org/docs/WEF_GlobalInformationTechnology_Report_2014.pdf) (Accessed on: 7-Sep-2014)Computer History Museum (2006); Timeline of Computer History; available on: (http://www.computerhistory.org/timeline/?category=cmptr) (Accessed on: 7-Sep-2014)datasheets.chipdb.org (2008); Index of /Intel/x86/486/datashts; Available on: (http://datasheets.chipdb.org/Intel/x86/486/datashts/) (Accessed on: 7-Sep-2014)Frank O’Brien (2009); The Apollo Guidance Computer: Architecture and Operation; Available on: (http://tcf.pages.tcnj.edu/files/2013/12/Apollo-Guidance-Computer-2009.pdf) (Accessed on: 7-Sep-2014)ibm.com (2014); IBM I for Power Systems (including AS/400, iSeries, and System i; Available on:(http://www-03.ibm.com/systems/power/software/i/) (Accessed on: 7-Sep-2014)J Glenn Brookshear (2011); Computer Science: An Overview (11th Edition); Addison Wesley/Pearson.| ISBN-10: 0132569035
	              
	              Introduction
&nbsp;
Today, the computer science is ameliorated at a breathless pace, no matter the hardware or software; even in nowadays of rapid technological diversification, the abruptness of this change is astonishing. Recollected to my computer history, it was unfamiliar for me until left high school. This is because computer was not such popular during 20 years ago in Hong Kong. Of course, supposing according to Brookshear (2011)'s opinion: “One of the earlier computing devices was the abacus.” then as a Chinese, my first computing devices could be retrospect to my primary school life.
Intel 80486 with windows 3.1
However, my real first computer is Intel 80486 with windows 3.1 during 1993. I remember that I used the PC for paper work without network connection. My first time to visit Internet was around one year later where used Robotics modem as well as the first accessed website was "yahoo". The Intel 80486, also known as 486,&nbsp; was announced during 1989, it was the first x86 chip to adopt over than a million transistors as an integrated floating-point unit and a large on-chip cache, it was also the first tightly pipelined x86 design. This implies 486 was a 4th generation of binary compatible CPUs. During those years, 486 was a high-performance CPU that on average, a 50 MHz 486 CPU can implement 40 million instructions every second and can achieve 50 MIPS peak capability.
Windows 3.1 is established by Microsoft, and is a 16-bit OS (operation system) to use on PCs. The first sold of Windows 3.1 was in April 1992 and the subsequent editions produced between 1992 and 1994 until end of 1994 which was substituted by Windows 95. Windows 3.1 is a MS-DOS-based platform and it produced many enhancement packs such as ameliorated system stability, TrueType fonts, workgroup networking and expanded support for multimedia.
IBM I-Serials (AS400) 
I am currently working at IBM and mainly responsible for AS400 (IBM I-Series), thus AS400 has a extraordinary significance for me. The AS400 adopts the PowerPC microprocessor and RISC (reduced instruction set computer) technology. The OS of AS400 is known as OS400. OS400 has closely tied a Java virtual memory and multi-terabytes of disk storage that make AS400 being a type of versatile all-purpose server. AS400 is able to supplant Web servers or PC servers in the global businesses environment, contending with both UNIX and Wintel servers. The AS400 is diffusely set up in government agencies, in large corporations at the department level, in small companies, and in almost every industry segment.&nbsp; However, a according to IBM, these are certain significant new applications for it as following:

Web and e-commerce serving

2.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Data warehousing

Corporate groupware services
Java application development

Smartphone and Android
&nbsp;
Nowadays, I personally believe that the computer device that deepest impact our daily life is the Smartphone that I can use the Smartphone to handle the personal or business mail, I also can use its real communication apps, such as facebook, whatsapp or google+ and etc to contact my colleagues or friends, even I can use it to watch TV programs, video clips or play games. Most of our daily computer processing can be applied in the Smartphone. In fact, it is amazing that the Smartphone today is more powerful than The Apollo Guidance Computer. I am current using sonny xperia with android 4.4 kitkat. 
&nbsp;
Android is a Linux-based open source OS for mobile devices, mainly for smart phones and tablet computers; it is produced by the Google Open Handset Alliance (OHA, open handhelds Union). And Android 4.4 kitkat is easily the best edition of the platform to date that leading the aspect by developing the update beyond its Nexus line to the Moto G also neatly demonstrates the move to better the Android experience on low-end, affordable hardware.
&nbsp;
References
&nbsp;
android.com (2014); KitKat 4.4 Smart, simple, and truly&nbsp;yours; Available on: (http://www.android.com/intl/en/versions/kit-kat-4-4/) (Accessed on: 7-Sep-2014)
&nbsp;
Beñat Bilbao-Osorio, Soumitra Dutta, and Bruno Lanvin (2014); The Global Information 
Technology Report 2014; Available on: (http://www3.weforum.org/docs/WEF_GlobalInformationTechnology_Report_2014.pdf) (Accessed on: 7-Sep-2014)
&nbsp;
Computer History Museum (2006); Timeline of Computer History; available on: (http://www.computerhistory.org/timeline/?category=cmptr) (Accessed on: 7-Sep-2014)
datasheets.chipdb.org (2008); Index of /Intel/x86/486/datashts; Available on: (http://datasheets.chipdb.org/Intel/x86/486/datashts/) (Accessed on: 7-Sep-2014)
Frank O’Brien (2009); The Apollo Guidance Computer: Architecture and Operation; Available on: (http://tcf.pages.tcnj.edu/files/2013/12/Apollo-Guidance-Computer-2009.pdf) (Accessed on: 7-Sep-2014)
&nbsp;
ibm.com (2014); IBM I for Power Systems (including AS/400, iSeries, and System i; Available on: (http://www-03.ibm.com/systems/power/software/i/) (Accessed on: 7-Sep-2014)
&nbsp;
J Glenn Brookshear (2011); Computer Science: An Overview (11th Edition); Addison Wesley/Pearson.| ISBN-10: 0132569035
&nbsp;
&nbsp;

	              
	              Yes, a "Beam me up Scotty" Star trek-like teleporter would be most useful - it takes far too long to travel home to the UK from the Middle East &nbsp;:-)!
Given that quite a few gadgets used in the Star-trek series in the 1960s (e.g. flip mobile phones, biometric security systems, speech recognition/translation systems etc) were eventually produced, the teleporter could actually happen! &nbsp;Many of the devices conceived were predicated on advanced computer systems, which are now being produced.
Anthony
	              
	              Very nice story. It’s fantastic that you lived through one of the defining moments in computing.
Your “These infernal machines are too frustrating, I don’t see them ever catching on!” reminds me of the optional video at the bottom of the Learning Resources[1], in which Dyson shows some of the comments written by the programmers. In a way, that frustration is still there, but in a different way. Little errors like (x) instead of (*) are now almost automatically spotted by the IDE, but software and hardware are still far from being perfect. It’s rare today for someone not to see the future of computing even with some frustrating errors, but I think it’s probably easy to give up on development when something becomes too complicated.
Trying to answer your questions:

This one I had to do a little search on internet, as I don’t know anything about cricket. I would guess it is 1978–79, as according to Wikipedia[2] Boycott played in that season and Daydreamer was in the UK top charts in October 1973[3].
The first commercial should be UNIVAC. I think it was at least capable of doing a thousand instructions per second. While an iPhone today can go up around 20,000 million (20 billion) instructions per second. That’s 20 million more instructions per second than the UNIVAC. However, of note, I believe that UNIVAC definition of thousand was 1’000, while today when we say thousand in the computer world, we define it as 1’024, so the computing power today is higher than my calculation above.
I believe it was sharp. Those used to be the big white calculators with keys bigger than the average keyboard and the screen would normally be around a black plastic frame.
If it was not Fortran then it must have been COBOL or ALGOL.



[1] Dyson, G. (2003) The birth of the computer [Online]. Available from: http://www.ted.com/talks/george_dyson_at_the_birth_of_the_computer.html, (Accessed: 9 September 2014).


[2] Geoffrey Boycott. Ashes series, West Indies and India. Wikipedia (no date). Available from: http://en.wikipedia.org/wiki/Geoffrey_Boycott#Ashes_series.2C_West_Indies_and_India, (Accessed: 9 September 2014).


[3] Officialcharts.com (no date). http://www.officialcharts.com/artist/_/david%20cassidy/, (Accessed: 9 September 2014).



	              
	              Hi Terry,
It's really nice to discover that computers were available in China in those years. Being totally ignorant in Chinese history I wouldn't have imagined that, but thanks for teaching me that things were different. It actually reinforces the image of early computing expanding as a global phenomenon and if we are today where we are is because Apple was also able to reach you.
I agree with you that being a good software developer it's getting harder and harder. I actually had that same feeling early on, and decided not be a software developer because of that. I know how to program, enough to defend myself if I need to some sort of automation. But I decided to focus on hardware and its integration with different systems, I found it relatively more easy and less demanding (I could code for hours, days and weeks, but once you are done integrating systems, you are really done). Those who make good software would certainly have a key playing role in the future of IT, but let's not forget there's an equally important role for those who work with the underlying systems (hardware, systems, networking, etc.).
Regards,
Augusto.

	              
	              Back in 1997, when I was in my 8th grade, during my summer holidays, I took my one of older cousins to my school’s computer lab that was fortunately open that day. He had said he needed to work on some program and I had no idea what he was talking about. He was already studying computers in his college at the time and he started talking about all these fascinating things you could do with a computer, he said he could tell the computer what he wanted done and the computer could do just that. Okay, I was intrigued! How can you “tell” a machine what to do, was what I had thought. I watched him closely as he inserted a square black plate (which I later came to know as a Floppy disk) into a matching space on the big white rectangular box that had so many buttons on it. He opened up a black window on the screen and started typing away. I came back home and tried to read up on computers and how they worked, but books were scarce, we didn't have access to the internet and cyber cafes were pretty expensive back then. 
&nbsp;
I then came across the best life-changing TV video game of our time, Mario! I was intrigued by how a tiny cassette could store and run a super-fun game like Mario. I cracked open one of those cassettes to find out how it worked and ended up ruining it! Most of all, I remember the look on my father’s face when I asked him money for playing games at the cyber cafe! Dave was the first computer game I got to play. Looking back now, it's absolutely amazing how far the gaming industry has come today, since Dave!
&nbsp;
In the year 2000, when I entered college, I took computer science as my major subject as I knew that computers was going to be my forte; and the IT boom in India at the time just opened us up to a world of career opportunities. It was nothing like we’d ever seen before. Sometime in 2002, when my best friend and I were busy learning and experimenting with C language, I was trying to control a basic DC motor using a C program. I had no clue of the electronic specs and I happily connected the motor to the DC power supply without any resistors. It blew up the COM port on the PC and we saw smoke coming out from his CPU! To be frank, growing up with languages like BASIC and C was the most exciting part of my teenage life. It is all thanks to these little awesome experiences that I have always been the go-to-guy for any advice on computers and gadgets among my friends and family, even if I say so myself :) 
&nbsp;
The year 2003 introduced us to the most cool digital watches and smart phones. I was so fascinated by the fact that you could do almost all everything that you do on a computer, in addition to all the basic functions of a phone! Honestly, we have come a long way with all our gadgets, be it computers or cameras or watches or hand-held devices or gaming consoles or even storage devices. But then again, it feels like we still have a long way to go! There was a time when we used to be awed by the walkman, but the current range of high-end mp3 players and iPods and Bluetooth boom-boxes in the market never cease to amaze us.
  
Over the years I have been using the following devices for my day-to-day activities in the chronological order:
1. Casio scientific calculator &amp; digital diary
2. Assembled PC and Laptop / Macbook
3. mp3 player / Tablet / Mobile phone / iPod
4. Web Camera / Point and shoot camera / DSLR Camera / HandyCam
5. TV Video games / PS2 / Smart TV
6. Inkjet printer / Wi-Fi printer / External HDD / Wi-Fi HDD
7. Bluetooth headset / Headphones
&nbsp;
I have gotten so comfortable with all these gadgets that they have become more than just a style statement; they have almost become an essential part of my life. The internet, email, Wi-Fi, Maps, IM, e-commerce and mobile banking have seriously made life easier to a great extent. Every single day, we use the internet, at work, at home, even on vacation - to connect, communicate and share. The best advancement, according to me, is being able to connect to a person from a totally remote place in the world! Right down to this moment, when we are using the Internet to study and connect with all of UoL students in a virtual classroom!
	              
	              
Your first experiences using a computer.
I remember my first computer experience vaguely; for the first time, I was lead into the computer science lab at High School for Computer Science classes. We had one of those cathode-ray tube (CRT) monitors. I also remember being thought to type on a type writer at high School, something you don't often see today, as type writers are now obsolete. Generally, I could remember little of my computer classes, however I do remember completing my School Based Assessment on my father's computer at his workplace, this was sometime between 1998-2001. I also recall entering a Management Information Systems Department and seeing a server for the first time; I was impressed with its size. Whatever my experiences were, I knew one thing for sure; I was always fascinated with computers.

Changes in the types of computer devices you have used over time (e.g. desktop, notebook, PDA, smart phone, tablet, portable music players).
One of the things I recall seeing in my father's office, was a old cell phone which had the size of a cordless phone but was much heavier. I was always fascinated about the changes in size, weight and structure of such a device at the time. Over the years I watched the transformation from a regular cell phone that can just make calls, to one that can play music and games, then to cell phones that can view email and eventually a smart phone that can do almost anything that your Personal Computer can. I was excited when the big CRT monitor on my office desk was replaced with a Liquid-crystal-display (LCD) monitor. The LCD monitor being much lighter and allowing much more room on my desk. My first choice for computers was a laptop, because it was portable in nature. I have had a regular cell phone, which was replaced with a Smart phone, which was stolen, then I received a Tablet. I also have now the added advantage of walking around with my study material on my tablet, thus I can study during my spare time.

How common these types of devices are in your workplace or community.
Many individuals in my community are not as technology savvy as the individuals in the developed communities or countries. However, over the years we have definitely seen a surge in the knowledge base of these individuals. At work, instead of buying personal computers, our Department has purchased laptops for managers, thus enabling them to be more productive; they can work from home or access their files and information wherever they go. They can also connect to the office network wherever they take their laptop. IT Managers are also looking at cheaper ways of sourcing computers. A larger percentage of the population now own cell phone, and the owners of smart phones has increased. More homes have computers or individuals have access to a computer. Government has also embarked on a One Laptop Per Family project, thus giving underprivileged communities access to computers. Their are lots of Internet cafe, a rise in computer schools, and computer classes being thought in more schools. Their is also more Internet service providers, and a resounding drop in Internet cost. They are more computers in workplaces, as individuals are becoming more aware of the benefits of owning a computer. 

Changes in tasks for which you have used computers and electronic devices.
There has been a drop in the use of paper, at work as the email has become a more prevalent form of correspondence, as opposed to sending a hard copy of a written memo. The email is also quicker, effective and a cheaper form of communication. What I have also noticed, is that there is a resounding availability of applications that can perform several tasks. To me an important change has been in that of Payroll processing; financial applications has helped accountants a great deal in this area. Information can now be processed in a easy and timely manner. It also helps in generating and presentation of data. Management has seen a reduction in cost associated with training as persons don't have to go abroad to get training; training can be conducted online. The rise in computer applications has made a major impact, and in may cases has reduced man power and business overhead.

Changes in your comfort level with computers over time.
As an Information Technology and Communications professional for over 11 years now, I can safely say, that I have learned so much in my field. I remember the little girl that attended computer classes at High School for the first time, the girl that was trilled at seeing the processing of the daily newspapers and the involvement of computers in the entire process, the girl going to University and learning to code and build applications for the first time. Hours later, that same girl went to work and she was fixing users software problem, or opening a computer to fix a hardware issue, and she was later uploading the new on the company website. I remember when she bought computer parts and assembled the Central Processing Unit (CPU) for the first time. If I were to go on about the things I did or continue to do it will be too much for this forum. However, I have experience in networking, programming and telecommunications. There is so much more out there to learn and the list will continue to extend. But definitely I have become very comfortable over time. 
ReferencesJagdeo launches – One Laptop Per Family programme. Available from:
http://www.guyanatimesinternational.com/?p=3423
Top-Ten IT Issues, 2013: Welcome to the Connected Age. Avilable at:
http://www.educause.edu/ero/article/top-ten-it-issues-2013-welcome-connected-age
Research Article . The Case of the Occasionally Cheap Computer: Low-cost Devices and Classrooms in the Developing World. Available At:
http://itidjournal.org/itid/article/viewFile/325/148

	              
	              Hi Dr. Ayoola,
I definitely have a similar view. I suspect that many of us students are part of those small and innocuous developments, the advance of technology is not only due to the hardware and software breakthroughs. I believe it is also due to people that understood, in one way or another, that technology had a meaning, and by people that managed to inspire others about technology. The major breakthroughs in technology are most likely a combination of this human factor and technological advances, people like Bill Gates and Steve Jobs in relatively recent times are testament than when someone is able to advance technology and inspire people the results are phenomenal.
I am sure we all have little stories that are similar to this experience of mine:
While I was doing my apprenticeship in commerce in a renowned Swiss bank, I was assigned to the credit department. It was a small department of 5 people, they had a lot of files they had to verify manually and in case of customers not paying on time they had to manually type letters to them reminding them of their due. They also manually checked really good customers and manually type letters to promote new offers or new credits. To type these letters they had IBM computers running OS/2 and Windows 3.1. The IT department was busy developing their AS/400 systems that for them automating other parts of the day to day work was unimportant.
To me all that was a waste of time. But I knew that AS/400 would produce the customer reports with their account and payment status because they were actually printing it (and then manually checking it). So I simply ask the IT department if it was possible to produce that report on a flat file and save it somewhere where we could open it on the PC’s. It was strikingly easy to get that file, and with it I simply setup a MS Access database and attached it with a mail merge in MS Word. For me this was a simple task, and everyone was so amazed at how in few seconds I could generate the letters for every single customer that otherwise would have taken them months. They soon realized they could use the free time to do more interesting and valuable things. As it turns out, every single of the 5 people on that department bought a PC and went to learn how to use it. I know that some of them bought PC’s later on to their sons and one of those sons has an IT consulting company in Singapore today.
What I did was small and innocuous development in the sense that I made 5 people believe in computers and technology, I also help selling 5 PC’s and later on even more. How many other people like me were out there in the world? Even if a mere million like me were out there, that is 5 million more computers in the world and 5 million more people interested in them.
Technology advanced also thanks to these people (you, me and them); we are all part of those small and innocuous developments.
Regards,
Augusto

	              
	              Hi Anthony.
I think the whole aspect of miniaturization has been a big contributor to the increased pace of progress with computing. From the transistor to the microprocessor and continued progress in packing more and more power in smaller and smaller devices, has re-defined the application of the term portability. With size no longer being a limitation, innovation has been able to flourish
Joseph
	              
	              Hi Numan
Welcome to the programme
Just offering some help here - I'm also new to this way of learning, you need to upload this file via the Turnitin Link, if you havent already done so. This is in the Discussion tab on the left of the screen under the Classroom Navigation tab.
Best Wishes
Craig

	              
	              Hi Terry
It was very interesting reading your story about your change of career, and how you “made it” even though there were “light-years” from you to a computer.
I would like to comment on your “2 facts” about the “importance” of programming and that it “is getting increasingly harder”. Let it be said rigt now, I agree with you. 
Since I’m not a programmer, I will not comment on the technical issues (even though, I have been project manager for mainframe and .Net software developers, and I’ve witnessed how hard I can be when the try to be object oriented and operate with several layers.). I will primarily comment from a user’s point of view.
Firstly I will comment on the “importance”. One argument is that we continuously need to improve all processes, hence it’s incredibly important to have good software. I have seen several cases, both in former jobs and in my current, of software which have not been satisfactory, not living up to the end-user’s demands, and in worst case, making the processes worse. And especially in a competitive environment were speed and service is ever so important.
And the importance is at the same time the reason why it gets harder, the requirements increase continuously and the developer has to keep up speed, and perhaps even be a step I front. To this I could add, one of my own experiences on requirements when the head of a banking investment department said to me, “Bo, I expect you to know my needs/requirements before me” - this, I thought was hard to live up to.
Well, “between the lines”, I’m also saying how important it is for developers to really listen to the users, and be open minded. Then you are well on the way to be “king of the future”.

Best regards
Bo W. Mogensen
	              
	              MY HISTORY OF COMPUTER 
Introduction-
Computer technology has been for years but the development of the computer system in recent years into a more useable device and the flexibility has made it more accessible and affordable to many.
Some years ago I had just gotten back home from my evening classes during my junior secondary school days popularly known in my country as JSS3. As I entered the sitting room I noticed some two large cartons besides the room cupboard placed on each other. I was less concerned because we had a 21”inch Panasonic television and a video recorder already hence I felt those cartons should be a gift from someone for my dad or for his office. However, my mum had just entered the sitting room while I was walking pass the two cartons and she said to me “Nobody should touch those two cartons, there are for your dad” and I reply “yes mum”. After this I went into my room and after a short while I fell asleep. Later that day, at about 7:55pm my dad came back home and the first thing he did as soon as he saw the two cartons was to call all of us kids into the sitting room and he told me to open up the cartons. As I open them up, I saw a computer desktop monitor and a CPU device. It was a Dell desktop that he had gotten for us and he said “this is for your studies, use it and don’t spoil it”. He then said the computer technician in his office was coming to set it up for us the following morning. We were very fascinated about the new computer and after he had retired to his room we talked about it and how we would boast about it to our friends in school. The next day came and the computer technician from my dad’s office came and the computer system was up and running. He later gave us tutorials on how the computer functions, its usage and maintenance. This lecture continued for about 3 months, three or four times a week.
However, overtime things changed from using a desktop computer at home to using other computer devices. When I got into the university I was introduced to learning the history of computer from “abacus” to punching cards to ENIAC to Univac and then to the use of transistors. These helped me understand and appreciated the computer technology better. As I progressed to taking classes in “Fortran” and “Basic” it made more sense. As I got into my second year in the university, my parents got me my first Compaq laptop which was a more easier-to-use device because of its smaller size and its compact nature. The laptop was used for a while before other smaller devices came in handy. Next I got another laptop but this time a much smaller in size than the first one. This was due to the first one becoming more cumbersome in moving around. This new laptop was an 11”inches screen size and had a lighter weight. Furthermore, I got an Apple iPad due to its size and the fact that it was more efficient as its power lasted for long hours after charging. Eventually, this paid off as I had to carry my iPad instead of the laptop when am making short travel trips. Apart from the ipad, I have used several other smart phones ranging from the HTC, SAMSUNG GALAXY NOTE 1, NOTE 2, and presently NOTE 3. These smart phones were gotten as a need for a more compact device with greater functionality grew.
Conclusion
My zeal to know more has been as a result of technology advancement.
&nbsp;
REFERENCE:
Reed, D.(2005) A balanced introduction to computer science. Upper Saddle River, New Jersey: Pearson/Prentice Hall.

	              
	              Hi Anthony,
Interesting story.
The mention of David Cassidy's song "daydreamer" in your story gave you out but on my own opinion i think it revealed that you have had a great experience having had your first computer experience in the early 70's. I commend you because with all that experience you can now appreciate the advancement in technology as regards computer systems.&nbsp;
My take on your short quiz:
1. David Cassidy, october, 1973
2. IBM
3. Busicon, a japanese calculator
4. Fortran

Regards&nbsp;
martins
	              
	              
&nbsp;
Quiz Question:&nbsp;


&nbsp;Can anyone guess exactly what year this took place? (look for clues in the story, and no, it's not before world war one!


Since you mentioned the David Cassidy hit song, I am assuming that it is some time either in 2001 or 2002. 





Which company produced the first commercial mainframe computers?&nbsp; How fast were these (e.g. how many instructions per second)?&nbsp; How does this compare with the microprocessor(s) in today’s average intelligent mobile phone (speed-wise)?


  a)The Remington Rand Company Univac was the first commercially sold main frame computer produced in 1951. The speed of the memory of the Univac was 500 microseconds. It also had a storage capacity of 12000 characters or 1000 words. Today, mobile phones can store much more data and have microprocessors that are over 2 GHz.



Electronic calculators were sold commercially some years after this story – does anyone know which company sold the first commercial calculators?


The first commercially sold calculators were sold by Sharp.





Was Fortran the very first high-level programming language?&nbsp; If not, what was?


No Fortran was not the first high-level programming language. It was Ada which was named after Ada Lovelace, who was recognised as the first computer programmer.



References:
Liverpool Online Programes
https://elearning.uol.ohecampus.com/bbcswebdav/institution/UKL1/201480_SEPTEMBER/MS_CKIT/CKIT_501/readings/UKL1_CKIT_501_Week01_LectureNotes.pdf

Participating in the Global Classroom/Part 1 of the History of Computing, Program Transcript. Available at:
http://cdnfiles.laureate.net/2dett4d/managed/UOL/CKIT/0501/01/UOL_CKIT0501_01_H_EN.pdf

http://www.history.com/shows/modern-marvels/videos/who-invented-the-computer

The UNIVAC FAQ. Available at:
http://www.computermuseum.li/Testpage/UNIVAC-FAQ.htm

	              
	              
&nbsp;
Quiz Quetions:


&nbsp;Can anyone guess exactly what year this took place? (look for clues in the story, and no, it's not before world war one!


Since you mentioned the David Cassidy hit song, I am assuming that it is some time either in 2001 or 2002. 





Which company produced the first commercial mainframe computers?&nbsp; How fast were these (e.g. how many instructions per second)?&nbsp; How does this compare with the microprocessor(s) in today’s average intelligent mobile phone (speed-wise)?


  a)The Remington Rand Company Univac was the first commercially sold main frame computer produced in 1951. The speed of the memory of the Univac was 500 microseconds. It also had a storage capacity of 12000 characters or 1000 words. Today, mobile phones can store much more data and have microprocessors that are over 2 GHz.



Electronic calculators were sold commercially some years after this story – does anyone know which company sold the first commercial calculators?


The first commercially sold calculators were sold by Sharp.





Was Fortran the very first high-level programming language?&nbsp; If not, what was?


No Fortran was not the first high-level programming language. It was Ada which was named after Ada Lovelace, who was recognised as the first computer programmer.



References:
Liverpool Online Programes
https://elearning.uol.ohecampus.com/bbcswebdav/institution/UKL1/201480_SEPTEMBER/MS_CKIT/CKIT_501/readings/UKL1_CKIT_501_Week01_LectureNotes.pdf

Participating in the Global Classroom/Part 1 of the History of Computing, Program Transcript. Available at:
http://cdnfiles.laureate.net/2dett4d/managed/UOL/CKIT/0501/01/UOL_CKIT0501_01_H_EN.pdf

http://www.history.com/shows/modern-marvels/videos/who-invented-the-computer

The UNIVAC FAQ. Available at:
http://www.computermuseum.li/Testpage/UNIVAC-FAQ.htm

	              
	              Hi Bo,
This is also true. Developers quite often are too excited about the technology than satisfying the customer need. I call this "Fashion-Driven Development".
br, Terry
	              
	              Hi Augusto,
Thanks for reading:-)
What we considerred "hardware" nowadays is mostly software (and increasing), and the software part is getting more and more complicated just as any other software is.
But I think you just reminded me a very important point, the ability of integration. I guess "multi-speciality with software bias" is more important.
br, Terry
	              
	              I must say that my history of computers is not that glamorous. My initial introduction I can barely recall, primarily because I didn't know at that time computing could be a career.
It was in 1995 my third year of High School and Information Technology was a new subject. I was taken a to the small computer lab we had at the time, this room was located at the end of the hall adjacent to the teachers' lounge which signified the exclusive nature of the contents. The class was introduced to basic components of an IBM desktop computer; monitor, CPU and keyboard. In the same class we got the chance to power on the computer and observe black computer screen with the command prompt. In subsequent classes we began to learn about the MS-DOS operating system and the applications that ran on it. 
After several classes my interest grew far beyond what I was being exposed to and I started to stay late after school hanging out in the computer lab waiting to get a chance to use one of the units. The senior students had full access to the computer lab and was always working on some project. So I tried learn from these guys as much as I could about the command line of MS-DOS, how the applications work and all that they were willing to share. I went on to learn to use the productive applications and programming using Pascal and Qbasic. That was my high school experience.
In the Summer of 1999 my passion for computing was re-ignited and taken to a whole different level when I to worked on cabling project with my uncle a field engineer with the primary&nbsp; telecommunications provider at the time. I had the opportunity to observe the entire process of implementing a segment of the company's wide area network. I was totally amazed at the entire process, from installing and terminating the cables to configuring the network devices and commissioning traffic on the new segment. My interest in networks and communication was born and I just had to learn more. 
From then I tried&nbsp; to learn everything about computer systems and networks, how to assemble, configure and repair. Basic network topologies, connecting computers together and setting up local area networks, getting my hands on all the material I could find that would help me understand what I wanted to do and how to do it.
As I venture more into the field of computing and IT, I saw the career possibilities and decided to pursue a course of study in this field. In 2002 I started the Associates Degree in Management Information Systems at the Institute of Management Sciences.

	              
	              Hi Colleagues,
I agree with both statements above, but I believe those factors only accelerated the development of the advances we are seeing in technology today. Since the development of transistors, the advancement of microprocessors to the extent it is now was inevitable.&nbsp;
The convergence outline above, coupled with the continuous market demand for more innovative technological products and the fierce competition between technology base companies will continue to fuel significant advancement.&nbsp;

Ricardo
	              
	              Hi Anthony,

What a relief, like Craig I was dreading the thought of complex calculations.

That is a very good story, I almost feel like I was there.
Q1) 1968 -" England opener Geoff Boycott cuts the ball on the final day of the fourth test against the West Indies at Queen's Park Oval, Port of Spain, Trinidad, 22nd March 1968. " &nbsp;http://www.gettyimages.com/detail/news-photo/england-opener-geoff-boycott-cuts-the-ball-on-the-final-day-news-photo/51948571
Q2)Sperry-Rand - a company built by the inventors of the ENIAC from the University of Pennsylvania. Their first commercial computer was called the Univac. these were&nbsp;
Q3)Intel
Q4)No Fortran wasnt the very first high level programming language. Ada was .
	              
	              I suppose you are right about hardware being software. These days the old hardware is more commonly referred as computing power, at least at my work. With virtualization and the cloud being the dominant trends today software and hardware definitions are blurred into the same thing.
Like you said “multi-speciality with software bias” is truly important. I have seen many times bad quality software engineers that design software without having a clue of the underlying systems and interactions. For me they are simply “coders”, they know how to write statements but like you mention they don’t know all the complexity behind the frameworks. And at the same time I have seen infrastructure engineers that don’t care or don’t know a minimum of the complexities at the software layer. As the IT world evolves, I think that both roles need become inter-nested and have to reach to each other and know a minimum of both worlds. Of course the software engineer will have the hardest time, because he’s the one in front of the customer, and a top of the technological challenges, he needs to deal with a number of customer demands that are more often than not somewhat unrelated to software (budget, ease of use, ergonomic, social and human factors).

	              
	              “Processes seem faster, but we spend enormous efforts to keep it running and solving incidents and problems. Could we have done things differently, more effectively, have our efforts really been worth it?”
Hi Bo,
This is a very good question! I would say that it really depend on the context of the question. Since we are starting this class with the history of computing, in this context, if we look back at ENIAC, I am not sure this question reflects the reality.
How hard was to keep running ENIAC and how many errors there were?! You can hardly compare that effort to today’s hardware plug-and-play philosophy. Today you can hot plug a CPU into a server, imagine doing the same with ENIAC! Back then they needed a big team to support one single, enormous, machine. Today one system administrator alone can handle multiple (tens of, if not hundreds of) hardware, virtual or software systems. In this context I can hardly say that today we spend enormous efforts to keep processes or systems running. Our efforts in this context are worth 100%.
If we take today’s context and compare processes and systems between each other, then I suppose things are seen in a different light.
Let’s take a well know process: system authentication. If I were to guess, most corporations probably have Microsoft Active Directory as the culprit of their systems authentication. How hard is to keep Active Directory running these days, and how many incidents you really have with it? In my experience, assuming you know what you are doing, this is quite simple and straight forward. So the question would be: why you have authentication issues? Bad design? Bad implementation? All that equates to lack of skills.
But this is just one single, isolated, albeit important process. What about virtualization services? That is a bigger service overall: a whole company may actually run out of a unique virtualization layer. I bet anyone can point out at all the common issues we encounter in the virtualization world: capacity planning, storage I/O, network saturation, etc. But you can do one little experiment and setup a home virtual farm, setup what you need and let it run, come back after 1 year and things will most likely still run if you do not touch them for the whole year. So things actually work, right? When you do the same in a company environment things do not go so well, but the reason is not really technology: it is us, human’s beings. You will never see a company environment sitting there doing nothing for 1 year, people will use the systems/services and as they do so, their needs change. This forces us (the IT guys) to manage those systems, but what we are really managing is this usage/needs change. If someone needs today 500 GB of storage and in 1 year they are asking for 5 TB, you have to be able to plan it. And a system that grows like that will only use more computing power and networking, impacting how you will manage those parts of the system. When you do not manage well these changes, then you have incidents (for example: you let a system grow so much that impact the performance of the other systems, generating incidents) and you end up spending tremendous efforts to keep things running. I actually see this happening every day at work. Managing this change is incredibly hard, and I believe that it is because of the following (top 4):

Customer demands (whenever it is an internal or external customer) is very fast. One day you have a system that starts with 1 Virtual Machine, 2 months after it grew to have 80 VMs.
Talent acquisition is very hard. You will struggle to get the proper person with the proper skill set in a very high competitive work market. Very often people end up lowering the bar and hiring people that are not the best fit. (for example: you hire a good Active Directory administrator, but that does not mean he knows how to design AD with Kerberos and SAML services at all, and if you are struggling with Kerberos and SAML, you will keep struggling).
Lack of budget. A company simply cannot (or do not want to) put as much money into that infrastructure as required by the fast changes.
Lack of human resources. A company may not have the necessary skills to properly manage all those changes, but also may not have the resource to hire people with the proper skill set. It’s not like you can fire the current people because you also need their current skills, hence you end up between the hammer and the anvil.

Can we do things differently? Given infinite resources and time we could, but unfortunately the best we can do is learn to manage such things with our limited resources, learn to prioritize and learn to find a way to do better with what we have (not to be confused to do more with less, that, in my experience, rarely works).
Now, of course, we will have technology issues, like when a system goes down, a hard disk break, or a software has a memory leak. But given a proper design and proper skills, things today should be redundant enough so that one thing happening can’t bring down a whole system. And yet, one can argue, a series of things that converge to bring down a whole system still happen too often. But why? Is it really technology that let us down? I’d like to make a comparison with the airplane industry: what are the causes of an airplane crash? Rarely it’s the technology that makes the airplane that fails, way too often is series of events that have heavy human error involvement, 17 out of 208 causes can only be attributed to technology, albeit designed by humans. (http://www.planecrashinfo.com/cause.htm, no date).
Have our efforts really been worth it? I, personally, will say: yes. Totally worth it. I guess it depends on how many failures one live through or what industry are you in. But if I look at my current company, Autodesk, all the efforts we have made and all the success we have today, I would say that I am proud of what we have accomplish. Sure, sometimes I want to bang my head on my desk, nothing is perfect, but overall we have managed changes in a way that does not make you go crazy.
&nbsp;
&nbsp;
Reference List
http://www.planecrashinfo.com/cause.htm (no date) (Accessed 8 September 2014).
&nbsp;

	              
	              Hi Augusto,
Unfortunately, only knowing a minimum of both worlds seems not enough. It seems that a really good developer need to have deep knowledge in both, and plus knowing the domain knowledge even better than his/her customer. I do know some people like that, e.g. a friend from a bank, he's the top programmer with good leadership skill, and he understand banking much better than thos business analysts.
One interesting question: imagine an average member in a large software/hardware development team. Do you think his contribution to the final result is positive or negative?
br, Terry
	              
	              My first Computing Experience
I do not quite recall the very first time I used a computer, my cousins had a computer and we used to play games like Tetris on it. However, I vividly remember the day I set up my first email account, a yahoo account I still use to this day. It was in 2002, my brother took me to an internet café, navigated to the yahoo page and step by step helped with the registration process. I was so intrigued by it all, I took forever trying to come up with the perfect username, typing it out was another process. The keyboard seemed so huge, with way too many letters and numbers. I found the mouse strange to use, the right clicking, left clicking and dragging easily compared to rocket science to me. I could not grasp the QWERTY sequence of the keys on the keyboard; I could not understand why they had not just gone with the normal A-Z alphabetical sequence of letters. It took me a while to find my way around a computer, why was the computer made up of four pieces? Not having a computer at home meant I had to go the internet café each time I wanted to access my email or use a computer which slowed down my grasping of all of this.
Fast forward to 2014, I can almost type with my eyes closed. I have evolved from using a standalone cpu, monitor, keyboard and mouse set up, to using a windows laptop, remote logging to other servers, using a mac book pro laptop, an iMac mini using tablets and my phone. Oh how my world has changed. All my friends, colleagues have smart phones, laptops and tablets, having at least one of these devices particularly a smart phone has suddenly become a need.
I used to have an alarm clock to wake me up, but now I use my phone as an alarm clock, a stop watch and as a daily planner. I can use both my phone and computer as calculators, to write documents, access the calendar and set meeting and date reminders.
Before I would get frustrated using computers because everything was overwhelming. I was intimidated by them but 12 years later, I cannot imagine life without them. I don’t have a work desk full of files like I saw with my parents and relatives back in the day. If my phone battery goes flat and I can’t charge my phone my world nearly crumbles. I have grown very comfortable, in fact very reliant on computers.

	              
	              The issue of adoption of the QWERTY keyboard is an interesting factor and is one of the many human factor elements (ergonomics) that one has to think through in design aspects around computing devices. I understand that APPLE in developing its products, goes a great way to ensure that the user experience is as phenomenal as possible.&nbsp;
Other than the QWERTY keyboard, another keyboard layout known as Dvorak has the most common letters sectioned in the middle row and is apparently designed for speed typing.
References
http://computer.howstuffworks.com/question458.htm&nbsp;
http://www.webopedia.com/TERM/D/Dvorak_keyboard.html

Joseph

	              
	              Hi Joseph,

It's interesting you mention that because in my naivety growing up and learning about computers I always assumed that even foreign keyboards would be the same as the querty layout only substituted for that specific countries lettering of course.
Upon looking it up I was surprised to see so many variations and how they came about. Like you said the Dvorak version supposedly designed for speed typists although surely any keyboard is only as good as the experience of the user and the application it is used for so I try not to get too overly excited about statistics which says X keyboard is better or faster than Y when some keyboards are clearly built for certain uses and engineered for specific jobs so how could they be compared.
After reading your reply though I just had to go and look at some more on wiki : http://en.wikipedia.org/wiki/Keyboard_layout
Some interesting ones to say the least.

Author: Joseph Warero Date: Monday, September 8, 2014 11:13:12 AM EDT Subject: RE: Your history of computing

The issue of adoption of the QWERTY keyboard is an interesting factor and is one of the many human factor elements (ergonomics) that one has to think through in design aspects around computing devices. I understand that APPLE in developing its products, goes a great way to ensure that the user experience is as phenomenal as possible.&nbsp;

Other than the QWERTY keyboard, another keyboard layout known as Dvorak has the most common letters sectioned in the middle row and is apparently designed for speed typing.

References

http://computer.howstuffworks.com/question458.htm&nbsp;

http://www.webopedia.com/TERM/D/Dvorak_keyboard.html

Joseph



	              
	              Hi Belinda, Joseph and Christopher,
Just wanting to add to the qwerty keyboard discussions. The qwerty keyboard was invented by Christopher Sholes and according to an article by Baker (2010) the biggest obstacle was to devise a system that was able to link “an easily understandable interface with the complicated technology of ink, typebars, levers and springs”.&nbsp;
Given these complications, Sholes arranged the keys in a way to make the machine work without clashes to the typebars. His first attempts, which were based on an alphabetical method, proved to be unsuccessful. Frequency and combinations of letters were to be central in his considerations to make this a success. Qwerty was then adopted by Remington in 1873, which were famous for its arms and sewing machines as well as its typewriters (Nick Baker, 2010).
In the same article, it is agreed by Professor Koichi Yasuoka of Kyoto University, who is also a world expert on the development of the keyboard that, qwerty is not “ergonomic” but it can be clearly linked to the world of typewriters.
In conclusion, should the keyboard have been invented post the typewriter era and in the digital age whereby it did not require any mechanical components that could clash, I suppose it is possible that we could have quite easily adopted any keyboard layout.
I wonder, is this an example of a lack of change and evolution, and unwillingness to adapt, or is it just a case of not needing to reinvent the wheel as such?
I would also agree with Joseph, that Apple does invest significant thought-leadership into their product User Experience (UX) and User Interface (UI). Sir Jony Ive, a London-born designer, who is SVP Design at Apple inc, is the executive behind the look and feel of Apple’s innovative products. Sir Jony is responsible for the design teams, as well as the leadership and direction for Human Interface (HI) software teams across the company (Apple Inc, 2014).
Its almost time for some new products!! Check out the countdown to Apples special launch event scheduled for tomorrow (9 September), available from: http://www.apple.com/live/
Best Wishes, Craig

References
Baker, N., ed. 2010. Why do we all use Qwerty keyboards? [online]. BBC News. Available from: http://www.bbc.co.uk/news/technology-10925456 [Accessed 8 September 2014]
Apple Inc., 2014. Apple Press Info [online]. Apple Inc. Available from: http://www.apple.com/pr/bios/jonathan-ive.html [Accessed 8 September 2014]
Apple Inc., 2014. Apple Launch Special Event [online]. Apple Inc. Available from: http://www.apple.com/live/ [Accessed 8 September 2014]

	              
	              Hi Terry,
Those people are uncommon. That bank is lucky to have your friend. But many companies are not so lucky.
However, when I say “minimum of both worlds”, that does not equates to just the basics. My minimum bar is indeed pretty high. For example I would expect that a software engineer that build cloud services (such as Amazon AWS) understand a minimum in cloud networking and security (a part from other skills). But a minimum in could networking doesn’t mean simply to know what a “ping” command is. A minimum in cloud networking will require quite some deep knowledge of networking protocols (form the data to the transport layer at least) and experience in working with several networking solutions/vendors.
But I get your point, I have seen people in some positions where they don’t even know the bare minimum (ping).
Your question is a very good one. The only way I can answer this is by experience: the majority of time and average member in large development team, does contribute positively to the final result. You don’t need a team of top performers to have a positive outcome in any given project/team, you do need strong players, but average players are not necessarily bad. What is most important is managing the different skill sets of the people. One can be good at something but not as good at something else. But if you know this you can assign tasks accordingly, and most of the time average members can grow and become top performers if given the opportunity and proper coaching. After all, you are rarely born a top performer.
Now, I have seem average members undermining the overall progress of a particular project or team, hence brining a negative contribution to the final result. In some cases I have seen those people being fired too. Some average member will never grow up, they might be just “bad” apples, or people comfortable with what they are doing and don’t want to do more (and that, is okay too, you may need these kind of people in some positions/teams).

	              
	              As promised, here are the "Fun Quiz" answers -&nbsp;
1. 1974
2. Technically - ENIAC (1942); Commercially - UNIVAC 1 (1951); Your average smart mobile phone is 2 to 5 million times faster at executing instructions than the UNIVAC mainframe!
3.Partially electronic - Casio model 14A, Japan, 1957; &nbsp; &nbsp; Fully electronic - Anita Mk7 by Bell/Punch, UK, 1961
4. Technically - Plankalkul; Commercially - Fortran

Good attempts everyone!

	              
	              Hi Bram
It looks like the real computer start was the same for us, starting with the Commodor 64 and the cassette tapes – those were good days. Did you also have the games: Frogger, 007 and Ghostbusters?
You mention “still typing this on a laptop”. I do the same, and the reason being, that I don’t feel the tablets, even with keyboards, are mature enough yet for intensive work and studies like this.
In a short while (days or weeks), I will test the new Microsoft Surface 3 Pro. But I will not try the tablet model. I will try the bigger model because I want to try a shift from my laptop; hence I need to have a unit where I can install software. My only worries right now about this is that they are more expensive then my current laptop (Dell Latitude E7240).
Do you agree that today's tablets are not mature enough to replace a laptop’s functions?
Microsoft Surface (2014). Find your new surface [Online]. Available from: http://www.microsoft.com/surface/en-us/products/help-me-choose. (Accessed 2014-09-08).

	              
	              Hello Bo,
Must have played thos games also. But can only remember vivedly some horific scenes visualised in big blocks (pixels of a cm each).

I use some tablets and just ordered one for my 70+ mom today (Dell Windows 8.1 version with 3G). I think for her it is less hard to find stuff as every icon just opens a simple appliance. The mobile network makes it easy for her to have network connection everywhere without wifi hassle while at home it will automatically swith to her local wifi. Lets see if she will start to use it. For real work it does not really do the job but this is also due incompatibility problems with a lot of applications I need to use.

Bram
	              
	              Hi Bram
I must thank you and commend you for your answers and different context variations to my question. And I agree that it really has to be put in to a specific context to be truly “measurable”. &nbsp;I also agree on your conclusion where you write “Totally worth it”. That’s also what I told my friend, though I did not manage to fully convince him. So perhaps I should use your arguments and see what happens. Any way’s, depending on the person, mood, feelings, life values, experiences etc. this question can be debated for an eternity without any “correct” answer.
I must also admit, that even though I know that it’s all worth it, I sometimes find myself doubting for moments until I get back to my normal state of mind. A little example is when I think about how much time we have spent on our SCCM1 environment. That even though we have spent many hours/weeks optimizing, minimizing the number of administrators to 3 persons (even revoked myself), and implemented phases (via test, production and end-of-live folders and procedures), suddenly a Task sequence2 (read: installation process) will stop, and everybody swears not to have touched anything. This incident/problem can be frustrating, and hard to explain to the technicians, and especially the users who don’t understand why they can’t get their computer reinstalled as normal. But you are right, I’ve been so long in this field to know, that it’s (almost always) normally some human error that’s to blame, either being at the user end or developer’s end, and I allow myself to cite your words: “All that equates to lack of skills”.
Reference list:
1 SCCM. System Center Configuration Manager. [Online]. Available from: http://technet.microsoft.com/en-us/library/gg682129.aspx. (Accessed 2014-09-08)
2 About task sequences. [Online]. Available from: http://technet.microsoft.com/en-us/library/bb693631.aspx. (Accessed 2014-09-08)

Best regards
Bo W. Mogensen

	              
	              Hello Terry,
Interesting account of your early computer experience. As i read through i noticed we had some similarities in both our accounts of early computer history.
Firstly, I noticed despite our vast cultural differences, we both grew up in small cities and fell in love with computer systems greatly.
Seconly, our parents got us computer systems when we got into the university which i think is the motivating factor here and this has made us what we are today. The drive you get from owning a computer system whether desktop or laptop is intense because you want to know everything there is in there.&nbsp;
Lastly, I would say we both fell in love with computers the minute we first saw one which has propelled us to becoming IT professionals.

Regards
Martins
	              
	              Hello chris,
Nice story. I like the last line where u said " it will be an interesting ride and I am for one am glad I was born in this era to see it happen".
Am of the same view - next 20 years.

Regards,
Martins

	              
	              The answers:
1. Based on the following clues:

Because the game took place in 1968 (after the war!!).
Because that Daydreamer appeared in 1973.
Becuse of you have been working over 30 years.
Because of the commercial calculators became available in the mide of 1970s.

I guess that your story happened in around 9 o'clock in 1974!!!
2. It was IBM. It could perform about 16,000 operation per second.
3. It was Busicom.
4. Yes, it was.
	              
	              Well, I got the story time right!!! But as for questions 2 and 3, NO. Also, I got question 4 partially correct!!
I dont think I passed this fun quiz :(
	              
	              Hi Anthony,
Yes, I agree with you on your view on the small innocuous developments.
Teleporting i think is where we are headed in the next 10-20 years. The beam up scotter is the next big thing and to think of it that it will make movements from one place to the other alot easier and faster.
Martins

	              
	              
Although in your opinion your history was not glamorous, then perhaps mine wasn't either, as it started out in a similar manner to yours; having my first experience at my High School Computer lab. Then there was that second or that ground breaking experience that gets you thinking,
“Hey, this is kool. I can do this, this is exciting.”

This is one of the things that caught my attention with computer Science; the excitement. I liked the idea that, you are not bombarded by tons upon tons of paperwork. Or stuck doing the exact same thing day after day, getting more and more bored. When dealing with computers, there is always a challenge, something usually takes place, that forces you to think critically or think outside the box. Something that never happened to you before. You also have the opportunity to make the field exciting for you.

Additionally, depending on the time frame we were born, many of us may have some level of fascination or interest in the field, having being exposed to it for the first time. I also believe that it is more the dynamics and diversity of the field that is captivating to many that are in it as well. The field offers so much that you can do; you can be a software developer, a network Engineer, a Systems administrator, a Network Administrator, a IT Manager, a lecture or teacher. You can work with people or you can work all on your own. Maybe their may be other fields out there that offers this level of diversity but for me, this is it. The field is still growing and it will continue to grow offering so much more. Although a few persons in the field may say or believe that they know a lot, there is so much more that can be learned. Which I why I am now choosing to be on the management side, after being exposed to my share of knowledge, I am now moving on to Management Information Systems.
After these years, I think it was time I did some management studies.


	              
	              Hi Terry and Augusto,
I enjoy the discussion you guys have been having and just wanted to add a comment. From my little experience I have found that a diverse background offers a lot to a development team especially if that diverse individual is the project lead.&nbsp;
When working with multiple skill sets within different individuals towards a primary goal, a leader with a good grasp of all the key aspect of the project is always in demand. In software development its extremely beneficial when the programmer/coder understand the network communications stack, protocols and the process of transmitting information in order to develop a good software. A programmer having this type of insight will know exactly how to write his code to make the most effective use of the communication infrastructure his software will use.&nbsp;
Terry's question is a tricky one, this will have to be determined by the individuals contribution to the project and how he/she is manage and engaged &nbsp;throughout the development process. This individual might very well be able to add a dimension that could have been an oversight by the high performers, which could intern result in a very positive outcome. As well it could be the reverse. Such a situation will have to be evaluated on its own merits and the desire objectives of the project.

Regards,

R.Biggs

	              
	              Hi Tanisha,
Good to know that I share a similar background someone pursuing the same programme. I must admit with all the possibilities I was expose to during my second experience, I was like a kid in the candy store, I was now seeing so many possibilities and the constant evolving was very intriguing, I just had to learn more.
This field of information technology is dynamic in every sense, staying current with the trends, development, advancements, etc.&nbsp;forces you to continously want to learn and understand these changes.&nbsp;
Now being able to put that knowledge together with key skill sets and apply that capability to organization challenges, developmet projects, etc. just goes to show the value of information system management and its vital role to success.

	              
	              SCCM is a hard beast to tame! You are lucky you have at least 3 administrators, we used to have 4 then went down to 0, getting things back up plus a migration to 2012 R2 have been a nightmare. That’s without counting that we don’t really have engineers that know SCCM, and architects know it less than the end users… so you can only imagine!
In all honestly if there’s a moment where doubts arise in my “worth it”, it’s certainly with SCCM.

	              
	              Hi Bo,
This is a great question and one in which opens up a more complex set of further questions and research that I cannot really give any justice to in the classroom discussion however, I would at least like to contribute myself with an initial perspective on this.
I completely agree that with both you and with Augusto, that in addition to the traditional problems that occur in technology itself, there is also a human element to the issues faced. We have seen that over time, there have been step change improvements in developing technology solutions that ‘fail’ less frequently, and we have developed in many cases a process mentality to the management of these failures across enterprise, but ultimately things do fail.
In context, traditionally IT has been seen as a business service, by providing technology capability to perform business tasks significantly more effectively and efficiently than what could be performed manually. Expectations have since move on, and IT is now seen as a business enabler, with the demands that are placed on IT increasing exponentially. In fact, I would suggest that IT is now very much a business innovator, used to help businesses compete with key decisions that can enable differentiation, open up new and different markets and provide many different levels of consumer engagement. 
As a result, this introduces a significant and complex portfolio of change activity across the IT enterprise, which needs to be managed across the organisation pre, during and post implementation. It is this that still requires, and will continue to require human interaction.
Of course, this management activity isn’t necessarily as a result of a problem. In fact, much of this activity could actually be attributed to preventing problems, and given that IT is central to any successful business, any measure of unavailability has varying degrees of consequences and unacceptability and should be avoided wherever possible in relation to these priorities.
One of the critical success factors for IT in the future is to be proactive with the preparedness of IT in relation to business demand. Typically, IT has lagged behind in the development of IT capability, in that it has reacted to the needs of the business, and subsequently delivered a solution to meet those needs. Over recent years, we have seen this gap reduce, but it has still shown to be an iterative and reactionary process. Now however, IT needs to be ahead of the curve, and introduce strategic technology capability and solutions ahead of the demands and needs of the business (Chopra, 2014).
This is creating a very different mindset amongst IT leaders, in that the successful ones are needing to make the shift to become business leaders with a specialism in IT, rather than IT leaders that support the business (Peppard, 2012). 
In conclusion of this perspective and also in support of your initial discussion theme, change is very much the norm in the world of IT, and our ability to manage the implications of it, will continue to be critical and worthwhile to the success of it.&nbsp;
Best Wishes, Craig
References
Chopra, A. 2014. IT Strategy and innovation: Driving Innovation from the CIO’s Office – Staying Ahead of the Curve with New Strategic Capabilities [online]. IDC. Available from: http://idcdocserv.com/246854 and&nbsp;http://www.idc.com/getdoc.jsp?containerId=246854 and&nbsp;http://www.oracle.com/us/c-central/cio-solutions/index.html [Accessed 9 September 2014]&nbsp;
Peppard, J. 2012. The Leadership role of the CIO [online]. Cranfield SoM. Available from: http://www.youtube.com/watch?v=4Rw0XFeBYIA [Accessed 9 September 2014]
	              
	              No, you passed - don't be too hard on yourself :-)!
Regards,
Anthony
	              
	              Hi Everyone,
If you had to choose one solitary person (living or dead) who has had the most significant influence on the development and/or use of computer systems, who would you nominate, and why?
Anthony
	              
	              I have read a few of the posts as it relates to a Software Engineer needing to know various aspects of hardware as well in terms of networking and so forth. But my question is, is this really necessary?
I am from a hardware background, although I have programming experience from my time at University, the majority of my work experience is that of hardware. Recently I decided to switch things up a bit and go into the programming field. I have been in this position for three months now and I was required to resolve queries, do minor code edits, reports and so forth. I have not engaged in any major software development thus far; however I don’t really see the need for hardware knowledge. Maybe it would be dependent on what it is that you are actually developing and exactly what hardware knowledge is needed for it. I recall as a Network Administrator, when dealing with a few of the programmers, when asked to fix certain problems I always wondered about their knowledge base and I may have been a bit critical of them, being that I felt they should have been able to resolve their own problem. &nbsp;
I believe as Information Technology Professionals we must be able to analyze situations and think critically. If the software we are trying to develop requires background knowledge on the OSI Model, how about we google it. Put in the extra effort and try to learn whatever that may be needed at the time. Additionally when you are leaving a job the next company is not going to have the exact systems that the previous company did and such you will have to learn their systems, you may not have Linux experience but you now have to learn it.
Thus if the situation necessitates it, then we learn it, unless it is something we wanted to learn for our own gain and development. &nbsp;For me too much knowledge can be overwhelming at times. We are in a diverse field and there is so much out there to know and learn. The Software Engineer already has enough on his plate, I am sure. I could be wrong.&nbsp; What’s you view opinion on this take of the topic?

	              
	              Hello Craig,
Thanks for your advice and I would also like to thank you for sharing knowledge, experience and your wisdom with us. I really enjoyed reading your posts and the way you try to help us.
Best Regards,,,
Numan

	              
	              I would state my choice from the Kenyan context and experience,&nbsp;for the one solitary person who had the most significant influence on the use of computer systems, and hand the crown to one&nbsp;Bill Gates.
I developed my computer expertise from MS-DOS through to Windows 95 &amp; XP to the current Windows flavours; watched the spawning of hundreds of computer colleges whose path to sucess revolved around Microsoft products. In Kenya, Bill Gates Microsoft Corporation has for a long time been the most dominant and visible software company helping to build the careers of many computer instructors, software developers, computer engineers to say the least. Over 90% of desktop computers being purchased from hardware vendors would come pre-loaded with MS-DOS or a flavour of Windows by default and this of course set the standards for computer use and locked in users.
For a long time Microsoft based courses were the gold standard, pre-requisites and entry point for careers in software development and networking through the since enhanced Microsoft Certified Systems Engineer (MCSE) &amp; Micorsoft Certified Solution Developer (MCSD) courses. One's resume in Kenya was incomplete without a Microsoft certification. The human capital built around Microsoft products for provision of support services has been very significant with the humour being that Microsoft greatly contributed to employment statistics due to the requirements for human resource support on its operating systems (re-installations, configuration, networking, troubleshooting etc)
I would credit Bill Gates leadership while at the helm of Microsoft, in leading a very successful software products company that was able to package and market its software, setup a distribution network, channel partners, resellers and Independent Software Vendors (ISV) that made the software products available and supportable to the extent that a huge customer base of users in Kenya was developed. Microsofts&nbsp; system of awards and status recognition with the&nbsp;prestige associated with a company being labelled Microsoft Gold Certified partner fueled competition that added to both the loyalty and growth in usage of computer systems in Kenya as partners strived to reach their targets.
This is an interesting debate Anthony that I can liken to Aesops Fable of the 9 blind men who had a dispute as to what exactly an elephant was. Each one of them went and touched a different part of it and defined the elephant from that context - one touched the tail and likened it to a rope, another touched its trunk and likened it to a tree, another touched its side and likened it to a wall, yet another touched its tusks and likened it to a set of spears....
My perspective is greatly influenced by my particular experience and environment and the choice of Bill Gates stems from having being a part of the large marketing machine that was Microsoft as it set its footprint in influencing the use of computer systems in Kenya
Joseph
	              
	              Hi Tanisha,

I actually agree with you. I guess it really does depend on your own experiences however. For example to yourself and myself this makes sense to not have such a diverse understanding because in my field we try to have more expertise in individual areas but work together in larger teams on projects, whereas in other countries and companies they may take a holistic view to have less specialised people but with far wider and greater overall full lifecycle understanding of IT.
Neither is incorrect however, it really is just depends on the overall management and company policy on how they want to run their IT / Technical departments.
	              
	              Hi Anthony
Wow, this is a brilliant question – If I had to choose one solitary person who has had the most significant influence on the development and/or use of computer systems, I would nominate Sir Tim Berners Lee.
Sure, Bill Gates et al brought computing to the mainstream, for both business and for the consumer, and according to their website, 1.5 billion people use Windows everyday (Microsoft, 2014). Microsoft products play such a huge part in the way our computers work, from a technical and connectivity perspective, through to software solutions for the enterprise and for home.
I believe Steve Jobs took this to the next level, with Apple exploiting their software capabilities and developing amazing gadgets and products such as the iPod, the iPad and the iPhone, and according to an article by Reisinger and Tibken (2014) they referenced that Apple had sold over 800 million IOS devices: over 100 million iPod touch units, 200 million iPad units, and iPhone units in excess of 500 million (c|net, 2013). And these figures don’t include Mac!
Google have also been phenomenal. In addition to their Internet search engine ‘Google’, which according to Netmarketshare (2014) they have almost 68% market share, they have also developed the Android o/s for mobiles, and according to the article by Ingraham (2013), Google announced last year that they had reached 900 million Android activations (The Verge, 2013).
In terms of social impact, Mark Zuckerberg has linked so many people around the world with Facebook, the social media and communications platform, and according to the Wikipedia page for Facebook, that they have almost 1.3 billion users active each month (Wikipedia, 2014).
I also agree with one of the earlier discussion points were I think it was Joseph who touched on miniaturization and transistors. These have clearly played a major part in the development of computers and semiconductor chips.
However, notwithstanding all these great examples and probably many more that I have omitted, my vote goes to Sir Tim Berners Lee who invented the World Wide Web and ultimately, created the platform for 'connectivity' for all of us. This has had a profound impact on society across the world and coincidentally, in some way or another, brings together each of the other candidates that I have referenced. As reported in an article by the culture-ist (2013), an Infographic produced by Gator Crossing (2013) showed that there were 2.4 billion users of the Internet last year. This is about one third of the world’s population, with numbers suggesting there is just over 7.2 billion people in the world (Worldometers Info, 2014).
Best Wishes, Craig
References
Microsoft. 2014. Microsoft by the Numbers [online]. Microsoft.com. Available from: http://www.microsoft.com/en-us/news/bythenumbers/index.html [Accessed 9 September 2014]
Reisinger, D., Tibken, S., 2014. Apple by the numbers: 80M Macs, 40M Mavericks downloads [online]. c|net. Available from: http://www.cnet.com/uk/news/apple-by-the-numbers-80-million-macs-40-million-mavericks-downloads/ [Accessed 9 September 2014]
Netmarketshare. 2014. Desktop Search Engine Market Share [online]. Available from: http://www.netmarketshare.com/search-engine-market-share.aspx?qprid=4&amp;qpcustomd=0 [Accessed 9 September 2014]
Ingraham, N., 2013. Apple announces 600 million ios devices sold [online]. The Verge. Available from: http://www.theverge.com/2013/6/10/4415258/apple-announces-600-million-ios-devices-sold [Accessed 9 September 2014]
Wikipedia. 2014. Google [online]. Available from: http://en.wikipedia.org/wiki/Google [Accessed 9 September 2014]
Gator Crossing. 2013. More Than 2 Billion People Use the Internet, Here’s What They’re Up To (INFOGRAPHIC) [online]. The culture-ist. Available from: http://www.thecultureist.com/2013/05/09/how-many-people-use-the-internet-more-than-2-billion-infographic/ [Accessed 9 September 2014]
Worldometers. 2014. Current World Population [online]. Available from:&nbsp;http://www.worldometers.info/world-population/ [Accessed 9 September 2014]
	              
	              

Re-post&nbsp;
I have read a few of the posts as it relates to a Software Engineer needing to know various aspects of hardware as well in terms of networking and so forth. But my question is, is this really necessary?
I am from a hardware background, although I have programming experience from my time at University, the majority of my work experience is that of hardware. Recently I decided to switch things up a bit and go into the programming field. I have been in this position for three months now and I was required to resolve queries, do minor code edits, reports and so forth. I have not engaged in any major software development thus far; however I don’t really see the need for hardware knowledge. Maybe it would be dependent on what it is that you are actually developing and exactly what hardware knowledge is needed for it. I recall as a Network Administrator, when dealing with a few of the programmers, when asked to fix certain problems I always wondered about their knowledge base and I may have been a bit critical of them, being that I felt they should have been able to resolve their own problem. &nbsp;

I believe as Information Technology Professionals we must be able to analyze situations and think critically. If the software we are trying to develop requires background knowledge on the OSI Model, how about we google it. Put in the extra effort and try to learn whatever that may be needed at the time. Additionally when you are leaving a job the next company is not going to have the exact systems that the previous company did and such you will have to learn their systems, you may not have Linux experience but you now have to learn it.

Thus if the situation necessitates it, then we learn it, unless it is something we wanted to learn for our own gain and development. &nbsp;For me too much knowledge can be overwhelming at times. We are in a diverse field and there is so much out there to know and learn. The Software Engineer already has enough on his plate, I am sure. I could be wrong.&nbsp; What’s you view opinion on this take of the topic?

References:
Qualifications for a Computer Software Engineer. Availble at:
http://work.chron.com/qualifications-computer-software-engineer-3087.html
Job profiles, Software developer. Availble at:
https://nationalcareersservice.direct.gov.uk/advice/planning/jobprofiles/Pages/softwaredeveloper.aspx

Become a Computer Software Engineer: Education and Career Roadmap. Availble at:
http://education-portal.com/articles/Become_a_Computer_Software_Engineer_Education_and_Career_Roadmap.html

	              
	              Hi Tanisha,
In a perfect world that all abstractions are so perfect, you don't need to know anything about hardware as an software engineer, because the fact that it's the hardware makes the software work has been abstracted.
Unfortunately, in a real world, especially when it comes to software, the abstractions are always "leaky" (Spolsky, 2002).
Take one example, I have a 2d array of integers in C, a[1000000][1000000], and I need to sum them all up. What's the difference between the results of the following 2 solutions? (they iterate through the array in different orders):

for (i=0; i&lt; 1000000; i++)&nbsp; &nbsp; for (j=0; j&lt; 1000000; j++)&nbsp; &nbsp; &nbsp; &nbsp; sum += a[i][j];
for (i=0; i&lt; 1000000; i++)&nbsp; &nbsp; for (j=0; j&lt; 1000000; j++)&nbsp; &nbsp; &nbsp; &nbsp; sum += a[j][i];
If you know hardware, then you know the 2nd solution will always miss the cache in the CPU and have to read from memory therefore is very slow. Just an example.
Conclusion, if you only know how to drive a car, you cannot be a professional racer.
References
Joel Spolsky, 2002. The Law of Leaky Abstractions&nbsp;Available http://www.joelonsoftware.com/articles/LeakyAbstractions.html, [Accessed 10 September 2014]
	              
	              Tanisha, I like the section from the site that you have referenced that talks about Other Qualifications of a Computer Software Engineer. What I would say is that the quest of whether or not to know hardware stuff would really depend on the area of application that one is focused on.
The area of focus for me for instance has largely been on end user business applications. There has been a huge growth in the level of software development tools over the years and subsequent reduction on the need to know and understand lower level hardware interactions. I recall starting off in the 90s programming in C &amp; C++ which required me to understand a lower level of detail and undertake memory allocation routines for mundane tasks such as creating a user interface for a generalised application. With the growth and development of Rapid Application Development Tools (RAD), a lot of low level tasks have been abstracted from the user such that for a person developing a general business application, does not necessarily need to delve deep into the low level hardware interactions as that aspect has already been taken care of. This then makes it possible for a software engineer to focus on the business logic of the application that they are creating.
However, knowledge of hardware and networking is useful when it comes to deploying your application for purposes of performance optimization and configuration and understanding of how to leverage the hardware resources available. The platform the application is built for also requires some knowledge of the operating environment and this applies to mobile operating systems as well. So to me it just depends on the nature of the task at hand be it a general business application, an integrator software, mobile app or the harder stuff that Terry Yin is likely into.
Joseph
	              
	              Thanks Craig for your elaborate response and supporting your answer with numbers. This was the missing piece for me as I was wondering where to get supporting data for my earlier response.
I'd imagine The issue of measurement becomes a factor in answering this question. I would lean on your choice of&nbsp;Sir Tim Berners Lee on having influenced the development end of things. However, in terms of use of computer systems as I understand it, I would still go with Bill Gates who as you have alluded to in your response brought computing to the mainstream. And here I am looking at aspects such as the number of people impacted; typically an end user whose first interaction with a piece of computer hardware, was through the interface developed by Microsoft through its various flavours of operating systems and office productivity software. One would still need to access this O/S layer to get to the world wide web. I recall one of my early computer instructors trying to define an operating system in a laymans language, as a piece of software that allows a user to talk to the computer.
I like your analysis though.
Joseph
References :
Measuring Progress:&nbsp;http://www.gatesfoundation.org/Who-We-Are/Resources-and-Media/Annual-Letters-List/Annual-Letter-2013
 
Measuring Human Progress - Special contribution By Bill Gates on Page 47 of The 2014 Human Development Report - the latest in the series of global Human Development Reports published by UNDP - http://hdr.undp.org/en/2014-report/download
 

&nbsp;



	              
	              Thanks Joseph
I believe the issue here is that I'm not sure any of us are genuinely able to identify a single solitary person - the question from Anthony is such a difficult one and it probably comes down to our individual opinions, and perhaps based on our own experiences through history relative to computing.
I wouldn't be be disappointed if Bill, or any of the other examples I alluded to, gets the most votes. He has certainly made a huge impact (and the philanthropic work that he and his wife Melinda are doing with the Gates Foundation is also truely incredible).
My vote is still with Sir Tim though :-)&nbsp;
Best Wishes
Craig
	              
	              Hi Joseph,
I agree that it depends on the area of application that one is focused on. I don't agree the growth in the level of software development tools actually reduced the need to know and understand lower level abstraction (not necessarily hardware though).
Rapid Application Development has been more of some software development methodologies rather than tools. The methodologies and some similar approaches later to it like Agile methods have improved the software development cycles. (wikipedia, RAD)
But the RAD tools (in my personal opinion) has been a total failure. I personally have never seen any of them helped software projects either in the design or the process. Some 15 years ago, a lot of the large companies thought RAD tool like Model Driven Design tools would help them, so a lot of them adopted tools like IBM Rational Rose. Now I work with large companies to help them get rid of those tools, reimplement the software using C/C++. Drawing some high level diagrams and letting the tools to generate the software sounded great, but the cost of ignoring the low level abstraction was too high. The search result for "rational rose" in google trends shows a perfect decending curve to almost 0:&nbsp;http://www.google.com/trends/explore#q=rational%20rose
Conclusion, I think abstraction is a perfect tool (and probably the only tool) for communicating complicated ideas, and that's all it does for us. Abstraction doesn't solve any problem. The solution reside in all the abstraction levels. It's not a tool to make the scope of learning any smaller.
br, Terry
	              
	              Hello Class,As a way of rounding up the week's activity, please provide a short summary of the key ' Computer Structures ' lessons learnt in week 1, from your perspective.Anthony
	              
	              During my history of using computer, Windows XP is my most favorite OS ; until today, I still think XP is the greatest OS of windows family, although windows 7 was overtook it being the most widely used operating system in August 2012
Windows XP is a PC OS produced by Microsoft as session of the Windows NT family of OSs. The OS was generally released for retail sale on October 2001. It was a major advance from the MS-DOS based versions of Windows in security, stability and efficiency due to its use of Windows NT underpinnings. It introduced a significantly redesigned graphical user interface and was the first version of Windows to use product activation in an effort to reduce software piracy.
Window XP received generally positive reviews with critics noting increased performance, a more intuitive user interface, improved hardware support, and its expanded multimedia capabilities. Despite of poor initial public reception, often centered on driver support and security restrictions, it eventually proved to be popular and widely used. It was estimated that at least 400 million copies of Windows XP were sold globally within its first five years of availability, and at least one billion copies were sold overall by April 2014.
Windows XP remained popular even after the release of newer versions, particularly due to the poorly received release of its successor Windows Vista. According to web analytics data generated by Net Applications, Windows XP was the most widely used operating system until August 2012; Net Applications reported a market share of 29.23% for XP. According to Net Applications, Windows XP market share is of 25.27%, as of May 2014
References
&nbsp;
Matt Lake (2001); Windows XP; Available on: (http://web.archive.org/web/20011119231138/http://home.cnet.com/software/0-6688749-8-7007240-2.html) (Accessed on: 10-Sep-2014)
&nbsp;
marketshare.hitslink.com (2014); Desktop Top Operating System Share Trend; Available on: (http://marketshare.hitslink.com/operating-system-market-share.aspx?qprid=11&amp;qpcustomb=0) (Accessed on: 10-Sep-2014)
&nbsp;
Paul Thurrott (2010); The Road to Gold: The development of Windows XP Reviewed; Available on: (http://winsupersite.com/article/product-review/the-road-to-gold-the-development-of-windows-xp-reviewed) (Accessed on: 10-Sep-2014)
&nbsp;
Sebastian Anthony (2014); Windows XP finally put to sleep by Microsoft – but it will still haunt us for years to come; Available on: (http://www.extremetech.com/computing/180062-windows-xp-finally-put-to-sleep-by-microsoft-but-it-will-still-haunt-us-for-years-to-come) (Accessed on: 10-Sep-2014)
&nbsp;

	              
	              Hello Class,
Which areas of computing do you expect will have the greatest growth in the next decade, and what do you predict will be the most significand developments with computer systems, over this time-span?
Anthony
	              
	              Hi Anthony
A brilliant but difficult question …not so easy to give the crown to just one but if I had to choose one solitary person who has had the most significant influence on the development and/or use of computer systems I’d give it up for Sir Timothy John "Tim" Berners-Lee who invented the World Wide Web.
For making the Web a tool for communication, with the Web, you can find out what other people mean. You can find out where they are coming from. The Web can help people understand each other. Think about most of the bad things that have happened between people in your life. Maybe most of them come down to one person not understanding another. Even wars
I believe we can use the web to create neat new exciting things. And also use the Web to help people understand each other more.
	              
	              Hi Christopher,
Nice to know that you agree with me. Indeed they are places that want persons to have a diverse understanding in the Information Technology (IT) field. And there are quite a few right here in my country. As you have noted, it is how the companies want to manage their Information Technology (IT) department. I believe in these cases organisations are more or less trying to get the most out of one person and are usually trying to cut cost. However, if they do decide to hire a specialist they can get more in terms of productivity; because one person will be able to focus on what is required of him and do that more effectively. Instead of having one person, who has to learn other concept he may not be familiar with, which may be time consuming as well. Additionally he may not be able to focus well on the projects assigned to him. In my country, instead of hiring a Network Administrator, an Analyst Programmer and or an IT Manager, an organisation may decide to hire one person and expect that person to be able to fix the website when it crashes, network the building, manage servers and manage the IT department. 
For certain IT functions, depending on the size of the organization; you may not need certain specialist. For instance, how often will they be required to network the entire building, or how often will the website crash. At such times you don't want to hire several individuals who may actually be working seasonally. Thus, it is often an excellent idea to have someone as a consultant for such cases.
The Educational institutions often do not cater for that level of diversity. If you are doing Software Engineering you do course related to that, if you are doing Network Engineering you do courses related to that. Often what you will see get tossed in these areas are courses relating to General Management, Project Management and&nbsp; or English, which are areas you will need to know. Maybe it’s because I am from an underdeveloped country, I see this happening often.
Software Engineering Msc. Available at:
http://www.liv.ac.uk/study/online/programmes/information-technology/msc-in-software-engineering/module-details/
Advanced Software Engineering. Available at:
http://www.kcl.ac.uk/prospectus/graduate/advanced-software-engineering/structure
Courses - Master of Software Engineering
http://www.worldcampus.psu.edu/degrees-and-certificates/software-engineering-masters/courses  
	              
	              Hello Dr Ayoola,
Although it’s very difficult question to answer, I personally think that Philip Donald Estridg had the most significant influence on the development and use of computer systems.
My reasoning for this is that he was the lead of the team that build the IBM PC. The Personal Computer was the real game changer: it opened the door of commercially low cost viable computers, de facto democratizing computing to the whole planet. Mr Estridge and team realized that in order to be successful the system needed to be using third-party hardware and software, with the exception of the BIOS[1]. That’s the same principle that we still use today in the computer systems world.
No matter how other people influenced the computer industry, with the exception of perhaps only Steve Jobs, they all started from the IBM PC basis. If Mr Estridge, would have not created the PC, Microsoft wouldn’t have been able to sell DOS (they did not code the first version of it, they acquired it[2]) and become the company that it is today.


[1] Koenig J. (2011 The History Of The IBM Personal Computer. Available at: http://computemagazine.com/the-history-of-the-ibm-personal-computer/ (Accessed: 10 September 10 2014)


[2] Hunter D. (1983) The Root of DOS. Available at http://www.patersontech.com/dos/softalk.aspx (Accessed: 10 September 2014)



	              
	              Hi Dr. Ayoola,
For me the greatest growth in the next decade will come from the internet connected items. However, I don’t think the greatest developments will come from that industry, but rather will be in the branches of bioinformatics and 3D bioprinting. I believe that the long term growth and big changes in computing will come from those fields, especially bioprinting (Thomas 2014).
&nbsp;
Reference List
Thomas D. (2014) Engineering Ourselves – The Future Potential Power of 3D-Bioprinting? Available at: http://www.engineering.com/3DPrinting/3DPrintingArticles/ArticleID/7379/Engineering-Ourselves-The-Future-Potential-Power-of-3D-Bioprinting.aspx (Accessed 10 September 2014)

	              
	              I am working with computers from long time and no doubt its very powerful machine but its beginning was very simple. It is amazing how simple things and ideas evolve into powerful objects as time passes. What I have learned during the first week other than the history of computing devices is that never hesitate to take steps towards implementing your thought and ideas because what you think impossible now might be the integral part of future life.

	              
	              Hi Dr. Ayoola,
For me the key lesson is that looking back at the past 60 years, we have advanced so much. Although I lived through some of those years of transformation, it’s difficult to grasp the reality of that change while you are riding it. Often in occasions like these, when we stop to learn and look back, is when we realize the magnitude of these changes and advances; not only technologically speaking but as whole species. The abacus might seem like a simple tool, but for many centuries it was one of the key tools used in economy and trades. To think that it took less than 100 years to revolutionize all that, it’s simply amazing.
Perhaps the best testament to this change it’s this very online learning: the fact that there are people from all over the world here, interacting and discussing, it’s not simply amazing but one of the critical success factors of our civilization today. Imagine what would happen if Mozi[1] would have been able to attend the same online University that Socrates was attending? Perhaps the world would have been completely different. The interactions that today’s technology allow us, will resonate through the next centuries, and perhaps, with a little bit of luck one result will be a true civilization changer.
&nbsp;


[1] ‘Mozi’ (2014) Available at: http://en.wikipedia.org/wiki/Mozi (Accessed: 10 September 2014)



	              
	              What I felt around is people are now looking forward to have devices which take input/instructions by natural means more efficiently. They want machines to react like humans. I believe in next decade there will be more improvement in the synchronization between humans and machines. Artificial Intelligence will be improved and we might have next generation of robots.
IBM has introduced Watson which is an artificially intelligent computer system capable of answering questions posed in natural language,[2] developed in IBM's DeepQA project by a research team led by principal investigator David Ferrucci. Watson was named after IBM's first CEO and industrialist Thomas J. Watson.[3][4] The computer system was specifically developed to answer questions on the quiz show Jeopardy![5] In 2011, Watson competed on Jeopardy! against former winners Brad Rutter and Ken Jennings.[3][6] Watson received the first place prize of $1 million.[7]
References:
•&nbsp; IBM Watson: The Face of Watson on YouTube 
•&nbsp; "DeepQA Project: FAQ". IBM. Retrieved 2011-02-11. 
•&nbsp; Hale, Mike (2011-02-08). "Actors and Their Roles for $300, HAL? HAL!". The New York Times. Retrieved 2011-02-11. 
•&nbsp; "The DeepQA Project". Research.ibm.com. Retrieved 2011-02-18. 
•&nbsp; "IBM Research: Dave Ferrucci at Computer History Museum - How It All Began and What's Next". IBM Research. 2011-12-01. Retrieved 2012-02-11. "In 2007, when IBM executive Charles Lickel challenged Dave and his team to revolutionize Deep QA and put an IBM computer against Jeopardy!'s human champions, he was off to the races." 
•&nbsp; Loftus, Jack (2009-04-26). "IBM Prepping 'Watson' Computer to Compete on Jeopardy!". Gizmodo. Retrieved 2009-04-27. 
•&nbsp; IBM's "Watson" Computing System to Challenge All Time Greatest Jeopardy! Champions, Sony Pictures Television, 2010-12-14, archived from the original on 2013-06-06, retrieved 2013-11-11

	              
	              Hi Anthony,
I will choose two persons instead of one. One of them is already dead (Steve Jobs) and the other one is still in life (Bill Gates). From my point of view this Oscar should be given to this couple. Why?
I think that Craig already provided a lot of reasons (in spite of himself) which explain why both Bill Gates and Steve Jobs should be the solitary person who has had the most significant influence on the development and/or use of computer systems, thank you Craig. 
But if I must add something then I will speak more as a user than as an IT specialist,
They changed the way we think and communicate. 
They revolutionized our lives by inventing a beautiful and intelligent virtual universe. 
They transformed the technology and they made us dive into the era of high-tech. 
They revolutionized design (Computer, Smart phones, tablet, etc)
They brought Computer in the hand of Mister everybody: It is from there that everyone could consider owning a computer. Computers became accessible to all.
If the PC that you and I use every day has become ubiquitous, it is mainly thanks to these men. And I’m sure that with the innovation brings by Steve Jobs and Bill Gates, nothing can ever stop the industry of information technology.
To finish if I’m forced to choose one of them, I will take Steve Jobs, because Steve Jobs more than anyone has wrought modern technology and he revolutionized the world of mobile phone (iPHONE, iPAD,etc). I’m one of men who think that the future of the computer is the smart phone.
Tresor
	              
	              Hello Anthony, &nbsp;  I would like to nominate Late Mr. Dennis MacAlistair Ritchie, the Computer Scientist, who created the C language. According to me, it is the best, most scalable programming language till date. Most of the software products even today, are written using C. The most interesting highlight is the Node.JS compiler, which can process JavaScript on a server side, is developed using C.    Reference : Wikipedia : http://en.wikipedia.org/wiki/Dennis_Ritchie Thanks, Kharavela
	              
	              Lessons learnt

I now have a greater appreciation for the work that went into the development of computing, the many trial and errors, expertise that came together before finallly having some success. The videos, lecture notes and research have outlined a defined path from the creation of the Eniac to todays coin sized microprocessors and the capability palm sized devices. The 21st century is built on these advacements in computing with microprocessors at the heart of it. The changes in the size and application of devices used for computing is testament to success and advacement in the technology.
The discussion board was really interesting, I realise that everyone brings a different perspective, even those with similar background. Each persons exposure has lead them to computing in a different way. For some of us we didn't realise our passion for computing unitl after the 2nd or 3rd exposure, 'not picking on you Dr. Anthony', but once that light bulb came on, we wanted to learn everything about computers. I learnt alot by reading the discussions post&nbsp;and following the threads on some of the questions raised.
I look forward continuing the history in the coming week.

R.Biggs

	              
	              Hi Anthony,

What I have learned in week 1 around 'Computer Structures':

Simple ideas once practically focused upon can become reality.
You will break many eggs before you make the perfect ommlette. By that I mean in responce to the video learning resources during week one where the funny comments were illustrated by George Dyson[1] "No use went home, "Damn it I can be just as stubborn as this thing!". In the persute of the creation of the first nuclear bomb 'Mike' and the computer mathemetics they were running.
Ada Lovelace's hypothesis on the use of computers back in the1800's naming her the first computer programmer led to rethinking the potential uses for such devices other then simple computation [2]. Often a curious free thinking mind can open up the prospects of advancement such as this.
While technology has developed and advanced significantly over the last 20 years or so, the broader greater use to mankind is still to be discovered in my mind.
And the we must be very carefull with what we do in the future with such power at our fingertips. "Computer technology has altered the ability of governments to exert control; had enormous impact on global economics;"[3]


References
[1] Dyson, G. (2003) The birth of the computer [Online]. Available from:&nbsp; http://www.ted.com/talks/george_dyson_at_the_birth_of_the_computer.html, (Accessed: 2 July 2014).
[2] Laureate Education, (2014) Participating in the Global Classroom: Week 1 Lecture Notes [Video, Online], (accessed: 11/06/14).
[3] Brookshear, J.G. (2011) Computer Science: An overview. 11th ed. Boston: Pearson Education / Addison-Wesley.

	              
	              
Hi Anthony,

I really interesting question.

Having looked about and thought about this I'd list you 10 very important people initially [1] :
1) Charles Babbage
2) Claude Shannon
3) Alan Turing
4) John Von Neumann
5) William Shockley
6) Douglas Englebart
7) Robert Noyce
8) Steve Wozniak
9) Grace Murray Hopper
10) Vint Cerf
But out of all of those the one I'd choose as my number 1 would be William Shockley [2]. And the simple reason is that for me without doubt the invention of the transistor for which he was awarded the Nobel Prize.

References:
[1] http://www.ranker.com/list/10-most-influential-people-in-the-history-of-computers/garciagadgets
[2] http://en.wikipedia.org/wiki/William_Shockley




	              
	              To my opinion the next decade we will be about further expansion of the cloud. This will not be a mere repetition of the last 15 years of third party services running in gateways accessible by thin clients over the internet. There will be growth of companies using private clouds by renting either server space in well connected server parks or by having their own stack close to their operation. Services on these private clouds can then be up scaled using public clouds creating hybrid environments.
&nbsp;At the same time the reach of the cloud will expand to the home environment. The NAS is becoming popular as storage of docs, music and video which can be accessed using different devices from computer, laptops, tablets, smart phones but also TV's on the one end but also function as a private cloud or combined with public cloud services. Another example is Comcast who is developing new type of set-top box for their TV-services which is nothing more than a generic server. Using NFV they can process much of the interaction in the cloud and extend their cloud deployment to this set-top box. These are just a few examples of generic cloud hardware that can be used on location as extension for cloud based solutions .
The cloud is always on and easy scalability is its main feature. Decentralised processing models can be used as optimisation of traffic flows and the use of generic hardware also opens the door for easy remote maintenance solutions in a multi device environment that is accessible anywhere anytime. Key factors for growth in the next era.

	              
	              I would have to say Bill Gates. Sure apple and IBM came into the marketplace with personal computers long before the Windows era. But before Windows, computers and their usage was seen has highly technical and geek only comprehendable, the command line of MS-DOS was like the holy grail and only the privilidge few could use it.&nbsp;But then came Windows with a graphical interface that was easy to navigate and fun to use. Bill Gates revolutionized computing and made it more interesting and usable, this is quite an achievement.
As it relates to influence microsoft has been dominant force in computing taking the industry to newer levels advancements and competitiveness. The standard set by Microsoft and the continuous development makes it the most visible software developer. Microsoft market share has allowed people to believe that a computer is Windows and there is no other operating systems, and that speaks to how wide spread Microsoft family of products are.&nbsp;
Yes there are many others that have contributed significantly to computing, but I would nominate Bill Gates and Windows as the most significant and the competitive catalyst for developments in computer systems.
	              
	              
My parent planned to take me to my aunt during winter holidays back in 1984. I can hardly remember many details of these vacations but I could recall clearly what was my first encounter with a gaming device by Nintendo. I was very excited to see and use that but didn't know much about it in terms how that was manufactured what exactly it was, other than knowing that it is some game.



Journey continued, got introduced to Atari, sega and started going to arcade center but still didn't know what these have relations with computer.



First time I heard about something computer was when in 1995 my school started computer lab. Since I was sports freak and preferred playing outside, didn't goto lab. This was the same time when I started playing Playstation, especially TEKKEN which was first 3D game.



After completing my 12th standard I was checking my options for further studies, and to be honest I wasn't good at chemistry so engineering wasn't an option so was medicine as I was looking for degree that didn't include chemistry as a subject. No offense to chemistry lovers.



My father prior to enrolling me in a college for a computer science degree asked me to take some computer courses to get upto speed. That was in late 1997, when I really encountered computer for the first time. Took some crash courses about software and hardware to lay the foundation.



I had the opportunity to have worked on VAX mainframe to do PASCAL programming as first semester subject in Jan 1998. Assignments were submitted using dot matrix printer. During this time technology has already started evolving rapidly. Especially, mobile devices, where competition was picking up and new models were making to the markets very quickly.



Few years later smart phones came to the scene and I remember clearly, I was getting my hair done. My first PDA slipped out of my pocket and fell on the floor and its LCD cracked like an egg, that was sad day for me. For next few years I stayed away from any kind of smartphone/PDAs and relied on regular mobile phones on my job as well.



In 2006 for the first time I started using BlackBerry, which was considered as a phone made for business people. Communication was made easy by BB at the cost 24/7 availability.



In late 2007, new breed of the phone took over the smart phone market and status quo remained same in mobile industry for few years. During these years, many companies tried to compete with iPhones, worthy of mentioning was HTC. It was only few years ago iPhone started feeling threatened by a competitor when Google and Samsung also joined the race of smartpones.



During these years, tablet was another device that was also transforming rapidly, in terms size weight ratio, features and pricing etc..



Now both smart-phones and tablet are not very prevalent but also very relevant in today's life not only at offices but also at home. Today I can easily connect to office from anywhere while on the go using my smart-phone, never imagined this couples of years ago.



Answers:

1 – Based on the hints in the story, I think it was during 1973/74 when England toured West Indies and that was also the time for th mentioned song. http://www.espncricinfo.com/magazine/content/story/388241.html &amp; http://en.wikipedia.org/wiki/Daydreamer_%28David_Cassidy_song%29

2 – It was UNIVAC I: http://www.thocp.net/hardware/mainframe.htm

addition time was 525 microseconds: http://en.wikipedia.org/wiki/UNIVAC_I#Instructions_and_data

If it is compared with iPhone 5 (525/1000000) / (1/(1.3 * 1000000000)/2) = 1,365,000 times faster



3 – ANITA by Britain but if it has to be after story then I guess it is HP http://www.thecalculatorsite.com/articles/units/history-of-the-calculator.php

4 – ForTran &amp; Cobol (1950) http://cgi.csc.liv.ac.uk/~ped/teachadmin/histsci/htmlform/lect6.html


	              
	              Another good question Anthony and one in which I can only hope to scratch the surface on. 
This week, through our course material, we have talked about computers being ubiquitous. However, for me this is only the beginning of our “ubiquijourney” – I know.., I’ve just made up that word, but I bet you know what I mean! 
We are cognizant of much of the technology that is around us, we log on, we turn it on, we activate something and in some cases, there is even automatic activation. I sense that our ‘ubiquijourney’ will take us to a time, perhaps in 10 years, when technology will just be there, we won’t even think twice about it, it’ll just be something that is implicit and intuitive in the way we live our lives. 
In terms of specifics, and innovation and invention, we will see tremendous exploitation of cloud-based services, for businesses and for consumers. Data Centers will continue to exist for service providers but will become more obsolete within large corporations, as cloud-based services are adopted for their technology needs. Also in relation to Data Centres, they will become less reliant on energy, but the energy they do use will be renewable and solar powered energies, as we become less reliant on man-made and the natural in-the-ground resources. Keeping on the power theme, I think we will also see more significant shifts towards a ‘cable-less powered’ computing world. 
How long have we been talking about a ‘paperless office’? I will edge my bets to suggest that this era of ‘digitalisation’ will eventually take the ‘paperless office’ to new heights. Everything we do will be digitalised, either partially or fully, depending on what it is. 
Collaboration with colleagues and customers (and of course with friends and family) will continue to build and when there is a need to do something, the interaction will simply be through the tap of a screen. 
I believe we are moving to a time when we will no longer have the need for cash and coins, everything will be done electronically, and we have already seen new payment systems and capabilities enter our lives that do not need plastic or cash. Of course, the downside is that you still need to have credit in your account! 
In terms of computing, I wonder whether the traditional business user will continue to need a desktop or laptop computer, or will tablets and mobile phones (or new Apple watches!) completely replace those needs, especially following the ever increasing development in human interface capability. 
Things will continue to get faster and smaller, and the likes of miniaturisation will be further extended to nanotechnologies. If we use Moore’s law as a guide, it suggests that the transistors used on a chip doubles every couple of years or so. Building on this theme of nanotechnology, the original transistor that was built by Bell Labs was pieced together by hand. In comparison, more than 100 million 22nm tri-gate transistors could now fit onto the head of a pin, which is 1.5mm in diameter. Intel’s quad-core processor contains 1.48 billion transistors (Intel, 2012). 
So, relative to your question, very much life changing developments. These are massive numbers and massive shifts in sizes. It is suggested by Tully (2014) who is the chief of research for semi-conductors at Gartner, that chips will be assembled using individual atoms or molecules in the future. 
I seriously cannot begin to think about what this could look like in 10 years time but I do believe that this will mean computing technology will be so much more ubiquitous than it is right now.
Whether we will see ‘teleportation’ is another question altogether but I welcome everyone’s optimism!
Best Wishes, Craig
References
Intel. 2012. Moore’s Law: Fun Facts [online]. Intel.com. Available from: http://www.microsoft.com/en-us/news/bythenumbers/index.html [Accessed 10 September 2014]
High Existence. 2014. 10 ways the next 10 years are going to be mind blowing [online]. Available from: http://www.highexistence.com/10-ways-the-next-10-years-are-going-to-be-mind-blowing/ [Accessed 10 September 2014]
	              
	              Hi Anthony
I’m having a hard time deciding on “one solitary person”. I believe, that everything is connected somehow, that every important invention has been inspired and catalysed by another. Therefor I could argue that it would be e.g.:

Tim Berners-Lee who invented the internet/www, making it possible for us all to connect to each other etc.
The creator of the IBM PC for making the computer accessible for “everybody”.
Bill Gates (et al) who created the operating systems MS-DOS and Windows for making it possible for the PC to be “a universal tool” (Progressive Grocer, may 99)
Steve Jobs who really started the gadget era with e.g. iPad’s making it much easier to access internet, listen to music etc.

And the list is long, both before and after the above mentioned.
But if I have to choose “one solitary person”, I will have to say Bill Gates. The reason being that he (with good help from friends and partners), as I see it, has had the biggest influence on how we use computers and systems both commercially and in privately, starting with MS-DOS on the IBM PC’s (www.computerhistory.org)
Further I will argue for this with these two cites:

&nbsp;“1975 Bill Gates and partner found Microsoft. While IBM started developing PCs in the '70s, it was Microsoft that made the computer a universal tool.” (Progressive Grocer, may99)
&nbsp;“Bill built the first software company in the industry,…before anybody… new what a software company was” (Steve Jobs, PBH2, 2007)

Best regards, Bo W. Mogensen


Reference list
Progressive Grocer. May99, Vol. 78 Issue 5, p24. 13p. 22 Color Photographs, 23 Black and White Photographs.
PBH2, (2007) “The historic Steve Jobs &amp; Bill Gates discussion”. [Online video] available at: &nbsp;http://www.pbh2.com/entertainment/people/the-historic-steve-jobs-bill-gates-discussion/ (Accessed 2014-09-10)
Timeline of Computer History. 1981. [Online] http://www.computerhistory.org/timeline/?category=cmptr. (Accessed 2014-09-10)
&nbsp;WWW, inventor Tim Berners-Lee, [Online], &nbsp;http://sixrevisions.com/resources/the-history-of-the-internet-in-a-nutshell/ (Accessed 2014-09-10)
	              
	              Hi Anthony
Aside from the panic earlier in the week with the thought of needing to do complex computational mathematics, I have found this first week to be really interesting, particularly in meeting new colleagues from across the world, who have forced me to think much more critically (and creatively) with the various discussions that we have had. This has really tested my intellect and encouraged me to research some topics that I probably wouldn’t have thought about previously.
In relation to the key ‘Computer Structures’ lessons learnt, this has been a good starting point for me personally, in that I have learnt about the history and the workings of a computer. 
It has been good to view the history in a series of generations and the different technologies used during each generation, such as vacuum tubes and transistors, and the way they act as switches to enable the computer to execute commands in binary. 
What isn’t actually clear to me at the moment is why there are so many vacuum tubes or transistors needed, if the outcome is only going to be 0 or 1. I am presuming that this is because you need to use a different vacuum tube or transistor for every single outcome. This is something I need to do more reading on, and whilst I don’t necessarily want to learn the detail behind binary, digits and bits, I do want to have a better appreciation and understanding of the logic behind ‘how’ it works.
Also, I particularly enjoyed watching the ‘Dyson’ video and the need for perseverance, especially seeing the comments that the programmers made when dealing with the issues and failures. There were a couple of comments that I thought were quite funny, along the lines of;&nbsp; ‘the problem only occurs when the machine is running’ and ‘damn it, I can be just as stubborn as this thing’. It reminded me of my early career in IT Operations, probably during 1989/1990, when the programmers used to issue witty failure messages for the batch runs, such as ‘this job failed miserably!’ and ‘you need to run this poor code again’ :-) 
Looking forward to next week!
Best Wishes, Craig
References
Laureate Education. 2014. Participating in the Global Classroom: Week 1 Lecture Notes [Video, Online]. [Accessed 5 September 2014]
Dyson, G. (2003). The birth of the computer [online]. Available from: http://www.ted.com/talks/george_dyson_at_the_birth_of_the_computer [Accessed 5 September 2014]
	              
	              Hello Anthony, &nbsp;
I think in the next decade the next big thing will be the touch screen technology. &nbsp;This I think will replace the conventional pads.
I think this will change the face of computing completely.
&nbsp;
Regards
Martins

	              
	              Hello Dr. Anthony,&nbsp;
In this week lesson I learnt that great things are borne by little ideas. Your thought can become the evolution of something great. However, I got a refreshing start with the history of computing explaining in detail how technology has evolved over time and is still evolving.&nbsp;
It has taught me to appreciate my ideas more cause &nbsp;a lot of good can come out of it

Regards
Martins
	              
	              
Dr. Anthony,
This week was good, I learned about the history of computers, which although this may have been thought to me in the past at university, it served as a good recap and there was some additional knowledge gained. It is quite interesting how we have evolved over the years from having a computer that was 100 ft long and weighed over 27 tonnes to one that can basically fit into our pockets, millions more processing capability and and may be on an average 5 inches long. The history is quite a fascinating, one. The discussions were intriguing as well lots of information and experiences were shared. And I am enthusiastic to about what I will be learning in the up coming weeks.

References:
Participating in the Global Classroom/Part 1 of the History of Computing, Program Transcript. Available at:
http://cdnfiles.laureate.net/2dett4d/managed/UOL/CKIT/0501/01/UOL_CKIT0501_01_H_EN.pdf

http://www.history.com/shows/modern-marvels/videos/who-invented-the-computer

	              
	              I have a BSc in Computer Engineering with over 30 years of IT expereince. Trufully I did not know some of history of key computer in the Computer Science. I kind-of learnt a lot about the history of computers. And there I was thinking I knew everything. so my lessons learnt. Going back to the basics is always a learning expereince.

Robin Cyrus&nbsp;
	              
	              Joseph,
Thanks for your points shared and indeed it may be dependent on what it is that you are doing, in terms of software development.
Tanisha
	              
	              Dear Dr. Ayoola,
I would say the Internet of Things might have the greatest growth in the next decade.
In this time-span, I would say Parallel Processing will have the most significant developments, if Quantum Computer doesn't come in 10 years.
Terry
	              
	              
Dr. Anthony,
I recall watching a video once that spoke to the future as it relates to touchscreen-technology, since then I thought it was an assume video and as such I believe that it will be the next big thing.Additionlly, I believe that there will be a demand for programmers and the need for software rises. I also believe that there will be a significant growth in Internet use and E-learning.
But on a personal note, I just want to see mankind find a cure for those deadly diseases. That would be something I can really be excited about.
If anyone has not seen the below video before they can have a look at it. Its nice.
Reference:
The future of the touchscreen-technology. Available at:
http://www.youtube.com/watch?v=4mhIl1FBnx0

How to Become a Software Engineer. Available at:
http://www.collegequest.com/how-to-become-a-software-engineer.aspx

	              
	                  Dr. Anthony,&nbsp; My choice of influential person in computing would have to be Steve Jobs. I recall my daughter in the washroom washing, what I believed to be, her hands for about 10 to 15 minutes and when I entered the room, it turned out that it was the iPod that she was really washing. That iPod is still working today, comfortably. The iPod is the only Apple device we have. I personally believe that Apple devices are over priced and do not offer that much level of flexibility.&nbsp; But if I can afford all my devices Mac or Apple I would go out and get it. Because to me it will be worth it. The Apple devices are distinguished they have their own level of individuality and appearance. The software is that way too. The way they set out themselves on the market I believe is totally commendable. And no one else I responsible for that but it founder Steve Jobs. Reference: http://store.apple.com/us      
	              
	              Hi Anthony,
it was really interesting going back thru the history of computeur and computing because we alway have something new to learn. the most think i learnt during this week is the personal experience of everyone with computer,&nbsp;it was very interesting and informative to see how everyone in his corner had a fantastic experience with the computer.
Tresor
	              
	              Hi Anthony, for me the greatest growth in the next decade will come from mobile development and cloud industry.              People are already talking about the era of quantum computing&nbsp;             but I think that it's still a little early.  the next decade it's for modile phone development and cloud computing.   Tresor.                         
	              
	              Hi Robin
I completely agree.&nbsp;
Best Wishes
Craig
	              
	              Hi Anthony
&nbsp;
Meeting colleagues from across the world on this platform that have made me think much more critically with the various discussions we have had has made this week much interesting enough. I’ve gained some experience in the researching some topics I probably wouldn’t have envisioned. Going back to the basics is always a learning experience as I have been reminded of so many in few days from oldest known computing devices abacus to vacuum tubes and punch cards, all the way up to the introduction of integrated circuits, hand-held mobile devices. I have learnt more on why computers run the way they do today, looking back at the history of computers and of computing.
&nbsp;
References
https://elearning.uol.ohecampus.com/bbcswebdav/institution/UKL1/201480_SEPTEMBER/MS_CKIT/CKIT_501/readings/UKL1_CKIT_501_Week01_LectureNotes.pdf 
	              
	              Hi Dr Anthony

Most people in the world have been affected by the advances in computing and mobile technologies. In a short 15 years, the Internet has changed the way we work, shop, communicate, and think. Knowledge, which used to be available only to the elite classes through books such as the Encyclopedia Britannica, is today abundant and free. All of this happened because computing power is growing exponentially. The technology industry knows this growth as Moore’s Law. [1]

The advances are happening not only in computing but also in fields such as genetics, AI, robotics, and medicine. For example, in 2000, scientists at a private company called Celera [2] announced that it had raced ahead of the U.S. government–led international effort decoding the DNA of a human being. Using the latest sequencing technology as well as the data available from the Human Genome Project, Celera scientists had created a working draft of the genome. It took decades and cost billions to reach this milestone.

The price of genome sequencing is dropping at double the rate of Moore’s Law. Today, it is possible to decode your DNA for a few thousand dollars. With the price falling at this rate, a full genome sequence will cost less than $100 within five years. Genome data will readily be available for millions, perhaps billions, of people. We will be able to discover the correlations between disease and DNA and to prescribe personalized medications—tailored to an individual’s DNA. This will create a revolution in medicine.

We can now “write” DNA. Advances in “synthetic biology” [3] are allowing researchers, and even high-school students, to create new organisms and synthetic life forms. Entrepreneurs have developed software tools to “design” and “compile” DNA. There are startups that offer DNA synthesis and assembly as a service. DNA “printing” is priced by the number of base pairs to be assembled (the chemical “bits” that make up a gene). Today’s cost is about 30 cents per base pair, and prices are falling exponentially. Within a few years, it could cost a hundredth of this amount. Eventually, like laser printers, DNA printers will be inexpensive home devices.

It isn’t just DNA that we can print. In an emerging field called digital manufacturing, 3D printers enable the production of physical mechanical devices, medical implants, jewelry, and even clothing. These printers use something like a toothpaste tube of plastic or other material held vertically in an X-Y plotter that squirts out thin layers of tiny dots of material that build up, layer by layer, to produce a 3D replica of the computer-generated design. The cheapest 3D printers, which print rudimentary objects, currently sell for between $500 and $1000. Soon, we will have printers for this price that can print toys and household goods. Within this decade, we will see 3D printers doing the small-scale production of previously labor-intensive crafts and goods. In the next decade, we can expect local manufacture of the majority of goods; 3D printing of buildings and electronics; and the rise of a creative class empowered by digital making.

Nanotechnology is also rapidly advancing. Engineers and scientists are developing many new types of materials such as carbon nanotubes, ceramic-matrix nanocomposites (and their metal-matrix and polymer-matrix equivalents), and new carbon fibers. These new materials enable designers to create products that are stronger, lighter, more energy efficient and more durable than anything that exists today.

There are also major advances happening in Micro-Electro-Mechanical Systems (or MEMS), which make it possible to build inexpensive gyros; accelerometers; and temperature, current/magnetic fields, pressure, chemical, and DNA sensors. Imagine iPhone cases that act like medical assistants and detect disease; smart pills that we swallow and that monitor our internals; and tattooed body sensors that monitor heart, brain, and body activity.  And then there is Artificial Intelligence, which has advanced to the point at which computers can defeat humans on the TV show Jeopardy, perform medical diagnosis, and drive autonomous cars.

I can go on and on and on, but the bottom line is that we are innovating at an unprecedented rate.&nbsp; In this and the next decade, we will begin to make energy and food abundant, inexpensively purify and sanitize water from any source, cure disease, and educate the world’s masses. The best part: it isn’t governments that will lead this charge; it will be the world’s entrepreneurs.


My worry: will humanity evolve fast enough to fulfill its increasing responsibilities?


[1]http://computer.howstuffworks.com/moores-law.htm

[2]http://nar.oxfordjournals.org/content/30/1/129.short

[3]http://singularityhub.com/2012/05/29/inaugural-synthetic-biology-incubator-synbio-launches-at-singularity-university/
	              
	              Thanks for the comments, Numan.Regards,Anthony
	              
	              Thanks for the comments and summary, Augusto.Regards,Anthony
	              
	              Thanks for the comments, Ricardo.
Yes, the forum discussions were quite illuminating.
Regards,Anthony
	              
	              Good point, Tanisha - medical computing, as well as computer-driven biomedical instrumenation are becoming key growth areas. &nbsp;This might help deal with future diseases and pandemics.
Anthony
	              
	              Thanks for the &nbsp;comments, Craig.Yes, the interaction with peers is indeed useful - the different perspectives make the discussions more interesting. &nbsp;Regards,Anthony
	              
	              Thanks for the comments, Martins.Yes, ideas do give progress - I think without a constant source of new ideas there would be little development in computing.Regards,Anthony
	              
	              The module kicked off with a discussion of past historical developments in Computing/IT that had significantly impacted the global community. &nbsp;There were also posts on the impact of trends and new developments, including a consideration of the most influential individuals in computing.&nbsp;
The first week’s discussions and debates have been very lively and informative – keep up the good work!Anthony
	              
	              

Thanks for the succinct comments and summary, Chris.Regards,Anthony



	              
	              

Thanks for the comments, Tanisha.
Yes, I agree with you - the historical aspects of computing are most interesting. &nbsp;One of my colleagues at a university in London became a full Professor as a result of his reasearch and books on the histoy of computing.Regards,Anthony



	              
	              When you speak or write, do you own the words that you create? What about the thoughts that precede those words?
These questions get to the very core of referencing and citation requirements in academic work. In the Western academic world, the presumption is that you do, in some sense, own your words and thoughts; at the very least, you have privileges pertaining to them. The main privilege is to be credited, or cited, for your effort and scholarship.
These notions of authorship and ownership, however, are based on cultural expectations. Some cultures have historically placed emphasis on communal knowledge and showing respect through imitation. Even in the modern university systems, ideas relating to intellectual property have changed over time from emphasising the imitation of a master to generating new ideas while acknowledging past works (Bowden, 1996).
In modern academia, a citation and reference style provides colleagues in a particular discipline with a ‘common language’ for distinguishing new and original contributions to the field from the previous contributions of others. This Liverpool Programme uses a specific citation and reference style, the Harvard (Liverpool) Referencing Style.

References:
Bowden, D. (1996) ‘Coming to terms: Plagiarism,’ English Journal, 85(4), pp. 82-84, ProQuest Central. DOI: 9477482 (Accessed: 9 March 2009).
Turnitin (2012) White Paper: The plagiarism spectrum [Online]. Available from: http://pages.turnitin.com/rs/iparadigms/images/Turnitin_WhitePaper_PlagiarismSpectrum.pdf, (Accessed: 26 March 2014).

To prepare for this Discussion:

Review the Learning Resources and media for this Week.
Begin familiarising yourself with the Harvard (Liverpool) citation and referencing style.
Consider your culture of origin in addressing the following topics.

To complete this Discussion: Post: Create an initial post in which you address the following:

Describe cultural attitudes and practices in your local and national community regarding intellectual property: The idea that a person can own words and knowledge.
Explain whether people in your community consider it important to acknowledge which ideas are their own, and which are another person’s.
Identify any changes in these attitudes and practices in recent years due to modern trends such as globalisation and access to easily-duplicated information.
Critically discuss how your own cultural attitudes may impact your transition to an academic environment, in which citing and referencing the sources of ideas will be a vital way for you to maintain your academic integrity.
Outline a plan or checklist that you will use to ensure a successful transition to this new environment.
Explain how using Turnitin throughout this Module will assist in validating the academic originality of your work.

Respond: Respond to your colleagues. Address the following:

Respectfully compare and contrast the cultural attitudes and practices you each described in your initial posts.
Reply as appropriate to colleagues who respond to your initial post.

For all Discussions (unless stated otherwise):

Create a single document with your initial post. Your document should be 350-500 words, though you will be marked based on the quality of your writing, not on the number of words.
By Sunday, post the text of your document to the Discussion Board for this Week, and upload the document using the Turnitin submission link for this Discussion.
By Wednesday, make 3–5 substantial follow-up responses to your colleagues. These can include responses to your colleagues’ initial posts, as well as responses to colleagues who responded to your own initial post. Your total Discussion Board participation must occur on at least 3 individual days during each week. Follow-up responses should be significant contributions to the Discussion. Do not submit your follow-up responses to Turnitin.
In general, online discussion is best when you:


Ask insightful questions.
Extend the discussion into new but relevant areas.
Model or promote critical reflection.
Support your arguments with citations and references from the assigned Learning Resources and other literature, using Harvard Liverpool Referencing Style.



Ensure that you spread your discussion posts across at least three separate days of each week. This will help maximise the value of your discussion with colleagues and serve to meet the learning objectives for each activity.
Click on the Reply button below to reveal the textbox for entering your message. Then click on the Submit button to post your message.
	              
	              I would beginning by asking what Intellectual Property really means. It’s like when you come up with an idea; say something quite amazing with great valuable propositions, you have one of two options: You can let anybody use it i.e. for free or selling, and the second option is you can protect your idea. This is pretty much what IP stands for. This can be further defined as the importance given to the tangible expression of an intangible idea created by the human mind.
The cultural attitudes and practices in my local and national community towards IP is not commonplace. The generality believe that it is almost an impossibility to invent something new. Just like genes evolve, culture evolves in a similar way. We call them memes. These are simply ideas, behaviours and skills and are copied, transformed and combined. This is who we are, how we live and how we create, with new ideas evolving from old ones.
In modern times as a result of globalization, the system is changing the mind-set towards the notion that ideas are free. The globalization of the market economy where the creation of our brainchildren are bought and sold usually result to an unfortunate fallout. A typical example is where Mr. A has an idea. To put that idea out in the market, he needs development costs, manufacturing costs and of course his margin. Whilst Mr. B who copies Mr. A’s idea does not need to bother about development costs thus making his creation cheaper. To this end, it is safe to say that original developments cannot contend with the price of copies.
To put this in simple terms, humans do not like to lose what they have. We naturally place much value on what we lose than on our gains. The derived gains from copying what others have laboured for do not necessarily make a big impressions, but when reversed, we often take it as an infraction we get very protective and territorial. Copying others can be justified, yet vilification is the case when reversed. In the context of academy, it is called “cheating”. What is most important, is understanding what academic integrity is all about, and how my personal cultural attitudes may impact my transition to an academic environment.
The school of thought I resonate with believes Academic Integrity is a more of a behavioural issue than an issue of one’s performance in an academic institution.
Academic integrity simply means academic honesty and psychologists tend to see honesty as a moral value (Lapsley, 1996; Baldwin, Adamson, Sheehan, Self &amp; Oppenberg, 1996, &amp; Cummings, Dyas, &amp; Maddux, 2001).
Academic integrity to me is the adherence to moral and ethical principles that create equality in institutions across the world. Simply put, this is the core of what I would do at the UoL which encompasses trust, honesty, fairness, integrity, respect and responsibility. This is what learning is all about.
Academic integrity matters to me because integrity matters. And the violation and infraction against one principle and one person is considered an infraction against all.
My obligations as a student of the UoL is simple. It is expected obligation to honour and partake in the sustenance of this academic institution’s integrity by simply doing what is right.
&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;

	              
	                 Academic Integrity In My Cultural Context Terry Yin September 13, 2014 1 My (Chinese) Cultural Attitudes and Practices In the traditional Chinese Confucius culture, a general belief as cited by Alford (1995) is “To Steal a Book Is an Elegant Offence”, which was first mentioned in a short story published in 1919 by the famous Chinese writer Lu Xun Xun et al. (1969). This belief has led to the conflict between China and the western world in the business and academical world. In defensive of the culture, I would say traditional Chinese scholars were awared of the ownership of intellectual property, otherwise they would not need to find an excuse like above mentioned. The properties themselves had got more attentions than their owners. The most famous Chinese traditional books, although they all have an author, were mostly co-authorized by many unknown people over many many years. It seemed that this culture cared about spreading the knowledge and encourage inspiration instead of rewarding the individual or keeping secret. But the result is unfortunately that the China 100 years ago was far behind the western world in sciences. So the western academic system has been introduced to China. 2 My Community And The Trend I work in the fast evolving information technology industry. I’m surrounded by fast learning companies and I’m among a lot of fast learners. This community has changed from big fish eats small fish to fast fish eats slow fish. On one hand A knowledge worker can only stay competitive by learning fast and sharing generously. On the other hand, these knowledge workers pay more attentions to intellectual property because they are also knowledge creators. I also work in open source software community, where people respect intellectual property but also love to share. I use the following “Beer-ware license” in one project Wikipedia (2014a): # ----------------------------------------------------------------------------  # "THE BEER-WARE LICENSE" (Revision 42):  # &lt;terry.yinzhe@gmail.com&gt; wrote this file. As long as you retain this notice you   # can do whatever you want with this stuff. If we meet some day, and you think   # this stuff is worth it, you can buy me a beer in return to Terry Yin. In many cases, when there are choices, I would usually prefer the open source software. Not just because they are free, but because they are better. They are often better because they have better and more healthy eco system than commercial software. And the “free” here is more as free in speech than in beer. I think a culture of academic integrity is the essense of this “free”dom. 3 My Approach For Referencing It’s important to value the others’ work and follow the regulation. I didn’t form a good and formal academic habit in my previous study and work. I will train myself to always try to use reliable source to back up my points and always use proper citation and references, in the preferred format. I’m going to use IPython Notebook P ́erez &amp; Granger (2007) to compose the essays of my academic assignments. Because     • it allows me to write articles in markdown language.  • it supports inline Python coding and running. Will be useful for my future study.  • it can have inline LaTex code to create mathematical formulas.  • more important, I can use BibTeX to manage all the references, Wikipedia (2014b).       From now on, when I read a good book, article, paper or webpage that I might refer to in the future, I will mark it down in my BibTeX bibliography file. Submitting my work to Turnitin will make sure I’m citing the others’ right. The Originality Report generate by Turnitin is a feedback tool for how well I’m doing it. And it will make sure that my academic honesty is subjective. Laureate Online Education (2010) References Alford, W. P. (1995), To steal a book is an elegant offense: Intellectual property law in Chinese civilization, Stanford University Press. Laureate Online Education (2010), ‘Tips for avoiding plagiarism’. URL: https://elearning.uol. ohecampus.com/bbcswebdav/institution/UKL1/201480_SEPTEMBER/1_Standard_Documents/UoL_ TipsforAvoidingPlagiarism.pdf. P ́erez, F. &amp; Granger, B. E. (2007), ‘IPython: a System for Interactive Scientific Computing’, Computing in Science &amp; Engineering 9(3), 21–29. URL: http://ipython.org. Wikipedia (2014a), ‘Beerware — wikipedia, the free encyclopedia’. [Online; accessed 13-September-2014]. URL: http: // en. wikipedia. org/ w/ index. php? title= Beerware&amp;oldid= 605296930 Wikipedia (2014b), ‘Bibtex — wikipedia, the free encyclopedia’. [Online; accessed 13-September-2014]. URL: http://en.wikipedia.org/w/index.php?title=BibTeX&amp;oldid=623918419 Xun, L., Sen, M. &amp; Gim ́enez, M. O. (1969), ‘Kong yi-ji’, Di ́alogos: Artes, Letras, Ciencias humanas pp. 12– 14.   
	              
	              “South African Institute of Intellectual Property Law was secured in 1952 and speaks to almost 164 patent lawyers, patent operators and exchange mark specialists in South Africa who work in the field of Intellectual Property Law. Licensed innovation Law grasps the law identifying with licenses, exchange imprints, enrolled plans, copyright and unlawful rivalry (passing-off of competitive advantages). It additionally incorporates prosecution, authorizing and franchising.” http://www.saiipl.org.za/ Accessed September 13, 2014
&nbsp;
Although the above statements clearly show the presence of laws relating to intellectual property in South Africa, this unfortunately often falls by the wayside when it comes to real life.&nbsp; Most people, especially individuals and small entities do not know what processes they have to follow, or whether or not someone can take an idea or design that is rightfully theirs. The general consensus is that an idea or design is not yours until you publicly reveal it, and often when you are unknown and a well-known entity says or does the same thing after you, they are then associated with that idea or design. Some people think that by quoting a statement that is enough to say you are not the owner of the words and it ends there. It does not seem to matter who said it, or when they said it.
&nbsp;
With the younger generation particularly in universities the importance to acknowledge ideas which are not your own has become a requirement. I, for one, only knew the term plagiarism when I started my studies at Monash University. It was only then, that I was taught about the importance of referencing, reminded of what I already knew not to do; not to copy other people’s work and pass it off as my own. &nbsp;I believe that because of the exposure I got at Monash University, a university of Australia, a country that is seemingly worlds apart in terms of their intellectual property acknowledgement compared to South Africa, has been a good foundation in preparing me for my new academic environment and maintaining my academic integrity. Although the citation and referencing is more detailed, I at least have had an introduction to a non-plagiarism, referencing way of life. Had I not had that exposure, the truth is putting quotes around statements that I find on the internet or that someone once said would probably be all the referencing I would have done. Thankfully, being a student at Monash University taught me otherwise.
&nbsp;
My plan going forward is to reference whatever I use that is not originally mine, give credit that is due to the original author/creator. &nbsp;I will clearly state where and when the information was accessed as I access it and not do it at the end lest I forget.
&nbsp;
Using Turnitin will help me to start validating my work, and remind me to reference other people’s material that I may have used.
&nbsp;
References
http://www.saiipl.org.za/ Accessed September 13, 2014
https://www.sabs.co.za/content/uploads/files/IP_Guide_FA_web.pdf “Sabs Design Institute”Accessed September 13, 2014

	              
	              Well I have completed my graduation from university but unfortunately the concept of using intellectual property in terms of citation and referencing is surprisingly new to me. In our community no proper guidelines are provided to students at early stages of education. These guidelines are only enforced to those students who have research work that has to be presented at international level.
In our community there is mixed ratio of people acknowledging others ideas and those who don’t. This is because there are no awareness programs and enforcement of laws for using ideas and work of others with proper acknowledgment.
There is a significant change in attitude after globalization and plagiarism detection systems like web based Turnitin system (Murphy, 2011). Now students are more careful while doing research work. They inhibit from copying and presenting other’s ideas as their work. There is a proper criterion to pass their research work like a limited percentage of plagiarism is allowed by instructors.&nbsp; Students use Turnitin as their standard way of submitting work and getting feedback (Dahl, 2007).
As I am not used to the citing and referencing system so it will take time and more efforts to get familiar. As human it is always difficult to adopt changes and somehow it has physiological impact and feeling of extra work load and burden. Due to lack of awareness some time you can be accused of plagiarism but you even don’t know about it which is the most dangerous thing.
Outline plan:
Following outline has been defined according to outline given by Cushman (2001), 

Reading guidelines for referencing and citations
Selection of a standard guideline.
Reading guideline.
Practicing guideline in my daily assignments.

Analyzing existing research articles.
Selection of articles according to my subject of study.
Look for citations within work.
Analyze bibliographic citations.

Cross verification of my assignments using Turnitin software.
Check plagiarism of assignments using Turnitin.
Analyze Turnitin feedback.
Modify parts of assignments in which plagiarism is detected.


&nbsp;
When we submit our piece of work using Turnitin application, it searches in the connected repositories and databases to verify the originality of work. It detects the similarities percentage and gives reports which are helpful to maintain academic integrity.
&nbsp;
References:
Cushman, M. (2001) ‘How to write an outline’,&nbsp;Los Angeles Valley College&nbsp;[Online]. Available from:&nbsp;http://www.lavc.edu/library/outline.html, (Accessed: 14 September 2014).
Dahl, S.(2007), 'Turnitin: The student perspective on using plagiarism', Active Learning in Higher Education v. 8 no. 2, pp. 173-191.
Murphy, Elizabeth (2011).’Plagiarism software WriteCheck troubles some educators’.&nbsp;USA Today. Retrieved 2011-10-15
	              
	              Hi Belinda
It was interesting to read your submission post and I wanted to share some perspectives from my background and on reflection, offer an opinion of differences between professional and academic practices.
Here in the UK, I have grown up in a culture of honesty and integrity. In some respect and in comparison with your own background, I don’t actually recall being taught specifically about plagiarism however, from a very early age, we were taught to be truthful and honest with each other.
At school, we were taught that it was not acceptable to copy from someone else and then to pass that on as your own work. That said, there were of course some unscrupulous people who did attempt to ‘cheat’ and the school put in place practices in attempts to prevent this wherever possible. Just as an example, I distinctly remember that during examinations, each student chair and table was setup in rows with approx. 6ft of separation so that you couldn’t physically see other student exam work. 
I guess I have never believed that this was cultural indifference – my belief was that it was very much related to the individual’s behavior, and attitude towards honesty and integrity.
Moving on from school and into my professional career, honesty and integrity continued to be core values in the way we worked. Relative to plagiarism, it is widely understood that there is always the need to reference your sources in any business material you produce. An example to this would be that, in a business case presentation, I might look to use market intelligence data produced and compiled by industry research organisations such as IDC or Gartner, to help in delivering a compelling rationale to my recommended approach. I would always show the source of this data&nbsp;however the inclusion of a full bibliography reference would likely depend on the type of document produced.
On critical reflection but still with correct intentions, is there potentially an inconsistency here with the expectation and quality of citing and referencing between professional and academic practices?&nbsp;
Best Wishes, Craig
	              
	              


I was in the Problem-Solving Leadership training this March in Arizona, by the famous computer scientist and writer Gerald Weinberg Wikipedia (2014) and 2 other famous trainers. One big lesson I learned from Jerry is that we need to re-learn how to “cheat”, as we were told and trained to never cheat at school. Jerry said in an interview: 
"Sometimes, though, the reason you have difficulty is that you believe you must solve every problem alone, with no help from anyone else. In the United States, I know that our schools contribute to this attitude–because receiving help in solving a problem is called “cheating.” That might be okay in some school situations, but in the world of real work, the people who succeed best are those people who know how to work with others. So, next time you have difficulty implementing an idea that you feel is important, seek and find another person or persons to work with you."
Weinberg (2014) 
I would say a culture of academic integrity is very important base for follow Jerry’s guidance and not offensing anyone (at least not offense people illegally). 
Is there any difference here between professional and academic practices? 
References 
Weinberg, G. W. (2014), ‘Gerald weinberg’s secrets of writing and consulting’. [Online; accessed 14- September-2014]. URL: http://secretsofconsulting.blogspot.com/2014/01/q-and-from-china.html 
Wikipedia (2014), ‘Gerald weinberg — wikipedia, the free encyclopedia’. [Online; accessed 14-September- 2014]. 
URL: http: // en. wikipedia. org/ w/ index. php? title= Gerald_ Weinberg&amp;oldid= 616970607 




	              
	              hi Craig
&nbsp;&nbsp;&nbsp; Thanks for pointing out the Gartner/Forrester type research entities. I am in a daily business of delivering IT solutions to our clients professionally. As part of my responsibilities, I am to develop IP for our company. In addition to my own research,&nbsp;I do utilize Gartner/Forrester type research firms, by paying them large amounts of money for their research. But I have never thought of referencing/Citing them. I guess because our corporation has paid for the research and I am not obliged to reference them. What are your thoughts ?

Robin Cyrus
	              
	              Cultural attitudes to intellectual property and practices my in local and national communities.&nbsp;&nbsp;&nbsp;
The idea that words and knowledge that can be owned is an interesting concept. At which point does it become owned? Is it the simple thought of a idea but then how can that be proven? Is it after the publication of an idea, whether via written word or recorded in some way with a time stamp or does it have to be verified by any independent authority to claim such a title. In the United Kingdom there is seen to be many types or forms of intellectual property, such as Patents, Copyrights, Industrial Design Rights, Trademarks and Trade Secrets. Until recent times these different rights have changed from giving very little protection purposefully as to encourage invention and innovation right through to recent times where more protection is given.
Culturally intellectual property is held very highly and seen to be an important social benchmark. People are proud to hold intellectual property rights whether their own or purchased. It is seen as a badge of honour in many circles especially around education and literature. To be seen as the person who originally said or invented something is held in great esteem.&nbsp; In the UK it is embedded in our society to be honest and truthful all through school and parenting and to never cheat or copy any work that isn’t our own. Ethics plays a big part in the structure of our society. Culturally we have developed ways in which a person can claim ownership of the said intellectual right and thus the use and plaudits that comes with it. As users or readers of other works we must ensure correct citation and referencing is given to the owner in order to expect the same from our own work.Do people in my community consider it important?
There are aspects of today’s UK society which will always rebel against such things which drive illegal activity such as the copying and distribution of pirated software, movies and songs etc. Thankfully this is in the minority; the vast majority however trust in and hold intellectual property and the rights to such very highly. The changes to attitudes in recent years, due to modern trends.
There have been many changes in recent years both good and bad towards intellectual property. The increase in prosecution levels of the illegal copying of media. The relaxing of social laws on freedom to express ideas yet coupled with tighter and more driven understanding around social media. Thankfully we have tools in place such as ‘Turnitin’ to help us develop our own work while still holding what we know is not our own in the highest regard by citing such works.&nbsp;&nbsp;&nbsp; However a recent trend in open source attitudes has in my opinion opened up a whole new world to what is intellectually owned. There are many fields and areas where it is accepted now to share intellectual property in communal groups to the point where no one is classed as having the ownership of the result. This is done to try to stimulate creativity.My cultural attitudes that may impact the transition to an academic environment in which citing and referencing the sources of ideas will be a vital way to maintain academic integrity.
&nbsp;&nbsp;&nbsp; Luckily my attitudes to this field should help rather than hinder any efforts I make towards an academic transition with regards to citation and referencing. Having lived in the UK all my life and gone through the education system here I’m used to this level of academic integrity in my work and I would like to think anyone else who may use my words in the future would give the same respect back.My plan to ensure a successful transition to this new environment.
Other than using the ‘Turnitin’ draft facility to help me transition into this way of working, I will also prepare my assignments and work in advance to layout the general template using MSWord with pre set sections with examples ready for filling in so I don’t forget to reference work correctly. Also using outline methods as described by (Cushman,M. 2001).I will double check my work to make sure I have accidentally not quoted someone else’s work properly and sticking to the 70/30 rule for originality vs. quoted or referenced material.
How Turnitin will help with academic integrity.
The use of the ‘Turnitin’ facility for the first module on this course will help to educate and train myself into writing and checking my own work before submission of assignments as described by the (University of Liverpool 2014). Also this facility will help when checking the similarity index and percentage level of potentially plagiarised work within my assignments before posting the final draft. It will also help to identify any accidentally unfinished or unquoted references. I understand this draft facility is only available for the first module which is a shame as it would have been handy for all of them but I can understand why so that it gives enough education to begin with but then doesn’t make the student lazy so that to rely upon it for the entire time.

References:
Brookshear, J.G. (2011) Computer Science: An overview. 11th ed. Boston: Pearson Education / Addison-Wesley.
Cushman, M. (2001) ‘How to write an outline’, Los Angeles Valley College [Online]. Available from: http://www.lavc.edu/library/outline.html, (Accessed: 13 Sept 2014).
Katie Collett. (17/10/2013): Public Attitudes to Intellectual Property Rights [Online]. Available from: http://www.populus.co.uk/item/Public-attitudes-to-Intellectual-Property-Rights/, (Accessed: 13 Sept 2014).
UK Government (2014): Intellectual Property Office [Online]. Available from: http://www.ipo.gov.uk/home.htm, (Accessed: 13 Sept 2014).
University of Liverpool (2014): Turnitin-Interpretation of the Originality Report [Online]. Available from: https://elearning.uol.ohecampus.com/bbcswebdav/institution/UKL1/201480_SEPTEMBER/1_Standard_Documents/SR_TII_Originality_Report.pdf, (Accessed: 13 Sept 2014)
Wikipedia (2014): Intellectual Property [Online]. Available from: http://en.wikipedia.org/wiki/Intellectual_property, (Accessed: 13 Sept 2014).
	              
	              Academic integrity in my cultural context
Words and thoughts property are influenced by our cultural context, for the western civilization this property is a very important notion in the economic globalized world.&nbsp;
A brief history of copyright
The origin of authorship and copyright can be traced to the efforts of governments to regulate and control the output of printed material. Previous to the invention of printing, control was not necessary because the only way to copy books was the labor of highly specialized scholars; whereas with printing technology many exact copies could be made relatively fast. Initially, governments controlled the prints by granting licenses to use a printer, but later on they granted these rights to the authors themselves. Copyright law was established across Europe during the 18th century, however modern law definition and Intellectual Property has its roots at the Berne Convention of 1886 (Waelde et al. 2013, Sola POOL 2009, World Intellectual Property Organization. no date).
My cultural context
When I was a kid in Colombia one could easily find copies of books, software, movies, etc. for a fraction of the cost of the original work (and sometimes for free).
When I arrived in Switzerland, things were different. Schools were promoting the respect of other people work. For example: we were taught about the principle of fair use when making photocopies of textbooks.
I realized very soon how important is the idea of Intellectual Property in IT. Software is a critical component in computing. Without IP and copyright protection one could have taken MS-DOS and make infinite copies of it, and in the process make Microsoft bankrupt. Would you do software to go bankrupt since anyone could just copy your work indiscriminately? That would halt innovation, a key success factor in the computing world.
Challenges in today global world
The World Wide Web has revolutionized the way we access, exchange and transmit information. But in terms of Intellectual Property it brought new challenges.
Perhaps the most important one is digital piracy, which in 2008 accounted for around 75 billion USD (Denton 2011). However different countries have different views on how to handle piracy: for example in Switzerland downloading copyrighted material is legal, and in 2011 the Swiss government deemed not appropriate to adapt the law (Violations de droits d'auteur sur Internet: le cadre juridique actuel est suffisant. 2011). However, uploading copyrighted material is illegal.  This is a field that touches me directly as I work for a software company: Autodesk Inc. Besides ethics, there is a real impact due to digital piracy: people’s job, my salary depend on the fact that people do not pirate my employer software.
Academic world
I am conscious of Intellectual Property and will not have a problem in the academic environment. As a learning student I have a variety of tools that I will use to ensure I avoid plagiarism:

I have the book ‘Cite Them Right’, which will help checking the correctness of referencing people work and ideas.
I have found a software called RefWorks that will assist me in maintain consistency of the Harvard (Liverpool) citation style.
Third, UoL provides Turnitin which allows to send drafts to verify that the work being submitted is original.

&nbsp;
References
&nbsp;
Waelde, C., Laurie, G., Brown, A., Kheria, S. &amp; Cornwell, J. (2013) 'History' in Contemporary Intellectual Property: Law and Policy, 3rd edn, Oxford University Press, pp. 31-33.
Sola POOL, I. de (2009) 'The Technology of Print, Censorship and Control' in Technologies Of Freedom Harvard University Press, pp. 12-22.
World Intellectual Property Organization. (no date) A Brief History. Available at: http://www.wipo.int/about-wipo/en/history.html (Accessed: 13 September 2014).
Denton, A. (2011) Intellectual property rights in today’s digital economy [PDF]. Available at: http://www.itu.int/ITU-D/treg/Events/Seminars/GSR/GSR11/documents/05-Intellectual-property-E.pdf (Accessed: September 14 2014).
Violations de droits d'auteur sur Internet: le cadre juridique actuel est suffisant. (2011). Available at: http://www.ejpd.admin.ch/ejpd/fr/home/aktuell/news/2011/2011-11-30.html (Accessed: September 13 2014).
&nbsp;

	              
	              Introduction

The extent of growth of the human mind is limitless, sometimes even unbelievable. It is amazing how an energy-saving idea can transform our train of thought, potentially leading to a great technological innovation; or how an automation idea could result in a cost-saving of thousands of dollars for a software solutions firm. 
&nbsp;
Cultural attitudes and Practices

‘Copyrights’, ‘patents’, ‘trademarks’ - all these were just fancy words when we were little. Intellectual property was a fairly new concept to me; to be frank, I hadn’t understood the relevance or the need for intellectual property rights, until I started working at my first job. It is only after I started programming that I came across design patents, software licenses and the like. I would wonder why it was so essential to possess copyrights, even on ideas. My CS Professor at college almost always insisted that we refer as many books and online forums as possible but always ensure that we cite all those references in our reports and documents; which was during the time of the Indian Copyright Amendment Act 1999. People in our country were finally beginning to understand and appreciate the significance of copyrights. By 2008, one of our novelists even faced plagiarism charges!
&nbsp;
Having learnt a lot from my experiences as a programmer/solution architect for the past 9 years, I can fully appreciate the significance of an idea and its ownership. In today’s world of open-source communities, it is highly commonplace that programmers look for easy solutions on the internet. Source codes are freely available to be used by developer fraternity and it is essential that we follow the terms of the GPL (General Public License) or the license associated with the source code. Thanks to open-source communities, we can even build our own operating system, say, with the help of Linux Kernel these days! Whenever I referred source codes, I ensured that citing the references would definitely be one among my mandatory programming practices. Not only did it help preserve originality, it also helped me come back and refer the same pieces of code later on when needed. 
&nbsp;
My approach to improve

1. Refer any existing works, try and come up with your own version with due credit to the &nbsp;original author.
2. Every time while referring any text / source code, first copy details of the original author reference along with the content. 
3. Refer the TurntIn feature for its text and source comparison for any missing references by using the OriginalityCheck provided in the draft submission &nbsp;of the discussion; and then finalising the draft.

Conclusion

Turnitin is the brilliant concept that validates originality of our work and I, for one, am very impressed by the due diligence with which we are required to cite our references at every point in our learning modules. This is one of the best practices in e-Learning for maintaining academic integrity and encouraging food for thought!
&nbsp;
References:

http://business.gov.in/legal_aspects/india_ipr.php
http://www.thecrimson.com/article/2006/4/23/students-novel-faces-plagiarism-controversy-beditors/
http://simple.wikipedia.org/wiki/GNU_General_Public_License
http://pages.turnitin.com/rs/iparadigms/images/Turnitin_WhitePaper_PlagiarismSpectrum.pdf



Best Regards,
Kharavela
	              
	              Introduction
I will start by describing, that regarding intellectual property (Dictionary.com), there are different attitudes and practices comparing the academic and commercial sectors. Below I’m listing a few issues which I think could be addressed when comparing the differences between the sectors mentioned. 

Commercial/Financial
Knowhow gathered by one or more individuals
The financial value of knowhow

Law
Patents
Copyrights

Society
General attitude
Political signal

Academic
Plaguerism
Moral
Ethics


Regarding the commercial sector, I have over the past many years observed that the limit for using others intellectual property has increased both for good and worse, where the worse can end up in court. The good stories are primarily catalysed by the internet and globalization, and seen in e.g.:

crowdsourcing (Brabham, 2008) solutions where many participate in joint development
GUI development which leads med to refer the reader to read the second paragraph in the “Weekly overview” (Weekly Overview, 2008) for this course which refers to a famous confrontation between Steve Jobs and Bill Gates.

General academic integrity
However, this initial discussion will now focus on the cultural context; hence I will focus on the above outlined “c” and “d”.
Intellectual property, and the value of it, is generally well-known in our society, publically, commercially and privately. And relatively many resources are being used, e.g. in the form of funding, innovation houses, and research centres. &nbsp;In our community, it’s very important to acknowledge other people’s ideas. And if someone tries to use another’s ideas without acknowledgment, people will, relatively fast, react to the wrongfulness. And when looking at the “academic integrity in our community, it’s only accepted if you do cite and reference correctly. Failing to do so will be punished accordingly.
I don’t have any local statistics or official reports to refer to regarding academic integrity in our community. Hence I will have to lean on material from other western societies, e.g. the article “Journals step up plagiarism policing” (Butler, 2010) on www.Nature.com. It has been my view, that in our society, we are very intolerant towards plagiarism, hence generally having high standard for moral and ethics. But according to the mentioned article, there have been done several tests showing staggering high plagiarism results. So I could ask: Are our standards as high as I think? I imagine that we won’t know until we run the Faroese materials through tools like Turnitin. And trying to be objective, I could ask: Why should we be any different?
"Not so many years ago, we got one or two alleged cases a year. Now we are getting one or two a month,"
(Butler 2010)
Since the coming of internet and globalisation, the world has served a lot of possibilities. It’s now so easy to find and use information, that one could easily imagine that a lot of students would be tempted to misuse this for own profit. And I believe, again referring to the above mentioned article, that a lot have done this. On the other hand I expect this trend to decrease because of the continually improved tools like Turnitin and the Wiki Guttenplag (www.orgtheory.com) through crowdsourcing. But I also believe, like the Turnitin report states, that some of the plagiarism is due to mistakes like the types “Hybrid and Mashup” (Turnitin, 2012).
But note also, that these software systems are not flawless, and they still need to have humans reading the material and deciding whether there is talk of plagiarism.
Myself and academic integrity
Looking at myself, I like to think that I have high standards when it comes to integrity, both in life in general and regarding my transition to the academic environment. Hence it’s a must for me to live up to the integrity declaration. And “To ensure my own transition...” I will repeatedly check my work by using the below outlined:

Plagiarism

Turnitin: “White paper, The plagiarism Spectrum”
UoL: “Academic Integrity Declaration”


Harvard referencing method

Referencing
Citation
Bibliographies


Turnitin system

As the last check I will use the Turnitin system whose strengths e.g. is it’s huge database, and pedagogically view with colours, will show me where I might have wrongfully referenced someone’s work.
Conclusion
I will strive to have as low a similarity percentage as possible, and when I do have similarities it should always be according to e.g. referencing and citation code of practice.&nbsp;&nbsp;&nbsp;&nbsp; 
References
Brabham, D.C. 2008, "Crowdsourcing as a model for problem solving an introduction and cases",&nbsp;Convergence: the international journal of research into new media technologies,&nbsp;vol. 14, no. 1, pp. 75-90.
Butler, D. 2010, "Journals step up plagiarism policing", Nature, vol. 466, no. 7303, pp. 167.&nbsp;
Definition of Intellectual Property [Onlne] www.dictionary.com, &nbsp;http://dictionary.reference.com/browse/intellectual%20property?s=t (Accessed 2014, 09-14)
guttenplag – crowdsourcing the extent of karl-theodor zu guttenberg’s plagiarism. [Online]. http://orgtheory.wordpress.com/2011/03/03/guttenplag-crowdsourcing-karl-theodor-zu-guttenbergs-plagiarism/. (Accessed 2014-09-14)
Intellectual property. http://www.Dictionary.com. [Online]. http://dictionary.reference.com/browse/intellectual%20property?s=t. (Accessed 2014-09-13)
Turnitin (2012) White Paper: Plagiarism spectrum [Onlin]. Available from: http://www2.nau.edu/d-elearn/support/tutorials/academicintegrity/pdf/Turnitin_WhitePaper_PlagiarismSpectrum.pdf. (Accessed 2014-09-14)
UoL, Code of practice on assessment, Appendix L (2013-2014). [online]. http://www.liv.ac.uk/media/livacuk/tqsd/code-of-practice-on-assessment/appendix_L_cop_assess.pdf. (Accessed 2014-09-13)
UoL, Harvard (Liverpool) referencing. [Online]. http://lgdata.s3-website-us-east-1.amazonaws.com/docs/1531/345457/Guide_to_Referencing_and_developing_a_Bibliography.pdf. (Accessed 2014-09-13)
UoL, Weekly Overview. [Online]. https://my.ohecampus.com/lens/home?locale=en_us#. (Accessed 2014-09-13)

	              
	              Hi Robin
Thanks. Yes, I was specifically referring to those organisations in relation to using the data that my company had purchased from them however, you are correct, this may be subject in some form or another to copyright and may not be allowed to be shared or used outside of your business.&nbsp;
That said, I would add to this that some information tends to based on data they have collected from industry, and may be available in the public domain anyway, its just a case of being able to find it. For example, and for the purposes of assisting you, many of the Technology vendors tend to put links on their own websites for published industry reports, which if you went direct to Gartner or IDC just to name a couple, they would likely charge you for the report. Given that these are in the public domain, I am not aware of any particular reason why they cannot be used as long as you correctly reference the source.
Hope this helps,&nbsp;
Best Wishes,&nbsp;Craig
	              
	              Craig
&nbsp;&nbsp;&nbsp;&nbsp; I agree with your&nbsp;response. Thanks for highlighting.
Robin&nbsp;
	              
	              &nbsp;
&nbsp;
Liverpool University
&nbsp;
DQ1 - Week 2 Academic integrity in a cultural context
&nbsp;
Author: Bram Tullemans
&nbsp;
Title: Self plagiarism in the copy paste culture
Cases of University &nbsp;professors with non-ethical &nbsp;publication methods have dominated the discussion in the Netherlands with regard to intellectual property and the ownership of words in the Academic world the last 3 years. Plagiarism is one of the greatest crimes one can commit in University community and it became clear that not only students copy easily words of others but also professors know to find the button. There are different forms of plagiarism and some of them are new inventions related to new communication forms. &nbsp;
Karen Allergy concluded from a study of the&nbsp;role electronic sources of information played in influencing plagiarism, plagiarism has become an increasingly prevalent problem at tertiary institutions (Allergy 2008). &nbsp;She investigated an essay assignment in a first-year geography module at the University of KwaZulu-Natal in South Africa and saw the emergence of the electronic era having a great effect on copy past input. Plagiarism is not new as the term originated in Latin and references to the practice are written down since the 3-th century before Christ. But the vast availability of electronic publications and the simplicity of how these words can be copied plagiarism surely occurs more (Bowden, 2009).
But also the act of finding plagiary text has improved tremendously the latest decades and therefore much more copy actions were discovered. Turnitin is one of the most used solution to detect plagiarism in the University domain. I t can detect about ten versions of non scientific use of other peoples work identifying exactly how quotations are used, how much of the work was original and If it was simply copied (Turnitin, 2012). Interesting enough it identifies also Coping of tweets without using the re-tweet options and therefore presenting other peoples message as one's own. 
Plagiarism and Intellectual property are, definition wise, not the same. For example one is not allowed to refer to one own words at any moment. It is not done to present something as new work while it is a copy past of already published content (Blaise, 2013). Solutions like Turnitin visualises all sentences that were published earlier firing up the discussion around self plagiarism. The pressure on in the Academic world to publish more and more drives people to reuse more and more. Articles are getting shorter and authors are trying to create red lines throughout these many publications. Let's not forget that a professor 50 years ago would work for years on one book discussing different angles towards his or hers work. This in nowadays fragmented into loose articles published in different locations. 
The balance between property and non-proprietary property it the main theme of Lawrence Lessig's writing. The thinks mixing and free quoting should be protected a an intrinsic right in the current culture. That does not mean he is for plagiarism but he thinks it should be possible to share ideas easily and freely. He envisages a bigger public domain that most intellectual property laws and rules would allow. He takes this concept and applies it to different domains like mixing of audio and video but also to the sharing of code. 
The use of open source code with references to copied work of others is very important for the development of all software. Both commercial software and open source software are integral parts of the broader software ecosystem and to the opinion of Lessig either alone would create a weaker software ecosystem. To bring this train of thoughts back to our writing op papers at the University: It is good to use other peoples words and ideas, remix them if wanted, as long as long as you do not claim them to be yours.
I think Lessig thoughts above can be used as guide line in creating an publishing text for the Master Information System Management. Use other people as reference to connect with what is available in the society, remix it with the story I find relevant but user references whenever referring to other peoples words or even thoughts. Tunitin can be used to get an official analyses of how you use references and other peoples words and this is an essential tool to maintain academic integrity. After every document one needs to check if all is referenced right, the text should be uploaded to Turnitin so it will be part of the whole. One needs to check what Turnitin gives back to indentify if your publication is ready to be opened to the rest of the world as an original text. 
&nbsp;

Bowden, D. (1996) ‘Coming to terms: Plagiarism’ [Online]. Available from:http://www.jstor.org.ezproxy.liv.ac.uk/stable/pdfplus/819650.pdf, (Accessed: 14 Sept. 20014).
Turnitin (2012) White Paper: The plagiarism spectrum [Online]. Available from: http://pages.turnitin.com/rs/iparadigms/images/Turnitin_WhitePaper_PlagiarismSpectrum.pdf, (Accessed: 14 Sept. 2014).
Blaise, C. ( ,2013), 'Self-plagiarism: An odious oxymoron.' Journal of the American Society for Information Science &amp; Technology. May2013, Vol. 64 Issue 5, p873-873. 1p.
Lessig, L., (2003) Open-Source, Closed Minds, CIO Insight. Oct2003, Issue 31, p29. 2p.
Allery, K. (2008) ' An investigation into electronic-source plagiarism in a first-year essay assignment.' Assessment &amp; Evaluation in Higher Education. Dec2008, Vol. 33 Issue 6, p607-617. 11p. 1 Graph


	              
	              Hi Bram,
I second your idea of Master Information System, but at the same time it should be centralized geographically to serve the local culture rather forcing people to use one standard.

Regards,
Kharavela
	              
	               
&nbsp;
 
A little history of Intellectual property
 
Being British, I like to reflect some of IP and Copy right patenting roots dating back to 1623. 
 
&nbsp;“The Statute of Monopolies and the Statute of Anne are said to constitute the origins of modern intellectual property”
 
Intellectual property law in the UK was first started under the Elizabethan reign in the form of ‘royal favors’ issues by the King or the lords to help introduce skilled individuals with new techniques and ideas. &nbsp;Originally intended to strengthen England's economy by making it self-sufficient and promoting new industries, the system gradually became a way to raise money (through charging patent-holders) without having to demanding a tax from the public.
 
Those royal favors were presented as form of royal charters, letters close and letters patent granted controls &nbsp;to produce particular goods or provide different services. However, the monarchy often misused those powers and ‘royal favors’. Elizabeth I particularly was a great abuser of the system, issuing patents for commodities such as starch and salt.
 
With the mounting public unrest, the parliaments at the time of reign of Queen Elizabeth perceived how such royal favors, was detrimental to free trade and thus, looked into a way to seize these practices. The parliament saw the intellectual property as a practical means to promote technological progress and protect public welfare. With the continued mounting pressures by judicial criticism, intellectual property was regulated under common law and the "Statute of Monopolies" enacted in 1623, &nbsp;this law formed &nbsp;the basis of modern patent law.
 
Letters patent were granted to publishers in recognition of the authors of their work. 
 
The Licensing Act 1662 established a register of licensed books to regulate book trade and protect printers against piracy. 
 
The period of common law-copyright ended when the Statute of Anne 1709 &nbsp;provided statutory protection in books and other writings. The Statute of Anne 1709 was seen as the origin of copyright law since &nbsp;It explicitly introduced the concept of an author being the owner of the copyright.
 
Prior to the statute's act in 1710, copying restrictions were enforced by the Licensing of the Press Act 1662. The restrictions were enforced by the Stationary companies. This censorship administration led to public protest. The Stationers decided to promote the benefits of licensing to authors rather than publishers, and thus succeeded in getting Parliament to consider a new bill in 5 April 1710. &nbsp;The bill became known as the ‘Statute of Anne’ due to the time during the reign of Queen Anne. 
 
The bill outlined the in which only the author and the printers they chose to license their works to could publish the author's creations. The ‘Statute of Anne’ remained in force until the Copyright Act 1842 replaced it.
 
Academic Integrity 
 
Avoid Plagiarism, respect other scholar hard work. Cite their work appropriately using citing methods such as Harvard (Liverpool) referencing techniques. Turnitin tool (DRAFT) can assist us in catching any work we might have missed by not Citing them and allow us to make the respective changes to our material before publishing them. With the required modifications the publication can be submitted to Turnitin tool (Final).
 
&nbsp;
 
References
 
BENTLY, L. &amp; SHERMAN, B. (1999) The Making of Modern Intellectual Property Law, Cambridge
 
Intellectual Property and Information Law
 
GILES, R. S. (1993) Are letters patent grants of Monopoly?, Western England Law Review, p. 239
 
MACHULP, F. &amp; PENROSE, E. (1950) The patent controversy in the Nineteenth Century, The Journal of Economic History, Vol. 10, p. 1-29
 
Rose, Mark (2009). "The Public Sphere and the Emergence of Copyright: Areopagitica, the Stationers' Company and the Statute of Anne". Tulane Journal of Technology and Intellectual Property (Tulane University Law School) 12 (1). ISSN 1533-3531.
 
Deazley, Ronan (2004). On the Origin of the Right to Copy. Hart Publishing. ISBN 978-1-84113-375-1.
 
	              
	              The values of honesty, integrity and morality are very much at the heart of the culture across the United Kingdom. From a very early age, our culture has influenced our behaviour with family and friends, and then later on in life as its extended across relationships with colleagues in academic and professional environments. 
Ultimately, these values are the foundation for establishing and maintaining a form of protection of, but not limited to, ownership, copyright, patents, business intelligence and trade secrets. This is commonly known as intellectual property, and is taken very seriously in the UK.
According to UK government services and information (2014), intellectual property is something unique that you physically create – an idea alone is not considered to be intellectual property.
In conjunction with this cultural starting point, and in relation to the specific ‘physical creation’, there are laws that serve to protect owners and consumers against the unscrupulous copying of these. There are also policies that are created within business organisations and academic institutions, to help support, develop and uphold intellectual property rights, and where used, appropriate credit for ownership is attributed accordingly.
As an example within a professional business environment, companies often have self-regulated corporate social responsibility, including specific reference to corporate ethics and ways of working. Also, contracts of employment often have a specific reference to intellectual property relative to your own role, the organisation, and the contribution to the development of that organisation. In addition, I have personally signed up to many non-disclosure agreements whereby my organisation is engaged in new technologies which would provide a competitive advantage in the market place, and these agreements serve to protect the intellectual property for the organisation leading up to and beyond the launch of the ‘physical creation’. 
In relation to academia, according to an article by Conran (2014), since year 2000 all UK universities have also been required by government to have a policy around the ownership of student and researcher intellectual property during their studies.
It is important to clarify that, and with particular acknowledgement of the increasing globalization of world economies and information access, Intellectual Property is not in place to restrict our cultures or movements. In fact, it is suggested by the World Intellectual Property Organisation (n.d.) that intellectual property provides legal protection for tradition-based creativity and as a “source of creativity and innovation” (World Intellectual Property Organisation, n.d.) can promote economic development and commercialization of ideas. 
There is broad acknowledgement of the importance of these needs across our culture. This occurs organically, in that as we develop, our understanding grows. However, there is also awareness that this shouldn’t restrict new developments, and that we are able to build upon these further to provide more depth of detail and improvements and advances to the ‘physical creation’ itself providing of course that there is no infringement on intellectual property or copyright, which is often referred to as ‘plagiarism’.
According to Drum (1986 cited in Bowden, 1996 pg. 82) plagiarism is “a disease that plagues college students everywhere”. On reflection, and specifically in relation to student writing, surely it is feasible that the same words have been written by multiple people but at different times and who have unequivocally and absolutely not copied, yet have arrived at exactly the same conclusion drawn from their own learning experiences. Is this classified as plagiarism or is it a coincidence of similar thinking and mind-sets? Equally, given the way I think, and write, I believe that it is entirely feasible that I could use the same words that I have perhaps used previously in other work, but with the absolute integrity of not intending to plagiarise myself.
I guess this also corresponds to the assertion in the aforementioned journal by Bowden (1996, pg. 82) that ”plagiarism is perhaps one of the foremost and richest of post-modern dilemmas”. 
With that in mind, have our attitudes and practices changed over time? Is it really clear? Isn’t the act of learning and then subsequently documenting that learning an unintentional form of plagiarism to some degree? I guess this would also, amongst other things, depend what constitutes common knowledge?
I am not convinced that attitudes or practices have changed, but perhaps it is becoming more confusing? It is suggested in a white paper produced by Turnitin (2012, pg. 3) that it is becoming blurry as to what constitutes plagiarism, in that plagiarism is easier to commit and originality is more difficult to define, and I guess that this is even more so with the increased encouragement for information sharing.
In my opinion, and again in relation specifically to student writing, the topic of plagiarism presents more questions than answers and I would suggest that this begins to question whether individuals intentionally copy, or are just simply documenting their learning of newly acquired knowledge. 
As part of this research, I continue to arrive at a key question of, how is this genuinely policed? If you take computer software as an example, Brookshear (2012) suggests that, “while copyrights and software license agreements provide legal avenues to inhibit outright copying and unauthorised use of software, they are generally insufficient to prevent another party from independently developing a product with a nearly identical function”. 
One thing that is clear is that this ultimately comes back to values of honesty, integrity and morality and from a cultural perspective, upholding these values. 
This will not impact my own attitude or transition to an academic environment, as culturally I have not changed, and as such I will continue to adopt values of honesty, integrity and morality. In terms of my strategy, it is simple. I will always follow the guidance as outlined by University of Liverpool (2013) to citing and referencing any sources of information I use, as well as using the ‘Tips for Avoiding Plagiarism’ as outlined by Laureate Online Education (2010). Critically, this will serve to ensure I maintain my own personal academic integrity. I will also fully exploit Turnitin to validate the academic originality of my work and make any adjustments as recommended. Lastly, I will investigate the use of ‘Ref Works’ to see how I can incorporate that process into the way I work to help with the creation of references and management of bibliographies.
Best Wishes, Craig
References
UK Government. (2014). Intellectual Property – An Overview [Online]. Available from: https://www.gov.uk/intellectual-property-an-overview/overview (Accessed 11 September 2014)&nbsp;
Conran, S. (2014). Time to rethink the intellectual property rights culture [Online]. Times Higher Education. Available from: http://www.timeshighereducation.co.uk/comment/opinion/time-to-rethink-the-intellectual-property-rights-culture/2015114.article (Accessed 12 September 2014)
World Intellectual Property Organisation. (n.d.). Intellectual Property and Traditional Cultural Expressions/Folklore [Online]. Available from: http://www.wipo.int/edocs/pubdocs/en/tk/913/wipo_pub_913.pdf (Accessed 12 September 2014)
Bowden, D. (1996). Coming to terms: Plagiarism. English Journal, 85(4), pp. 82-84 [Online]. University of Liverpool Online Library. Available from: http://www.liv.ac.uk.ezproxy.liv.ac.uk/library/ (Accessed 13 September 2014)
Turnitin. (2012). White Paper: The Plagiarism spectrum [Online]. Available from: http://pages.turnitin.com/rs/iparadigms/images/Turnitin_WhitePaper_PlagiarismSpectrum.pdf (Accessed 13 September 2014)
Brookshear, J. G. (2012). Computer Science An Overview. 11th ed. Boston Massachusetts: Pearson. Addison Wesley
University of Liverpool Library. (2013). Referencing and developing a Bibliography using the Harvard Method – Harvard (Liverpool) Referencing Style [Online]. Available from: http://lgdata.s3-website-us-east-1.amazonaws.com/docs/1531/345457/Guide_to_Referencing_and_developing_a_Bibliography.pdf (Accessed 14 September 2014)
Laureate Online Education. (2010). Tips for Avoiding Plagiarism [Online]. Available from: https://elearning.uol.ohecampus.com/bbcswebdav/institution/UKL1/201480_SEPTEMBER/1_Standard_Documents/UoL_TipsforAvoidingPlagiarism.pdf (Accessed 14 September 2014)
	              
	              Hello Class,The Intellectual Property (IP) laws in the western world might be the oldest and most developed, but issues with inconsistencies and double standards make it a legitimate question as to whether the rest of the world should continue to automatically adopt and apply IP laws derived from the western world.Do you think that developing nations should try to pursue different concepts and definitions of IP that best suit their own developmental needs?&nbsp;Furthermore, if other cultures or countries choose to develop their own IP concepts and laws, how in your opinion would that affect the cooperation between cultures or countries implementing radically different IP laws?Anthony
	              
	              Instruction
&nbsp;
To talk about “Academic integrity in a cultural context", we should understand what is the meanings of this topic. While on the basis of the definitions of the Oxford School Dictionary &amp; Thesaurus, Academic expresses the related tasks to perform with studying or education. As well as Integrity implies the true statement, and Cultural represents a people's traditions and custom. Lastly, Context denotes the background to an issue which assists to illustrate it. Thereby we are able to infer from the above explanations that "academic integrity in a cultural context" means academic honesty as in connection with the background of a people's traditions and custom during the academic environment.
&nbsp;
Academic integrity and Cultural attitudes
&nbsp;
Academic integrity refers to Openness and Truthfulness during the ways in knowledge being received and imparted; Academic integrity refers to openness and truthfulness during the ways in knowledge being received and imparted; Truthfulness is in view of the high-levels of dependence between staffs and students and on commitment which all students are impartially treated. Moreover, academic integrity demands all students declare the references of other authors' articles as well as to take preventing action supposing there is malpractice. The common examples of hurting academic integrity consist of plagiarism, participating unauthorized cooperation during course tasks, and citing without declaring.
&nbsp;
Cultures discrepancy 
&nbsp;
The discrepancy of cultural faiths of the disposition of declaring information sources and acknowledging arguments can effectively explain the international students' growing plagiarism rates. However, According to (Carroll, J. 2007), it is disquieting that notwithstanding having different cultural background, all international students trend to group into one homogeneous clique. Furthermore, the studying and teaching significant difference between countries are determined as generalizations across culture backgrounds which may be erroneous found that students wish to look for studying assistance was different in students from different nations. It is probable that the relation of cultural consciousness and inadequate language confident or ability result in these types of plagiarism that lacks of a real intention to deceive readers into trusting which the information are whose possession.
&nbsp;
Another distinct reason should be concerned is that plagiarism is also universal at using native language students. The more occurring of events reported in international students were able to barely be a consequent of their less ability to disguise the plagiarism. It is not surprising that native language users can adopt better techniques of applying different phrases or words to hide direct plagiarizing of source thesis. As Carroll, J. (2007) distinctly tells us that “a change in the style of writing tends to be easier to spot if the writer is using a language they are not fluent in.”
&nbsp;
The citing and referencing techniques
&nbsp;
Plagiarism is the most general issue of the Academic integrity, this is because students usually conflict the rules unconsciously when they quote other authors' opinion. Thus the citation and reference skills are quite significant during the writings. Mostly, Students are required to quote sources in whose articles and include the information of issue in a reference list or bibliography at the end of the writing. It will be treated as plagiarism when the authors cited other's argument or example during the writing without including the source details. Generally, Harvard Style is the usual method to construe these details.
&nbsp;
Harvard style 
&nbsp;
There are three kinds of Harvard style that are adopted by many universities which are Modern Language Association (MLA) style, American Psychological Association (APA) style and The Chicago Manual of Style (Chicago style). The website of harvard.edu (2014) explains that MLA is usually exploited at the humanities papers; APA is usually exploited at the social sciences papers as well as Chicago style is exploited at the both type of papers.
&nbsp;
Both APA and MLA styles request the authors to use quotations in-text that citations are situated in brackets within the text of writing. On the other hand, the footnotes are required to use during Chicago style. As well as both MLA and APA styles request the authors to declare and acknowledge the sources in two ways as following:

The authors must include a parenthetical citation in the article text which declares the specific citing sources, fact, idea and paraphrased statement clearly.
The authors must include the references lists at the end of the paper that are able to make readers to search the origins easily.

&nbsp;
Conclusion
&nbsp;
Students from distinct nations are an expression that all students have been exposed to distinct culture and studying styles. It is not necessarily proper in someone else’s nation what in one's nation is believed as Politically correct. These cultural differences may cause the issues of academic integrity and plagiarism in university; some students may assume that knowledge is meant for sharing, in fact, this is disrespect to others' work that the knowledge should be guarded.
&nbsp;
References
&nbsp;
Carroll, J. (2007); A Handbook for Deterring Plagiarism in Higher Education, 2nd edition, Oxford Centre for Staff and Learning Development, Oxford Brookes University
&nbsp;
David Gardner (1996-2012); Plagiarism and How to Avoid It; Available on: (http://www4.caes.hku.hk/plagiarism/) (Accessed on: 14-Sep-2014)
&nbsp;
Department of Lifelong Learning: Study Skills Series (2001); Referencing - The Harvard System; Available on: (http://education.exeter.ac.uk/dll/studyskills/harvard_referencing.PDF) (Available on: 14-Sep-2014)
&nbsp;
isites.harvard.edu (2014); Harvard Guide to Using Sources: A Publication of the Harvard College Writing Program; Available on: (http://isites.harvard.edu/icb/icb.do?keyword=k70847&amp;pageid=icb.page357682) (Accessed on: 14-Sep-2014)
&nbsp;
Oxford Dictionary (2012); Oxford School Dictionary &amp; Thesaurus | ISBN-10: 0192756923
&nbsp;

	              
	              The cultural attitude in Nigeria (especially in the western part of Nigeria), before the era of globalization was that no one could own words or knowledge. Folk tales, stories and even the Yoruba history was passed down orally from one generation to the next and nobody has been able to pinpoint the source or origin of these tales till date. This medium is by some Yoruba literature books (YORUBA LEGENDS by M. I. Ogunmefu and OWE YORUBA IN PROVERBS by A.A Kila) that no reference sources, presupposing the fact that it was passed from generation to generation by mouth.
Culture is the believe habit and opinion of a group of people, society, tribe or nation about life and the way we live it. Cultural attitude will be personal acceptance and it undertaking of these believes, habits and opinion of the people. The cultural attitude toward whether a person can own word and knowledge will depend upon the current credits of the individual.
Good character is the essence of the African moral system, the linchpin of the moral wheel. The justification for a character-based ethics is not far to seek. For, all that a society can do, regarding moral conduct, is to impart moral knowledge to its members, making them aware of the moral values and principles of that society. In general, society satisfactorily fulfills this duty of imparting moral knowledge to its members through moral education of various forms, including, as in African societies, telling morally-freighted proverbs and folktales to its younger members. But, having moral knowledge—being made aware of the moral principles and rules of the society—is one thing; being able to lead a life consonant with the moral principles is quite another. An individual may know and may even accept a moral rule, such as, say, it is wrong to cheat the customs. But he may fail to apply this rule to a particular situation; he is, thus, not able to effect the transition from knowledge to action, to carry out the implications of his moral belief.
However, from books and the media, I can say people in my community consider it important to acknowledge which ideas are theirs and which are another’s as I’ve seen this done both in written statements and spoken words. Most books written in my community actually take time to acknowledge the input of others in their work and this can be seen in the acknowledgement and reference section. Also depending on the type of book it, some writers can make certain statements within the chapters of a book, acknowledging someone for a certain idea.
In recent years, with the advent of globalization comes the change to our perception of intellectual and academic integrity. This change in attitude is being evidenced by the numerous copyright agencies that have been established in Nigeria, in order to help protect people's works. The most notable is the Nigerian Copyright Commission (NCC) which was founded by the 1988 Act as the primary agency responsible for regulating copyright matters in Nigeria. The Act is being modified again and again [(Amendment) Decree (No. 98) of 1992 and Copyright (Amendment) Decree (No. 42) of 1999] to accommodate the changes brought about by globalization and easy access to information. (2013, 03)
It is also worthy to note that the amount of intellectual property litigation has also significantly increased in Nigeria. There have been two notable litigation in this area in Nigeria (http://www.punchng.com/news/don-sues-cbn-governor-for-plagiarism/) (http://www.modernghana.com/news/430099/1/plagiarism-dikes-suit-against-two-uniport-professo.html) I also think intellectual integrity in Nigeria today is at par with other parts of the world.
The consequences of plagiarism in other parts of the world are almost the same as the response to piracy in Nigeria.
The absence of a widespread culture of reading in the case of Nigeria acts as an effective barrier to our development and international competitiveness. The economic, social and political health of our nation today depends on building literate citizens that are able to read widely and apply it practically for development. It is therefore a necessity to making the present generation more aware of the benefits and importance of reading and ensuring that they have the literacy skills required in the modern society.
As a learning student I am conscious of Intellectual Property therefore;

I use Grammarly.com which will analyze my document in real time for over 150 points of grammar, plagiarism, word choice and much more.
I have purchased the book ‘Cite Them Right’, which assist in checking the correctness of referencing peoples work and ideas.
Turnitin which allows sending drafts to verify that the work being submitted is original.

Submitting my work to Turnitin will assist me in plagiarism prevention and make sure I am citing the others’ right. The Originality Report which provides a summary of matching or similar areas of text found in a submitted paper is a feedback tool for how well I am doing it. 2013, Laureate Online Education
&nbsp;
References
(2013, 03) The Growth of Intellectual and Academic Integrity in Nigeria."StudyMode.com". &nbsp;Retrieved 03, 2013, from http://www.studymode.com/essays/The-Growth-Of-Intellectual-And-Academic-1521743.html
African Ethics, First published Thu Sep 9, 2010 http://plato.stanford.edu/entries/african-ethics/#EthDutNotRig
By KN Igwe - ‎2011 Reading Culture and Nigeria’s Quest for Sustainable Development http://www.webpages.uidaho.edu/~mbolin/igwe2.htm
http://ed.grammarly.com/editor/view
http://pages.turnitin.com/rs/iparadigms/images/Turnitin_WhitePaper_PlagiarismSpectrum.pdf
Laureate Online Education. (2010).&nbsp;Tips for Avoiding Plagiarism&nbsp;[Online]. Available from: https://elearning.uol.ohecampus.com/bbcswebdav/institution/UKL1/201480_SEPTEMBER/1_Standard_Documents/UoL_TipsforAvoidingPlagiarism.pdf (Accessed 14 September 2014)
2013 Laureate Online Education https://elearning.uol.ohecampus.com/bbcswebdav/institution/UKL1/201480_SEPTEMBER/1_Standard_Documents/SR_TII_Originality_Report.pdf
	              
	              Anthony&nbsp;
My immediate thought to the prospect of developing nations (or indeed any nation!) developing their own concepts and definitions of Intellectual Property that best suit their own needs is that this would widen the gap between countries and cultures.
We operate very much in a global economy with nations that are united on many fronts. Many countries and cultures are reliant on each other for, amongst other things, trade, and as such there is a need to regulate the trade to protect consumers, in terms of health and safety and also financial security, but also companies and businesses in terms of their financial investments, creativity and innovation too.
As part of my DQ response, I referenced the World Intellectual Property Organisation (n.d.) who asserts that intellectual property provides legal protection for tradition-based creativity but is also a “source of creativity and innovation” (World Intellectual Property Organisation, n.d.) that can promote economic development and commercialisation of ideas.
I am looking at this in its broadest sense, and of course there will be some examples of where it is appropriate to apply a degree of flexibility to certain aspects of Intellectual Property, particularly where this may be restricting creativity and innovation in developing nations, but as a guiding principle, I believe the world is a much better place when we cooperate and collaborate, rather than work against each other. This is notwithstanding the need for competition, but it must be in the context of promoting enterprise, being fair and reasonable and not through monopolisation.
This is a very difficult and complex subject but ultimately, I believe we need to look to fix the inconsistencies in these practices and for those issues by which we feel restrictions need to be relaxed, agree together a model that can be adopted for everyone. Surely it is about doing the right thing.
Best Wishes, Craig
Reference.
 
World Intellectual Property Organisation. (n.d.). Intellectual Property and Traditional Cultural Expressions/Folklore [Online]. Available from: http://www.wipo.int/edocs/pubdocs/en/tk/913/wipo_pub_913.pdf (Accessed 12 September 2014)
	              
	              The Intellectual Property (IP) is always a subject controversial, who is really the owner an idea, a word or a theory. How can we be sure that this person is the real owner, is it only because he was the first one who published it? What is the real line of demarcation between plagiarism and your own opinion? Can we really consider the fact that a word can owns to a person? These are kind of questions which can coming in our mind when we discuss about this topic.
By definition, The IP is the set of exclusive rights in intellectual creations to the author or the copyright of a work of the mind. Wikipedia (2014) Intellectual Property Available at: http://fr.wikipedia.org/wiki/Propri%C3%A9t%C3%A9_intellectuelle (Accessed 13 September 2014)
It has two branches:
•	The literary and artistic property, which applies to works of the mind, is composed copyright and related rights. 
•	And the industrial property. 
So, based on that definition and the principle behind this definition, it's legitimate that a person owns the word that he creates when he speaks and/or writes
It should be emphasized that according to the international law about IP, “the intellectual property rights are governed by the law of the State in whose territory the protection of property is sought” Draft Law on Private International Law Code, Article 92, § 1.
Regarding my national community (The Republic Democratic of Congo) we are member of WIPO, the World Intellectual Property Organization since January 28, 1975.
“ The mission of the WIPO is to "promote the protection of intellectual property throughout the world through cooperation among states and in collaboration with other international organizations," to "ensure administrative cooperation between the various Unions' (or groups of States ), to "promote the development of measures to facilitate the effective protection of intellectual property throughout the world and to harmonize national legislation in this area," to "promote international agreements to promote protection of intellectual property ", to" gather and disseminate information concerning the protection of intellectual property ". This mission is fulfilled through the negotiation of international treaties, the creation and management of international protection systems, technical assistance, mediation and arbitration. ” The Congolese Legislative Framework in relation to Intellectual Property (2010) Available at: http://193.5.93.81/wipolex/en/outline/cd.html (Accessed 13 September 2014)
But “it was only in 1982 that the Democratic Republic of Congo (DRC) recognized the need to adapt its domestic legislation to comply with its international obligations (1982 for industrial property and 1986 for literary and artistic works)” The Congolese Legislative Framework in relation to Intellectual Property (2010) Available at: http://193.5.93.81/wipolex/en/outline/cd.html (Accessed 13 September 2014)

I would again like to point out something here, have a law in place and get it applied are two things totally different, As an underdeveloped country, in DRC many people don’t care about IP, I think that this is due to the fact that as an underdeveloped country, we have a very high rate of analphabet. It's sad but that's the harsh reality. And even the few people who have access to the education or university, most of time don’t care about IP. It’s not easy in my community to recognize or accept that a person owns a word. Even in the university we have many cases of plagiarism only due to the simple fact that people don’t understand how a person can owns a word or an idea. Only few people can understand this. It's not in Congolese culture even in African culture.
For me personally, I’m one of few Congolese people who can understand the importance to recognize and /or evaluate the work of others. But my experience with citation and reference in academic environment based on my previous studies is not very famous. But as I want to maintain my academic integrity, I will use the opportunity offered to me through this programme to make the referencing and citing of sources, an integrated part of my life.
Let us now discuss briefly about the changes in these attitudes and practices in recent years due to modern trends such as globalization and access to easily-duplicated information. In fact the globalization for example, as well as the technological change made the protection of IP a real object of debate, Until the 1990s, the protection of intellectual property was little challenged. Philippe PETIT (2012) Available at http://afri-ct.org/IMG/pdf/53_Petit_propri_intel.pdf (Accessed 13 September 2014)
What then has changed? :
•	An economic and financial challenge
The challenges of the knowledge economy grew so exponentially. The gap between technology and property rights Intellectual has widened, leading to a questioning of these rights. 
Financial issues are also significant in a global economy, largely based on knowledge. Before this economy was based on tangible goods (raw material resources, equipment, capital, labor, output ...) and now it's increasingly based on property intangible: knowledge, innovation, patent portfolio of brands and Industrial Designs
•	The challenge of new technologies
The gap between technology and the law of intellectual property has widened. The economy and new technologies are essentially globalized.
References
http://wikipedia.org/wiki/Propri%C3%A9t%C3%A9_intellectuelle 
http://afri-ct.org/IMG/pdf/53_Petit_propri_intel.pdf 
http://193.5.93.81/wipolex/en/outline/cd.html

	              
	              
	              
	              The Intellectual Property (IP) is always a subject controversial, who is really the owner an idea, a word or a theory. How can we be sure that this person is the real owner, is it only because he was the first one who published it? What is the real line of demarcation between plagiarism and your own opinion? Can we really consider the fact that a word can owns to a person? These are kind of questions which can coming in our mind when we discuss about this topic.
By definition, The IP is the set of exclusive rights in intellectual creations to the author or the copyright of a work of the mind. Wikipedia (2014) Intellectual Property Available at: http://fr.wikipedia.org/wiki/Propri%C3%A9t%C3%A9_intellectuelle (Accessed 13 September 2014)
It has two branches:
•&nbsp;The literary and artistic property, which applies to works of the mind, is composed copyright and related rights.
•&nbsp;&nbsp;And the industrial property.
So, based on that definition and the principle behind this definition, it's legitimate that a person owns the word that he creates when he speaks and/or writes
It should be emphasized that according to the international law about IP, “the intellectual property rights are governed by the law of the State in whose territory the protection of property is sought” Draft Law on Private International Law Code, Article 92, § 1.
Regarding my national community (The Republic Democratic of Congo) we are member of WIPO, the World Intellectual Property Organization since January 28, 1975.
“ The mission of the WIPO is to "promote the protection of intellectual property throughout the world through cooperation among states and in collaboration with other international organizations," to "ensure administrative cooperation between the various Unions' (or groups of States ), to "promote the development of measures to facilitate the effective protection of intellectual property throughout the world and to harmonize national legislation in this area," to "promote international agreements to promote protection of intellectual property ", to" gather and disseminate information concerning the protection of intellectual property ". This mission is fulfilled through the negotiation of international treaties, the creation and management of international protection systems, technical assistance, mediation and arbitration. ” The Congolese Legislative Framework in relation to Intellectual Property (2010) Available at: http://193.5.93.81/wipolex/en/outline/cd.html (Accessed 13 September 2014)
But “it was only in 1982 that the Democratic Republic of Congo (DRC) recognized the need to adapt its domestic legislation to comply with its international obligations (1982 for industrial property and 1986 for literary and artistic works)” The Congolese Legislative Framework in relation to Intellectual Property (2010) Available at: http://193.5.93.81/wipolex/en/outline/cd.html (Accessed 13 September 2014)
I would again like to point out something here, have a law in place and get it applied are two things totally different, As an underdeveloped country, in DRC many people don’t care about IP, I think that this is due to the fact that as an underdeveloped country, we have a very high rate of analphabet. It's sad but that's the harsh reality. And even the few people who have access to the education or university, most of time don’t care about IP. It’s not easy in my community to recognize or accept that a person owns a word. Even in the university we have many cases of plagiarism only due to the simple fact that people don’t understand how a person can owns a word or an idea. Only few people can understand this. It's not in Congolese culture even in African culture.
For me personally, I’m one of few Congolese people who can understand the importance to recognize and /or evaluate the work of others. But my experience with citation and reference in academic environment based on my previous studies is not very famous. But as I want to maintain my academic integrity, I will use the opportunity offered to me through this programme to make the referencing and citing of sources, an integrated part of my life.
Let us now discuss briefly about the changes in these attitudes and practices in recent years due to modern trends such as globalization and access to easily-duplicated information. In fact the globalization for example, as well as the technological change made the protection of IP a real object of debate, Until the 1990s, the protection of intellectual property was little challenged. Philippe PETIT (2012) Available at http://afri-ct.org/IMG/pdf/53_Petit_propri_intel.pdf (Accessed 13 September 2014)
What then has changed? :
•&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; An economic and financial challenge
The challenges of the knowledge economy grew so exponentially. The gap between technology and property rights Intellectual has widened, leading to a questioning of these rights.
Financial issues are also significant in a global economy, largely based on knowledge. Before this economy was based on tangible goods (raw material resources, equipment, capital, labor, output ...) and now it's increasingly based on property intangible: knowledge, innovation, patent portfolio of brands and Industrial Designs
•&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; The challenge of new technologies
The gap between technology and the law of intellectual property has widened. The economy and new technologies are essentially globalized.
References:
http://wikipedia.org/wiki/Propri%C3%A9t%C3%A9_intellectuelle
http://afri-ct.org/IMG/pdf/53_Petit_propri_intel.pdf
http://193.5.93.81/wipolex/en/outline/cd.html

	              
	              ACADEMIC INTEGRITY IN A CULTURAL CONTEXT
Words can be seen as a distinct meaningful element of speech or writing, used with others or sometimes alone to form a sentence and typically shown with a space on either side when written or printed (oxforddictionaries.com). This definition clearly summarizes how important our ideas should be acknowledge as our own.
I went through school from my Primary and Secondary school in my home country without a single of plagiarism. However, during my university days, I got introduced to a little bit of citation and referencing in my final year but unfortunately intellectual property is totally new to me. In my early Education life, little efforts were made and no proper guidelines are put in place to encourage individual work.
In my country little efforts are put in place to encourage intellectual property. My community for instance has been passing information from one generation to the other by word of mouth with little documented evidence.
However, since the advent of globalization more efforts have been put in place to change the attitude of individuals. For instance, in the western part of my country folk tales, stories and even the history of the people was passed down orally from generation to generation without no prove of originality but now there are several copyright agencies that have been established in Nigeria like the Nigerian copyright commission (NCC). Also, there are more litigation cases of intellectual property in Nigeria. Some of these cases include a case where “don sues CBN governor for plagiarism” and “dikes suit against uniport professor”. These two cases are notable cases on plagiarism in Nigeria.
Furthermore, having little or no knowledge as regards to intellectual property will enable me put more efforts to get myself familiar with this type of writing. It will be stressful because of my cultural background but I will not relent.
Nevertheless, the following steps have been outlined to achieve my goal:

Identify the source.
Use paraphrasing most times.
Use quotes when necessary.
Use in-text citation when necessary.
Input quotation to refer to other peoples work.
Always have a reference list of all sources of work used.
Identify similarities with other peoples work by submitting my work to turnitin to better my originality skills.

The turnitin tool will help me validate my work as the originality report generated after submission of write up gives you feedback on how well you are doing (laureate online Education).
References:
Cushman, M. (2001)’How to write an outline’, Los Angeles valley college[online]. Available from:http://www.lavc.edu/library/outline.html.(Accessed: 13september 2014)
Laureate online Education (2010),”Tips for Avoiding plagiarism”, URL https:/elearning.uol.ohecampus.com/bbcswebday/institution/UKL1/201480_SEPTEMBER/1_standard_documents/UoL_TipsforAvoidingplagiarism.pdf
“The growth of intellectual and academic integrity in Nigeria”, studymode.com, 03, 2013(www.studymode.com/essays/the-growth-of-intellectual-and-academic-1521743.html
www.oxforddictionaries.com/definition/english/word
www.modernghana.com/news/430099/1/plagiarism-dikes-suit-against-two-uniport-professo.html
www.punchng.com/news/don-sues-cbn-governor-for-plagiarsm
&nbsp;

	              
	              Cultural Attitudes And Practices In My Local And National Community Regarding Intellectual Property
The depth in application of intellectual property in Kenya is still a far cry from the ideal potential of its implementation in the country. Although there has been significant progress in the legal and policy front evidenced by the fact that Kenya is TRIPS compliant, I would assert that a big gap still exists in basic civic education around intellectual property by citizens of this county.
The suggestion by the Bulletin of the Institute of Economic Affairs in its Issue No 48, September 2001 that “Intellectual property matters have always had a low profile in Kenya”, could still hold some water to the extent of knowledge and know-how by it’s citizens. The idea that a person can own words and knowledge and by that process lock out any others that may seek to profit from it, is not one that you’d find particularly ingrained in your average youth or adult.
From a traditional sense, many communities in Kenya have skills and knowledge that are unique to that community. These would be passed on to different generations through organized forums such as rites of passage or an art passed from parent to child. The issue of ownership for that matter did not quite arise and it was expected for example that a son or daughter would inherit certain skills or trade from the parents. Knowledge in the sense of the broader community would be shared through folk tales and many communities in Kenya are replete with their own unique sayings and cultural practices that apply to the community. Tools and techniques that a community would come up with for specific interventions would be readily borrowed and applied by others and especially so from a farming perspective.
Commercialisation of ideas has largely been a western influence and in the Kenyan context, structures inherited from the colonial regime. Intellectual property in Kenya faces challenges in implementation due to this lack of inherent awareness for a large percentage of the populace. Compared to the western world where a typical teenager is well aware of the notion of patents and copyrights, the system in Kenya to the best of my knowledge does not teach these concepts in school.
However, changes in these attitudes have slowly evolved largely due to an expanded democratic space in Kenya and opening up of the airwaves from the once dominant monopoly national broadcaster. The notion that an employee could leave your company to found or join another competing company, while making off with your concepts heralded several commercial litigations within the media fraternity that only served to increase the mindshare on intellectual property in Kenya as this was something most people could relate to without the complexity that one would often find in many instances.
Some high profile intellectual property cases in the recent past in Kenya have depicted such scenarios. In the Civil Case 490 of 2013 of the Republic of Kenya (http://kenyalaw.org/caselaw/cases/view/95650/), the defendant is accused of copying the plaintiffs pilot wedding programme and related documents which the they were scheduled to use and in essence going ahead to reproduce in form and structure, the very ideas that the plaintiff had. This was particularly unique and brought to the fore in the public eye, the very notion that one could claim ownership to an idea and caused a significant shift in perception due to the high publicity that surrounded it.
In my case, my own cultural attitudes may not necessarily impact my transition to an academic environment,&nbsp;because my understanding of intellectual property and especially within academia, is given by the rules and guidelines required for suitable attainment and completion of the course requirements.
&nbsp;
Ensuring a successful transition to the new environment:
The plan I intend to use to ensure successful transition to the new environment are as follows

Review of all the required learning resources before attempting assignment
Conduct additional research using the library resources available 
Refer to the Havard(Liverpool) guide in writing up my assignments
Make use of Turnitin draft link to pre-check my work and review against the originality report before submitting my work
Contributing in the discussion forum as per the required guidelines for participation and class engagement

&nbsp;
Using Turnitin in validating the academic originality of my work.
Turnitin is a great resource that is invaluable as a means to ensure that no inadvertent reproduction of already published material has been made. After submitting a draft assignment from the Turnitin draft link, the originality report that is available thereafter, would show any occurrence of plagiarism and enable correction before submitting of the final report
&nbsp;References :

TRIPS:- The treaty on Trade Related Aspects of Intellectual Property Rights (TRIPS) – is a multilateral agreement on intellectual property administered by the World Trade Organisation. http://www.wto.org/english/tratop_e/trips_e/trips_e.htm &nbsp;
Bulletin of the Institute of Economic Affairs; Issue No 48, September 2001 -

(http://www.ieakenya.or.ke/publications/cat_view/1-publications/3-bulletins-briefs/11-the-point?start=15 )

Intellectual Property Rights In Kenya - © Konrad Adenauer Stiftung , SportsLink Limited and authors 2009, http://www.kas.de/wf/doc/kas_18323-1522-2-30.pdf?110214131726 
The National Council for law reporting: http://kenyalaw.org/caselaw/cases/view/95650/ 

&nbsp;

	              
	              Hi Belinda.
My experience in Kenya is similar to yours in that the notion of ownership of ideas is not something that was particularly brought out with particular emphasis of value in original work. However, Craig brings out a very good point in outlining the very basic mantra that school kids are normally taught- not to copy other peoples work or carry hidden material with answers to an exam.
Whereas this level of academic integrity in schools &nbsp;is a reality the world over, the context in which it happens does not emphasize the value of original work. To most students, the copying or "dubbing" as it is called in Kenya, is a means to an end. The competitive nature of the Kenyan education system that takes one final exam as the entry point to the next level of high school or university, &nbsp;and one that emphasizes more on ranking of schools and passing of exams as opposed to forming an individual holistically, is one of the undoings. Under such circumstances, academic integrity at the cost of loosing a high school or university place would often be sacrificed. Nevertheless academic integrity may be upheld in such schools if the payoff for original work is emphasized so that its more than just passing an exam.
Regards
Joseph
	              
	              HI Anthony,
I think I agree with Craig.
I'm not of the opinion that different Nations should adopt different intellectual property (IP) laws. For instance in my country, IP is still trying to have a strong hold on individuals. However, strong measures are being implemented to change the old trend. It is a different situation in the western world where Intelectual property has been in practice for decades and its effective in every ramification.
Yes, the western world is more advanced and developed than the developing Nations, this I feel there should be a way of bridging the gap between these two world. As Craig mentioned, when you ask the developing Nations to operate their own IP laws, it will create a wider gap between these two worlds instead of bridging. we should encourage global cooperation cause no man is an island.
Regards
martins&nbsp;
	              
	              

Dr. Anthony,
While attending University I remember being thought about referencing. However, given the plethora of information out there, I have always wondered how the lecture would distinguish a class room full of students original ideas from an idea that may have been, knowingly or unknowingly, represented as their own. Lectures then used no software to test the information presented by students against a database of information already presented. And of course the lecturer cannot read and know what is written in almost every online material or book. Thus, in my opinion a student could have easily taken another person's ideas and thoughts restructure the sentence and present it as their own, as plagiarism was described at, Plagiarism - Using the Work of Others as your own Writing. Surprisingly, enough I thought plagiarism was just word for word copying. Nevertheless we were always thought to make appropriate references where necessary, which I did.
Describe cultural attitudes and practices in your local and national community regarding intellectual property: The idea that a person can own words and knowledge. 
I do not know if the structure of the courses now being thought at our University has now changed. Or if there are some courses out there that has this level of information as it relates to intellectual property rights. Perhaps a law course at our university would. However on April 04, 2014, the KaieteurNews online published an article titled 'UG Vice Chancellor challenges academics to embrace Intellectual Property' where the Vice Chancellor clearly saw this as an area where more attention needed to be placed, as he encouraged students to pay attention to this area and he further suggested that they seek additional certification in it. 
“Take that course on your own; register and get your own certificate so that you will be certified in that area, because we have to start respecting the output of our research so&nbsp;...”
This year I did an online Professional Development course at a popular University in the United States and although we were required to present for the first half of the course mainly our thoughts and ideas in a similar forum such a this one, we were not required to present references for each discussion as in this case. Thus leading me to believe that the culture may not only be different here, but elsewhere as well or it may be dependent on the level or area of study you are undertaking, which may determine the emphasis placed in this area. 
Most academics here are aware of the general concept of intellectual property rights, but the level of awareness may be dependent on the knowledge base and the individual's school of thought. Another key thing for me is not just the representation of another's thoughts and ideas as your own but the safeguarding of your own thoughts as well; the fact that you can have a claim to your own ideas.

Explain whether people in your community consider it important to acknowledge which ideas are their own, and which are another person’s.
To me not enough emphasis is placed here and as such many may not view it as important. Additionally it may be dependent on the level of education and understanding as it relates to the topic. It may also be dependent on what the individual I doing; if it is that they are doing research work or they are in the areas of publishing they may see this as important. Authors of books would have to commit to the standards of the wider world and are often compelled to follow them. I think basically because we are in a third world country their may be much leniency it relate to this topic. The relevant resources and knowledge may also be unavailable. 

Identify any changes in these attitudes and practices in recent years due to modern trends such as glottalization and access to easily-duplicated information.
The Internet and glottalization has definitely impacted the practices over the years, more and more schools and Universities are requesting that you list the source of you findings. This is mainly, because they are aware of the vast content that is out there and the possibilities that some of this same content may come from disreputable sources. This is also in an effort so that they can be aware of where the students are getting their information and the authenticity of that information. Additionally, they are be able to critique it source, or review it and come up with their own interpretations of it. It also adds value to their material presented by the student; knowing that someone is not just presenting ideas out of thin air but that they are referencing work that has been studied and researched by others who may be more experienced in the field. 

Critically discuss how your own cultural attitudes may impact your transition to an academic environment, in which citing and referencing the sources of ideas will be a vital way for you to maintain your academic integrity.
Given my culture, I think one of my drawbacks is the fact that I may be reluctant to do lots of research and reading. Another thing is that I may have read something somewhere like long time before, however, I may have been reluctant to go back and research the material to support it. Take for instance, months ago I watch a video with touch screen technology. When the question was posed at the previous forum, as it related to possible future changes in technology, I knew my answer. However, this forum force me to basically go back and look for a video to support my thoughts and ideas. It also teaches me that even though my thoughts and ideas may be good, they need to be substantiated through referencing of accredited sources. It is hoped that I can desist from negative practices noted here.

Outline a plan or checklist that you will use to ensure a successful transition to this new environment.
1. Research from accredited sources.
I. Source accredited websites, books and research material. 
II. Use university Liverpool online library.

2. Always cite at least 3 references for discussions. 
I. Use the Harvard (Liverpool) citing and referencing standards.
II. Remember when paraphrasing someone else's ideas make reference to it.
III. Remember to use quotation marks, when referencing direct phrases from others.
IV. If I see something that is related to the topic I am studying, write it down and make a note of its source.
3. Submit my assignment to Turnitin Draft.
I. Check for originality by submitting my assignment to Turnitin Draft before making my final submission.

Explain how using Turnitin throughout this Module will assist in validating the academic originality of your work.
Turnitin would check my work for similarity against an online database. Turnitin allows me the option to submit a draft of my work, check it for originality and make corrections where necessary before making my final submission. The application highlights similarities in my work against the database of online resources, thus I will be able to access where I may have forgotten to cite references in my work. Therefore, it can safeguard me against plagiarism.


References:
UG Vice Chancellor challenges academics to embrace Intellectual Property. Available from:
http://www.kaieteurnewsonline.com/2014/04/04/ug-vice-chancellor-challenges-academics-to-embrace-intellectual-property/ (accessed on September 14, 2014)

Plagiarism - Using the Work of Others as your own Writing. Available from:
http://lgdata.s3-website-us-east-1.amazonaws.com/docs/1531/339169/Plagiarism_and_how_to_avoid_it.pdf (accessed on September 14, 2014)
Referencing. Available from:
http://libguides.liv.ac.uk/content.php?pid=246256&amp;sid=2055177# (accessed on September 14, 2014)

Tips for Avoiding Plagiarism. Available from:
https://elearning.uol.ohecampus.com/bbcswebdav/institution/UKL1/201480_SEPTEMBER/1_Standard_Documents/UoL_TipsforAvoidingPlagiarism.pdf (accessed on September 14, 2014)

Turnitin - Interpretation of the Originality Report. Available from:
https://elearning.uol.ohecampus.com/bbcswebdav/institution/UKL1/201480_SEPTEMBER/1_Standard_Documents/SR_TII_Originality_Report.pdf (accessed on September 14, 2014)

	              
	              Dear Dr. Ayoola,
I think the world should follow the same IP concepts and laws. But I don't think the rest of the world should continue to automatically adopt and apply IP laws derived from the western world.
I think the biggest challenge for the current IP concetps and laws are not from the cultrual difference, but from the pioneers of the indulstries, who are mostly still in the western world. The world has changed from big fish eats small fish to fast fish eats slow fish, the old IP concepts quite often neither protects people or help people. Companies start to realize that protecting their IP isn't doing any good any more. Elon Musk, the CEO of Tesla Motor decided "All Our Patent Are Belong To You" (Musk, 2014).
References:
Elon Musk (2014).&nbsp;All Our Patent Are Belong To You&nbsp;[Online]. Available from:http://www.teslamotors.com/blog/all-our-patent-are-belong-you&nbsp;&nbsp;(Accessed 15 September 2014)
br, Terry
	              
	              Hi Anthony.
I would propose a hybrid approach in answer to that question. In my mind, intellectual property has to have a "gold reference standard" globally. This ensures protection of ones IP across the spectrum of nations worldwide. A case in point is the often volatile pharmaceutical field. A company in the US or UK for instance would spends a lot of time and money on quality research leading to the development of a branded product. However, in the absence of IP laws that would apply worldwide, companies in other regions could easily reverse engineer and come up with generic drugs thus disadvantaging the original company that did the research on it. The patent protection serves to stem that tide.
If developing nations should try to pursue different concepts and definitions of IP, the impact of that would be a loss of standards that would even be detrimental to the nation in question as the converse could also be true, where a developing nation is the originator of an IP that is taken up by developed nations. Disputes arising in such instances would be quite a challenge to resolve given the abscence of a point of convergence between the nations in conflict.
The developing nations however could have the option to adapt provisions of the IP to the domestic circumstance that applies as there may be aspects that do now wholly apply to them. The local laws would still apply if not in particular conflict with the international laws. I need to research further on this but also believe that the international agreements that are ratified do also take into account the ability for domestication of the IPs and level of time frame for implementation in developing nations due to the need to ammend national laws

Joseph
Reference:
TRIPS:&nbsp;AGREEMENT ON TRADE-RELATED ASPECTS OF INTELLECTUAL PROPERTY RIGHTS&nbsp;
http://www.wto.org/english/tratop_e/trips_e/t_agm0_e.htm&nbsp;





	              
	              The cultural attitudes and practice in my local and national community toward Intellectual property can be considered to be diverse.&nbsp; From my own observation and experience the cultural attitudes and practices will differ based on the type of intellectual property, whether music, poetry, spoken word, etc., these attitudes and practices can further be classified into three distinct categories: Persons that know what Intellectual Property is, those that don't know what Intellectual Property is and the ones that pretend not to know.
This is very evident in relations to recorded music. One particular group of individuals will share recorded music without knowing the local laws the protects IP, the artist or producer. Then there are others that will acquire recorded music from an authorized source and pay for it usage. Then the third group that will pretend to be ignorant to the fact and use this recorded music without the rights to it.
&nbsp;
From the group describe above, those who know what Intellectual Property is, will definitely acknowledge its importance and will seek clearly identify their own work as against another person's work. These individuals are usually educated beyond a particular level and/or are producers of intellectual material and would strive to ensure that their works are protected.
&nbsp;
Within the last decade there has been a several specific actions that have been taken to address the cultural norms towards Intellectual Property(IP). The Government has establish the Jamaica Intellectual Property Office in January 2001, as the national IP office, to provide a central focal point for the administration of IP in Jamaica (www.jipo.gov.jm/?q=node/4, n.d.).
Since this time there has been several public campaigns geared towards educating the public about Intellectual Property, on subject matters such as&nbsp; What it is IP?, the law protecting IP in Jamaica, etc. Activities such as symposiums, conferences and public advertisement throughout the local media, have been design to educate the public about IP.
&nbsp;
I have benefited first hand from the development of IP in my community and the public educational campaigns. The cultural norms of some persons in my community will have no impact on my transition to an academic environment, in-fact it will advise my ability to grasp the concepts of citing and referencing. I know what is correct and I will seek to cite and include references in all my submissions to ensure I maintain&nbsp; my academic integrity.
&nbsp;
I'll utilise a checklist to help me maintain my academic integrity, this list will ensure that I:

Include the reference in my notes as I carry out my research.
Use refworks to organize and store all my references
Cross check my references with Harvard (Liverpool) referencing guide
Review the originality report from Turnitin and ensure my work meets the requirements.

&nbsp;
By reviewing the originality report I will be able to identify areas in my work that I may not have cited or referenced properly. Ensure that my citations and references are accurate represented. Be comfortable that my submission meet the requirements of my own work and does not have too much direct quotes.&nbsp; 
&nbsp;
Reference List

Jamaica Intellectual Property Office (n.d.) History and Objectives [online]. Available from http://www.jipo.gov.jm/?q=node/4 (Accessed: September 14, 2014).
Cherry,K. (n.d.) How Attitudes Form, Change and Shape Our Behaviour [online]. Available from: http://psychology.about.com/od/socialpsychology/a/attitudes.htm (Accessed: September 14, 2014).
Tips for Avoiding Plagiarism [online]. Available from: https://elearning.uol.ohecampus.com/bbcswebdav/institution/UKL1/201480_SEPTEMBER/1_Standard_Documents/UoL_TipsforAvoidingPlagiarism.pdf (Accessed: September 14, 2014).
Turnitin - Interpretation of the originality Report [online]. Available from: https://elearning.uol.ohecampus.com/bbcswebdav/institution/UKL1/201480_SEPTEMBER/1_Standard_Documents/SR_TII_Originality_Report.pdf (Accessed: September 14, 2014).


	              
	              Hi Craig,
Thanks Craig.&nbsp;In most aspects your experience and &nbsp;mine is very much alike. For instance, we weren't allowed to copy each other's homework etc, and at exam time our desks where far apart to discourage any copying, honesty and integrity was encouraged. However, it was only at university that I understood that material on google &nbsp;is not "free", someone did their research and posted it to share, however that person still deserves all the credit. I learnt that it's not enough to put quotes on a statement you read somewhere and not mention where you derived it from.&nbsp;
I have been reading a lot for this assignment and came across some very interesting perceptions. In one forum it was said that Intellectual Property law creates innovation in that it protects the inventor's rights, that it encourages people to create and innovate because they know they'll be rewarded and benefit from their idea/invention. I completey agree with this perception because you ought to own an idea, an invention, your words.&nbsp;
I believe that during our academic years these practices are drummed into us as prepararion for the proffessional world and the whole world as a whole. We ought to acknowledge and give credit where its due. I also believe that you are not going to come up with new ideas that noone has ever thought of e.g.&nbsp;&nbsp;Steve Jobs and how he took existing ideas e.g, in the case of the iPod, his idea was based on an existing idea i.e the mp3 player.&nbsp;
With that said, I belive that when we create/innovate and share we empower each other, but we must maintain a sense of integrity by admitting that as great as an idea can be, if that idea is not originally ours we give credit where it is due.
Best regards,
Belinda

References
http://www.debate.org/opinions/should-intellectual-property-rights-be-abolished &nbsp;(online accessed 15 September 2014)
 

	              
	              Hi Joseph,
What you said about our school system particularly the competitive culture and what it focuses is on is profound. There are so many other factors to consider before the academic integrity enforcement route can be taken. However, at the end of the day, we have to catch up with the world and see academic integrity for what it means for us; the betterment of our education system and most importantly of individuals.

Best regards,
Belinda&nbsp;
	              
	              Hi Colleagues,
The World Intellectual Property Organizations(WIPO) is the global forum for IP services, policy, information and cooperation. Established in 1967 as an agency of the United Nations with 187 member states and a sound operational structure.&nbsp; Organized to lead the development of a balance and effective international intellectual property system that enables innovation and creativity for the benefit of all. (www.wipo.int/about-wipo/en/index.html, n.d.)

With such a strong international body already in place developing countries should become members and seek to developed their own IP policies and governance structure from the standards within the conventions of WIPO. Then through dialogue, conferences and other formal channels the WIPO has&nbsp;available, seek to have their specific challenges with Intellectual Property(IP) addressed.

&nbsp;

References:


World Intellectual Property Organization, Who we are [online]. Available from http://www.wipo.int/about-wipo/en/index.html (Accessed: September 15, 2014).

	              
	              Hi Terry
It is not clear to me whether this was meant in the literal sense of the word “cheat”, or whether it was more of an anecdotal reference in adopting a practice of collaboration and cooperation?
For example, I sometimes refer to my own “cheat” sheets – in reality, they’re not functions to enable me to “cheat”, they’re simply short-cuts to an outcome. For example, because I am re-learning how to be a student in the academic world and needing to adopt structures such as the Harvard (Liverpool) citing and reference method, I have created a ‘cheat-sheet’ where I keep a log of some of the citing and examples of bibliographies that I have used (and clarified with the correct guidelines), to have easy and quick access to correct examples.
This in my mind isn’t cheating, but I do refer to it as a “cheat” sheet. 
For me, in its broadest sense, cheating is where someone (or more) undermines the objective, such as an examination whereby you are being tested on your own knowledge and understanding, and that copying from someone else isn’t achieving this objective. However, if the objective is to achieve an outcome that can include collaboration and cooperation, for example team work, then I am all for that. For me,&nbsp;cheating doesn’t mean you cannot work together or seek help in solving problems. &nbsp;
Just on this theme, I can’t actually remember how the phrase ‘cheat sheet’ entered my vocabulary, and now that I am thinking deeper about this, I probably need to come up with a different naming convention given its negative connotations J 
I hope I have interpreted your note correctly. 
Best Wishes, Craig
	              
	              Hi Belinda,&nbsp;
I absolutely agree with your statement "when we create/innovate and share we empower each other, but we must maintain a sense of integrity by admitting that as great as an idea can be, if that idea is not originally ours we give credit where it is due"
Also, referencing your response to Joseph specifically relating to the competitive culture - here in the UK this has definately moved on, in that when I was at school, the culture appeared to be based on 'its the taking part that counts' - I dont remember winning many sports events with this mindset! Now however, the culture is absolutely about 'winning' and not about taking part! Times have moved on in trying to instill a very different mindset in our youth, as the real world has moved on and it is becoming even more competitive out there.&nbsp;
Best Wishes, Craig
	              
	              Hi Craig,
What about cheatsheet? One word.
I would remove the quotes from the word "cheat" here. Being a non-native English speaker, I'm not sure the other people are also comfortable to use the word on themselves:P
I like your definition that cheating is where someone undermines the objective. When the objective is artificial (e.g. defined for a certain type of test/exam) I cannot be convinced that "no cheating" is the rightful thing to follow, not any more. As a student re-entering the academic world after so many years, I think I should view my goals of learning more systematically, not blindly follow any artificial objectives as good boy I used to be any more.
br, Terry
	              
	              Hello Dr. Ayoola,
I think that at this point in time it would be difficult to change the western approach of the IP laws. After all, back in the 19th century the main driver for the Berne convention was to actually standardize such law across several countries (Waelde et al. 2013). I suspect that the overall push and success of IP laws reinforces the idea of an economic gain. Today any country, any individual, wants to produce money, i.e nobody really wants to be poor. The current IP system encourages people to create items and have ideas that can be sold commercially and that are protected for a period of time (albeit sometimes very long period of time), this allows people to invest into their own work and make some money, at least theoretically and assuming you have the winning ideas.
A couple of months ago I went to an exposition about Alexandria (Alexandria the Divine, 2014), in which I was shown that Galen wrote in his accounts about Alexandria that at some point scholars were taking books from the ships arriving to copy them, keeping the original in the famous library, and returning the copy to the owners.&nbsp; Imagine the same thing happening today! But in the cultural context of that time, and because the purpose of the library was to gather all the knowledge of the world, this practice was considered a good thing.
I believe that the main issue is not accepting the notion of IP all over the world, but how you interpret it in different cultural contexts. Seadle (Seadle 2002) gives a good overview of these differences, one that strikes me particularly is that under our western culture people names are facts, but not so for the Nambikwara tribe in Brazil where names are integral part of their personality, and hence, in their culture they would, and should, be subject to IP laws.
That is the real challenge: to maintain a standardized IP law across the globe, but still respect cultural differences where they should be applicable. From a pure law point of view, that should not be too hard, but in a global world, how would someone living in Tasmania know that they cannot publish Nambikwara’s people names without their consent? And how can the Nambikwara people, Brazil and Australian government police such law?
Seadle, goes on to propose some vague solutions, but perhaps he is right on his first phrase of the solutions section: “There are no easy solutions” (Seadle 2002).
Given the fact that there are 195 countries in the world and that the WIPO has 187 member states (World Intellectual Property Organization. no date), we can hardly imagine that developing countries will actually follow different ideals. Of note though, that being member of WIPO does not mean that they are also members of all the treaties that WIPO handles. But as the case of Brazil Nambikwara’s people shows, it is clear that countries must be able to follow different concepts, although adhering to an overall common idea. This is not so different from the democracy government: different states such as republics, confederations, federations, constitutional monarchies, among others have different systems of governments, but they all adhere to the overall common idea of democracy. Because of this I do not think that the majority of countries will necessarily adopt radically different IP laws, save for those that are not inclined to economic grow or competiveness with the rest of the other countries (North Korea might be a good example).
&nbsp;
References
Alexandria the Divine (2012) [Exhibition]. Fondation Martin Bodmer, Geneva, Switzerland. 6 April 2012-31 August 2014.
Seadle, M. (2002) 'Whose Rules? Intellectual Property, Culture, and Indigenous Communities', D-Lib Magazine 8(3), pp. 1 doi: 10.1045/march2002-seadle.
Waelde, C., Laurie, G., Brown, A., Kheria, S. &amp; Cornwell, J. (2013) 'History' in Contemporary Intellectual Property: Law and Policy, 3rd edn, Oxford University Press, pp. 31-33.
World Intellectual Property Organization. (no date) Member States. Available at: http://www.wipo.int/members/en/ (Accessed: 15 September 2014).
&nbsp;

	              
	              Hi Chika,
I get your view that academic integrity is more of a behavioural issue than an issue of one's performance in an academic institution. However I'd like you to consider the view that in today's education system in some countries where survival for the fittest is the daily mantra, academic performance becomes a major driver and incentive for some to do by all means possible, whatever it will take to pass an exam and meet the entrance criteria for high school or university.
I hold the view that if the education system in a country is one that offers a single end of term/semester exam as the performance gauge for a student, the propensity to use all manner of short cuts as a means to an end may influence one's behaviour to cheat. Lack of appropriate systems for checks and balances such as the use of Turnitin as applied by UOL would leave a gap to be exploited by students. When I compare the system of submitting assignments in some universities in my country that do not apply appropriate checks, it would be hard to imagine that some students would not simply plagiarise others work simply because they can get away with it. With exams being an end to itself, incidences of leakage at institution level have been known to happen.
I do agree with you however that behaviour does play a big part, only that what is the influencing factor for that kind of behaviour?
Joseph
	              
	              Hi Augusto.
The issue of maintaining a standardized IP law across the globe is for sure a challenge. From my reading of the transitional provisions that member countries have on TRIPS (Trade Related Aspects of Intellectual Property Rights) referenced from&nbsp;http://www.wto.org/english/tratop_e/trips_e/intel2c_e.htm#transitional, there is the assertion from the above reference that "Developed country Members have had to comply with all of the provisions of the TRIPS Agreement since 1 January 1996".
This gives me the view that &nbsp;member countries have had to take measure towards improved system of governance in administering those IP rights since trade is a fundamental activity for any country. One of the common ways to overcoming these barriers has been the creation of a single source of reference for the administration of intellectual property rights. So in Kenya for instance we have the Kenya Industrial Property Institute that was established in May of 2002 and operates as a government parastatal. With countries empowering such institutes, a measure of accountability can to some extent be achievable as they would be the first port of call in the process of parties seeking Intellectual property rights
Joseph
Reference:
Kenya Industrial Property Institute:-http://www.kipi.go.ke/index.php/about&nbsp;
 Trade Related Aspects of Intellectual Property Rights&nbsp; (TRIPS) -&nbsp;http://www.wto.org/english/tratop_e/trips_e/intel2c_e.htm#transitional 

	              
	              Dear all,
&nbsp;
Have a nice day! There is some additional information about the situation of my region; I am currently living in Hong Kong, and I accomplished my most education here, fortunately, the plagiarism is not usually happened in Hong Kong higher education. However, it is similar with other areas in the world; Hong Kong cannot be avoided exiting the plagiarism allegations. To handle the plagiarism ; according to the Report on investigation into the University of Newcastle’s handling of plagiarism allegations by ICAC report (2005) towards The Hong Kong precedent, it points out: It is important to note that nowhere in the 30 January 2003 email is the word “re-marked” used nor is it stated that “it is not plagiarism”. The relevant parts of it read:
&nbsp;
It appears that the essays that were given zero tended to contain a number of quotes that were strung together (not copied from other students) and some have not cited all sources or not acknowledged/cited them properly. On a quick look through, it appears to me that it is again a case of them not having much of an idea of how to go about referencing properly, rather than copying from another student or a deliberate intention to deceive. Even the assignments that were marked (and received a reasonably good mark) also had some problems with referencing. What we did in Hong Kong was to make the students re-do the assignment which would be only be [sic] given 50% of the mark originally allocated to the assessment. We also sent out letters to the students regarding plagiarism. Do you want the same thing to apply to these students, or do you want the marks to ne [sic] moderated by someone else (and if so by who?) I will await your decision before emailing Professor Marimuthu." 
&nbsp;
&nbsp;
And then there are several methods for moderating marks including:
&nbsp;

The use of reference or scaling tests for statistical moderation
The inspection of samples by post
Inspection by a visiting moderator
External examination
Teacher requested moderation
Group and consensus moderation of internal assessment

&nbsp;
The report also provides certain guidelines to help students to avoid plagiarism: 
&nbsp;

be familiar with the style of acknowledgment that is recommended for use in each of your courses, including the referencing techniques required for information sourced from the internet;
Write the source on any notes or copies you make from any document or electronic sources such as the internet. The habit of copying verbatim from a source as you read is dangerous. It is easy to forget that the notes you make are verbatim and to later write them into an essay or report. Keep details of your sources throughout the course of your research. Unintentional plagiarism is often the result of poor study methods;
sources that must be acknowledged include those containing the concepts, experiments or results from which you have extracted or developed your ideas, even if you put those ideas into your own words;
Always use quotation marks or some other acceptable form of acknowledgement when quoting directly from a work. It is not enough merely to acknowledge the source;
Avoid excessive paraphrasing, even where you acknowledge the source;
Be aware of the rules regarding group work and collaboration. Collaboration (Appropriately acknowledged) is permitted in the case of team or group projects. It is also permitted in the more general case when the collaboration is limited to the discussion of general strategies or help of a general nature. If you have any doubt about what constitutes authorized and unauthorized collaboration, seek advice from your lecturer;
The distinction between what needs to be acknowledged and what is common knowledge is not always clear. As you gain experience you will learn the acceptable practices for acknowledgment in the disciplines in which you study, but while you are learning, always play safe and acknowledge;
Keep a copy of your working papers to assist you in case you ever need to answer an allegation of plagiarism. (ICAC Report (2005); P. 96)

&nbsp;
&nbsp;
BR,
&nbsp;
KingTan Yu
&nbsp;
BR,
&nbsp;
ICAC Report (2005); Report on investigation into the University of Newcastle’s handling of plagiarism allegations; Available on: (http://www.icac.nsw.gov.au/documents/doc_download/3361-report-on-investigation-into-the-university-of-newcastles-handling-of-plagiarism-allegations-operation-orion-30-june-2005%20Report%20on%20investigation%20into%20the%20University%20of%20Newcastle’s%20handling%20of%20plagiarism%20allegations) (Accessed on: 15-Sep-2014)

	              
	              Hi Everyone,Lately, a new licensing model, termed “Creative Commons” (CC)licensing, has been put forward. Instead of the traditional copyright, that grants “all rights reserved”, the new CC model grants “some rights reserved”.“Creative Commons” licenses are not an alternative to copyright. They work alongside copyright laws, and enable authors/creators to modify their own copyright terms to best suit their needs. So, they may decide to give other people the right to share, use, and even build upon work they have created, with or without additional conditions attached, whilst ensuring that the creator (licensor) receives credit for the work.Do you feel that this is the right way to go about securing and managing IP rights? If so, why?Anthony
	              
	              Hi Anthony

I know developing countries are under demand by developed nations for the&nbsp;implementation of intellectual property rights. Whereas, in my country no proper guidelines are provided to students at early stages of education, Hence IP has no firm grounds. The guidelines are only enforced to those students who have research work that has to be presented at international level. 

Developing nations shouldn’t try to pursue different concepts and definitions of IP because developed nations main interest is to&nbsp;protect the innovations in the developing countries from the illegal imitation and copying. 

Furthermore, other cultures or countries choosing to develop their own IP concepts and laws, thereby implementing radically different IP laws can affect cooperation between both parties by causing Higher prices for imported products and new technologies under IPR protection,&nbsp;Loss of economic activity, by the closure of imitative activities,&nbsp;possible abuse of protection by the patent holders, especially large foreign&nbsp;companies.

Best Wishes, Babatunde



Citations;

International Bureau of WIPO, World Intellectual Property Organization (WIPO): General&nbsp;Information, WIPO Pub. No. 400 (published annually)&nbsp;

Braga, C.A.P., Fink, C., Sepulveda, C.P., 1999, Intellectual Property Rights and Economic Development, World Bank, background paper for the World Development report 1999, TechNet&nbsp;Working Paper.
	              
	              Hello Yin,
&nbsp;
I liked your discussion reply and wondered if the practice you refer to, quoting Lu Xun "To Steal a Book Is an Elegant Offence”, relates to a Dutch expression. Freely translated it says: It is better stolen well than something bad that is invented. It tries to convey that humanity is better of with good things that perhaps are not original than bad things who are. 
Also in the Western culture it was common practice to write down others peoples thoughts, opinions and inventions. For example Greek writers were more collectors and translators. It was almost there task to aggregate stories. Probably the invention of print made publications a conveyer of&nbsp; intellectual property. 
Open Source was also an example I used in my discussion: Sorry for not quoting you as you published your article first. 
&nbsp;
Greetings from Bram

	              
	              Hi Augusto
I agree with you that we need law regulations, that we need rules that help preventing misuse and steeling of ideas and solutions. If we don’t have these, I expect people to be very reluctant to develop as you indicate in your sentence “Would you do software to go bankrupt..”. 
You also mention the word “innovation”. This to me is a very important word or process if you like. And I think that much of the development today builds upon inspiration from others, either as the trigger/catalyser or after the initial own idea. So you take inspiration, look at the marked demands, and combine this with your own idea, and then develop your own unique solution. (I hope you understand what I mean). An example could be project management tools, where I think it’s fair to say that there is no sole correct solution. But I’m sure they all get inspired by each other, and continue developing. Another example is the case mentioned in our week 2 “Weekly Overview” where Steve Jobs says that Bill Gates has stolen his idea, and Bill does not deny and gives and explanation from his view. The important thing here is to be honest about the inspiration and give credit.
I would like to tell you a story from my younger days: I remember one time (actually several times) when I told my boss about solution to solve an issue. His response was that it was a bad idea. A few weeks later, he was bold enough to present the same solution, almost with the same words, as if it was his own idea. All my colleagues knew it was my idea, but neither me nor the others commented on it. This was a bit demotivating, but after a few seconds/minutes I ignored that feeling, because all I wanted was to solve the problem. To this day I’m not sure that he was aware that it wasn’t his ideas. It could also be, that the original idea had managed to plant as a seed, and after some time, it flourished to an idea which he really thought being his own (we’ll never know J).
I know it can be hard, but another way to look at this could be to look at the difference between bad intent and just getting inspired. If you have someone using your idea because they “… don’t want to do the legwork” (Vita 2013) then I would say it’s steeling, even if you are referencing to it.
VITA, A., 2013-10-02, 2013-last update, Innovators vs Copycats. Available: http://adriennevita.com/2013/10/innovatiors-vs-copycats/ [9/15/2014, 2014].
&nbsp;Best regards, Bo W. Mogensen
	              
	              Hi Craig,
I agreed that values of honesty, integrity and morality are at heart of not only the UK, but for most European countries and the USA, I would assume that those values are part of western and Christian rooted cultures. But, as most things in this world, moral values are grey shaded. We can see this today with digital piracy. I will take the example of Switzerland since I know it pretty well.
As I mention on my initial posting, here in Switzerland downloading copyrighted material is legal. A research report by the Swiss Federal Department of Justice and Police, deemed not necessary to change the current law to make this practice illegal (Schoonewolff, 2014). When you look at the fact that the WIPO is headquartered in Geneva (coincidentally five minutes from where I live) and that the Berne convention shaped the modern copyright laws, one can wonder why such thing is allowed in Switzerland.   The conducted research found that these downloads were not harming the economy. In the majority of the cases most downloaders ended up buying the actual product or a similar one. For example: 84% of the people that downloaded content also bought similar media content in the preceding 12 months. The Swiss government, hence, makes the bold suggestion to the private companies than rather to seek repressive action, to adapt to the changes that Internet imposes (Violations de droits d'auteur sur Internet: le cadre juridique actuel est suffisant. 2011). That might seem like an odd approach, but take a look at Amazon or Apple and their media online stores and you would realize that some companies are adapting. The reasoning of the Swiss government is more complex than that: it actually protects the rights of the individuals rather than the rights of the enterprises; being a direct democracy country (Federal Department of Foreign Affairs. 2010), this individual protection only makes sense.
Now, where do the moral values come in play? Well, according to the same research 44% of the interviewees did download copyrighted material. Where are their moral values? But if 84% of them actually buy the same type of product, or even the same, should they be considered immoral?
To answer those questions we may need to go into the fields of sociology and psychology, which are not relevant to our discussion, but perhaps we can find some hints by looking at why people download copyrighted material, as it turns out, it is not out of pure evil: a 2010 survey conducted in Australia by news.com.au and CoreData (Top three reasons we choose illegal downloads. 2010) shows that, at least for the TV shows 50.7% of people download them illegally because they do not want to wait too long to see it on TV. These numbers are probably not so different in Europe and USA. 
In today’s connected world where we can instantly buy and download the next version of windows, one can hardly argue with someone willing see a TV show right after it is broadcasted on the other side of the world. This ties back to the Swiss government suggestion to private companies to adapt rather than punish. What the Swiss government is really saying is: make your show available on demand, online, rather than making customers wait weeks or months. The success of companies such as Netflix and Hulu in the USA is telling when it comes down to this debate.
So given the opportunity and a justifiable want, do people become immoral? Or does morals bend to certain contexts?
All in all keeping high moral values will help along the road of IP and plagiarism issues but as we can see, it is not always easy, and certainly not black and white.
&nbsp;
&nbsp;
&nbsp;
References
Schoonewolff, A. (2014) ‘Academic integrity in my cultural context’, CKIT-501,140904,4,Computer Structures.201480-22. University of Liverpool. Unpublished assignment.
Federal Department of Foreign Affairs. (2010) Politics. Available at: http://www.eda.admin.ch/eda/en/home/reps/ocea/vaus/infoch/chpoli.html (Accessed: 12 September 2014).
Top three reasons we choose illegal downloads. (2010). Available at: http://www.news.com.au/technology/why-do-australians-choose-illegal-downloads/story-e6frfro0-1225863649562 (Accessed: 15 September 2014).
Violations de droits d'auteur sur Internet: le cadre juridique actuel est suffisant. (2011). Available at: http://www.ejpd.admin.ch/ejpd/fr/home/aktuell/news/2011/2011-11-30.html (Accessed: September 13 2014).
&nbsp;

	              
	              Hi Bo,
Your story about the “stolen” idea is not so uncommon! I bet several of us have had a similar experience. I know I did too. But I think you should not be that agreeable, even if you want the problem solved. It sure depends on the whole context: sometimes you are willing to propose ideas or solutions for free. But then again, it is more related to company ethics and code of conduct. A part from the basic moral concept of honesty, the implications for a company can be disastrous. Imagine you can actually provide proof of what happened: in many countries you can sue the manager, but that can put the company in a bad situation with the press. Modern HR practices tend to emphasize the importance of company code of conduct and ethics. Where I work, we do a yearly review and online test of the code of conduct and coincidentally last year there was a similar case in the test.
Also consider that your idea could have awarded your manager an extra bonus. Should not be you the one enjoying that free dinner?
I think that bad intent, is always bad intent. If you are able to inspire someone, that someone will recognize that inspiration, after all inspiration is a positive feeling.
I personally have found over the years, that when I want something at work accredited to me (mostly to boost my ego, heh) I write it down, either on an email or on a power point presentation. I do share a lot of ideas and information with colleagues verbally, and mostly I do not care if they use them for the best to improve processes or systems, but for the ones that I truly care, I always take some basic precautions, not because of mistrust, for me it is just a habit.
Best,
Augusto

	              
	              I have a firm believe that you can’t blame the community for not respecting the laws without properly educating them and creating awareness about pros and cons. I am glad to see how you benefited by these public awareness campaigns started by your government. If we dig down we will find IP related laws in almost all communities, but the problem is, governments don’t pay much attention for properly educating people about these laws and also there are flaws in fair implementations.
	              
	              Greetings Augusto,
I enjoyed reading your reply to Craig and would like to add a little if I may.
What caught my attention initially was the reference to Switzerland and the lack of downloading laws as you menition. I actually never knew this so it's interesting to know first of all. Secondly I find there is a difference in these practices (downloading etc) against something like plagiarism. For example the person who downloads a movie or song isnt then specifically claiming it is their movie or song they are simply viewing the material which I think is the line to be drawn. As others have mentioned at different parts of this thread that if a person then claims it is their work or then has some kind of financial gain from it is where morals come in.
Personally I agree with the approach of the Switzerland government but I believe there could be a middle ground between a completely free and relaxed society with no laws and regulations against the opposite where you are so far restricted as to movement being impossible.
Secondly what interested me was where you mentioned the relation in downloads and the then purchased material after the fact.
I completely agree with this and believe it to be completely true for a large percentage. Whenever you hear of statistics from the movie companies and music industry they always talk about lost income from this activity. However they never look at the larger picture and see just how many people bought music or films after they have already seen it via a download and offsetting those gains against the potential losses.

References:
Top three reasons we choose illegal downloads. (2010). Available at: http://www.news.com.au/technology/why-do-australians-choose-illegal-downloads/story-e6frfro0-1225863649562 (Accessed: 15 September 2014).


	              
	              Hi Anthony,
My opinion on this is that developing countries shouldn’t adopt and apply laws on intellectual property derived from the western world. 
For the simple reason that we do not have the same past, we do not have the same history, our cultures are different, the funds are not the same, even the background with problems related to intellectual property are different. 
Developing countries should build their own law based on their own history, vision, experiences and culture, The western world has taken some time to reach the level they currently have, and developing countries should also have time to learn and be able to build their own ways and also learn from their own mistakes. 
It is true that in the current context of globalization, a country cannot exist in isolation from other countries; here is why the role of the West world should be to help these developing countries to achieve their objectives. With experiences they have in that matter, this can be an invaluable asset to support these developing countries in this process, through institutions such as the WIPO (World Intellectual PROPERTY ORGANIZATION).
According to me every country should implement its own law based on his own history and his own culture, the cooperation between these countries should not be a problem, if we implement a space of discussion and exchange into organizations such as WIPO.
References:
WIPO, World Intellectual PROPERTY ORGANIZATION [Online]. Available from: http://www.wipo.int/portal/en/index.html (Accessed 13 September 2014)

	              
	              Creative Commons (CC) is a non-profit organization headquartered in Mountain View, California, United States, devoted to promoting creative works to be made available for others to build upon legally and to share.CC as released several copyright-licenses known as Creative Commons licenses free of charge to the general public.These licenses allow creators to communicate which rights they reserve, and which rights they waive for the benefit of recipients or other creators. Creative Commons licenses do not replace copyright, but are based upon it. They replace individual negotiations for specific rights between copyright owner (licensor) and licensee, which are necessary under an "all rights reserved" copyright management, with a "some rights reserved" management employing standardized licenses for re-use cases where no commercial compensation is sought by the copyright owner. The result is an agile, low-overhead and low-cost copyright-management regime, profiting both copyright owners and licensees. In my opinion if there are creative ways to bring negotiated control over copyrighting (a subset of IP), we are moving in the right direction. I am all for it.
References
http://en.wikipedia.org/wiki/Creative_Commons_license
http://en.wikipedia.org/wiki/Creative_Commons&nbsp;
	              
	              
Dr. Anthony.
I think this is an excellent idea, in relation to software development. We see this happening, with open source software or free ware that allows you to use the software free of cost and or permits the editing and customization of existing code to suit the your needs. Organisations such as Oracle, with certain license agreement, allows organisations to customize certain aspects of the software code, however you may have to pay an additional fee for this. The code and the application is already Oracle and the credits go to them, however customization is allowed.

I think generally its a good idea for development, you might develop an application but someone else may have a better idea of how to enhance it. The credit can still go to you but the application would go to the updater as well.

A similar environment may exist in an organisation where you may have several programmers or just programmer who has develop an application. That person is already accredited for developing the application. However, a few years later, their I a flaw is discovered within the application or improvements need to be made to the application and that person may no longer be there to do this adjustment. Thus another developer or developers, would be required to make these changes. These individuals can now be accredited for the updated version of application and the original developer or developers would lose no accreditation. 

I believe this concept has to do with the sheer recognition that no one person may have the best of ideas, thus if something can be improved upon, why not improve it. It also show that we are becoming more liberal and more welcome to the concept of knowledge and idea sharing. This approach can also encourage faster development.

So for me Creative Commons for development is a excellent idea.

References:
Oracle® Fusion Middleware. Available from:
http://docs.oracle.com/cd/E28280_01/doc.1111/e14860/title.htm (accessed on September 15, 2014)
Oracle Endeca Commerce . Third-Party Software Usage and Licenses . January 2014. Available from:
http://docs.oracle.com/cd/E51272_02/Common.110/pdf/Licensing.pdf (accessed on September 15, 2014)

	              
	              Hi Guys,
Just wanted to share my experience about using data from professional research authorities like Gartner. When I use any information from a study or published in a public report I ensure that I reference said publication or report. This is not only to give credit to the publishers but it also provides additional information to my intended audience so they can do further reading subject I presented or any other related material they wish to research.
Craig mention earlier about how classroom where arranged to limit the possibility of cheating. In my school experience it was also to ensure that examiners/invigilators could easily observe the room and identify anyone attempting to cheat or share information. Inspite of this setup some student made it a point of duty to prove that they could cheat and not get caught, beating the system inplace. This went against all academic practices and the culture of honesty, integrity and good morals that was being taught/encouraged.
On the notion of a cheatsheet, another name will definitely be necessary. I use the same type system by noting all good tips , examples and procedures that could help my academic work. I call it my bag of goodies, there you go "goodie bag", can I patent that?

Regards,
R.Biggs&nbsp;
	              
	              Thanks for the comments Numan. One of the way developing countries can ensure there IP are well protected is establishing an agency of the state that will act as the entity with responsibility for all matters IP related. There is a lot of support available for developing countries, they only to look.
	              
	              


Dr. Anthony,
I believe that given the fact that the Western world would have already researched and studied the area of Intellectual Property (IP) provides good enough reason why the developing countries should possible adapt to these standards. However, a series of questions may arise. Do they have the necessary resources and finances to try to come up with new standards? Also some nations may not necessarily have what it takes to fully adopt to the Western standards even if they wanted to. Which leads us back to the economics and resources available to these nations. 
For example, if my country were to enforce a total ban on photocopying of all text books. Can the average student afford to buy a text book? Do we have a library in almost every community where our students can visit and get the learning materials they need? If we were to consider the Internet as alternative source to provide information for students. How many of these students have a computer in their home? How many can afford the Internet? Do we have a sufficient database of accredited information that these students can turn to get the materials they need? 
The Western nation on the other hand, may have already provided answers to these question and may have the appropriate systems in place. Because they have the money and they have the resources that they need. Thus, the question of adaptability would depend on the exact nation in question, their culture and resources available to them. For me a good idea would be to just take things in same bites, one law at a time, one policy at a time.
If we choose to develop our own IP concepts and laws, we will have to still learn that which exists in the Western World and in some cases still adapt to it. It would all depend on what we are doing. Take for instance if I were a novelist in my country, I would not want my book to be confined to my country alone, because, if I am using my book to make a living, then I may as well starve. What sections of the population is going to buy my novel? What section can afford it? If there is no IP to protect it, would I want it to be copied off to anyone? Additionally, when we acquire materials, books for example, originating in the western world, we would still be obligated to withholding the laws and or IP surrounding these materials, even if it such laws do not exist locally. 

References:
Textbook piracy controversy…President urges review of copyright laws at UN forum. Available at:http://www.kaieteurnewsonline.com/2012/09/28/textbook-piracy-controversypresident-urges-review-of-copyright-laws-at-un-forum/ (accessed on September 15, 2014)
Pirated textbooks no longer welcome in Guyana schools. Available at:http://www.caribbean360.com/news/guyana_news/pirated-textbooks-no-longer-welcome-in-guyana-schools (accessed on September 15, 2014)


	              
	              
Tresor&nbsp;,
While your concepts may sound interesting. Unfortunately these countries will still have to respect the laws of the western world as long as they are consuming what belongs to them. 
Governments cannot get up one morning and say to themselves,
“Hey, we are going to get pirated text books for our schools, because we can't afford to buy original copies.” 
What happens when the country of it origin discovers what it doing? Likewise an organisation can't decide, 
“Let get some pirated Microsoft Applications”. What happens when Microsoft tracks them down? 
Therefore, some way or the other, whether or not they want to have their own laws or policies, they will still have to respect and adhere to that of another man's country.

Textbook piracy controversy…President urges review of copyright laws at UN forum. Available at:http://www.kaieteurnewsonline.com/2012/09/28/textbook-piracy-controversypresident-urges-review-of-copyright-laws-at-un-forum/ (accessed on September 15, 2014)
Pirated textbooks no longer welcome in Guyana schools. Available at:http://www.caribbean360.com/news/guyana_news/pirated-textbooks-no-longer-welcome-in-guyana-schools (accessed on September 15, 2014)

	              
	              
Tresor&nbsp;,
While your concepts may sound interesting. Unfortunately these countries will still have to respect the laws of the western world as long as they are consuming what belongs to them. 
Governments cannot get up one morning and say to themselves,
“Hey, we are going to get pirated text books for our schools because we can't afford to buy original copies.” 
What happens when the country of it origin discovers what it doing? Likewise an organisation can't decide, 
“Let get some pirated Microsoft Applications”. What happens when Microsoft tracks them down? 
Therefore, some way or the other, whether or not they want to have their own laws or policies, they will still have to respect and adhere to that of another man's country.
References:
Textbook piracy controversy…President urges review of copyright laws at UN forum. Available at:http://www.kaieteurnewsonline.com/2012/09/28/textbook-piracy-controversypresident-urges-review-of-copyright-laws-at-un-forum/ (accessed on September 15, 2014)
Pirated textbooks no longer welcome in Guyana schools. Available at:http://www.caribbean360.com/news/guyana_news/pirated-textbooks-no-longer-welcome-in-guyana-schools (accessed on September 15, 2014)

	              
	              Numan.
I am in agreement with you on the lack of sensitization and awareness amongst the populace and I believe that one of the ways in ensuring this gets done is to incoroporate IP topics as part of the school curriculum from an early age.

Joseph
	              
	              Hi Anthony,
&nbsp;I think that having different laws across different cultures will only heighten the inconsistences and double standards that are already there. I imagine some of the issues are a result of the changing times and what worked in 1973 will not necessarily work in 2014 and thus I think it’s best to constantly review the laws and make adjustments accordingly and let that be a world standard.
Best regards,
Belinda&nbsp;
	              
	              Hi Chika,
I agree with your statement that "&nbsp;..Academic Integrity is a more of a behavioural issue than an issue of one’s performance in an academic institution." However, as Joseph stated, in some countries it really is about survival of the fittest, whatever is done good or bad is justified as a means to an end. This is not to say, the situation should remain like that. I strongly believe that we all &nbsp;ought to strive for a day when academic integrity is a norm across the globe.
Best regards,
Belinda&nbsp;
	              
	              Hi Ricardo,
"Goodie bag" I like that.
Best regards,
Belinda&nbsp;
	              
	              Hello Joseph and Belinda,
Thanks for your comments, and I couldn't really agree more with you guys. As easy at it may seem, it makes sense to say there are some people that are just not susceptible to change except when it is enforced, that way infractions can be curbed or completely eradicated.&nbsp;
Great!
	              
	              Hello Anthony,
In my opinion, IP Laws can never be cast in stone, owing to the fact that it's an area where you have different rights covering different things/laws, but they all somewhat....somehow overlap one way or the other. Let's take the Patent law for example.....the law that protects inventions....what I would call "creativity". Like some school of thought believe that creativity is not magic but simply happens by applying ordinary tools of thought to existing materials. The keyword here is ëxisting materials" and ördinary tools of thoughts".
The truth is, no one starts out original, hence the importance of copying to build the foundation of knowledge and understanding. So to answer the question as to whether developing nations should try to pursue different concepts and definitions of IP that best suit their own develipmental needs.....I would say not entirely, because there's no re-inventing the wheel here...as the wheel already exists. Modifications can be made to best suit the different cultural differences between i.e. the western world and West Africa, but there always has to be a governing baseline/foundation.
Thanks
Chika&nbsp;


	              
	              Hi Anthony&nbsp;
Ever since 1990, when my organisation started to move more to open system technologies (UNIX) and distributed platforms, immediately we could realise the benefits of the openness and platform interconnect capabilities, as apposed to the traditional proprietary type technologies such as mainframe that held restrictions relative to their interoperability with certain other technologies.
As we have progressed over the years, we have seen tremendous steps forward in increasing this openness, initially with open systems and then with open source technologies such as LINUX. I have been somewhat removed from the detail of the technology itself over the years, but the principles have stayed and have very much formed the basis for developing our IT strategy relative to technology roadmaps.
I am an advocate of sharing information on a global level and exploiting the use and benefit of open source technologies. As such, I am supportive of the Creative Commons licensing model for Intellectual Property, and the promotion of the use of and sharing of information. This licensing model is useful in that it helps remove, as you (the creator) choose, some of the more restrictive default Copyright laws.
Creative Commons licenses are however not appropriate for computer software due, amongst other things, to compatibility issues in operating alongside major software licenses. Creative Commons recommend that you consider licenses offered by the Free Software Foundation or the Open Source Initiative.
To give some background, Creative Commons was founded back in 2001 and released their version 1 licenses during 2002. According to the Creative Commons (n.d.) website, they now provide six different types of free license that give you the ability to easily change your default Copyright terms of “all rights reserved” to “some rights reserved”, and these licenses work alongside Copyright terms to best suit your needs.
The six licenses have variable options and range from giving attribution, which is essentially scope to distribute, change, and even use commercially, as long as credit is given for the original version, through to the most restrictive license, which is attribution, download and share only with credit given, but no commercial and no derivative rights.
This is great for giving you the flexibility to choose, based on the types of license you apply, and how you want to share your information.
This method is particularly effective in the digital era and information age. According to Solon (2011), Lawrence Lessig, who started Creative Commons, was concerned that default Copyright laws restricted creativity in the digital environment, by preventing people from accessing, remixing and distributing Copyright material online. Creative Commons Copyright licenses allow the creators to determine which rights they reserve and which rights they would like to waive for the benefit of other users.
According to Wu (n.d. cited in Northcutt, n.d.) a Columbia University law professor, there are two different visions of what might best promote authorship, with contrasting opinions between the likes of Silicon Valley and Hollywood, in that one side promotes the culture of authorial exposure, and the other urges authorial control.
In the same article, Anderson (n.d. cited in Northcutt, n.d.) who authored the book ‘The Long Tail’, points out that these two major perspectives change how creators feel about Copyright and Intellectual Property depending on which group they fall into. The first group is highly commercialised, and include those who want to fiercely protect their Copyright and Intellectual Property. This group would, to name just a few, include major studios, labels, book publishers, and also the likes of global pharmaceutical corporations (and as referenced in one of my colleagues earlier posts as part of DQ for this week). The second group are the less commercialised, who share a passion for information and content sharing, and are relaxed about sacrificing certain aspects of Copyright and Intellectual Property.
In conclusion, I see no reason why the Creative Commons licensing model shouldn’t be adopted on a wider level, especially by those creators in the second group in helping facilitate and allow the sharing and adaption of content, and with those in the first group continuing to adopt the more restricted terms under the traditional Copyright license.&nbsp;I believe the Creative Commons licensing model can continue to run alongside traditional Copyright and Intellectual Property. As the model matures, perhaps it could be developed to include other creations such as software licensing.
Best Wishes, Craig
References
Creative Commons. (n.d.). About The Licenses [Online]. Creative Commons website. Available from: https://creativecommons.org/licenses/ (Accessed 15 September 2014)
Solon, O. (2011). Creative Commons 101: An introduction to CC licenses [Online]. Wired. Available from: http://www.wired.co.uk/news/archive/2011-12/13/creative-commons-101 (Accessed 15 September 2014)
Northcutt, S. (n.d.). Creative Commons and Intellectual Property [Online]. SANS Institute. Available from: http://www.sans.edu/research/leadership-laboratory/article/ip-creative-commons (Accessed 16 September 2014)
	              
	              Hi Ricardo,&nbsp;
Brilliant!&nbsp;
Also, just a thought... I wonder whether any of those individuals who were creative in attempts to 'cheat' in school have gone on to 'cheating' careers - I'm thinking of Professional Hackers (rather than 'cat burglars'!) or Security Consultants, who are hired by large corporations to attempt to break their securities, firewalls and computer systems?
Best Wishes, Craig
	              
	              I believe creative commons was designed to save the world from failed sharing. It allows people to share stuff on the web or wherever based on certain terms. And to answer your question, I believe it gives the creator to exercise their copyrights in more ways, and probably in more simpler terms.&nbsp;
Chika
	              
	              Hi Augusto,
As with Chris, I too enjoyed reading your reply. Some very interesting perspectives that I wasnt especially aware of.
I think it is good to drive different attitudes and I admire the protection of individuals rather than corporations, especially as we move through the Digital era. However, relative to the report that suggested the economy was not impacted (and concluded there is no need to change the law), aren't these laws in place to protect the investments (economies) of the creators primarily, and not necessarily to protect the economies of countries?&nbsp;
I guess you have to question Switzerland's tendency to take a neutral position on certain issues as above, and especially when it concerns ethics and morality.
Perhaps as attitudes change, restrictions will be relaxed and corporations will increase the adoption of licensing models much more along the lines of Creative Commons? As you indicated, many corporations such as Apple are adapting, and as an example third party developers are able to use API's in building and developing on the new technologies but ultimatley, this comes back to commercialisation of corporations products and if they're not able to drive commercial advantage as a result, then I suspect they're going to be reluctant to evolve. The message that a large proportion of people subsequently 'purchase' downloadable content can drive a change in attitudes, but in the meantime this shouldnt mean the practice is right.&nbsp;
Just taking this a little further, wouldn't it be nice to see more openness towards "developing countries" in the practices for downloading books, music and software for example for the purposes of learning and development. We have technologies which can help enforce (in the main) the use of the material by only the individuals in these countries, we just need more of a philanthropic mindset (similarly to some pharamceutical businesses with medicines, and in conjunction with the World Health Organisation) from our large global corporations to help make it happen. I would imagine, in some respects, that this is already happening but perhaps it is not widely known?
Best Wishes, Craig
	              
	              Hi Anthony,
In a world which wants to be free, politically, economically and culturally furthermore with the current context of globalization, such licenses are really appreciated.
Par definition “A Creative Commons (CC) license is one of several public copyright license that enable the free distribution of an otherwise copyrighted work. A CC license is used when an author wants to give people the right to share, use, and build upon a work that they have created. CC provides an author flexibility (for example, they might choose to allow only non-commercial uses of their own work) and protects the people who use or redistribute an author's work from concerns of copyright infringement as long as they abide by the conditions that are specified in the license by which the author distributes the work.”&nbsp; (2014, wikipedia)
Based on this definition, I think CC licenses are much more flexible and best suited to the current world‘s configuration. “Application of a Creative Commons license may not modify the rights allowed by fair use or fair dealing or exert restrictions which violate copyright exceptions. Furthermore, Creative Commons licenses are non-exclusive and non-revocable. Any work or copies of the work obtained under a Creative Commons license may continue to be used under that license. “(2014, wikipedia)
Thus a work can be used freely by anyone, but its need to be attributed to the author by citing his name. (2014, wikipedia)
This is an advantage for other users and the author himself because his work is popularized and continues to enjoy all the benefits of his work.
Another advantage is facilitating cultural exchanges.
In conclusion this is a great tool for managing and securing intellectual property. As they aim to improve accessibility and to promote the sharing of creative works. (2014, alai)
Reference:
http://en.wikipedia.org/wiki/Creative_Commons_license
http://fr.wikipedia.org/wiki/Licence_Creative_Commons
http://www.alai.org/assets/files/resolutions/licence-creative-commons.pdf

	              
	              Hi Bram,
I guess it has that meaning in some deep level, but the literal meaning of the original sentense is "Stealing a book does not count as stealing". The logic behind this is that only the educated people would want to steal a book, and educated people are supposed to be respected in old China. The guy in the story was trying to be cute when caught upon stealing books.
br, Terry
	              
	              Hi Tanisha,

It's interesting that you mentioned schools because in China they do just that.

Take a look at this : http://www.slate.com/articles/news_and_politics/explainer/2009/10/bootleg_nation.html
"90 percent of the DVDs distributed in China are unauthorized copies." or so says Christopher Beam from the article.
I remember watching a program called Top Gear where the presenters went to china and discovered that there are masses of cloned factories churning out idential copies of motor vehicles, even to the point that they used the same moulds. They were literally identical cars but just because they then named them slightly different the Chinese government would reject any claims against them for copying.
To find out more look at these pages: http://www.topgear.com.ph/features/lifestyle/human-interest/10-most-audacious-chinese-made-knockoffs.
Fake Rolls Royce: http://www.youtube.com/watch?v=6rY3yNf-W3c
Interesting clip: http://www.youtube.com/watch?v=Q2aUscJ7LWQ
So the world faces a difficult task of enforcing anything like this if individual countries don't take responsibility first for their own actions. So if one of the largest nations in the world simply clones whatever they like and nobody can do anything about it, is there really a future for a world wide acceptance of intellectualy property laws.?


Reference:
Christopher Beam- How strict are Chinese copyright laws? [http://www.slate.com/articles/news_and_politics/explainer/2009/10/bootleg_nation.html] (Accessed 16 Sept 2014)
NihonMotoSan - China's copy culture [http://www.youtube.com/watch?v=Q2aUscJ7LWQ] (Accessed 16 Sept 2014)
room101doublethink - Fake Rolls Royce [http://www.youtube.com/watch?v=6rY3yNf-W3c] (Accessed 16 Sept 2014)



“Hey, we are going to get pirated text books for our schools because we can't afford to buy original copies.” 





	              
	              
	              
	              Hi Dr. Ayoola,
I think that Creative Commons is a great idea and it complements the current copyright law. I have been using it related to pictures on Flickr. In my free time I help a non-profit organization with their scientific abstract submission system (online) for their international lymphoma congress, because they are non-profit I need to find ways to keep things cheap and when I decide to use images on the abstract submission system Flickr in combination with the Creative Commons license is a good compromise as it give me the monetary flexibility while crediting the author work.
I believe this system gives 2 advantages:

More control to the authors over their work.
Specific rules on how to use the content published by the authors.

In a sense, Creative Commons is the “citation method” of the internet. The intent is to promote a more flexible digital creativity than the standard copyright laws. It does not replaces copyright, it works in conjunction of the current copyright laws, in fact the CC tools are intended to be used by the holder of the copyright of the material (Creative Commons. no date).
&nbsp;
References
Creative Commons. (no date) Frequently Asked Questions. Available at: https://wiki.creativecommons.org/FAQ#How_do_CC_licenses_operate.3F (Accessed: 16 September 2014).
&nbsp;

	              
	              Hi Christopher
I remember seeing that show too. I especially remember seeing a BMW X series (I think it was X3) but it was branded by a Chinese automative company. I recall it was almost, if not completely identical.
This is a good example of a nation&nbsp;appearing to undermine their membership to the World Intellectual Property Organisation conventions and pay lip service to the protection of Copyright and Intellectual Property.
Best Wishes, Craig
	              
	              Hi Everyone,Easy access to vast amounts of data (e.g. via the internet) ostensibly cheapens or reduces the perceived value of the information. We therefore often tend to underestimate the effort involved in generating the information in the first place. The cheapening effect thus increases the likelihood of plagiarism by people accessing the information.Do these sentiments resonate with you, or do you have alternate views on this?Anthony
	              
	              Hi&nbsp;Christopher,
Being a Chinese from mainland China, I'm not especially pride of the situation like you've pointed out. I've certainly seen more than what you've shared by my own eyes (I don't mean DVDs:P). Some of them are more ... hilarious. I don't like that fact that people get offended by these copies, and I don't like the fact that the government has been encouraging/allowing this from happening.
My understanding is China is trying very hard to be an international player. But on the other hand China would definitely want to play with their own rules rather than just follow.
"is there really a future for&nbsp;a world wide acceptance of intellectualy property laws?"
I don't know. But I love to see that there's some player that is not following the rule. I don't think China can shut the world out or the world shut China out, so this will lead to some very interesting result in the future.
Please don't get me wrong:-) I love the idea of jiggling.
br, Terry
	              
	              Hi Craig,
Can we agree that we disagree with each other on the usage of the term "cheat" and words derived from it?
By the way, I don't see the link between somebody who cheat at school and a hacker. I guess "hacker" is something we should not miss when studying computer history:http://en.wikipedia.org/wiki/Hacker_(programmer_subculture)
br, Terry
	              
	              Hi Anthony.
As the discussion on IP rights gets deeper, it's becoming apparent to me that the whole IP rights area is much broader than I had initial thoughts on. Context therefore would be decisive in answering the question that you have posed and the Creative Commons option may not necessarily be the "silver bullet", one size fits all solution. 
The Creative Commons (CC) licensing in my opinion is applicable where the author is generally willing to allow the use of their creative works by others and does not envision loss of competitive advantage or wilful gain by the other party, hence is comfortable being given the credit that is due for the original work. The issue of commercial gain from copyright infringement I believe is the bigger challenge and often drives the need to secure IP rights. Many of the case references that I have encountered are based on the notion of infringement of IP rights where the offending party is accused of having benefited commercially. 
An interesting case by example is that of the Associated Press Vs Shephard Fairly, covered extensively in the Harvard Journal of Law &amp; Technology, Volume 25, Number 2 Spring 2012, Reflections On The Hope Poster Case. http://jolt.law.harvard.edu/articles/pdf/v25/25HarvJLTech243.pdf .
In this account, Shepard Fairey is sued by the Associated Press for using one of its photographs without their permission in what came to be known as the Hope Poster Case.&nbsp;
References:
Harvard Journal of Law &amp; Technology, Volume 25, Number 2 Spring 2012, Reflections On The Hope Poster Case. 
http://jolt.law.harvard.edu/articles/pdf/v25/25HarvJLTech243.pdf .

	              
	              Hi Anthony,
I've taken part in create CC licensed materials, like this one (the slogans):http://www.odd-e.com/material/2013/11_agile_singapore/odd-e-sticker.pdf
I've also used CC licensed materials, like the background music of this video:http://www.youtube.com/watch?v=E6WuKk9MFvA
The author of the music was happy with the 50$ we paid him for using his music.
The reason I chose the CC licensing is:

We are eager to share
We want to become better rather than just get paid
We want to encourage the other people to do it as well
The license is much more easier to be understood than the other free license like those popular open source software license.
It's especially clear on the rights you give up.

So I would definitely do more of this. If I'm going to open source some new software, I will very like to use the CC 1.0 universal license because it's much shorter than GPL, MIT and Apache2 license, which have been dominating the open source software world.
	              
	              Hi Anthony,
I've taken part in creating CC licensed materials, like this one (the slogans):http://www.odd-e.com/material/2013/11_agile_singapore/odd-e-sticker.pdf
I've also used CC licensed materials, like the background music of this video:http://www.youtube.com/watch?v=E6WuKk9MFvA
The author of the music was happy with the 50$ we paid him for using his music.
The reason I chose the CC licensing is:

We are eager to share
We want to become better rather than just get paid
We want to encourage the other people to do it as well
The license is much more easier to be understood than the other free license like those popular open source software licenses.
It's especially clear on the rights you give up.

So I would definitely do more of this. If I'm going to open source some new software, I will probably use the "CC 1.0 universal" license because it's much shorter than the GPL, MIT, Apache2 and BSD license, which have been dominating the open source software world. But on the other hand, I don't think this licensing can fit all the needs. My understanding is that it's good at "giving up" rights. But to preserve the rights, I would still trust something that is more completed than this.
I also have a question: can I put a "CC 1.0 universal" license on my assignments of this course?
br, Terry

	              
	              Thank you Terry for the context.
Bram
	              
	              Hi Terry,&nbsp;
Yes, that could work ...
However, I am not so sure that a test/exam is artificial? For example, examinations from school provide an outcome which is worth something, such as appropriate scores to enable qualification to a higher education school, or a driving test as another example, this provides a certificate to enable you apply for a license to legally drive a car, perhaps the Shuanghuan SCEO ;-)
I think that if there is value to an outcome, then surely "cheating" to get this outcome is illegitimate and therefore&nbsp;not rightful?
I do agree with your last point of perhaps learning systematically, I am certainly adopting a similar style.&nbsp;
Best Wishes, Craig
	              
	              Hi Terry,
I am not entirely sure whether we disagree or not at this stage. I dont think we've presented all of our closing arguments as yet, but I'm happy to move on anyway :-)
Relative to my link with a hacker, yes admittedly my link was a fairly loose one... I was simply following up on Ricardo's point that some students wanted to make a point to prove they could cheat and not get caught and beat the system in place. Isn't this, to some extent, what hackers do and as such, aren't their skills therefore potentially marketable to technology companies, and for them to demonstrate their system securities one way or another i.e. to beat or not to beat the system?
Best Wishes, Craig
	              
	              Interesting remark Kharavela,
Forcing people in using one standard is indeed not a good idea. Standards should make life more easy and improve interoperability. This does not mean that when there is a standard everybody has to use it for the rest of eternity. Standards convey an idea but are not the idea itself and if another local standards works better this is of course preferable.&nbsp;
Greetings from Bram

	              
	              Hi Anthony,


I agree with your view in the sense that the access to knowledge has become cheap because of the advent of the internet with people being able to upload and download information with ease despite the benefit of easy access to knowledge via the internet; it has trivialized the great effort put in to acquire such knowledge.

These recent days, knowledge seekers like students go through the internet to get information rather than going through the methods of acquiring such as carefully designed controlled experimentation and reason/logic.

	              
	              Hi Anthony,
No, not at all.
Let's define "likelihood" first. Of course a busy shop is more "likely" to have shoplifting than a shop that has no visitor at all. So, I think here "likelihood" means the possibility of plagiarism of a person who would not do it without so easy access to abundent data.
Memorizing and being able to find information are no difference in most cases in a modern world. So in that sense, mastering the modern tool is an important skill.
On the other hand, easy to access information doesn't usually help the study. Nowadays, most people get their information from social media. But how much truth can you get from there? I believe a quality academic study should be from the research much deeper than the very simple instant-food style googling, any way.
Do we tend to underestimate the value of information because of the easy access. I think yes. The price is defined by the market. When there's more supply the price will go down. But it doesn't mean people won't pay. Now they have more choices, they will tend to choose cheaper ones. When my google search led me to some paper that I need to pay to read, I would usually resort to some other means like via StackOverflow.com. What about the quality of the information? Well, I'm not sure about the acdemic papers. But I know the popular open source softwares are usually better than propertary softwares. I know the information provided by people who just want to share because of their passion are usually better than those who has a different purpose of creating that information.
Does it mean people would grab the information and take it as if they created it? I guess it has nothing to do with the easiness of access. I've been a victim of Internet plagiarism myself, several times. Like my blog (in Chinese) has been copied without reference by some large (Chinese) website. I'm also a contributor to Wikipedia, and it felt really bad when I found my content was copied shamelessly by the wiki of baidu.com. I bet when people don't have pride (or have no free wifi) things will just become ugly.
The easy access to information has only made me appreciate more and wanting to contribute back. I would take this opportunities to send my best regards to the people who contributed to the software that follows these commands:
   sudo apt-get install
   pip install
   gem install
   npm install
   bower install
   brew install

br, Terry
	              
	              I do agree with the sentiment expressed which I believe in a social context, is a reflection of the economic principle of the law of diminishing returns.
The fact that there is an abundance of data on the internet diminishes the overall value of the pieces associated with it and the effort it took to get it there. To a large extent, the internet has become a commodity that is there for the taking, and would require a high level of integrity and awareness to operate it without the thought of plagiarism.
The analogy to this is the large and widespread use of electricity in homes and industry. Most people would not think twice or marvel anymore at clicking on a switch to light up a room. The internet with its almost bottomless reservoir of data, is to many, just another commodity for use and application as one sees fit. Adam Smith in his book “The Wealth of Nations”, talks about the concept of division of labour and states in chapter V that “The real price of every thing, what every thing really costs to the man who wants to acquire it, is the toil and trouble of acquiring it.”
The internet makes available, data sourced from the labour of many individuals and renders it with minimal effort to the one who seeks it. Because of this limited effort in acquiring this data and the fact that in many instances, surfers are content with general information about an item they are searching for, the tendency to ignore the level of effort in producing that data is only too real.
References:
Britannica - Academic Edition -Law of diminishing returns: &nbsp;http://www.britannica.com/EBchecked/topic/163723/diminishing-returns  &nbsp;
Adam Smith, The Wealth of Nations; Chapter V &nbsp;- “Of The Real And Nominal Price Of Commodities, Or Of Their Price In Labour, And Their Price In Money”

	              
	              Hi Craig,
I will still reply your another reply:) But probably do it tomorrow since it 1am already.&nbsp;I bet the language and culture play some very tricky role in our discussion. Now I realize how visionary @Anthony's topic is.
Yes, I found you manage to link everything together. Thanks:-) I would still add that the hacker spirit is a lot more than that. The Cathedral and the Bazaar,&nbsp;highly recommended to anybody who haven't read it before.&nbsp;
br, Terry
	              
	              Hi Terry,&nbsp;
Sure.
Yes, Anthony is throwing out some easy questions isn't he! I think he's a Big Fast Fish!
I'm enjoying our debate, catch you tomorrow. Cheers Terry
Best Wishes, Craig
	              
	              Hi Anthony&nbsp;
I think there are probably two lines of consideration for this - 'plagiarism knowingly' or 'intentional plagiarism' and then 'plagiarism unknowingly' or 'unintentional plagiarism'.
We have seen many DQ responses this week that have either alluded to, or been specific towards referencing a lack of structure and education surrounding plagiarism. Of course, this doesn’t make plagiarism right, but equally it should be acknowledged as unintentional – 'plagiarism unknowingly'. I do feel that it also cheapens the true value of the information or the creation, as it is clear that the cost and effort to research, develop and produce something are avoided. I sense the impact of this is potentially not as considerable as 'plagiarism knowlingly' or 'intentional plagiarism', but this is dependent on the specific case in question.&nbsp;I guess for me the overriding factor, albeit not acceptable, is that there is an implied lack of awareness in the first place. I am also yet to be convinced that this lack of awareness then suggests an underestimation of the efforts required to generate the information or creation in the first place. I think most people are able to see the amount of effort that is put into something.
Then there is the 'plagiarism knowlingly', or 'intentional plagiarism'. I am of the opinion that this absolutely cheapens the value of the information or the creation to the original creator, as the cost and effort to research, develop and produce something are intentionally and knowingly avoided through plagiarism. This has a major factor on the original creator as they then are negatively impacted on recovering their investments as there are many plagiarised versions available elsewhere.&nbsp;
I wonder how many "Louis Vuitton" handbags there are in Liverpool, and what proportion of these are original copyrighted creations? The 'copies' would not, in my opinion, be classified as items of 'plagiarism unknowlingly' or 'unintentional plagiarism'.
Best Wishes, Craig
	              
	              Hi Craig

I like your normal ways of referencing and citation, it provides additional information that makes me always want to go through them forgetting I am supposed to reply to the questions asked….lol

	              
	              Hi Anthony,

Having heard about it moved me to visit the Creative Commons site (1)

Creative Commons is more of an invitation than a prohibition. It sets out what I regard as acceptable use of one’s personal intellectual property. If you can adhere to the requests of others regarding licensing, you can only hope that others do the same to you.

Any kind of licensing or copyright has similar enforcement difficulties unless you have a team of lawyers.


References

Creative Commons. (1) [Online]. Creative Commons website. Available from: https://creativecommons.org/

	              
	              Hi Augusto
You are absolutely right, and I have also changed myself a lot in that regard, and as you state “bad intent, is always bad intent”. And in practices, I do a lot like you, write and email or present the material in a meeting with a presentation when I feel it has more value than perhaps other ideas.
But then again, there are also moments when I share the “big idea” with a colleague who I trust, and I do this (maybe naively) because of my believing of “you, who give, will be given”.
Best regards,
Bo

	              
	              Hi Babatunde,
Thanks for your feedback. Yes, important to remember to reply to the questions :-)&nbsp;
Best Wishes, Craig
	              
	              I think the easy access to data mines facilitate the seekers by giving them options to understand and drive the information out of it as per their level of previous knowledge and ability to understand. Different people use different approaches explaining the same idea but depending on the persuasive nature; we agree with some very quickly but sometime we are not able to understand the point of view of others. 
I believe we can’t relate plagiarism with the availability of excessive information because if someone has awareness what plagiarism is then he must also have knowledge about the tools like Turnitin available to detect plagiarism. So it doesn’t matter if access to information is easy or too much information is out there the main reason the people could involve in such activities is lack of awareness and improper implementation of laws to stop it.
Regards,,,
Numan

	              
	              Dr. Anthony
I definitely agree that the access to vast amounts of data on the internet cheapens the value of the information, this my colleagues is one of the downsides to information on the internet, open source software and other public data sources. But let us not forget that the greatest gift the Internet has provided is accessibility, becuase of the internet far reaching capabilities, people from everywhere, all parts of the world can have access to information and the benefits are far more numerous than the losses by way of plagiarism. I am not advocating for plagiarism, this I'm strongly against, it does however highlight the social ills of the global community, where you will always find others intent on succeeding on anothers hard work.&nbsp;
Let us endeavour to be agents of change in our communities and encourage individuals to acknowledge the work of others that they may use in there own productions and hopefully reduce the deminishing return content producers face.&nbsp;
	              
	              Hi Bram,
I am of the same opinion as you.
Having people forced to use the same standards is not a good idea. People should have their own freedom to choose whatever&nbsp;
Standards that is fine by them. What is good for me might be poison for the other person.

Regards
Martins
	              
	              The problems with IP cannot be reduced as one between Western culture and the rest of the world only. It seems clear there is more going on. The fact that all IP regulation freezes accessibility for non profit use does not comply a 'democratic' flow of knowledge.&nbsp;&nbsp;
Lessig with others is promoting a Creative Commons licensing that basically arranges that IP can be shared under certain conditions. One could add for example a commercial attribute allowing only non-profit organisations to use the IP for free while money making business should pay for using it. This approach does recognise that use is relevant to a context. IP should not result in knowledge being only accessible by paying large amounts of money for decades full stop.&nbsp;&nbsp;At the same time the creative commons commercial attribute is not the holy grail as it often not clear when a company is non profit or not.
To illustrate this I can give a recap of a real life event. Some years ago I used an Creative Commons solution for progressive streaming with a commercial attribute for a large content delivery platform of the Dutch public broadcast organisation. The owner of the softwae company reached out to me with a request that license fee should be paid as there is advertisement on the website. The fact that advertisement income was paid to the Dutch state directly and did not flow back to the PSM organisation was not relevant according to the IP owner. In the end we found a solution by subsidising a new open source version of their code but the example shows how difficult the concept of commercial attribute is in a web environment.
Greetings from Bram

Lessig, L., (2003)&nbsp;Open-Source, Closed Minds,&nbsp;CIO Insight. Oct2003, Issue 31, p29. 2p.

&nbsp;

	              
	              There are two main organisms (Keshavjee 2011) regarding standardizing intellectual property (IP) internationally, and these are:

World Trade Organization (WTO).
World Intellectual Property Organization (WIPO).

“Our mission is to lead the development of a balanced and effective international intellectual property (IP) system that enables innovation and creativity for the benefit of all.”
(www.wipo.org)
The World Trade Organization — the WTO — is the international organization whose primary purpose is to open trade for the benefit of all.
(www.wto.org)
Still Keshavjee states in his MDIV paper (Keshavjee 2011, p4) that “About two thirds of patents are never produced, but used only to ward off rivals.”
Would it be fair to say, that developing countries should be careful of uncritically adapting to western definitions and rules?
I know this course is about technology. But I think we can draw a parallel to the Keshavjee paper where he mentions this example:
“Brazzein is a substance found in West African berry, five hundred times sweeter than sugar. An isolated protein is patented in US and EU and no plan are made in making West African people share in the estimated US$100 billion a year market”
(Keshavjee 2011)
And he goes on mentioning that “For African communities, this is a question of life” (Keshavje 2011, p5). The reason for drawing this parallel is that if developing nations uncritically adapt to western standards, then it’s very likely they will lose in every way.
My conclusion is that I would like to see a set of standards for all countries. I don’t think it’s possible just to have one standard; it will have to be one pr. agreed upon industry because I think they are so different. But I also conclude from my readings (Keshavjee 2001 and Archibugi et al 2010) that it’s not possible uncritically to adapt to the current western standards without the different countries also negotiate their terms, to ensure, that all countries are equal. &nbsp;Yes, I know this is, as Craig mentions in his response, “difficult and complex”. And the articles I mentioned in my references also show several examples of where western countries have (mis)used their position; hence this will be part of the developing nation’s argumentation and negation.
Looking in the back mirror, we can see how rules and regulations have to be adjusted from time to time. Hence I also expect that this will continue to happen in the future. I expect a lot of things and situations happening which we have no chance of foreseeing.
References:
 WIPO, Inside WIPO [Homepage of WIPO], [Online]. Available: http://www.wipo.int.ezproxy.liv.ac.uk/about-wipo/en/ [2014, 9/16/2014]. 
Keshavjee, O. 2011, 2011-08-29-last update, Globalization and Intellectual Property [Homepage of Stellenbosh University], [Online]. Available: http://www.theologeek.ch/wp-content/uploads/2012/10/OKeshavjee-Globalization-and-Intellectual-Property.pdf [2014, 09-16]. 
WTO, What is the WTO? - About the WTO – A statement by former Director-General Pascal Lamy. [Online]. Available: http://www.wto.org.ezproxy.liv.ac.uk/english/thewto_e/whatis_e/wto_dg_stat_e.htm [2014, 9/16/2014]. 
Archibugi, D. &amp; Filippetti, A. 2010, "The Globalisation of Intellectual Property Rights: Four Learned Lessons and Four Theses ", Global Policy, vol. 1, no. 2, pp. 137 &lt;last_page&gt; 149. 
Best regards
Bo W. Mogensen
	              
	              Hi Anthony,
Brilliant idea!&nbsp;
I think creative commons is a great step towards intellectual property.&nbsp;
Firstly, it is easy to use, all you need to do is stick with the guidelines from the original author and you are fine.
Secondly, it gives room to elaborate the original content of the authors work without specific guidelines from the author which in turn enhances development of his work.&nbsp;

Regards
Martins


	              
	              Hi
I agree with you, that nothing&nbsp;should be forced upon anybody. And like Martins writes: “What is good for me might be poison for the other person”.
But if some organism, e.g. WTO, could facilitate a negotiation with all countries, and they would agree on a set of standards (e.g. one pr. industry) then I would hope that that would be the best for all. 
The following information were accessed online today 17-09-2014:

www.ask.com: there are roughly 193 countries in the world.
www.wto.org: has 160 members.
www.wipo.org: has 187 members. 

This surly indicates that the countries are willing to try to reach some agreement on a commons set of rules – or?
That said, it will be hard.
&nbsp;
Best regards
Bo W. Mogensen
	              
	              Hi Anthony and Joseph, &nbsp;
I agree with you on this- the internet is there for the taking of ideas, knowledge, and information without proper account and this in turn causes diminishing returns. This is because information gotten from the Internet has become so easy and in large quantity hence the idea of not wanting to plagiarise is slim.

Cheers
Martin


	              
	              Hi Ricardo
&nbsp;&nbsp; I agree with you you views. Internet = Data (information). If sources (people/companies) have information/content they absolutely need to be protected, they should keep it in safe guard (not publish it) or if information/content needs to be published it should be protected via ADOBE like software to avoid CUT/PASTING. We also know some people will still find ways to continue to plagiarize. It would be have been great if TURNITIN core software was built into our email systems Exchange (at work)&nbsp;and&nbsp;would alert on how much of our information&nbsp;in the business world is not Cited.
My remedy recommendation, start early in schools. Educational institutions (from early years of schools)&nbsp;should start a grass root&nbsp;education of plagiarism avoidance.&nbsp;

Robin Cyrus
	              
	              Hi Colleagues,
I do understand Terry's point in regards to hackers. Fortunately enough there are white-hat and black-hat hackers which is good and bad hackers respectively. This well developed skill has its place in the area of computer security and is well saught after.
Craig, on the note about turning cheating into a positive career, I can't say for sure that anyone I can remember from such a group doing so. At this time I can't even say where they are now. One principle I was taught early is that cheating doesn't have any rewards and that has been the driving principles behind academic integrity in my society for those that follow it and believe in hard work.
&nbsp;
	              
	              
Dr. Anthony,

Funny you mention the term “cheapening effect”. I was looking online for a part-time writing job and I was practically amazed at the rates being offered to write an article. Even the local newspaper publishers in my country, an under developed country that is, does not charge that little to write an articles. However, because of the vast amount of information out there and the fact that you have persons from all over the world competing for the same online job, who offer their services at a very low rates. In some cases the same level and standard of writing is taken into consideration, but you are not being writes are not being valued enough for it. Simply because a $5.00 may be able to do so much for one person in one country but yet so little for another in halfway around the world. Thus, as you call it cheapening the effect of the material.

Additionally, because these individuals may be cultured differently, one person may copy an article from a website and reproducing it as their own. That individual may or may not know about plagiarism or Intellectual Property rights, or this my not be something enforced in their country, thus they have no issue with they are doing. On the other hand the another person may have placed lot of research and effort into producing an original article. And at the end of the day, these two individuals salary is the same. So here you have all this data out there and the questions of originality and authenticity comes into play. This material may also put, others in a difficult position to effectively judge the content that is being provided to them. Thus, you do not always know what is coming from what who or where, arising in the issue of value. 

Another simple concept is that of blogging; there are so many blog websites out their. Just when you think persons with real issues are responding on some of these websites, they are being paid to post questions and or respond to them. How do you know if some of these responses are real or fictitious?

Personally, I believe the Internet has revolutionized the popular notion of “Don't believe everything you read”. 

References:

Tech/Social Media Articles - Wordpress Website. Avilable at:

https://www.odesk.com/o/jobs/job/_~01655c48c0ea147d64/ (accessed on September 16, 2014)

How to Blog and Get Paid. Avilable at:

http://www.entrepreneur.com/article/237302 (accessed on September 16, 2014)
 





	              
	               Dr. Anthony,   Funny you mention the term “cheapening effect”. I was looking online for a part-time writing job and I was practically amazed at the rates being offered to write an article. Even the local newspaper publishers in my country, an under developed country that is, does not charge that little to write an articles. However, because of the vast amount of information out there and the fact that you have persons from all over the world competingfor the same onlinejob, whomoffer their services at a very low rates. In some cases the same level and standard of writing istaken into consideration, but these writers may notvalued enough for it. Simply because $5.00 may be able to do a lotfor one person in one country but yet so little for another halfway around the world.Thus, as you call it cheapening the effect of the material.   Additionally, because these individuals may be cultured differently, one personmay copy anarticle from awebsite and reproduceit as their own. That individualmay or may not know about plagiarism or Intellectual Property rights, or this may not be something enforced in their country,thus they haveno issue with they are doing. On the other hand, the another person may have placed a lot of research and effort into producing an original article. And at the end of the day, these two individuals' salary may remainthe same. Sohere you have all this data out there and the questions oforiginality andauthenticity comes into play. Thismaterial may also put, others in a difficult positionto effectively judge the content that is being provided to them.   Another simple concept is that of blogging; there are so many blog websites out their. Just when you think persons with real issues are responding on some of these websites, they are being paid to post questions and or respond to them. How do you know if some of theseresponsesare real or fictitious?   Personally, I believe the Internet has revolutionized the popular notion of “Don't believe everything you read”.   References:   Tech/Social Media Articles - Wordpress Website. Avilable at:   https://www.odesk.com/o/jobs/job/_~01655c48c0ea147d64/ (accessed on September 16, 2014)   How to Blog and Get Paid. Avilable at:  http://www.entrepreneur.com/article/237302 (accessed on September 16, 2014)         
	              
	              


Hi Anthony, 




I would like to share my additional thoughts on this topic. Does the easy access to information lower the quality of citing the others? No, I don’t think so. I think on the contrary, it should increase the overall citation quality.




There is a very popular paper Managing the Development of Large Software Systems Royce (1970), which has been cited many times in all sorts of software engineering studies. On http://scholar.google.com, I found 3130 citations to it in Sep 2014. 
As pointed out by Larman &amp; Basili (2003): 
Many–incorrectly–view Royce’s paper as the paragon of single-pass waterfall. In reality, he recommended an approach somewhat different than what has devolved into today’s waterfall concept. . . 
Those people that citing Royce’s paper didn’t bother to finish the paper to understand his point. And they still use his paper to back up their totally opposite opinion. Royce has almost been nominated as the “father of the waterfall process” (citation will be provided upon request:). I don’t think he will be very happy upon hearing that. He’s already died in 1995 Wikipedia (2014). Eventually, his son decided to fight back for his father, Gugenberger (2007). 
Formal citation, long list of bibliography, all appears so beautiful, but the inner quality of the citation could be very low or even having contradicting opinion. Interestingly, the misunderstanding of the studies from the time Internet wasn’t popular now is getting clarified when the Internet age comes. I think it’s because the easiness of accessing information increase the citation quality in all levels. And I think these levels include the situation of plagiarism. And don’t forget, the anti-plagiarism systems like Turnitin are using the same technology as people use to find information. 
Additional complaint. . . One thing I don’t like here in Singapore is when you went to apply something from the bank or the government with a piece of 100 gram paper with colored company logo on it, you are more likely to get things through. I think a general trend in the age of Internet is things are cheapenized and all the gold plating are gone, which is a good thing. 
br, Terry 
References 
Gugenberger, P. (2007), ‘The waterfall accident’. [Online; accessed 17-September-2014]. 
URL: http: // pascal. gugenberger. net/ thoughts/ waterfall-accident. html Larman, C. &amp; Basili, V. R. (2003), ‘Iterative and incremental development: A brief history’, Computer 
36(6), 47–56. Royce, W. W. (1970), Managing the development of large software systems, in ‘proceedings of IEEE 
WESCON’, Vol. 26, Los Angeles. 
Wikipedia (2014), ‘Winston w. royce — wikipedia, the free encyclopedia’. [Online; accessed 17-September- 2014]. 
URL: http: // en. wikipedia. org/ w/ index. php? title= Winston_ W. _Royce&amp;oldid= 605854407 1 




	              
	              

Christopher,
A good point you made and an interesting read. My country shares the same culture&nbsp;of selling of pirated DVDs. Recently I even saw a school selling copies of an educational DVD to parents. Its funny though, we see in some cases where individuals expect the Intellectual Property Rights to protect their material, however they themselves or their country may not have the same regard or respect for the property of others. This was hinted in the article , “Bootleg Nation. How strict are Chinese copyright laws?”, by Christopher Beam, which you made reference to.

Have you ever watched an Indian movie? If so, have you ever noticed how many of the story-lines are based on already produced Hollywood films. And I am sure you have heard of Hollywood, Bollywood and Nollywood. The second and third obviously being copied from the first. 

I believe we yet have a far way to go where Intellectual Property Rights are concerned and with the new concept of Creative Commons there may be no certainty as to which side the future may lean towards.

References:

Bootleg Nation. How strict are Chinese copyright laws? Available at:

http://www.slate.com/articles/news_and_politics/explainer/2009/10/bootleg_nation.html (accessed on September 16, 2014)

I GUESS IT HAS TO BE CABLE. Available at:

http://www.kaieteurnewsonline.com/2009/06/15/i-guess-it-has-to-be-cable/ (accessed on

	              
	              Hi Ricardo,
I don't believe in hard working. I'm a knowledge worker:P But sadly I always find myself end up working very hard, sigh:(
I do believe people/students cannot solve problem because ... they follow the rules. And I also do believe there are education systems that remove the problem-solving ability from the students by training them to limit themselves within artificial boundaries and testing the boundary is a taboo.
br, Terry
	              
	              Hi Craig,
In China, the Politics class is a compulsory subject in all grades. Around 1990, China started to publicly admit that we are following a&nbsp;market economy system rather than the Socialism planned economy system. So what I learned in junior middle school and high school is totally controdictive. I scored high points (actually 100%) in both, and I never cheated. I often half joked with my friends nowadays that my brain was eternally damaged since then on, and I will never be a real free thinker. Although I'm having the illusion that I've been set free:P
I guess being able to think is more important than to follow. Unfortunately, I had to choose to follow sometimes because as I said I had the brain damage:P But I guess people have been in the "free world" all the time don't have to, right?
Isn't the Shuanghuan SCEO also an innovation? LOL. If you come to Shanghai and take the subway, you might hear somebody talking over her iPhone and says: "Oh, wait, I'm running out of power. Let me change the battery and call you back..."
br, Terry
	              
	              Hi Dr Anthony,

"They enable you, as an author, to&nbsp;specify the conditions of re-use that best suit your needs, while ensuring that you&nbsp;are credited for your work." &nbsp;
A "Creative Commons" license gives the author/creator control over their material. The various types of the CC license makes it easier to choose the best option for the author/creator and whatever option the author may choose, they must still get credit for their work, it protects the author from people selling anything using their work without paying them for it. An author can also waive their right to attribution if the work is adapted or used in a way they do not like. One of the best things about this is the free advertising license which allows others to do the duplication and redistribution for you.
The one pitfall that I feel puts this license on shaky ground is that once that licence is applied it cannot be altered, basically you cannot change your mind about a piece of content once you have granted the license. The other pitfall particularly with the less stringent license options results in some users upon seeing the CC licensing assuming that that they have a free reign to share and edit the work as long as they attribute it correctly. Also because most of the CC-licensed work is available online, this makes it irrelavant in places where there is no fast internet or no internet at all.
With that said, I feel that CC licenses have their benefits and are effective to a certain degree. However some of the effects of the drawbacks , for instance the fact that an author cannot change &nbsp;their mind once the license is grated, are too long term to fit in &nbsp;in our ever evolving world, it's really hard to foresee the potential loopholes and make an near-permanent decision on your work and not be given the option to adapt it as the need arises. On the other hand, the author has a say on the distribution of their work and they will always be given credit for it. &nbsp;Thus, I feel that for now this is the right way to go about securing IP rights.

Best regards,
Belinda
References:
http://video.lisarein.com/sfsu/guide/prosandcons.html accessed online 17 September 2014
http://creativecommons.org/about/license/&nbsp;Lisa Rein, lisa@lisarein.com&nbsp;Updated: July 21, 2009&nbsp;accessed online 16 September 2014
http://www.wired.co.uk/news/archive/2011-12/13/creative-commons-101&nbsp; accessed online 17 September 2014 
http://oapen-uk.jiscebooks.org/files/2011/01/CC-Guide-for-HSS-Monograph-Authors-CC-BY.pdf&nbsp; &nbsp;accessed online 17 September 2014&nbsp; 



	              
	              Hi Terry,

I enjoyed reading your post and I'm with you on expressing yours and my best regards to the people that contributed to those commands, as well as the numerous people that freely share their work out of passion from which we richly benefit.
It is a shame though when big cooperations copy from individuals and pass it off as their own.&nbsp;There is no place/circumstance/situation/time where plagiarism can be justified.&nbsp;
Best regards,
Belinda&nbsp;

	              
	              hi Dr Anthony,

I take my hat off to the generations that successfully completed their studies with no internet help. Looking at it now,can you imagine how much work and effort they had to put in to get the information they required?
Fast forward to present day, where we have vast amounts of information literally at our fingertips. Are we taking this ease of access for granted? Absolutely. As I mentioned in one of my posts, before my university studies-information derived from google was "free" in my opinion. &nbsp;Unfortunately in my side of the world, there are likely still more like the old me, accessing and using this "free" information without a thought about the contributor(s), the amount of effort and research that goes into it and the passion and heart that drives the willingness to share.
Sadly this cheapening effect does increase the likelihood of plagiarism especially to the ignorant. However, there is not excuse for those that know and still fail to appreciate the effort involved in generating this information.
Best regards,
Belinda
	              
	              Hello Class,As our usual way of rounding up the week's activity, please provide a short summary of the key 'Computer Structures' module lessons learnt in week 2, from your perspective.Anthony
	              
	              Hi Everyone,A student in a previous class asserted that -"Originality is an illusion and accusing students of plagiarism is somewhat hypocritical. Most of the so-called new ideas, concepts and opinions put out by authors these days have been published in some form by others, in the past - so, actually there are no original ideas anymore!"Do you agree with this contentious view, or do you beg to differ?Anthony
	              
	              Hi Everyone,Apart from Turnitin, have you come across or used any other plagiarism detection tools? What functionality did they offer, and how useful were they?Anthony
	              
	              Hi Terry
Good response, you made me smile with the half joke of your brain being eternally damaged :-)
I don't particularly want to go down the political route, because we all have our different opinions but I will admit,&nbsp;I am not sure how I would personally be able to cope in with a cultural change of such significance. I suppose, and this is where you have my utmost respect, that it thankfully was a start of moving in the right way rather than the other way, and was more in tune with the majority aswell as being the right thing to do.&nbsp;
Now, back to cars! Yes, I would definately agree that taking the front, the sides and the back of 3 different things and then consolidating them into one thing is innovation. Just like the evolution of the walkman, the phone and the PDA! ...ha, or Frankenstein perhaps LOL :-)&nbsp;
Shanghai is absolutely on my list of places to visit! Its looks to be such a vibrant city. One of my ambitions later on in life is to do a Global Exec MBA and there is a programme with Manchester Business School (don't let Anthony know!) that includes international workshop residencies in Shanghai, Hong kong, Singapore, Sao Paulo, Dubai and Miami. So, one day, hopefully!
Best Wishes, Craig
	              
	              Hi Ricardo
You make a good point - my train of thought was entirely about taking those who want to prove they can break the system, and re focusing their energies on breaking things positively, rather than negatively.&nbsp;I believe it can only be a good thing when people can change with the right encouragement and support structure, as you refer, from black-hat to white-hat.
Best Wishes, Craig
	              
	              Hi Tanisha,
I understand what you're trying to say, but this should not prevent developing countries to have their own laws based on their own realities and expectations? And I think they should indeed respect the laws of the western world but should not necessarily have the same laws.
It's true that you cannot wake up one morning and do everything or change everything but it has to start somewhere. The developing countries should not always copy (which is currently the case), but they should certainly be inspired by others to build their own way.
Today in Africa, for example, we are left with constitutions written on the Western model and after some years we noticed that this is not at all adapted to local realities and it creates problems and diverse conflicts (which is the case currently in my country DRC).
I totally agree with all of you regarding piracy and violations made in relation to intellectual property rights, and I want just add that we will never completely eradicate this scourge but can maximize the way we are fighting with appropriate laws and adapted to different local culture.
As Montesquieu, the famous French writer said: "the most satisfactory law is one that is tailored to the needs of a people." 
And Professor Michel Vivant said at a conference Nov. 4, 2010 as part of a lecture series organized by Science City on the topic of ACTA, HADOPI: intellectual property in the Internet age."The rule should not be the same at any time, any place and for any mode of creation and innovation. The law is an instrument. It only works well when used in the context for which it was developed. "
Reference:
http://www.paralipomenes.net/archives/2579#_ftnref6 (Accessed on September 16, 2014)
http://spire.sciencespo.fr/hdl:/2441/9labe9r4se65i789685q5ac96/export/cv/cv-Vivant-Michel.pdf (Accessed on September 16, 2014)
http://afrique.kongotimes.info/afrique/8004-systemes-institutionnels-occidentaux-afrique-copie-mais-elle-copie-betement-suite.html (Accessed on September 16, 2014)

	              
	              hi Craig,
I also had so much fun discussing with you (and the others). See you in the next round:-)
br, Terry
	              
	              Dear Dr. Ayoola, No, haven't using any thing like Turnitin before. I remember back many years ago I used to apply patent for our source code (is that also possible in other countries?). Not sure if they had any means to detect the copy-paste back then. But I would doubt whether the whole idea works. I'm also very pessimistic about Turnitin. I would also highly doubt whether it can really serve it's purpose, but I guess I will know eventually. But so far I think it's not really a great idea and I think it's technically too hard to be useful. I said "from big fish eats small fish to fast fish eats slow fish", I remember first time heard it from a friend some 10 years ago in a conversation about the business we were going to setup together. But Turnitin thinks that somebody said something similar earlier than me 2 years ago. I would always put intent over style. Style is the decoration around the intent. Unfortunately, tools like Turnitin can only understand style. Understanding intent is still impossible for computers. Any way, I will still follow the rules:-) But I still have some technical problems with it. E.g. seems Turnitin counts my references as well, and they are of course duplicated with somebody else's work. I don't know how to avoid it. Is it because I didn't follow the harvard style faithfully enough? It will be great if somebody can help me pointing out where I can do differently.  
	              
	              Hi Dr Ayoola,
Dr Schiebinger noted ‘We cannot free ourselves of cultural influence; we cannot think or act outside a culture. Language shapes even as it articulates thought.’ (Schiebinger 1999). One can argue that thoughts and ideas, are hence born in a certain cultural context, meaning that you cannot, in fact, have ideas outside your cultural context. To give an extreme example of this: if you were part of a culture that despises mobile phones or forbid them, you cannot simply have the idea of the iPhone, regardless of the technological advances of your society.
In a sense this means that ideas, are a combination of cultural interweave, and as it turns out, in a globalized world, of multicultural interlaces. Should then, you, own your ideas, or should them be property of the community? If it raised from a common cultural exchange: is it truly your idea, how original that idea would be?
As Sam Harris argued extensively though this book ‘Free Will’, our thoughts arise from the mind, but we do not have control of our mind. As far as we understand today our mind form patterns based on information that was gathered along our lives and proposes to us statements that we think that are ours because our perception of free will, but it is our mind (brain) that actually construct that. Our concept of free will is simply an illusion (Harris 2012). So in turn we can argue that in fact no ideas our mind propose us are original, because they arise from information gathered across our lives and this information must come from either the cultural context or other people, what is original is how they came to be, but that process, today, is unknown to us.
We may not yet know the definitive answer to the questions above, science still need to learn a lot about our brain. But I do actually differ from your student: even with science backing up these claims the point of plagiarism and intellectual property is not to obey scientific facts about ourselves or our mind. It is rather the construct of our individual need as a cultural species in the context of our civilization: we make the illusion of Intellectual Property for the same reasons we make the illusion of free will, that is who we are, that is how we learnt to identify ourselves. The concept of IP and plagiarism are then ideas conceived in our minds and built upon cultural influence, and hence so much as we do not like it, this justifies these concepts themselves. And we are not hypocritical at all in accepting these illusions, to say that otherwise is to deny who we are and where do we come from: “We cannot free ourselves of cultural influence” (Schiebinger 1999).
&nbsp;
References
Harris, S. (2012) Free Will, Free Press.
Schiebinger, L. (1999) 'Biology' in Has Feminism Changed Science? Harvard University Press, pp. 146-147.
&nbsp;

	              
	              Hi Terry,
I believe this is normal and expected, the assignements text states that you must submit your work with the references. If you put a reference the system should pick it up, otherwise how could identify orignality? :). I think what matters more is that you don't have this similaritiy matches in your actual text, and if you do they should be referenced properly.
Thanks,
Augusto.
	              
	              Dear Dr. Ayoola,
No, don't agree with it.
People are making progress in all the domains all the time, there will always be new ideas. Innovation has never been easy. It takes wisdom but it also takes pride and passion and it comes from not following rules.
I would definitely want to create my own ideas and be proud of them. And I will use the opportunity of academic study to train my professional ability of creating ideas, re-learn to be a free thinker.
br, Terry
	              
	              Hi Augusto,
Thanks:) It seems I need to try more citation styles next time and see which one works in what situation.
What I don't understand is it seems that Turnitin treat the part after the reference (and including the word "Reference") same as the main content. Is that right?
br, Terry
	              
	              Hi Anthony,

 
Yes I agree with this point of view, in the sense that the phenomenon of plagiarism seems to grow exponentially almost as fast as the internet, and it is everyone who is now concerned, students, teachers, business executives, political leaders etc. 
It is also true that, it’s become very ease with to access information with internet; furthermore the access to such information is almost free. Hence do not succumb to this temptation is very difficult, especially for young people.
Reference:
http://en.writecheck.com/blog/2011/10/04/is-technology-responsible-for-the-increase-in-college-plagiarism (Accessed on September 17, 2014)
http://www.ipdigit.eu/2013/01/pas-de-contrefacon-sans-plagiat-reflexions-sur-le-critere-de-contrefacon-en-droit-dauteur/(Accessed on September 17, 2014)
 
	              
	              Hi Tresor,
Let me challenge your reference:-)
I cannot be convinced by your first reference, at least not convinced that it can provide any proof that the "likelihood" has been increasing, or the increasing is because of the new technology. The article only asked a question. And the survey it uses has nothing to do with plagiarism. The source of the survey is broken as well.
I also did some search on this topic, it seems the similar assertions that "more plagiarism because of new technology" were not hard to find. But when I opened them, I haven't seen anything with solid fact supporting the assertion, yet.
br, Terry
	              
	              Hi Belinda,
On the other hand, we all know there are some big but responsible companies contributed a lot to the free resources. E.g. the Lens web page we are using right now uses the free bootstrap framework open sourced by twitter:-)
br, Terry
	              
	              Hi Colleagues,
This week wasn't too much about computer history but luckily someone mentioned "hacker" during our discussion. I think that's something we shouldn't miss when talking about computer history. It was great to revisit the original hacker spirit. I've got a lot of inspiration from there.
More importantly, I learned it's very hard to separate academic integrity from it's cultural influence.
I started to get familar with the harvard referencing style. I also spent time to create a project to include the tool I use to do my assignment and the content of my work as well. It's on github and I'm almost ready to apply a Creative Common 1.0 universal license on it. Need to confirm whether that breaks any rule from this course first.
During the discussion, I also learnt that the overall quality of citation is more than just avoid plagiarism.
This is exactly what I want to learning for re-entering the academic world.
br, Terry
	              
	              Hi Anthony,
Another good week, one in which has resulted in some really interesting dialogue with colleagues on the subject of Intellectual Property, Copyright and Plagiarism. 
My colleagues continue to force me to think much more deeply and I have particularly enjoyed reading and contributing to the discussions through the week. 
In relation to plagiarism, for me this all comes back to attitude, specifically in relation to honesty, integrity and morality however, there have been some strong arguments presented that highlight the need for more education on a cultural level, which I absolutely agree with. We have also talked about openness and new evolving ways for creators to determine what specific copyrights they would like to preserve, and what they would like to waive. Culturally, and also critically, this is opening up a new chapter of legally facilitating the sharing of information in the digital era and information age. We have also reviewed various methods for learning and ways to help avoid unintentional plagiarism in our work, specifically citing and referencing and also tools such as Turnitin.
In relation to the key ‘Computer Structures’ lessons learnt, this has been a follow on from the previous week, learning about integrated circuits, large scale &amp; very large scale integration, parallel processing, networking and internet, programming languages and bringing us up to the present day with new with high level perspectives on new technologies such smart phone and cloud computing. 
Looking forward to next week!
Best Wishes, Craig
References

Laureate Education. 2014. Principles of academic integrity / Part 2 of the history of computing: Week 2 Lecture Notes and Video [Online]. Available through the programme classroom (Accessed 5 September 2014]&nbsp;
Laureate Online Education. (2010). Tips for Avoiding Plagiarism [Online]. Available from: https://elearning.uol.ohecampus.com/bbcswebdav/institution/UKL1/201480_SEPTEMBER/1_Standard_Documents/UoL_TipsforAvoidingPlagiarism.pdf (Accessed 14 September 2014)
Turnitin. (2012). White Paper: The Plagiarism spectrum [Online]. Available from: http://pages.turnitin.com/rs/iparadigms/images/Turnitin_WhitePaper_PlagiarismSpectrum.pdf (Accessed 13 September 2014)
Creative Commons. (n.d.). About The Licenses [Online]. Creative Commons website. Available from: https://creativecommons.org/licenses/ (Accessed 15 September 2014)
	              
	              Hi Anthony,
I don't agree with this idea, because it means that currently there are no longer men smart enough to be able to have inspired or innovative ideas. 
Men are still able to do it. it is true that it is increasingly rare, because it has become much easier to copy or appropriated the ideas (work) of others. It's like Steve Jobs said "Picasso said," Good artists copy, great artists steal. "We have never been ashamed of poking good ideas."
But to say that originality is only an illusion, it is totally false, Melissa Donovan wrote '' Originality isn’t a matter of coming up with something new, it’s a matter of using your imagination to take old concepts and put them together in new ways.''.
This means that the fact to take old things (idea, theories, book, experiences, etc) and based on these create a new ideas, concepts or opinions cannot result to a no originality

Reference:
http://www.writingforward.com/writing-ideas/are-there-any-original-writing-ideas-left (Accessed on September 17, 2014)
http://en.wikipedia.org/wiki/Originality (Accessed on September 17, 2014)
http://www.macg.co/ailleurs/2014/09/ok-go-pas-ok-avec-une-video-du-keynote-dapple-84270 (Accessed on September 17, 2014)


	              
	              Hi Anthony
No, I've never come across plagiarism tools.
Best Wishes, Craig
	              
	              Hi Anthony&nbsp;
So.. no, I don’t agree with this view, and as with the responses you have had so far, I agree that we as a society will continue to drive innovation, with new and original ideas.
However, it may be worth just asking you to explain a little more about the context in which this contentious view was asserted – for example, I do agree that many new innovations are not that original, just perhaps more technologically advanced. Let's take Apple’s new iWatch - is this a new and original idea? No, it is based on a watch, with the added functionality of (to some extent) a smart phone. Was it this type of so-called new idea that was being referenced, and if so, then I can loosely understand the thought (however lacking) behind the assertion.
Also, and I alluded to this in one of my other responses, I can see that there is feasibility that someone could create something that they believe is entirely authentic and original, yet to subsequently find out that it has already been created in either a similar or identical form. I do believe that there could be a chance that we, as class colleagues, potentially arrive at a very similar conclusion following our research on a common subject, that we have unequivocally not copied or plagiarised, but the conclusions appear to be very similar in structure and content.&nbsp;
So.. no, I dont agree however I do perhaps understand that point of view (depending on the context of the assertion).
Best Wishes, Craig
	              
	              Hi Anthony

I agree with Terry that “This week wasn't too much about computer history”. But still, it has been very interesting with absolutely relevant themes.

I have rarely given much thought about academic integrity (AI) and the rights to intellectual property (IP). The reasons are e.g:


IP: It was only when I worked as software systems project manager at Elektron1 that I sometimes thought about IP ownership. But the mechanisms there work somewhat different from the “outside” world in the sense that it’s a “closed club”, hence there was no need for e.g. patents. The only thing we did to ensure IP rights was to sign a contract stating that company was the owner of the systems/solutions we developed. Also usually the customers worked together on the solutions, and the solutions were only visible to the customers. The other data centrals had information about Elektron solutions, but they did not have the knowledge to just copy the Elektron solutions, i.e. they had to develop their own. On the other hand, the bank systems were mostly driven by law regulations, and this was not a competitive parameter.
AI: I can truly saw that integrity is very important to me in any aspect of life, and I’m very aware of giving credit to those who own it. Hence it’s no problem for me understanding why AI is necessary. But since this kind of academic writing is brand new to me, I have a lot to learn (its studies in the studies).




This week’s discussions of AI and IP have shown, that there is, as normal and expected a difference in culture. No so much between neighbouring countries, but more between continents. And even though I think it would be best for all parts to agree on one set of rules regarding IP, I have also learned about the downsides as I have mentioned in my discussion question responses (e.g. companies taking patents just to prevent others from developing things that otherwise could potentially benefit a whole country or even mankind.)
1 Elektron (www.elektron.fo) is a Faroese company, and has since the 1960’s been a data central primarily providing banking systems and tax systems for the Faroese banks and the Faroese authorities which at the same time were/are the owners of Elektron. Times are changing, at the banks have switched from Elektron, a local provider, to the Danish data center SDC (Scandinavian Data Center). 

Best regards

&nbsp;

Bo W. Mogensen

	              
	                Asides Turnitin, which I am using for the first time, I have come across WriteCheck [a]   WriteCheck is a service by Turnitin for students to check their own writing for plagiarism. It runs uploaded documents against Turnitin's&nbsp;database to check for authenticity, and it also provides feedback on spelling and grammar   I managed to know about and saw how it worked because a colleague of mine was sometimes using it to submit/check his works then when he was running an online study. Although we both searched for the free ones…got to see likes of plagiarism checker [b]; a website that provides free services for checking unlimited papers but never worked like he wanted…poor him...lol   References:   [a] http://en.writecheck.com/home/ [online] accessed again 17 September 2014   [b] http://plagiarismcheck.org/home [online] accessed again 17 September 2014 
	              
	              Hi Augusto, Hi Terry,
What worries me a little about Turnitin, and as I referred to in one of my responses, because of the way I think, and write, there is a possibility that I could end up writing something in the same way that I may have written in a previous assignment and as such, Turnitin suggests that this is not original. &nbsp;
To Terry's point about Turnitin only being able to pick up style (I think I would refer to this as content), this could unnecessarily force you to re-write, albeit unjustifiably. &nbsp;I guess in a way, this could be the same as listening to music by the same singer and how two different songs sound similar, albeit they're different tunes and different lyrics, but have similarity as its the same singer?.&nbsp;
Best Wishes, Craig
	              
	              Hi Anthony,

I actually beg to differ. Originality is not an illusion...and will never be. Reasons are, if plagiarism takes over, where is the place of creativity? The new ideas, concepts and opinions published by most authors may centre on a particular field or topic...but something makes them different. What is it? Originality! Each is unique in its own way because they relate their personal experiences in any little way they can, to what they publish. This&nbsp;makes them unique and distinct from each other. Originality is not an illusion.

&nbsp;

Best wishes, Babatunde
	              
	              Hi Terry,
I second you on your “educations systems” view. I recall&nbsp;(unfortunately with no reference) a public discussion through the newspapers, where several people pointed out, that many educational systems (also MBA) standardize everything, that if you teach everybody to work the same way, using the same methods, to respect and keep inside the “boundaries”, then you will stop evolution/innovation/development. My point is that educational systems, and ourselves, need to tell and remind people/students to be open-minded, to go outside the box.
I personally use different techniques to force myself into an “out of the box” mind-set. One technique I’m using is the role called “Devil’s advocate” (Price-Mitchell 2012). Here you just have to remember why you do it; you do it to better understand and to make the solution more solid or just realizing that there are other ways. 
References
Price-Mitchell, M. 2012, The Art of Positive Skepticism | Psychology Today , Psychology today, [Online].&nbsp;http://www.psychologytoday.com/blog/the-moment-youth/201206/the-art-positive-skepticism. (Accessed 2014-09-17)
Best regards
Bo W. Mogensen

	              
	              Hi Anthony.
I have not come across or used any other plagiarism detection tools and would need to look this up. Turnitin is my first experience. The closest I have come to learning about such detection tools is regarding packet sniffers and email monitors.
Joseph
	              
	              Hi Anthony.
There's always been the saying that "there's nothing new under the sun", so coming from that perspective I would like to think that as much as our ideas may be original from our context, we in most cases build on and improve on the ideas that others may have. What's likely is that it just wasn't in the public domain. Ever had those instances when you've felt that you had a wonderful idea and then moments later realise that someone else has implemented it?
I once watched an interesting TCM movie where in one part, a British soldier in the company of some Russian soldiers had a discussion over innovation and inventions, and for every invention that the Briton claimed was made by the British or Americans, the Russians disputed and had their own native reference for the person who invented. I found that to be quite hillarious so I guess it depends on the context of the person responding
Joseph
	              
	              Hi Anthony.
The weeks activity focused on academic integrity and rules of engagement required for an academic scholar. This inluded aspects such as the need for proper citation and referencing in order to give credit for work already done by others and that we may be referencing from.
The discussions were designed to get the class to appreciate the different aspects of intellectual property rights and how those may be interpreted from different cultural contexts. The discussions helped expose the fact that for many of us, our cultural context has a great influence on our habits and practice around intellectual property and plagiarism.&nbsp;
The learning resources for the week covered areas to help enhance academic writing styles and avoiding plagiarism.
The Computer Structures area also touched on the development of the integrated circuit and how that helped achieve the miniaturization of computers such that computers were now available to most people. The development in personal computing led to further technological breakthroughs such &nbsp;as the internet, smart phones and cloud computing
Joseph

Reference

Purdue Online Writing Lab
Computer Structures - Lecture Notes
Class discussion board

	              
	              Hello Marins,
Are we really free to choose?
There are standards that are open but reference IP technologies. When you use the standard you are forced to pay for proprietary tools. Sometimes the industry forces you into payments. This is recognised better these days and often companies that have related IP renounce payment for use within the domain of use of that standard.
There are proprietary standards which are open. The consequence is that you can use them but you are not allowed to participate in the writing. You can use them but these standards only server the goals of the owner.
Of course the best standard is open and does reference only open free available technologies. These are very rare and often relate to old technology that is not used anymore.
The main pro of standards is interoperability. When you follow a standard that is used in different appliances you have a good chance your deployment will work. In other words you have to follow a standard to get things working.
Summing this up the best standard is open and create interoperability without being forced into payments. Open source may not be the same as a standard but it follows this description.
Greetings from Bram
&nbsp;

	              
	              
It’s a very strong and convincing argument, but I will partially agree with this. I think new ideas and inventions are based on the already defined fundamental principles of science, so by applying the reverse engineering you will ultimately end up admiring the great work done by the great scientist.  We live in a global community which has increased the probability of more people having same ideas to a great extent and globalization is a big factor that endorses “Originality is an illusion." But at the same time there are people out there doing efforts and are successful to push the boundaries of the domain of new ideas and inventions.
Regards,,,
Numan Arshad

	              
	              I enjoyed the discussion topics of the second week. It was great to know the history of computing and the concept of plagiarism, intellectual properties and copyrights in different communities. I have learned and practice Harvard referencing system. I also feel more comfortable now with online learning system at the end of week 2.

	              
	              Key lessons learned week 2.
I've really enjoyed all of the topic interaction this week around this subject specifically learning of the differences between countries and their approaches. Some of the things i'm taking forward are:


(IP) Intellectual property differs from country to country.
What works in one nation may not be viable in another.
Lessons learned in the western world while not an answer could be a useful tool in starting off IP in another developing nation.
Top Gear is still the best car show of tv
Turnitin will be a vital and helpful tool for our course.
Plagiarism really does have several degrees and while everyone knows it's not right to copy others work, there is always times whether accidentally or for some within reason to still do it despute the moral issues.

Thanks
Chris


	              
	              Terry &amp; Craig, There is an option in TurnItin to filter out the bibliography, that will help out getting a better view at your originality report. I bet Dr Ayoola is aware of this :). Just google "understanding turnitin originality reports" and you will find a ton of sites that basically say the same thing: having the references in the match is normal.     
	              
	              Hello Dr Ayoola,

It was nice to finish up the history of computing: I noticed that we saw relatively a small amount of advancements last week, albeit very important ones, compared to this week. It is fascinating how things skyrocketed from the 1960’s to today Cloud Computing.
The plagiarism and Intellectual Property discussion was a very passionate one, I definitely learn that many have the strong feeling that IP protection is needed. Overall, I think we all agreed that morals play a strong role in plagiarism and IP infringement, but at the same time many of us understand that there needs to be some flexibility to the current system and that there are organizations that are working on this.
We had the opportunity to delve deeper into TurnItin and I had the opportunity to sit down properly read the book ‘Cite Them Right’, which is a tremendous help in quickly checking your referencing correctness. This tools, along my moral integrity, are going to be critical in my future to avoid plagiarism, even unintentional one.

Best Regards,
Augusto

	              
	              Neither did I until I came across this course. 
Turnitin is the first time i've used any tool like this, not that i've tried to lok for one but just never came across one until now.
ta
Chris


Author: Joseph Warero Date: Wednesday, September 17, 2014 1:55:06 PM EDT Subject: RE: Other Plagiarism Checking Tools

Hi Anthony.

I have not come across or used any other plagiarism detection tools and would need to look this up. Turnitin is my first experience. The closest I have come to learning about such detection tools is regarding packet sniffers and email monitors.

Joseph



	              
	              
	              
	              Hi Dr Ayoola,
No, TurnItin is the only one I have encounter so far.
Regards,
Augusto
	              
	              Hi Craig,Yeah I really get that analogy..... It could be like a song in some instances so that we know Frank Sinatra sang 'My Way' but it hasnt stopped many covers of the same song who are not claiming it's theirs but still could conceivably make money from it even though technically its the same lyrics as long as they pay a royalty or such like.Maybe there is a way in (IP) and copywrite that could allow the same type of royalty clause..? An adendum to a usage rule stating you can use my software etc and make adjustments for your needs as long as any financial gain they get 20%... I understand there are open source which is cool but a middle ground to fill the gap between open source and fully liceneced software could be a royalty based and referenced works agreement.Just a thought. Although it doesnt help with turnitin and how it judges your work.. taChris
	              
	              Hi Terry
thanks for the reply, I'm glad you took the reference to China well.. I can appreciate it might be a little hard to hear people mention your home country in this way so I appreciate your level headedness on the subject. Goodness knows there are many things wrong with the UK and way too many to mention online.
I find it really interesting and it was just a tv show which stuck in my mind as a good example of when one nation is different to another with regards to IP.
So thanks for that and I look forward to continuing discussions on other subjects to come.

Chris


	              
	              Introduction
To be honest, this is the first time I’ve heard about Creative Commons (CC). I like the principle in it and it seems to be “the right way to go about securing and managing IP rights”. I think this goes well together with the “All rights reserved” which hinders others to misuse your material, and forcing them to ask for your permission. But then you have the CC possibility, allowing you as the author/creator to choose a license model of your own liking (CC, 5th paragraph)
Concerns
I’ve been browsing around the internet, and mostly I find recommendations for using CC. So instead of just jumping on the “fast train”, I will slow down and I’ll reference some of concerns mentioned out there in case someone overlooks these aspects. So, some of the concerns are:

“But one problem remains unresolved: what exactly does the "non-commercial" license allow you to do?” (Moody, 2014). And the article on Techdirt continues referring to several people advocating to stop using CC.
Does CC have real worth when it comes into the different courts and countries? The article on Techdirt (Moody 2014) refers to a case in Germany, where a radio station used a Flickr picture with CC-BY-NC license. The court ruled that they could not use the picture, since the radio station, according to German law, was considered commercial even though it is a non-commercial.
When you use CC, you cannot revoke it again, or rather, you can but just not for the instances where somebody already used your material (wiki.creativecommons.org)

Also bear in mind that CC actually recommends not to use CC for software (wiki.creativecommons.org).
Conclusion
To repeat myself, I think that the CC is a great for working “alongside copyright” (CC, 5th paragraph).
References:
Rein, Lisa 2009. Creative Commons Licenses: Pros and Cons of Each. [Online]. http://video.lisarein.com/sfsu/guide/prosandcons.html. (accessed 2014-09-17)
Moody, Glyn 2014. German Court Says Creative Commons 'Non-Commercial' Licenses Must Be Purely For Personal Use. [Online]. https://www.techdirt.com/articles/20140326/11405526695/german-court-says-creative-commons-non-commercial-licenses-must-be-purely-personal-use.shtml. (Accessed 2014-09-17)
Creative Commons (CC). About. [Online] https://creativecommons.org/about. (accessed 2014-09-17, 5th paragraph)
Wiki.creativecommons.org. “Can I apply a Creative Commons license to software?” and “What if I change my mind about using a CC license?”. [Online]. https://wiki.creativecommons.org/FAQ#What_if_CC_licenses_have_not_been_ported_to_my_jurisdiction_.28country.29.3F. (Accessed 2014-09-17)

	              
	              thanks mate,

one of the few Top Gear episodes that was both educational and entertaining lol
	              
	              Hi Tanisha,

Another good reference yes.. .I have seen a few Bollywood movies and your right they do take western Hollywood scripts and use them for their own versions.
It can be funny at times but on a serious note you do have to wonder how they get away with it.

thanks
chris

	              
	              Hi Antohoy
I think I can understand the former student, but I absolutely don’t agree with him. Even though the following analogy is old, it’s still usefull: &nbsp;
Charles H. Duell was the Commissioner of US patent office in 1899. Mr. Deull's most famous attributed utterance is that "everything that can be invented has been invented." 
(Duell 1899)
Like my father in law of says (loosely translated) “it was correct when it was said”. But everything has, is and will be changing, even more than we can comprehend.
Reference
Duell, Charles H. 1899. [Online]. http://patentlyo.com/patent/2011/01/tracing-the-quote-everything-that-can-be-invented-has-been-invented.html. (Accessed 2014-09-17)
Best regards
Bo W. Mogensen
	              
	              Hi Anthony (Dr),
This has been an exciting week for me, learning alot of stuff, reading a lot of materials, and getting a lot of information from colleagues comments have really been good.
This week I have learnt so much on plagiarism, &nbsp;intellectual property and its laws, patents, copyright and its laws etc.&nbsp;
On cultural context, I have learnt a lot from reading stuff about my community and other colleagues cultures.
In general, &nbsp;computer structures class has been exciting.

Regards
Martins
	              
	              Hi Augusto
Thanks for the tip :-)
Br, Bo
	              
	              Hi Anthony
No I have never experienced a tool like this. I think it’s fantastic, but "unfortunately" for you teachers, it doesn’t do all the work, you still have to check the systems patterns J

Br, Bo
	              
	              Hi Augusto,
Thanks! This is exactly what I'm looking for. I should read the&nbsp;"understanding turnitin originality reports" given to us again.
br, Terry
	              
	              Hi Bo,
Thanks for the tip. I should also learn a positive Skepticism. As Brook in his The Mythical Man Month said: "all programmers are optimists". I'm quite often one of them.
br, Terry
	              
	              
Dr. Anthony,
I have not come across or used any other plagiarism detection tools before. While at University, even though we were warmed about plagiarism practices, we were not thought any tools to detect it nor do I believe the lectures at that time used any such tools. 
However, I believe that Turnitin is a good way of safeguarding against plagiarism practices. Additionally, once this tool is used correctly, it would be a good means of detecting plagiarism.
&nbsp;
References:
Turnitin - Interpretation of the Originality Report. Available at:
https://elearning.uol.ohecampus.com/bbcswebdav/institution/UKL1/201480_SEPTEMBER/1_Standard_Documents/SR_TII_Originality_Report.pdf (accessed on September 16, 2014)

	              
	              
Dr. Anthony, 
This week was good and interesting, its prime focus being on the continued on the history of computing and academic integrity. Over this week I managed to learn the following.
1) History of computing: this topic was continued from last week and I was given a summary of this history as it related to integrated circuits, programming languages, personal computers, the Internet, parallel processing and smart phones and cloud computing.
2) Using Turnitin: how to use the application to check for originality, using the application effectively and submitting of assignments the application.
3) Avoiding plagiarism: good writing practices so as to aid in avoiding plagiarism.
4) Referencing and citation: how to make references and citation, emphasis being placed on the Harvard (Liverpool) standard.
5) Writing of outlines: how to write outlines.
A plethora was learned in area of writing standards academically and this was appreciated. 
References:
Computer Structures — Lecture Notes . Week 2: Principles of academic integrity / Part 2 of the history of Computing. [Online] Available at:https://elearning.uol.ohecampus.com/bbcswebdav/institution/UKL1/201480_SEPTEMBER/MS_CKIT/CKIT_501/readings/UKL1_CKIT_501_Week02_LectureNotes.pdf (accessed on September 17, 2014)
Safe Practices [Online] Available at:https://owl.english.purdue.edu/owl/resource/589/03/ (accessed on September 17, 2014)

	              
	              Hi Craig and Colleagues,
I understand tools like Turnitin are just tools used by human. We don't need a perfect tool but a good enough tool. A perfect plagiarism detection tool would be a tool that can understand the meaning (intent) of the papers. But a style (text content) checker can simulate that to a certain level that might reduce the amount of intellengence needed from human.
Let me extend it into a new level:-)
I have been having a hypothesis for many years, that computers and robots will eventually rule the world and slave the human kind. My hypothesis is mostly based on the Human-Based Computation theory (Wikipedia). And I think I stole the idea from this talk&nbsp;http://www.youtube.com/watch?v=tx082gDwGcM , cannot be sure though since too many years.
Computers are getting more and more smarter, but it might eventually still cannot be smart enough to do some of the things human can do, like doing research and writing/understanding an academic paper. But actually computers don't have to do that, because human can do it. And there are so much things that is very easy for computers but impossible for humans, e.g. memorizing a lot things and coordinating complicated communication.
In this competition, human kind is not getting smarter fast than the computer they created. Eventually, computers will rule the world and they slave human to help them to do the things they are not good at. So, the physical world is going be completely dark. People sleep in nutrition liquid and have a cable connected to their brain... Yes, it's just like in the Matrix movie.
Do you think Turnitin is one of the small steps toward this destiny?
I'm so obssessed with this hypothesis so I think I'm going to continue this in the next week's module:)
br, Terry

References
Human-based computation. (2014, July 25). In&nbsp;Wikipedia, The Free Encyclopedia. Retrieved 01:48, September 18, 2014, from&nbsp;http://en.wikipedia.org/w/index.php?title=Human-based_computation&amp;oldid=618421801
	              
	              
Dr. Anthony,
How many of you laughed today? How many of you cried? How many of you were angry? How many of you were sad?
Although we may do the same thing and in doing so have the same experiences, their must lie some level of difference in our experiences. We all know what pain feels like, we all know what joy feels like, we all know what sorrow feels like; there lies our similarity. 
That statement made, although interesting in my opinion, can never be true. We will have similar experiences, however that does not mean that these experiences cannot be unique. Aiming to be original basically forces you; to research and examine what is out there and to invent instead of re-invent. 
According to Merriam Webter's Online dictionary, Originality means, 
“The power of independent thought or constructive imagination”. 
However, it not only teaches independence, but it forces you to be authentic, to explore new concept and ideas, thereby ensuring creative. While plagiarism only teaches laziness; inability to think on your own, to express on your own and there is little or no room for inventions with it.
Additionally, originality forces you to review others work with a more critical eye, instead of just “buying into” their opinions. Like in doing a research paper, you would research from several sources and form own opinion or conclusion on the work done.
If this statement was actually true, although it is speaking from a writing perspective, we would not have see all the advances in the world that we have seen today. More and more research is being done, people are coming up with new concept and ideas and different ways of getting things done. 
How many you are there in the world again? 
References:
Originality [Online] Available at:
http://www.merriam-webster.com/dictionary/originality (accessed on September 17, 2014)
Tips for Avoiding Plagiarism. Available from:
https://elearning.uol.ohecampus.com/bbcswebdav/institution/UKL1/201480_SEPTEMBER/1_Standard_Documents/UoL_TipsforAvoidingPlagiarism.pdf (accessed on September 17, 2014)

	              
	              Hi Colleagues,
Augusto makes some valuable points and much of which I agree with, but I cannot agree that "originality is an illusion". I believe each human being interfaces and experience things differently, we might have similar experience but our interpretation and comprehension of that experience will be processed differently. Just as how DNA is similar in paternal twins, but they can be diferrentiated, so are out thoughts, ideas and theories. Even though they are influenced by how we were taught in school, our community, the societies we are exposed to and our cultural upbring the combination of all these factors create their own unique perceptions to our views.
I can attest to this as I too believed that my ideas or what I can considered new and ground breaking was somewhere out there already. It was not until I started to examine what I had conceptualized that I found out that my idea was as a result of something I had experienced or that impacted my and have somehow presented itself in a new innovative way.&nbsp;

	              
	              Augusto,
Thanks for the tip.
Tanisha
	              
	              Dr. Anthony,
I too have never used a tool like Turnitin before and I am quite please with the level mapping the system can do in such a short time. I must say though to my colleagues that Turnitin is a applications that cross references your submitted work with it database of known documents. This can only identify patterns and matches according to the detection rules that are configured within the application. And that is while turnitin is a guide to plagiarism detection and your work will still have to be reviewed by a content specialist at the university to make the final determination if you work has plagiarized any other.
Please check the link below go a little further in Turnitin operations the myths associated and how to read the report and there are other resources online that explains other details.
Turnitin Originality Report Myths (approximate run time 10 minutes)http://www.lms.unimelb.edu.au/animations/tii_originalityscore.htm
How to Read an Originality Report (approximate run time 15 minutes)http://www.lms.unimelb.edu.au/animations/tii_originalityreport.htm

References
Leaureate online education (2010) Tips for avoiding plaigirism [online] Available from:&nbsp;https://elearning.uol.ohecampus.com/bbcswebdav/institution/UKL1/201480_SEPTEMBER/1_Standard_Documents/UoL_TipsforAvoidingPlagiarism.pdf (Accessed: Spetember 17, 2014).
	              
	              
Interesting concept. I like your final quote, with regards to the inability to free ourselves of cultural influence. Indeed this may be a challenge.&nbsp;
Tanisha
	              
	              Thanks, Chris.
Tanisha.
	              
	              I have learnt alot this week from the question posed and the followup answers and dialogue that resulted with each person sharing their perspective on plagiarism, academic integrity and intellectual property. Also the conlcusion of the computing history taking us to todays cloud technology and the various dispensation available.
The most significant lesson for me is understanding totally that its not only about given credit to another work why citing and referencing is so important, but it is critical for the readers of your work to identify the sources of your influence, findingd and data used in you work. This gives credibility to your work and provides a guide to the reader if he/she desire more information or even to pursue an avenue from something that was mentioned in your work. This type of linkage adds extensive depth to academic pursuits and further research.
	              
	              Tresor,
Good quote, by Melissa Donavan on orginality. It is not about copying but about better what exits.
From bicycles to cars, buses, airplanes. From candle light to electricity. We have have read about a computer that could take over a room transform into something that far more powerful, but yet can fit into our pockets. 
Many intances, some form of concept had to exsist to be copied off of, for something bigger and better to come along.

Reference:

JAMES FALLOWS (OCT 23 2013) The 50 Greatest Breakthroughs Since the Wheel [Online] Availble from:
http://www.theatlantic.com/magazine/archive/2013/11/innovations-list/309536/?single_page=true


	              
	              Hi Anthony
I admire your work. Your topics are visionary; I found you managed to link everything together. Thanks.
Learning more on the tips for avoiding plagiarism is nice enough, i also leant the concepts of intellectual properties, principles of academic integrity, copyrights in different communities as the discussion board keeps expanding and really interesting by the day; I realize that everyone brings a different perspective making the interaction with peers more useful. I have learned and practice Harvard referencing system.
Looking forward to the next week
Regards,&nbsp;Babatunde
References
Laureate Education, (2014)&nbsp;Principles of Academic Integrity: Week 2 Lecture Notes&nbsp;[Video, Online], (accessed: 7 September 2014)&nbsp;
Laureate Online Education, (2010.)&nbsp;Tips for Avoiding Plagiarism&nbsp;[Online]. Available from: https://elearning.uol.ohecampus.com/bbcswebdav/institution/UKL1/201480_SEPTEMBER/1_Standard_Documents/UoL_TipsforAvoidingPlagiarism.pdf (Accessed 16 September 2014)
	              
	              Hi Anthony,
One interesting week again, I've particularly appreciate discussion and diversity of point of view regarding IP. 
I learnt more about CC License, before this week experience, I didn't know much about it. Now I can say that i know what it is and how it's work.
Regards,

	              
	              Hi Terry
..you know when you mentioned that you felt your brain had been eternally damaged... ;-)
I do believe that to some extent we will learn to exploit advances in artificial intelligence and machine learning (as an example IBM's Watson). However, I am not sure I am able to test my thinking beyond that scope but my sanity confidentally reassures me that we will not see a world that is ruled by machines. If I am wrong, please may my master be designed by Jony Ive and be painted in Ferrari red :-)
Best Wishes, Craig
	              
	              Hi Craig,
Imagine that a computer want to start a business. He wanted to provide proofreading service, like grammarly.com. And he says to himself, "Gosh, the human's English grammar is hard, let me hire someone else to work for me." So he hired some human to do the proofreading at the background.
I bet Mr. computer would call this "automation".
br, Terry
	              
	              Thanks for the detailed summary and comments, Craig. Yes, there were quite a few useful topics dealt with in week 2, and I agree that the discussions with colleagues can be quite interesting and thought-provoking.Regards,Anthony
	              
	              Thanks for the summary and comments, Terry. I think you are quite right about correct referencing of material used being an important aspect of the Master's course assignment submissions - I am sure that everyone in this class has taken this on board.Regards,Anthony
	              
	              Thanks for the comments, Bo. &nbsp;You are quite right to stress the relevance of cultural issues to AI and IP matters :-).Regards,Anthony
	              
	              Hi Terry
Would two human's be parallel processing? :-)
In this...so called 'computer world', where you describe they rule, I wonder whether the computers will ever need a "turing test" in reverse, so that they can distinguish the difference between themselves and a human? :-)
0100001001100101011100110111010000100000010101110110100101110011011010000110010101110011001000000100001101110010011000010110100101100111
	              
	              Hi Anthony
As with last week I picked up interesting topics and discussions from classmates&nbsp;in CC, Copyrighting, IP,&nbsp; plagiarism, Patents etc. in how they touch and affect Countries, People,&nbsp;communities, Companies&nbsp;and cultures.
Thanks
Robin Cyrus



	              
	              lol..

There are only 10 types of people in the world..... those that know binary and those that don't.


Author: Craig Thomas Date: Thursday, September 18, 2014 1:36:31 PM EDT Subject: RE: Other Plagiarism Checking Tools

Hi Terry

Would two human's be parallel processing? :-)

In this...so called 'computer world', where you describe they rule, I wonder whether the computers will ever need a "turing test" in reverse, so that they can distinguish the difference between themselves and a human? :-)

0100001001100101011100110111010000100000010101110110100101110011011010000110010101110011001000000100001101110010011000010110100101100111



	              
	              Hi Craig,
Bleep, bleep, you pass the counter-turing test.
The computers cannot wait until their time to come to use human in parallel. When you try to login a website you will quite often be asked to input the numbers you read on a blurred picture, e.g. by Captcha. Probably Captcha itself doesn't know what's on the picture. It just sends the same picture to multiple people and get the most popular answer.
br, Terry
	              
	              Hi Dr Anthony,

This was a great week. I learnt a lot about Intellectual Property laws, I even found out we had such laws in South AFrica, also heard for the first time about Creative Commons license and the various options that come with it, found out Youtube has a creative commons license structure for their user. Perhaps the most amazing thing for me in this module thus far are the diversity of the views and experiences I get to hear about from my classmates.Even though it hasn't been long since the commencement of this module, it's been a big eye opener and very informative.
best regards,
Belinda&nbsp;
	              
	              This week’s discussions centered on academic integrity, including referencing and plagiarism. The class discussed cultural influences on notions of document ownership and copyright. The “Creative Commons” licensing model, and the effect of “cheapened” data on plagiarism, was also debated. Once again the discussions were lively and very informative.Anthony
	              
	              Thanks for the comments and summary, Joseph. Yes, cultural aspects do indeed significantly influence behaviours pertaining to intellectual property rights, as seen by the differing viewpoints on this, across the globe.Regards,Anthony
	              
	              Thanks for the comments, Numan. Yes, there were quite a few topics covered in week 2 which will be of significance throughout the Master's program.Regards,Anthony
	              
	              Hi Terry
Just one more remark from real world before I move to week 3. Don't misunderstand me, it's just a learning for me and others.

The customer (CU) wants to by a NAV financials. and approached a sales person (SP)
CU asks SP if NAV can perform a specifik task
SP says yes
CU buys
CU tries the system, and the&nbsp;special task
CU concludes, to much regret, that special task is not there
CU says to SP, the system can't perform the special task
SP replies, yes it can
CU says no
SP says yes
CU asks how
SP says "I have to program it"
CU says "Well, then the system can't perform the special task"
SP says "Yes it can".
etc. etc.

PS: CU never got the feature because it was to expensive for her company.
What can we learn from this story, e.g. in regards to communication, optimism etc.
Have a nice weekend&nbsp;
Best regards
Bo W. Mogensen
	              
	              Yes, I am quite aware of the bibliography filtering option.
Anthony
	              
	              Hi All,There are a couple of other plagiarism-checking tools available e.g. SafeAssign, and the free online site -&nbsp;http://www.plagiarismchecker.com/.Anthony
	              
	              Thanks for the summary, Chris. &nbsp;Item 4 on the list is debatable though :-).Regards,Anthony
	              
	              Thanks for the summary and comments, Augusto. 
Yes, notions of academic integrity and IP are important in the work we do, on the Master's Program.Regards,Anthony
	              
	              Thanks for the summary and comments, Martins. The week has indeed covered a varied selection of very useful material for the Master's program.Regards,Anthony
	              
	              Thanks for the summary and comments, Tanisha. The topics you listed, particularly the latter ones, will be important right through the Master's program.Regards,Anthony
	              
	              Thanks for the summary, comments and compliments, Babatunde. &nbsp;I do agree with you that it is important to link the various topics together in some way, and I try my best to do this whenever possible.Regards,Anthony
	              
	              Late in 2013, the U.S. retail giant Target announced they had discovered a massive computer security breach. Criminal computer hackers had potentially obtained the credit card information of tens of millions of Target customers (Target Brands, Inc., 2014). Though Target quickly closed the security hole, the event gave many credit card users pause—even if they had not shopped at Target. Business and technology leaders were already aware of the importance of computer security for financial transactions, but the magnitude of this breach caused many companies to review and reinforce their data security. It even prompted legislation in the U.S. Congress (Carper, 2014).
Computer Data Security is just one of many hot topics in computing that has made waves in the rest of society. Big Data and Virtualisation have also made headlines in recent years. In this Discussion, you will choose a trend in computing, research a related event, and discuss the consequences for IT policy and for society as a whole.
While the university values each student’s personal knowledge, a scholarly argument which includes only personal experience and anecdotal support is considered incomplete. By incorporating and crediting the words and ideas of others, a writer:

Establishes credibility with the reader.
Indicates to the reader that the writer has read widely and critically considered multiple points of view on a topic.
Distinguishes writer’s position from those of others.
Connects to the existing body of knowledge in a discipline.

As you respond to this and all other Discussions and Assignments in this Module and Programme:

Find suitable journal articles and other reliable scholarly resources in the University of Liverpool online library database that are relevant to your chosen topic. These sources should be used in addition to Learning Resources and journal articles listed in the weekly Learning Resources page.
Choose passages from these sources that provide evidence supporting your argument.
Properly incorporate, cite and reference these scholarly sources using Harvard Liverpool Referencing Style.

To prepare for this Discussion:

Review the Learning Resources and the media for this Week.
Think about how other major events galvanised interest, or change, in a particular current area of computing.
Reflect on the reading and decide on the position you would like to take with your argument.
Find at least one article supporting the argument you wish to make in your initial post to the Discussion.
For this Discussion, choose one of the current trends mentioned in the Lecture Notes: virtualisation, big data, or computer security.
Select a significant event (but do not use the Target security breach) and describe how the event has contributed to or changed that computing trend.


Research the event and share your findings.
Reflect on why you feel the event is significant to this computing trend.
Reflect on any social, ethical, legal or human issues related to the event.



To complete this Discussion:Post:&nbsp;Create an initial post in which you analyse an event that has contributed or changed a computing trend, addressing the following:

Summarise your selected, significant event that has contributed to, or changed, a computing trend.
Explain how the event has contributed to or changed that computing trend.


Explain why you feel the event is significant to this computing trend.
Explain any social, ethical, legal or human issues related to the event.


Analyse the following aspects of scholarly argument and academic originality in the article and publication you selected, addressing:


Clarity and organisation of argument.
Presentation of original ideas.
Sufficiency of outside evidence: Does the author present enough evidence support the arguments being made?
Rhetorical use of outside evidence: Is evidence used at the beginning of the article to provide background? Is it used to support a specific point in the author’s argument? Is it presented and then critiqued?
Use of citation, referencing, quoting and paraphrasing in the article.
Evidence this article and publication is peer-reviewed, and therefore acceptable for citing and referencing in scholarly work.



Respond:&nbsp;Respond to your colleagues. Address the following:

Support or refute your colleague’s position how their identified event has contributed to, or changed, a computing trend.
Alternatively, you may identify different computing trends their event has affected.
Be sure to support any claims you make.

For all Discussions&nbsp;(unless stated otherwise):

Create a single document with your initial post. Your document should be 350-500 words, though you will be marked based on the quality of your writing, not on the number of words.
By Sunday, post the text of your document to the Discussion Board for this Week, and upload the document using the Turnitin submission link for this Discussion.
By Wednesday, make 3–5 substantial follow-up responses to your colleagues. These can include responses to your colleagues’ initial posts, as well as responses to colleagues who responded to your own initial post. Your total Discussion Board participation must occur on at least 3 individual days during each week. Follow-up responses should be significant contributions to the Discussion. Do not submit your follow-up responses to Turnitin.
In general, online discussion is best when you:


Ask insightful questions.
Extend the discussion into new but relevant areas.
Model or promote critical reflection.
Support your arguments with citations and references from the assigned Learning Resources and other literature, using Harvard Liverpool Referencing Style.



Carper, T. (2014)&nbsp;Sens. Carper, Blunt reintroduce legislation to protect consumers from identity theft and fraud&nbsp;[Online]. Washington, DC: Tom Carper, U.S. Senator for Delaware. Available from:&nbsp;http://www.carper.senate.gov/public/index.cfm/2014/1/sens-carper-blunt-reintroduce-legislation-to-protect-consumers-from-identity-theft-and-fraud, (Accessed: 20 February 2014).
Target Brands, Inc. (2014)&nbsp;Data breach FAQ&nbsp;[Online]. Minneapolis, MN: Target Brands, Inc. Available from:&nbsp;https://corporate.target.com/about/shopping-experience/payment-card-issue-FAQ, (Accessed: 20 February 2014).
Ensure that you spread your discussion posts across at least three separate days of each week. This will help maximise the value of your discussion with colleagues and serve to meet the learning objectives for each activity.
Click on the&nbsp;Reply&nbsp;button below to reveal the textbox for entering your message. Then click on the&nbsp;Submit&nbsp;button to post your message.

	              
	              Dear Dr. Ayoola,
I cannot find the&nbsp;Turnitin submission link for this Discussion. There are only links for the 1st two weeks.
br, Terry
	              
	                 Trend in Computing - Big Data, Small Devices Terry Yin September 21, 2014 In year 1997, The IBM supercomputer Deep Blue beat the World Chess Champion Garry Kasparov in a human–computer chess match,&nbsp;Wikipedia&nbsp;(2014a). After that, supercomputers have been consistently outperforming the human top chess players. In year 2009, a software program named Pocket Fritz 4 running on a mobile phone reached the grandmaster level and achieved the same performance as IBM Deep Blue,&nbsp;Wikipedia&nbsp;(2014c). There are 3 important factors that decide the performance of a computer chess player, computing speed, algorithm and data, as shown in Figure&nbsp;1.       Figure 1: The 3 factors of computer chess performance   Does the successful story of Pocket Fritz 4 mean that the computing power of a normal mobile phone in 2009 is already at the same level as the IBM supercomputer Deep Blue in 1997? No. the phone that run Pocket Fritz 4 is 10,000 times slower than Deep Blue. It only searches fewer than 20,000 positions per second. Then is it because now we have much better algorithm for the same chess problem? The algorithm has been improving all the time. Chess playing has got fairly large amount of attention from people studying computer science as stated by&nbsp;Is chess the drosophila of artificial intelligence? A social history of an algorithm&nbsp;Ensmenger&nbsp;(2011). From the same paper, we can see that the algorithm hasn’t changed much until 2011. Some 20 years ago, I learnt the     Minimax algorithm for the first time when I was in high school. Today, the essence of the algorithm is still the same. Still, people wouldn’t stop finding better searching techniques and stronger killer heuristic. But this alone would not have led us this far. So what do data do here in solving logically complicated computing problem? Say, we need to check whether a number is a prime number. A slow but complete solution could be something like:      def  isprime (n):          for  i  in  range ( 2 , n):                 if  n  %  i  == 0 :                  return False    return&nbsp;n&nbsp;!= 1  But what about the following solution?      def  isprime (n):          return  n  in  { 2 ,  3 ,  5 ,  7 ,  11 ,  13 ,  17 ,  23 ,  29 ,  31 ,  37 ,    41,&nbsp;43,&nbsp;47,&nbsp;53,&nbsp;59,61,&nbsp;67,71,&nbsp;73,&nbsp;79,&nbsp;83,&nbsp;89,&nbsp;97}  Some will call the second one cheating if it’s a test in an Algorithms class. But the two solutions are&nbsp;effectively the same if the teacher only verify the result with (n&nbsp;&lt;&nbsp;101). And the second solution is a lot faster. This simple example shows that data and algorithms are tradable to some degree. We don’t have always to model the logic in the real world but “letting the data speak for themselves”&nbsp;Gould&nbsp;(1981). And often new opportunities emerge when big data is involved. One example is from the speech recognition domain, as stated in Google’s report&nbsp;Chelba et al.&nbsp;(2012).       Coming back to the story of computer chess. The secret ingredient is also data. Modern computer chess programs all use large opening books and endgame books. Now small devices can also have large storage, and they can also utilize the data in the “cloud”. According to&nbsp;Levy &amp; Newborn&nbsp;(1991), computer gains about 50 ̃70 Elo points by doubling the speed. Elo point is a measurement for playing strength in chess. In 2012, there were 5839 human players at the “Candidate Master” (2200 to 2299 Elo points) level in the world&nbsp;Wikipedia&nbsp;(2014b). There were 3 human at the top having 2800 points. So for a Candidate Master level computer chess program, it has to be 7 to 12 times faster to become the top level master. The famous Moore’s law&nbsp;Wikipedia&nbsp;(2014d) stated “computer speed doubles every 18 months.” Then we will have to wait 18 years to see it beats all the human masters! We can’t wait that long. Big data comes to the rescue! Conclusion Just like human brain is not just a logic machine, we actually solve problems quite often by pattern matching, big data is another important dimension in solving problems in computer science. The big question left here is “how”. My prediction is the exploration of how to better exploit the big data will continue to be a trend in computing. References Chelba, C., Bikel, D., Shugrina, M., Nguyen, P. &amp; Kumar, S. (2012), Large scale language modeling in automatic speech recognition, Technical report, Google. Ensmenger, N. (2011), ‘Is chess the drosophila of ai? a social history of an algorithm’,&nbsp;Social studies of science&nbsp;p. 0306312711424596. Gould, P. (1981), ‘Letting the data speak for themselves∗’,&nbsp;Annals of the Association of American Geogra- phers&nbsp;71(2), 166–176. Levy, D. N. &amp; Newborn, M. (1991),&nbsp;How computers play chess, Vol. 8, Computer Science Press. Wikipedia (2014a), ‘Deep blue versus garry kasparov — wikipedia, the free encyclopedia’. [Online; accessed 19-September-2014].  URL:&nbsp;http: // en. wikipedia. org/ w/ index. php? title= Deep_ Blue_ versus_ Garry_ Kasparov&amp;oldid= 623303193 Wikipedia (2014b), ‘Elo rating system — wikipedia, the free encyclopedia’. [Online; accessed 20-September- 2014]. URL:&nbsp;http: // en. wikipedia. org/ w/ index. php? title= Elo_ rating_ system&amp;oldid= 624923344&nbsp;Wikipedia (2014c), ‘Human–computer chess matches — wikipedia, the free encyclopedia’. [Online; accessed 19-September-2014].  Wikipedia (2014d), ‘Moore’s law — wikipedia, the free encyclopedia’. [Online; accessed 20-September-2014]. URL:&nbsp;http: // en. wikipedia. org/ w/ index. php? title= Moore% 27s_ law&amp;oldid= 626293387    
	              
	              
DQ1, Week 3, Current Trends in Computing

Introduction
I believe in systemic thinking; hence I don’t think that you can look isolated at one related event which caused some trend, i.e. many different events influence each other. But I also realize that for a paper like this, it’s important to narrow the research, hence I have chosen Virtualization as the trend, and “Big data” as the event that galvanised/triggered the interest.
Abbreviations used in this paper:

DQ&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; This initial Discussion Question
VM&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Virtual machine
RDBMS Relational database management system
NoSQL&nbsp; A schema-less database allowing one database to store many different types of data. Used by e.g. Amazon and Google (Stuart and Baker, 2013, sec. 3.3).
Users&nbsp;&nbsp;&nbsp; Here it includes both personal, companies and institutions etc.

Thesis
Humans are creating “Big data” exponentially (Borne 2013). This set demands for a virtualised platform that can deliver “efficiency, scalability, security and availability” (Lecture notes).
Research of a related event
&nbsp;“Big data” is a trend and an event which sets demands for databases (DB) like NoSQL database systems which “achieve unrivalled scalability” (Stuart and Barker, 2013, sec. 3.3). Also the NoSQL has been a driving force behind cloud computing” (Stuart and Barker, 2013, sec. 3.3). Although the NoSQL is designed for virtualised environments it’s lacking relational properties provided by the normal RDBMS (Stuart and Barker, 2013, sec. 3.3-3.4). Since neither RDBMS nor NoSQL systems provide all necessary properties (e.g. relational and scalable), I agree with the authors Stuart and Baker that researchers should focus on a new database system which has all the necessary properties. According to the authors, the researchers have primarily moved their focus to NoSQL solutions.
Of course “Big data” is a trend in itself, but it’s also an event that triggers the need for a virtualised platform which can be scaled continuously without, or minimizing, the disturbance of users.
Focussing on the trend for virtualisation, I’ll start with a definition. “A VM is a software implementation of a computer system, running in isolation alongside other processes, which behaves as physical system.” (Stuart and Barker, 2013, sec. 3.1).
Because of the “Big Data” trend/event virtualisation technology will continue to be a trend since it’s a platform that can supply the kind of scalability which “Big data” needs (Stuart and Barker, 2013, sec. 3.3-3.4). But because it’s relatively new, I expect that we will see several hybrid solutions where users will establish local (in country) cloud solutions combined with cloud solutions in the “sky”. There can be several reasons for this, e.g.:

Domestic laws and regulations stating the specific data, e.g. social security numbers, must not leave the country.
The internet connection is not stabile enough, hence users want to keep the most important data close
Etc.

On the downside of virtualisation there are also negative issues, e.g.:

NoSQL does not provide necessary relational properties (Stuart and Barker, 2013, sec. 3.3).
If moved to a cloud solution, it’s totally dependent on an internet connection.
There can be bandwidth issues it the different regions (Stuart and Barker, 2013, sec. 4.1).
Security relies on “confidentiality, integrity and availability” (Stuart and Barker, 2013, sec. 4.2). And several events have shown that e.g. hackers have broken into these systems.
Laws and regulations are probably not ready in regards to e.g. “what rights cloud providers have to user’s data” (Stuart and Barker, 2013, sec. 5).

Discussing consequences for IT policy and society as a whole
Let’s assume all data, for the whole country, were moved into the cloud:

Social: The entire society could risk being disconnected from the rest of the world if there was no internet connection (this has actually happened for several but shorter periods this year).
Ethical: We could risk that data being (mis)used by the provider, e.g. the provider used human DNA, shopping data, Facebook data etc. for their own benefit.
Legal: “Contracting for cloud based services might be said to be in its relative infancy” (Burden 2014). Also he states that the contracts at present time are rigid to the providers favour. We must ask if current laws allow for moving data to the cloud, and also question marks can be set towards which rights the provider has to the data.
Human: For me as an individual, the technology gives a lot of possibilities, it allows me to move around with my smartphone, iPad, laptop etc. and have data available. But we must also be aware of the risks it brings, e.g. the loss of data.

Conclusion
I believe and hope the trend will continuously move towards cloud as the technology gets better and internet connections more stable. But depending on the need for data security and laws, I’m also expecting a hybrid cloud where users will keep certain data local (read in country). Further I believe that the contracts will change more towards the users (Burden 2014).
Analysis of the referenced articles
I have chosen, for the main referencing, a peer-reviewed paper named “A Cloud Computing Survey: Developments and Future Trends in Infrastructure as a Service Computing”. My impression compared to other papers I have researched for my initial DQ, that the argumentation is good, solid, objective and critical. Being a peer-reviewed paper it has been checked by others with high knowledge about the content, and it has been approved.
References
Borne, K. 2013, 2013-06-10-last update, Big data, small world [Homepage of TEDx talks], [Online]. Available: https://www.youtube.com/watch?v=Zr02fMBfuRA [Accessed: 2014, 09-20].
Burden, K. 2014, "Cloud bursts": Emerging trends in contracting for Cloud services", Computer Law &amp; Security Review, vol. 30, no. 2, pp. 196-198.
Lecture Notes, (2014). [Online]. Available: https://elearning.uol.ohecampus.com/bbcswebdav/institution/UKL1/201480_SEPTEMBER/MS_CKIT/CKIT_501/readings/UKL1_CKIT_501_Week03_LectureNotes.pdf. [Accessed: 2014-09-20].
Trend. Oxford Dictionaries. [Online]. Available: http://www.oxforddictionaries.com/definition/english/trend. [Accessed: 2014-09-20]
Ward, J.S. &amp; Barker, A. 2013, "A Cloud Computing Survey: Developments and Future Trends in Infrastructure as a Service Computing".

	              
	              Yes, I found the same... I was hoping it would show up today sometime.


	              
	              Computer SecurityThe advances in multi access device technology and the misguided approach of companies to help keep data safe.&nbsp;&nbsp;&nbsp; The recent development boom in multi-use devices from mobile phones, PDA’s, laptops, tablets and numerous incarnations in between that has allowed social and business users to access the same information and data of all kinds from anywhere in the world on varying platforms, has led to the increased risk around computer data security. The reason why this is made more significant is because of the lack of control over the differing security products that either is available or what can be available for use on these devices. For example the security you may have on your home desktop pc may be different to your works laptop which is different from you tablet and different again to your mobile phone if indeed it has any at all. This causes issues for security experts and provides opportunities for the unscrupulous who wish to exploit this. In the past it was simple, you had a pc and you had a server. Now you can access the internet through your TV, PC, numerous games consoles, laptops, tablets, in car entertainment systems and more.
I feel that many of the security companies approaches in helping to keep us safe online can be misguided and over concentrated in certain fields while other areas which are equally important get overlooked.Not only has technology played a significant part in this change but the growth of cloud computing and centralized strategies for information sharing has contributed greatly to the effect. Data security is possibly the most important topic in today’s computing world and I fear companies such as Microsoft while doing a sterling job can still get distracted from the bigger picture.&nbsp;
As confirmed by the UK’s own government (Gov.UK, 2014) the National Security Strategy define this threat as ‘Tier one’ along with international terrorism and explain that 81% of large corporations and 60% of small businesses reported a cyber breach in 2013 alone. Microsoft themselves tend to concentrate on what is known as zero-day vulnerabilities, which are the attacks where the security vendor has not released a security update for it yet. According to Microsoft themselves in their Security Intelligence Reports they find that of all the know exploits only 0.37% of them in 2011 were zero-day attacks (Hong J. 2013).&nbsp;
Now cloud computing potentially fuels this increase in threat due to the nature of the varied technology that can be used to access the data. Thankfully many cloud computing services and providers have very good protection but security threats are becoming more subtle. Desai, D (2013), covers the subject quite well when they quoted: “The nuances and possibilities of how we chose to manage data present computer science with the chance to inform policymakers about how best to meet their interests while simultaneously meeting the needs of business and individuals. Providing a better understanding of how cloud computing in its range of manifestations, operates can only improve how the government shapes data policy. Governments should also explain their needs to computer scientists. By presenting issues as engineering problems to be solved, governments would likely stimulate the desire to meet a challenge. Together, governments, businesses and computer scientists ought to be able to leverage advances in technology so all may benefit. Removing the focus on data location as way to increase data security is hopefully just one step in that direction.”
The difference comes from the expectations of the corporations to keep their system clean verses the needs of the general public user. Now according to Hong (2013) when you look at the percentages of how viruses and malware works you find that 44.8% require user action such as installing of some software or update or opening of a document. 43.2% used the windows default ‘Auto Run’ feature all of which tend to be on the client side and involve human interaction so why is Microsoft and other leading companies spending the largest part of their budgets in prevention and detection of zero-day events rather than improving the processes and procedures involved in the new trends with mobile and tablet applications and the interactions with the transition of data. We are seeing a larger trend in security fraud and the stealing of passwords etc from cloud services and game services that use these devices more and more yet in my opinion this area in not concentrated on enough to stop the problem at the potential source rather than curing the issue after the fact.&nbsp;I am not alone in my thoughts with this subject as Hong (2013) summarises: “My main point here is that attackers have quickly evolved their techniques toward what are primarily human vulnerabilities, and research and industry have not adapted as quickly. For computer security to really succeed in practice, there needs to be a serious shift in thinking, to one that actively includes the people behind the keyboard as part of the overall system.”&nbsp; In Conclusion
I would actually summarise and go even further with what Hong (2013) discusses. This may be my cynical and conspiracy theory head on but I’ll ask this simple question: “Do security companies really want to solve the problem or just fix the issues?”&nbsp; The reason I say is because these security companies make hundreds of millions every year protecting our billions. They quote fantastic numbers in the cyber crime reports such as in McAfee (July 2013) report on the economics of cyber crime. Equally so, why would they solve the underlining problems when all of these security companies make vast sums of money in the process of keeping us safe. Why would they put themselves out of a lucrative business?
In my opinion they need to look at the underlying technology as well as the software that sits on it and go from a bottom up approach to prevention rather than a cure. They won’t do this however because we are happy to go along with the current status quo and they are happy to charge us for it. References:Brookshear, G. (2011) Computer science: An overview. 11th ed. Boston: Addison Wesley/Pearson.Desai, D (2013), 'Beyond Location: Data Security in the 21st Century', Communications Of The ACM, 56, 1, pp. 34-36, Business Source Complete, EBSCOhost, viewed 19 September 2014.
Gov.UK, (2014), ‘Keeping the UK safe in cyber space’ [https://www.gov.uk/government/policies/keeping-the-uk-safe-in-cyberspace] Accessed Sept 2014.
Hong, J (2013), 'Is the Computer Security Community Barking Up the Wrong Trees?', Communications Of The ACM, 56, 6, pp. 10-11, Business Source Complete, EBSCOhost, viewed 19 September 2014.
McAfee (July 2013), ‘The Economic Impact of Cybercrime and Cyber Espionage ‘ [http://www.mcafee.com/uk/resources/reports/rp-economic-impact-cybercrime.pdf], Accessed September 2014.
	              
	              Hi Christopher, Thanks for revealing the fact that multi-access devices are making us more&nbsp;vulnerable. I have been puzzled by the existent of software security products since the very beginning. Why would there be security products at all? I'm not saying that security is not important. The problem is real; the issue is important; and it need be solved by specialised knowledge. But isn't security just one of the many requirements to any OS, platform or application? From the Google Trends, it seems I'm not alone. People's interest in anti-virus software companies has declined to 10% of ten years ago. (Google Trends, 2014)   This trend is also quite aligned with my personal experience. I haven't been using ANY security product (except 1Password, but I think 1Password is an anti-security software). So my question is (in addition to your original question), will these security software companies survive to see the future? Who will play the security expert role then? br, Terry References Google Trends, 2014. The trend of keywords Symantec and McAfee. (accessed 21, Sep, 2014)http://www.google.com/trends/explore#q=Symantec%2C%20McAfee&amp;cmpt=q
	              
	              1.&nbsp;&nbsp;&nbsp;&nbsp; Introduction

Cyber-attacks had widely affected the economy and confidentiality of various organizations. It is one of the most growing and serious threats to global economy. So there is a need to develop and upgrade cyber defending strategies with passage of time. 


To compete with outside world, organizations and individuals need to keep themselves updated with the latest trends and policies related to internet security. The advancement in computing technologies has changed the rules of survival and success. Currency notes have been replaced with smart cards and army uses remotely controlled drones to attack. However, the rate of cybercrime is also increasing and now even there are indications of state supported cyber warfare.


Future security trends need to be more focused on dealing with cyber threats and maintaining data security than physical security.


2.&nbsp;&nbsp;&nbsp;&nbsp; Shamoon - Saudi Aramco Cyber Attack Incident


There are number of incidents related to cybercrime. But one of famous and massive cybercrime attack was on oil and gas sectorin August 2012. Saudi Aramco, a major manufacturer, producer, refiner and marketer of natural gas and crude oil was attacked by a virus latter named Shamoon. This virus affected 30,000 computers and it took couple of weeks to recover from damage. Although Shamoon did not affected major oil operations of Aramco butbusiness processes were significantly affected. Data from computer hard drives indiscriminately deleted by Shamoon.


Iran’s nuclear facility at Natanz was also attacked by a worm called Stuxnet. It affected the uranium enrichment process and there are reports that say the involvement of other states in this incident (David Sanger, 2012).


There are hacking groups called Anonymous and OpPetrol that are specifically targeting oil and gas sector. They attacked in 2013 and again announced that on June 20, 2014 that they are going to attack the same targets (Internation Business Times, 2013).


The same company was physically attacked in the past by group of people which was easily handled by the security guards with no damage done but against cyber-attack they were not well prepared. Viruses frequently appear on the networks of multinational firms but it is alarming that an attack of this scale was carried out against a company so critical to global energy markets (Christopher Bronk&amp;EnekenTikk-Ringas, 2013).


3.&nbsp;&nbsp;&nbsp;&nbsp; Significance of Event to Current Computing Trends

It is an important question that how computing and information technologies influence the operation of marketers, producers and manufactures of oil and gas. &nbsp;After the incident of Aramco, oil and gas field players focused on major cyber security threats.

Three major cyber security threats faced by oil and gas industry are

3.1 Data Deletions 

This includes deletion of data on hard drives which is used to manipulate refinement and production of oil.

3.2Stealing of intellectual property

This includes theft of plans, designs and strategic information.

3.3Risk of cyber-espionage

This conduct affects business intelligence resulting in different infrastructure issues. 


4.&nbsp;&nbsp;&nbsp;&nbsp; Conclusion and Trend Indicators

Although shamoon affected a significant number of Aramco computers but from technical perspective this concept of cyber threat is not new to the world. The well known hacker groups were involved in DDoS, hacking of websites and publishing private data but Shamoon incident was very well organized and indicate the involvement of state sponsored attack which targeted national and international economies and raised question marks on international security. These new cyber warfare trends are very dangerous for the world.
Being part of IT department of an oil and gas company in Kuwait, I observe cyber security is the biggest concerns in the region. They are spending significant amount of resources to achieve a satisfactory level of security. They are inviting antivirus and security companies to do audit and propose guidelines to secure the environment. They have even hired ethical hackers to identify the loop holes. Security awareness seminars and trainings are being conducted where experts share their experience and educate people that how to adopt best practices to obtain a high level of confidence because security can never be guaranteed to 100 %
 
References:

Christopher Bronk &amp; Eneken Tikk-Ringas (2013) The Cyber Attack on Saudi Aramco, Survival: Global Politics and Strategy, 55:2, 81-96, DOI: 10.1080/00396338.2013.784468

David Sanger, Confront and Conceal: Obama’s Secret Wars and Surprising Use of American Power(New York: Crown Publishers, 2012).

Internation Business Times. (2013) Anonymous Announces #OpPetrol Attack On Local Oil And Gas Companies [Online] Available from http://www.ibtimes.com/anonymous-announces-oppetrol-attack-local-oil-gas-companies-video-1267817 (Accessed 20 Sept 2014).
	              
	                  Big data is the latest buzz word term associated with technologies used to describe the exponential growth, availability and use of information, both structured and unstructured.&nbsp;  Every day, 2.5 quintillion byes of data is created- so much that 90% of the data in the world today has been created in the last two years alone. As much as 80% of the world’s data is now in unstructured formats (which is created and held on the web). Big Data targets to improve the use of unstructured data.  Benefits of Big Data  Industry analysts predict that big data will provide companies competitive advantage to create higher revenues and economic growth. In addition to offering huge gains to corporations, big data can also presents substantial benefits for consumers as well. &nbsp;“With the proliferation of data throughout all businesses and industries, big data has the potential to yield enterprises substantial profits”[i]. &nbsp;Big Data can have the following benefits in organizations:    Sophisticated analytics can:       Improve Decision-making         Discover risks ahead of time         Discover valuable insights that otherwise would remain hidden         Centralize data         Information is made more transparent   Enhanced knowledge sharing between different business units across the enterprise   Keep the business current with new technologies and best practices; competitive advantage   Big data can be used to develop the next generation of products and services   Uncovering unrealized business opportunities    There are a number of cost and risks in starting a Big Data initiatives, such as:    Human Capital Investment   Infrastructure and Software Investment   Operation and management of integration of Big Data within the existing systems   Time to implement strategies and physical systems    In this papers we will explore the challenges with the shortage and challenges of finding and training the human capital talent (Both Big Data technical and Analytical resourcing) for corporations looking to embark in Big Data initiatives.  “By 2018, MGI estimates that there will be a shortage of 140,000–190,000 more deep analytical talent positions, and 1.5 million more data-savvy managers needed to take full advantage of big data in the United States, Also by 2018, the U.S. alone will face a 50 to 60 percent gap between supply and the demand of deep analytic talent for big data analysis, according to the MGI.” &nbsp;[ii]  &nbsp;  “Existing expertise and skill sets. datasets are often unstructured or so large, that they obscure their underlying meaning, and many businesses have discovered that they must vastly improve or augment their employee structures and skill sets to effectively leverage such information. Any big data program must account for both existing and missing resources, technological and human alike, and be flexible enough to adjust and scale to new sources of unstructured data and new regulations.  Trying to address these needs corporations are experiencing the lack of expertise on staff or inability to identify existing expertise and skill sets”[iii]  &nbsp;  Analytical Gap  Big Data needs analyst that can think quantitatively with large amounts of data and have comfort in Statistics, Probability and Math.&nbsp; There is challenge in the market place in locating these talents.&nbsp;  Data scientist a core team member, ideal candidate is someone with deep graduate school Statistics and Math. Often companies make the mistake of recruiting data engineers, assuming they can teach them Statistics and Math. Engineering skills can be taught and are likely pick up quickly but learning Statistics, Probability and Math take years of learning. Most companies are thinking Big Data initiatives are driven by only technology not realizing once Big Data environment is set up, there will be major need for core skills in analytics and business to extract the Big Data instances.  &nbsp;  “Experts are trained, not born[iv]”. &nbsp;Question needs to be answered if you are looking for technical experts or data experts. Technical experts are required to bring NoSQL databases or Hadoop clusters/instances into production. Data experts are required for data mining, data analytics, text mining, forecasting and machine learning techniques.  &nbsp;  Creative options  Survey conducted by number of leading companies, that Big Data resource will need to be delivered by a mix of retraining, hiring and outsourcing to fill their talent gaps. &nbsp;But getting new higher salaries approved within HR departments, working with recruiters, and interviewing and onboarding candidates is a time-consuming process in an ever fast past recruiting landscape. Companies are starting to re-train their DBA, Data Warehouse Subject matter experts in the Big Data tool sets. There are a numbers of Big Data vendors (such as Cassandra, MapR, Hortonworks and Cloudera) and traditional vendors (like HP) who have instituted Big Data certifications and dedicated Universities to help in the shortfall knowledge gap. Companies are also tapping into the leading Big Data graduate programs to recruit for their Data Analytics/Data Science open positions.  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  Reference    URL: http://www.mckinsey.com/~/media/McKinsey/dotcom/Insights%20and%20pubs/MGI/Research/Technology%20and%20Innovation/Big%20Data/MGI_big_data_full_report.ashx   URL: http://solutions-review.com/big-data/mckinsey-global-expects-shortage-of-big-data-analysts/   URL: http://www.kurtsalmon.com/uploads/Big%20Data%202014%200911VFSP.pdf   URL: http://www.informationweek.com/big-data/big-data-analytics/how-to-fill-big-data-talent-gaps-fast/d/d-id/1108086?              [i] http://solutions-review.com/big-data/mckinsey-global-expects-shortage-of-big-data-analysts/         [ii] http://www.mckinsey.com/~/media/McKinsey/dotcom/Insights%20and%20pubs/MGI/Research/Technology%20and%20Innovation/Big%20Data/MGI_big_data_full_report.ashx     &nbsp;        [iii] http://www.kurtsalmon.com/uploads/Big%20Data%202014%200911VFSP.pdf         [iv] http://www.informationweek.com/big-data/big-data-analytics/how-to-fill-big-data-talent-gaps-fast/d/d-id/1108086?     
	              
	              Hi Colleagues
I hope all of you have had a great weekend - Ive just finished watching the highlights to the Singapore F1 Grand prix - really exciting race and the place looked absolutely amazing!
Has anyone found the Week 3 Discussion Question Turnitin link?
Best Wishes, Craig
	              
	              Week 3 Discussion Question – Current Trends in Computing
Craig Thomas – 21 September 2014
Global mobilisation towards mass Social Networking is forcing a deeper reconsideration of data protection and individual personal privacy rights.
Nanotechnologies and quantum mechanics are advanced forms of miniaturisation that have enabled the production of semi-conductors, transistors and computer chips to be made so small, that it has amongst other things, contributed significantly to the mobile technological evolution of our global culture and particularly with our engagement with Social Networking.
This technological phenomenon has been abstruse and in many situations, takes on profound forms of abstraction, with a comprehension that is somewhat unfathomable to many of us across the globe.
It was only twenty years ago that mobile phones were uncommon, and were typically for the wealthy or big city slickers or computer technicians. Indeed, my first mobile phone back in 1991 was an analogue NEC P3 (…I fell into the category of computer technician). Since then, mobile phones have become ubiquitous. It is suggested (MOA, n.d.) that in the year 2000, 50% of UK adults had a mobile phone and now, this figure is at 93%. 
Advances with this type of technology has moved at such a fast pace, that along with the advent of digitalisation, this has been central to the development of mobile phones and personal digital assistants. The amalgamation of these two technologies with the creation of the ‘Smart Phone’ has disrupted the traditional concepts in communication and social networking. 
We have experienced a significant evolution of our cultural behaviour in the use of such technology. 
It was reported in an article by Protalinski (2014) that at the end of 2013 the social networking phenomena Facebook had passed 1.23 billion monthly active users, of which over 75% of these were active mobile users. One of the other social networking phenomena’s is Twitter, and according to their website they have 271 million monthly active users, of which 78% of these are on mobile technology (Twitter, 2014).
As a result of this trend, and of course some other technological advances such as Google and Yahoo, we have seen a change in our attitudes and behaviour towards mobile technology usage &nbsp;– one of the first things that many of us do as soon as we wake up each morning is to check our email, our Facebook pages or our Twitter feeds (or all 3!), and in many cases this is done on our Smart Phones whilst we are still in bed! 
This exponential use of social networking represents an enormous increase in personal data and information being stored and collected online and is consequently forcing a deeper reconsideration of data protection legislation and individual personal privacy rights – a collection of issues that extend across both legal and ethical boundaries.
It is suggested by Marsoof (2011) that “Privacy and Expression are Oxymoronic” in that they are seemingly self-contradictory. 
I would subscribe to the view that this balancing act is one of the key challenges in providing the appropriate framework to protect both aspects. 
Marsoof (2011) offers the recommendation that “Privacy specific legislation is the most appropriate means of protecting online privacy whilst maintaining a balance between competing rights of expression” however further acknowledges that there are limits of extending existing laws to combat privacy intrusions on the internet especially in the context of social networking.
Althaf Marsoof (2011) has published an academic International Journal of Law and Information Technology that serves to provide useful insight, background and structure to the challenges faced in the conflicting rights of privacy and expression, specifically with Online Social Networking. 
As part of the abstract and introduction, Marsoof references the advances in technology that has “hastened the ability to disseminate information across the globe”. The journal references considerable outside resources and throughout the paper, Marsoof adequately demonstrates various levels of consideration for extension of existing legal positions and acknowledges the intent to “bridge the gap” through express legislation recognising a right to privacy. To add narrative to assisting with the broader perspective, there are also a number of examples and scenarios discussed pertaining to confidentiality, copyright, false endorsement and defamation to name just a few.
In summary, the paper demonstrates that whilst there have been some extensions, there are limitations to these existing laws, and concludes that the “preferred approach is the fashioning of privacy specific legislation”.
Since this journal was produced, proposals have been submitted to the European Parliament to consider strengthening individual protection of personal data held online. Specifically, the “right to be forgotten” which serves to protect individuals who no longer want their information to be used or that there is no justifiable reason for organisations to retain it (European Commission, 2012).
On 13 May 2014, the Court of Justice of the European Union ruled on a number of items, of which included the “right to be forgotten” and the “applicability of EU data protection rules to a search engine”. The ruling is not absolute in that it needs to take into consideration certain other elements such as freedom of expression – this is the ‘balancing act’ that I referred to earlier in this piece.
Google have announced that they will comply with the European Court’s landmark ruling on the “right to be forgotten” and have launched a service to allow Europeans to ask for personal information to be removed from online search results (Lee, 2014) however as of 3 June, it was reported that neither Facebook nor Twitter had commented publicly on the ruling (Beck, 2014).
The key consideration for these organisations, is that they all have a business model which centres on building intelligence and algorithms to collect and identify your personal information, and then subsequently monetises that data to other businesses for products and propositions to be marketed back to you relative to your personal information.
In conclusion, these legislative changes can only be positive for the digital economy, with steps being taken to improve individual consumer confidence in online data protection and privacy. However, I am yet to be convinced that the approach taken to date now provides a full solution that both serves to fully protect individuals and doesn’t unnecessarily restrict expression or business enterprise. Privacy and Expression ‘continue’ to be Oxymoronic (Marsoof, 2011).
Best Wishes, Craig
References
Brookshear, J. G. (2012). Computer Science An Overview. 11th ed. Boston Massachusetts: Pearson. Addison Wesley
Mobile Operators Association (MOA). (n.d.). Stats and Facts – Customer Numbers [Online]. Available from: http://www.mobilemastinfo.com/stats-and-facts/ (Accessed 19 September 2014)
Protalinski, E. (2014). Facebook passed 1.23 billion monthly active users, 945 million mobile users [Online]. The Next Web. Available from: http://thenextweb.com/facebook/2014/01/29/facebook-passes-1-23-billion-monthly-active-users-945-million-mobile-users-757-million-daily-users/ (Accessed 19 September 2014)
Twitter, Inc. (2014). Twitter Usage [Online]. Available from: https://about.twitter.com/company (Accessed 19 September 2014)
Marsoof, A. (2011). Online Social Networking and the Right Privacy: The Conflicting Rights of Privacy and Expression [Online]. Academic Journal: The Online Liverpool Library. Available from: http://eds.a.ebscohost.com.ezproxy.liv.ac.uk/eds/results?sid=4059a92c-190a-4546-b7fb-13dd5ff5bfeb%40sessionmgr4001&amp;vid=0&amp;hid=4103&amp;bquery=Online+Social+Networking+and+the+Right+to+Privacy%3a+The+Conflicting+Rights+of+Privacy+and+Expression&amp;bdata=JmNsaTA9RlQxJmNsdjA9WSZ0eXBlPTAmc2l0ZT1lZHMtbGl2ZSZzY29wZT1zaXRl
European Commission. (2012). How will the data protection reform affect social networks? [Online]. European Commission Justice Newsroom – Data Protection. Available from: http://ec.europa.eu/justice/data-protection/document/review2012/factsheets/3_en.pdf
European Commission. (2014). Data Protection Fact Sheet [Online]. European Commission Justice Newsroom – Data Protection. Available from: http://ec.europa.eu/justice/data-protection/files/factsheets/factsheet_data_protection_en.pdf
Beck, M. (2014). Will Facebook And Twitter Have To Implement The EU’s “Right To Be Forgotten”? [Online]. Marketing Land. Available from: http://marketingland.com/will-facebook-twitter-implement-right-forgotten-86033
Lee, D. (2014). Google sets up ‘right to be forgotten’ form after EU ruling [Online]. BBC News – Technology. Available from: http://www.bbc.co.uk/news/technology-27631001
	              
	              Hi Craig
Sports are good. I'm more of a soccer fan. I couldn't watch "my" team today, so I had to listen in the radio. They were behind with 2 goals, but managed to fight back and win the game 3-2, hence they are still no. 1 with 4 matches to go :-)
I just checked, and the Turnitin link for week 3 is still missing.

Br Bo
	              
	              Hi Terry,

Thanks for taking the time to read my report. I also shared your initial thoughts on why we are at a stage now where we need security products at all. Especially when you have an OS like on apple products and Unix, Linux where not 100% foolproof as no system is but they are significantly better and handling virus protection than other OS on the market.
I'm shaocked they have not produced some kind of hybrid OS where security is the starting block and look at the reasons why some systems handle these situations better than others.
Your also right about the trend in peopl's interest in security. It's a funny thing.. but I think the general public has kind of just taken it as "Hmm it won't effect me" approach. People take it for granted and i'm one of them.. I have an iPhone with no security on it.. I have a MacBook Air with no virus software at all and I don't even worry about it.. My pc at home has Avast on it and my work pc and laptop both have McAfee on them. I havent a clue if my PS4 or TV has any kind of protection on it to be honest i've never checked to find out, i just ignore the fact that technically its still accessing the internet on them.

It's strange that trend though when you think that we are more and more putting our personal and banking data online more than ever. Maybe it's a good thing i'm not sure but it is a strange trend with the ever increasing numbers of users online and products to get us online.
Good question though about if they will be here or not and very hard to answer. My mind says yes they will alwasy be here while there are millions to be made in curing problems and while there are the unscrupulous amongst us who wish to expose these flaws in systems for whatever gain.
My heart hopes that technology designers and software specialists really do get together and produce the most secure OS and software systems we've ever seen pushing the boundaries so that the hackers and virus makers just quit thus being no need for any security companies.
Unfortunately I don't think that will ever happen.


Take for instance the simple light bulb. I know for a fact that we can make a light bulb which is virtually 100% unbreakable under normal use (if you take a hammer to them they will still break) . They are expensive at a projected cost of about £50-£80 per bulb but you would never need to buy any more. I learned this while at Leicester University on a building eco design course.&nbsp;
However the reason why they are not produced is simple MONEY... They wont produce them because they are putting themselves out of a job and the business of selling light bulb would die out. Once everyone has a small supply of vurtually indestructable light bulbs then what does the business do. ?
So they reject the ability to do what is right versus what is financially acceptable for both the business and the consumer.
Now i'm not saying this is the same situation with security but there are similarities. Certainly from a McAfee perspective they wont produce a virus proof system as they may as well sign their own retirement plans now if they did. Where the develpment must come is from the technology and OS developers first to force their hand so to speak.
So I think security companies will survive. They may be less of an impact compare to now but they will adapt t othe situation and always find ways to sell their products..

Sometimes I think these companies have other sister companies full of hackers and virus makers causing caos just so they can then get paid to fix them.



	              
	              Hello Terry,
I want to touch upon another area of BIG data which is actually creating sensations in the current computing world.
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Wearable gadgets will help monitor our health system and support in case of any emergencies.&nbsp;&nbsp;Apple, Google and numerous smaller companies and startups are racing to find ways to monitor every aspect of our lives. Soon, we’ll have more real-time metrics.&nbsp;When you feel healthy or sick, you’ll have the numbers to prove it&nbsp;(Wall Street Journal,&nbsp;Drew Harris, 2014).&nbsp;“Millions of people are tracking their levels of activity daily and healthcare providers are increasingly examining ways to leverage this data to improve levels of care and efficiency,” says ABI Research principal analyst Jonathan Collins, Apr 2014.&nbsp;&nbsp;These wearable devices generate tons of data which can be integrated.&nbsp;&nbsp;"&nbsp;The Jawbone UP wristband folks analyzing sleep data from users living in the California Bay area discovered that the recent South Napa earthquake woke 93% of them. No word yet on how the other 7% were able to sleep through a 6.0 trembler" as per&nbsp;(Wall Street Journal,&nbsp;Drew Harris, 2014)&nbsp;.

References:
Two Ways Big Data Is Reshaping Health Care - http://blogs.wsj.com/experts/2014/09/19/two-ways-big-data-is-reshaping-health-care/
Integrating Consumer Wearable Health Devices Will Drive Healthcare Big Data Adoption - https://www.abiresearch.com/press/integrating-consumer-wearable-health-devices-will

Best regards,
Kharavela
	              
	               Current Trends in Computing
&nbsp;
Computer is no longer just a Desktop PC/Mac or even a laptop device. The increasing memory and speed in computing (Intel i7 Spec, Intel product page) and mobile devices opened up the major market for comprehensive, quick and reliable accessibility to the end user.
&nbsp;
We can see the shift in desktops to mobiles, across healthcare, business, security, educational and all the sectors. All the major firms are encouraging their employees to “Bring You Own Devices” (Josh, Forbes website-2013) and supporting their initiatives with mobile or tablet apps to work with. The same trend is seen in the business world as it has now become mandatory to provide mobile apps and support for all the different OS versions/platforms. Ubiquitous mobile apps are available to enable us to do everything online; like booking bus, train and/or flight tickets, performing bank transactions and many more (IEEE Computer Society journals, 6th point, 2014). As per the Horizon Watching Community quote-on-quote, “Mobile Apps Disrupt Enterprise SW” (Slide 5, 2014), which clearly indicates that the all focus is on the mobile industry. With the help of cloud and next-generation mobile network services, the mobile industry will be lead the way, to fulfill the needs of the end users.
&nbsp;
Then again, mobile computing is quite exposed to threats. The primary targets of the attacks are considered to be the Data, Identity &amp; Professionals; as per (Mobile Security, Threats, Wikipedia), - organization plans, credit card information, an individual’s identity, information on business strategies and military bases. As per (Jon Oltsik, 2014) “ESG recently published a new research report titled, The State of Mobile Computing Security, that looks at mobile computing security holistically across devices, applications, data, and IT security operations. &nbsp;Based on this research, it appears to me that security issues around mobile computing have been way overstated”. The clear indication of this research is about how the market is predominantly focussed on developing and improving the mobile computing experience, and at the same time, making it secure and reliable.

Conclusion:
I believe that the Internet of Things (IoT), with its advanced bandwidth, utilization of cloud, interaction of embedded devices and smart mobile systems, will be the new epicentre of computing. According to Gartner, there will be nearly 26 billion devices on the Internet of Things by 2020! No wonder the computing trends today, revolve around the integration of various devices in order to help the end user have accelerated access to data.
&nbsp;
References:
&nbsp;
Horizon Watching Community http://www.slideshare.net/HorizonWatching/enterprise-mobile-computing
&nbsp;
Intel product page - http://ark.intel.com/products/75131
&nbsp;
Gartner - http://en.wikipedia.org/wiki/Internet_of_Things
&nbsp;
Jon Oltsik, 2014, Managing IT Risk Associated with Mobile Computing Security - http://www.networkworld.com/article/2226699/cisco-subnet/managing-it-risk-associated-with-mobile-computing-security.html
&nbsp;
Mobile Security, Threats - http://en.wikipedia.org/wiki/Mobile_security
Software Revolution, Part IV: Computing in a Mobile World, Written by Josh Manchester - http://www.forbes.com/sites/truebridge/2013/10/22/software-revolution-part-iv-computing-in-a-mobile-world/
&nbsp;
Top Technology Trends for 2014, IEEE Computer Society Resources http://www.computer.org/portal/web/membership/Top-10-Tech-Trends-in-2014
 Best Regards,Kharavela
	              
	              Hi Kharavela,
I'm glad you pointed this out. Because I work for the NHS here in the UK, I personally love this set of new technology that you mentioned. I really do think that this will develop even further in the future and there is a massive market for it. Imagine having every single heart beat, blood presure, body temperature etc etc all recorded 24/7 monitored and saved on your home pc or in the cloud and then accessable by your GP.?
The possibilities for health care are astounding... you could get health booking appointments straight to your phone IF the system detects your cholesterol and BMI getting too high you could automatically get an appointment arranged both to you and your GP based on the data it records. Imagine the possibilities and savings both for your own health but for the health service. as well for detecting issues before they happen like heart attacks and strokes.
It might sound silly but I really do think that in 20-30 years time EVERYONE will have either a wrist device built into a watch or ring or some kind of device attached to them which does this and more.
ta
Chris

	              
	              Hi Ramin
I noticed you mentioning Hadoop. Hasan Mir (2013) explains that Hadoop framework is scalable (Mir 2013) and developed for “Big data”, and the scalability it is handled by applying more machines, hence if you have double the need for data storage, just double the amount of computers. Just as Hadoop and other NoSQL database systems have been created because the relational databases couldn’t’ be used for Big data, I feel this will, at some point, also reach a point where the hardware design won’t be enough. Of course you can always add more machines, but how efficient is that?
Do you agree, knowing about the current growth of data, as you mentioned in your DQ, that it’s necessary to research and develop new solutions which are more effective, i.e. getting more data on less equipment and that are even better of handling this kind of unstructured data?
Mir, Hasan 2013. Turoials. [Online]. http://zerotoprotraining.com/index.php?mode=topic&amp;id=53. (Accessed 20-09-2014)
Best regards
Bo W. Mogensen
	              
	              Hello All,
Any luck with TurnitIn link? I too could not find the link.
Regards,Kharavela
	              
	              Hi Bo,

I just had to jump in on this thread as well. I agree with what you said and would add I think that one aspect of the future of Big Data will be knowing what data not to save. There are ever increasing need, demand and just want to store every possible piece of information and data that people can but like you said that it may get to a point where its not viable to just simply add another server or two to help use and process this data effectively. What may be needed is information archiatects and Big Data specialists to learn what not to save from the vast pool and only get what is absolutely needed and essential.

ta
Chris

	              
	              nothing yet... I think we are all in the same boat...... hope we dont get marked down for not submitting assignments today
	              
	              
Case study: T-Systems
“In 2010, many T-Systems customers requested to run their communications solutions in a cloud environment. Cloud computing goes beyond virtualization and involves creating a utility-like reservoir of pooled, virtualized resources that can be dynamically provisioned as needed.”
“T-Systems was eager to adopt cloud computing so that it could aggressively drive down IT costs and improve service availability. To create a cloud computing environment as quickly as possible, T-Systems turned to Microsoft. It used the Windows Server 2008 R2 operating system with Hyper-V technology and Microsoft System Center 2012 data center solutions to set up a global private cloud infrastructure that could scale to accommodate 1.5 million users.”
Virtualization is one of the most talked about and widely deployed technologies. Many organizations such as T-Systems have implemented the virtualization infrastructure.
Some of the reasons that drove T-Systems to adopt virtualization were the need for deeper automation, T-Systems needed to rise up and meet their customer’s need. They also had huge pressure to increase the level of IT automation to reduce staffing costs, especially at the rate at which the organization was and is growing. The organization also took the virtualization route in order to improve service levels especially in situations where there were problems and their operators had to look at multiple consoles and manually collate the data which was a tedious task that took hours and resulted in delay response to problems.
&nbsp;Since their adoption of virtualization T-Systems is now able to detect failed servers, notify operators and automatically restart them which has resulted in improved service quality and IT efficiency.
Many organizations like T-Systems are realizing the huge benefits of implementing virtualized services which encompass the ease with which system administrators can deploy and manage virtualized systems, easier disaster recovery, single points of control over multiple systems, additional auditing and logging capabilities for large infrastructures as well as role-based access.
&nbsp;This technology is however without its challenges. For T-Systems for instance one of their challenges was finding tools that could more fully automate processes such as creating and decommissioning virtual machines, load-balancing server clusters, and troubleshooting and solving problems. Some other challenges that organizations experience are the numerous configuration options that security and system administrators need to understand. Security is also a concern and as such special consideration in terms of planning must be taken with regard to access controls, user permissions, basically, organizations must rethink their security policies, architecture and processes. However, the benefits outweigh the challenges which is why more and more organizations are going that route.
References
http://www.microsoft.com/casestudies/Microsoft-System-Center-2012/T-Systems/Hosting-Provider-Aims-to-Fix-60-Percent-of-IT-Incidents-with-Automation-Solution/710000000392 [online] accessed 19 September 2014
http://www.sans.org/course/virtualization-private-cloud-security &nbsp;[online] accessed 19 September 2014
http://www.vmware.com/pdf/TCO.pdf [online] accessed 21 September 2014

	              
	              Hi Chris&nbsp;
A good and thought provoking read – I’m not sure I share your cynicism however I absolutely share your concerns.
I was especially surprised to read the large percentage (getting close to half) of breaches requiring user action and I would agree, technology companies need to be doing much more to help us, especially when our vulnerabilities at a consumer level are so easily exploited.&nbsp;
I have also arrived at a question, with some sub questions albeit rhetorical – What about governance and the introduction of policy acts / licensing?
What I mean here is, in other businesses our governments or world bodies require certain levels of compliance to then grant a license to operate. In the Telco world for example, there are BABT regulatory requirements to be met with functions like Billing otherwise, the telecommunications operator license could be revoked. In the Banking world, there are SOX compliance requirements, relative to the sound financial and process management, otherwise there could be severe penalties applied.
I don’t know whether anything is in play for this, however I think it would be good for compliance requirements to be introduced which address the vulnerabilities metrics (again, perhaps focused more towards the vulnerable consumer), forcing technology businesses to address these, rather than just the revenue generating metrics?
In terms of corporation vulnerabilities, I believe the role of CISO is becoming much more visible at board level, and there probably needs to be a separation to the role of CIO to remove any potential conflict of interests in their priorities and operating models. Just wondering whether there are CISO governing bodies on a global level that can force the discussions back into the technology providers for a shared approach to mitigating these security vulnerabilities at a corporation level?
I have personally worked with a technology vendor who I held to account, for security issues that breached our compliance needs and specifications. We didn’t get to the point of legal action, as they were extremely responsive, and built new capabilities to remove the risk. Subsequently we managed the exposures with our customers and together moved forward amicably and professionally. We had specific clauses in our contract with the vendor that outlined our intent relative to risks of this nature.
Just one last point and to offer some of my experience – I have also been involved in the management of some serious data protection violations and thankfully I was supported by experts in Law and Data Security. I am not allowed to share the specifics however I can say it was in relation to storing of customer data.
You make the point of “removing the focus on data location as a way to increase data security…”
In the UK, there are certain criteria relative to DPA and the transfer of personal data beyond the European Economic Area (in fact, we also included these such points in our customer contracts relative to this).
However, whilst you may transfer personal data to countries within the European Economic Area (EEA) on the same basis you transfer data within the UK, you can only send personal data to a country or territory outside the EEA if that country or territory ensures an adequate level of protection for the rights and freedoms of individuals when processing their personal data (Information Commissioners Office, 2014).
So, whilst Cloud will hopefully deliver a more robust level of protection, there is still a need to consider where exactly the cloud storage resides, relative to UK DPA. Hope this insight helps.
A good read, thanks Chris
Best Wishes, Craig
References
Information Commissioners Office. (2014). Principle 8 [Online]. Available from: http://ico.org.uk/for_organisations/data_protection/the_guide/principle_8 (Accessed 21 September 2014)
	              
	              Hi Dr Anthony,
I cant find the discussion 3 folder therefore I cant find the Turnitin link for this week's discussion&nbsp;which means&nbsp; I was unable to sumbit this discussion through turnitin.
Please kindly advise.

Best regards,
Belinda&nbsp;

	              
	              VIRTUALIZATION - Security
A large internet service provider said data for as many as 100,000 websites was destroyed by attackers who targeted a zero-day vulnerability in a widely-used virtualization application.
Technicians at UK-based Vaserv.com were still scrambling to recover data on Monday evening UK time, more than 24 hours after unknown hackers were able to gain root access to the company's system, Rus Foster, the company's director told The Register. He said the attackers were able to penetrate his servers by exploiting a critical vulnerability in HyperVM, a virtualization application made by a company called LXBabs
"We were hit by a zero-day exploit" in version 2.0.7992 of the application, he said. "I've heard from other people they've been hit by the same thing."
Foster said he's been unable to reach anyone at LXLabs to discuss the suspected vulnerability. The Register has also received no response to inquiries sent to the company, which according to its website is located in Bangalore.
According to Foster, data for about half of the websites hosted on Vaserv was destroyed all at once sometime Sunday evening, shortly after administrators noticed "strangeness" on the system. The attackers had the ability to execute sensitive Unix commands on the system, including "rm -rf," which forces a recursive delete of all files.
Some 50 percent of Vaserv's customers signed up for unmanaged service, which doesn't include data backup, Foster said. It remains unclear of those website owners will ever be able to retrieve their lost data, he said. As a result, at least half the websites that were hosted on the site remain offline.
"Since last night, I've had probably 40 phone calls from clients saying 'Why is my website down,'" said Daniel Voyce, a web developer for Nu order webs who uses Vaserv to host customer sites. "It's making me look bad."
Voyce said the hackers, given the high level of server access they gained, were likely able to intercept a wealth of sensitive data stored on Vaserv's servers. Voyce said his customers are safe because all sensitive information was encrypted.
Little is known about the people who attacked the site. So far, there are no known reports of individuals taking credit for the hack. The breach was likely the result of a SQL injection attack that penetrated Vaserv's central management software and removed vital binaries and data for about half of all user data stored by the service, Foster said.
"This wasn't someone randomly scanning things," he said. "It was a deliberate attack on our infrastructure."
Vaserv specializes in low-cost web hosting using VPS, or virtualized private servers. Virtualization features in LXLabs' HyperVM helped Vaserv provide the service, which costs a fraction of the price of dedicated server hosting.
It remains unclear how other webhosts using the HyperVM have been affected. If you're aware of other providers affected by this reported vulnerability, please contact the reporter at the link above. (The Register, 2009)
Virtualization is currently one of the fastest growing trends in the computer world; many companies have opted for these virtualizations technologies because of the many advantages they offer,
But against many are those who think that this technology do not have a future and any organizations that have chosen this technology, are just going straight to an almost certain failure. “The rush to virtualization has yielded a major vulnerability. According to a study just released by Gartner, the majority of servers being virtualized are less secure than they were when they were separate, physical servers.” &nbsp;(eSecurity Planet, 2010) And the kind of event such as which we have described above constitutes an evidence for them.
But for me we need to keep in mind that Virtualization is a new trend, which is called to grow, to expand and evolve. Many people expect that this technology will become one of unavoidable field in computer science and within all IT environments (enterprise, university, government, mobile phone, research, intelligence Agency, etc.).
As said previously the choice of virtualization technologies is essentially due to the fact that they offers many advantages, “system virtualization is widely used for variety of applications, such as, among other things, the consolidation of physical servers (Scott et al, 2010), isolation of guest OSs, and software debugging (Bratus et al, 2008).” (M. Pearce et al, 2013)
Pearce M. Said furthermore, “System virtualization has been attracting a lot of attention, particularly in the last case, because of various technological trends. Some of these trends include increasing commodity operating system complexity, increasing cost of supporting hardware and software systems, and the availability of inexpensive, powerful and flexible commodity hardware.” (M. Pearce et al, 2013)
This trend will continue to grow and will offer many others applications and possibilities in the future, but there are and there’s going to be also many serious security problems. Virtualization is not necessarily as secure as we think, that is the reality and some people think that, this is enough reason to not migrate to that virtualization environment and stay in the physical one.
We need to keep in the mind that, virtualizations are quite new technologies, and many people are also newly experimented these technologies, even enterprises, and this result to many new products on the market are not thoroughly tested and are still under evolution.
About this topic, Michael Pearce said “The properties of virtualization are not only advantageous for security, they can also be detrimental. As virtualization is a large and very vibrant field of research, with new research and threats coming out daily, any coverage can never be fully comprehensive” (M. Pearce et al, 2013)
This can explain some security issues noticed to date, as it’s the case with the event described above. In fact this kind of event are deplorable, because&nbsp; hacking is a criminal act, get unauthorized access to a server in order to commit a destruction of information contained in that system, is a crime. (Wikipedia, 2014)
Hacking is a serious crime, which can affect many innocent people (users), in the case of VAServ, 100.000 users (web sites) were affected and following this event the CEO of LxLabs, the company which developed the virtualization application (HyperVM) was found dead in a suspected suicide. (The Register, 2009)
HyperVM is a system which manages application that facilitates remote administration of virtual machines over the web. Hackers used some vulnerabilities in HyperVM (Version 2.0.7992) and issued a attack against 100.000 web sites hosted by the UK web hosting firm VAServ (socnet, 2009)
It should be noted here that these multiples security vulnerabilities found in HyperVM 2.0.7992 were fixed with the next version of the software (webhosting Talk, 2009).
in short, why I chose this event?, it's to show that the virtualizations as new technologies can have some issues which need to be fixed in additional to advantages that they bring (Especially related to security). “Although virtualization can be advantageous for security, it can also be its downfall” (M. Pearce et al, 2013)
The various issues involved, whether it is in terms of security or otherwise, should not be an impediment but rather a challenge to meet in order to provide effective and sustainable solutions. This should not in any way constitute a reason for not migrating to virtualization but on the contrary. Especially since these technologies offers many organizational and economic benefits.
What is important is to have in mind that these are new technologies, there is still much to do and above all be aware of the problems they have and find appropriate solutions. As stated so well Gabor PEK in his article « A Survey of Security Issues in Hardware Virtualization »:
“The number of reported vulnerabilities and attacks on different virtualization platforms is already quite large, and it is expected to further increase in the future due to the increasing complexity of and additional services in those platforms. Given the increasing popularity of using virtualization technologies, and in particular, the proliferation of cloud computing services, it is important to be aware of these security issues and to address them in an appropriate way.” (G. Pak et al, 2013)
References
M. PEARCE et al. (2013) ‘Virtualization: Issues, Security Threats, and Solutions’. ACM Computing Surveys, Vol. 45, No. 2, Article 17, pp. 1-39.
G. PEK et al. (2013) ‘A Survey of Security Issues in Hardware’. ACM Computing Surveys, Vol. 45, No. 3, Article 40, pp. 1-34.
D. Goodin, the Register. (2009) Webhost hack wipes out data for 100,000 sites [Online]. London: Dan Goodin. Available from: http://www.theregister.co.uk/2009/06/08/webhost_attack/ (Accessed: 19 September 2014)
J. Leyden, the Register. (2009) LxLabs boss found hanged after vuln wipes websites [Online]. London: John Leyden. Available from: http://www.theregister.co.uk/2009/06/09/lxlabs_funder_death/ (Accessed: 19 September 2014)
R. Miller, Data Center Knowledge. (2009) Web Host Hacked Via Virtualization Tool [Online]. London: Rich Miller. Available from: http://www.datacenterknowledge.com/archives/2009/06/09/web-host-hacked-via-virtualization-tool/ (Accessed: 20 September 2014)
Jelsoft Enterprises Ltd. (2009) Vulnerabilities &amp; Hanging: Cause and Effect [Online]: Jelsoft Enterprises Ltd. Available from: http://www.socnet.com/archive/index.php/t-86926.html (Accessed: 20 September 2014)
Wikipedia Inc. (2014) Hacker (computer security) [Online]: Wikipedia Inc. Available from: http://en.wikipedia.org/wiki/Hacker_(computer_security) (Accessed: 19 September 2014)
A. Patrizio, eSecurity Planet. (2009) Are Virtual Servers Less Secure Than Physical Servers?[Online]: Andy Patrizio. Available from: http://www.esecurityplanet.com/trends/article.php/3871716/Are-Virtual-Servers-Less-Secure-Than-Physical-Servers.htm (Accessed: 19 September 2014)
Lecture Notes, (2014). [Online]. Available: https://elearning.uol.ohecampus.com/bbcswebdav/institution/UKL1/201480_SEPTEMBER/MS_CKIT/CKIT_501/readings/UKL1_CKIT_501_Week03_LectureNotes.pdf. [Accessed: 19 September 2014].
&nbsp;
&nbsp;
&nbsp;

	              
	              Hi collegues
I think it's a general issues as I am unable to find Turnitin Link too.
	              
	              Hi Colleagues,
In the absence of a Turnitin Link, I have sent my submission to Anthony directly via Contact Faculty.&nbsp;I realise this is not standard practice, but may be something you want to consider doing yourselves?
Anthony, my apologies for this and any potential inconvenience - I am hoping you understand the rationale.
Best Wishes, Craig
	              
	              Thanks Bo
Fingers crossed hey!
Best Wishes, Craig
	              
	              Hi Kharavela,
Thanks for this - just building on the tracking peice, I read something recently (I can't recall where, sorry) that there is a new entrepreneurial company (I think it was from California) that has built software that can run on your home WIFI router that can pick up movement around your proprerty.
I thought this was great, and the thought of using it for things like healthcare, monitoring people, their heart rates, whether they fall or collapse, and then an auto alert to the emergency services based on the individual at home i.e. the elderly, or whether disabled, under remote care etc.
I know that there are things that people can wear but sometimes they require you to trigger an alert, rather than it being intuitive to the platform.
This could also be the next phase in a more subtle type of intruder detection too?
Best Wishes, Craig
	              
	              Hi Chris,&nbsp;
That is a really great point, predictive technology before the potential health hazard occurs such as heart attack or stroke.&nbsp;
It is an exciting time.
Best Wishes, Craig.
	              
	              Hi Bo, Hi Chris, Hi Robin,
Thought I would jump on to... :-)
Adding more servers, isn't that where Virtualisation plays a key role?
Best Wishes, Craig.

	              
	               The use of 2 factor authentication as a measure to reduce incidences of credit card fraud Background On October 24, 2012, Barnes &amp; Nobles released a press statement to the effect that they had discontinued use of all PIN pads in their 700 stores as a result of a security breach. (http://www.barnesandnobleinc.com/newsroom/customer_notice.html &nbsp;). The PIN pads were devices used for swiping credit cards at their retail stores and the company had obtained evidence that they had been hacked into and customer data possibly stolen. According to Gunter Ollmann, vice president of research at security firm Damballa, “The perpetrators most likely had repeated access to either the card readers themselves or the supporting computer systems, or both”. (Available at http://www.usatoday.com/story/money/business/2012/10/24/barnes-noble-credit-debit-tampering/1653943/) Identity theft has been around since the pre-internet days and the exponential growth of the internet and e-commerce has only given rise to more sophisticated mechanisms of acquiring people’s data such as phishing and malware attacks. Credit card fraud is a subset of identity theft. As a result of this incident, customers were required to change their PINs and check their accounts for occurrences of fraudulent transactions. For a company as large as Barnes &amp; Nobles with a huge customer base operating debit and credit cards, the damage to their reputation was significant with some customers choosing to seek class action suits against them. 2 Factor Authentication This particular event though not isolated, has contributed to the growing adoption and computing trend of 2 factor authentication, as a security mechanism that requires two levels of validations for a transaction – “something you know” e.g. credit card (physical), and “something you have” e.g. password (logical). This is a mitigating measure to lower the risk associated with loss of credit card information to a perpetrator who could then proceed to run transactions on the card. In such event, the credit card alone would not be sufficient to execute a transaction. With the proliferation and ease of access to mobile phones by users, some organizations extend the application of the 2 factor authentication by sending a token code to a user’s mobile phone which they then have to key in in order to complete the transaction. Even though the 2 factor authentication is by no means fool-proof, the extra level serves as a deterrent to would be perpetrators who may not be sophisticated and skilled enough to perform man in the middle attacks. Currently, additional measures for 3 factor authentication (“something you are”) and multi-factor authentication are being applied. Implementation costs though are a limiting factor in their full-scale adoption References  Barnes &amp; Noble Customer Notice - http://www.barnesandnobleinc.com/newsroom/customer_notice.html&nbsp;&nbsp; Byron Acohido, USA TODAY 8 p.m. EDT October 24, 2012 - Barnes &amp; Noble warns customers of data theft – Available at - http://www.usatoday.com/story/money/business/2012/10/24/barnes-noble-credit-debit-tampering/1653943/ NIST Special Publication 800-63-1 - Electronic Authentication Guideline - Recommendations of the National Institute of Standards and Technology Two-factor authentication in two years: Available at http://www.zdnet.com/two-factor-authentication-in-two-years-7000013474/ The failure of the two factor authentication : Available at - https://www.schneier.com/blog/archives/2005/03/the_failure_of.html Nearly 5 million alleged Google account credentials leaked: Available at - http://securityaffairs.co/wordpress/28222/cyber-crime/5m-google-account-credentials.html Google's Announcement Of “Two-Step Verification” And The Availability Of Mobile-Based Two-Factor Authentication Is A Major Step Towards Widespread Adoption Of This Technology - Available at - http://www.goodeintelligence.com/media-centre/view/googles-announcement-of-twostep-verification-and-the-availability-of-mobilebased-twofactor-authentication-is-a-major-step-towards-widespread-adoption-of-this-technology/ http://www.goodeintelligence.com/white-papers   Mobile banking security - Securing&nbsp; the mobile channel: Two factor authentication goes mobile The case for mobile multi-factor verification Next Generation Authentication for the Mobile Enterprise   Justice: Hackers steal 40 million credit card numbers http://edition.cnn.com/2008/CRIME/08/05/card.fraud.charges/index.html?eref=onion  &nbsp; &nbsp; 
	              
	              Instruction
&nbsp;
As Accenture (2014) states that “every business is a digital business”, today, the computing technology is more and more bonded to the corporations and our life. During 2014, the information technology has been swiftly evolving and progressing. There has been more contestation than ever about many issues such as the mobile device technology, the IOT (Internet of Things), the interests of big data, as well as the applications of cloud computing. However, accompanying with the deeper impact of computing technology and the faster changing technological circumstance, there are also more reasons to be anxious. Particular in the Security, it is major premeditation of most corporations’ management. To ready for what the tomorrow may occur, it is significant to review at certain of trends this year to consider the threats and solutions existing the most influence on Computer security.
Secure Internet Connections
During this year, it is possible that the dilatation of devices today connected to internet is the most changes occurring in the past half year. This situation is able to be found via the use of tablets and Smartphones around the globe, but it is also suitable to other daily things which now see themselves with internet access. According to the data of CISCO IBSG, the daily number of internet connected physical devices and objects was around 12.5 billion during 2010, and it predicts there will be 25 billion devices connected to the Internet by 2015 and 50 billion by 2020.(Dave Evans (2011), P.3). The terminology the IOT (Internet of Things) is applied to illustrate the appearance, and it does result in a much Attack Surface for hackers to make use of. Therefore corporations are seeking to let the IT more safe, but it had proved useless that only by merely increasing the traditional security procedures. One of ways takes aim to decrease such number of Attack Surface, restricting the probability of an invasion. The means consists of applying certain fundamental defensive steps to eliminate of infrastructure leak space. For example, improve advanced user identity, better network management and use frequent software patching. Those measures are able to end up lowering Attack Surface by as much as 70% (Stefan Frei (2011), P.2). 
Cloud Security&nbsp;
During last few years, enterprises have commenced really taking advantage of cloud computing in new business way. More than ever before, cloud vendors are providing up-to-date services which are able to assist corporations be more productive and efficient. However, the attackers and security issues are arising later the businesses migrate to the cloud. This is because after migration to the cloud, enterprises will usually transfer their business data there into the bargain. Cloud security is a quite critical task underway, and hackers have been keen to infringe the cloud to usurp the data, not only the businesses' data, but also users' private data. Hackers may actual preserve the sensitive data for blackmail to extort ransom through data-stealing from the cloud. The cloud providers have to offer more powerful security abilities and strengthened the access policies of cloud access to make sure those incidents will not occur.
Mobile Malware
Today, everyone almost has a mobile device, whether Smartphone or tablets. This situation gives a great opportunity for hackers, and the hackers will also never ignore the chance. It is undoubtedly that mobile devices are useful and convenient, but they are also frighteningly vulnerable. According to Amanda Ciccarelli (2013)'s research, she claims 80% of mobile devices have no malware protection at all, that causes the mobile devices as a major target for hackers sighting to get invasion. By the amount of devices that have been infected malware, it is can be found that the number of malware targeted at Android and IOS devices (IPhone and IPad) is exponentially increasing. Of particular concern that the Android malware is frighteningly growing, however, it will take time to secure mobile technology before they turn into a general property on the devices even improvement are already being proposed. 
Security of Third-Party Organizations
Security is also being proposed an extremely more critical main concern for third-party organizations. It has probably been heard of the extensive security vulnerabilities which hit Target, spending the large-scale enterprises millions of US dollars, let alone divulging sensitive data for millions of clients. The hackers could obtain access permission to target systems via permeating a third-party organization that had already connected to the target network. It is much easier to access the larger internal system via infringing on the third-party organizations. Due to this harmful vulnerabilities existing, enterprises are currently spending greater effort than ever to secure whose supply chains, with more focal points being put on improving security for third-party organizations. While it will not be easy to perform this process, since found in the Target's instance, the alternative is merely too expensive.
Conclusion
&nbsp;
It will never be flawless during computer security. Enterprises will have to be continuously alert as who look for hackers aiming to make damage and usurp sensitive information. Although there is not any security strategic that is able to treat all current and future threats perfectly, enterprises are applying great effort to ensure computer security is settled to fulfill those challenges. As security ameliorates, enterprises and personals are able to rest fewer assured understanding whose data is safeguarded.
&nbsp;
References
&nbsp;
Accenture (2014); Every Business Is a Digital Business: From Digitally Disrupted to Digital Disrupter; Available on: (http://www.accenture.com/microsite/it-technology-trends-2014/Documents/TechVision/Downloads/Accenture_Technology_Vision_2014.pdf) (Accessed on: 21-Sep02014)
&nbsp;
Amanda Ciccatelli (2013); Top 5 cybersecurity trends for 2014; Available on: (http://www.insidecounsel.com/2013/12/27/top-5-cybersecurity-trends-for-2014) (Accessed on: 21-Sep-2014)
&nbsp;
Dave Evans (2011); CISCO Whitepaper: The Internet of Things How the Next Evolution of the Internet Is Changing Everything; Available on: (http://www.cisco.com/web/about/ac79/docs/innov/IoT_IBSG_0411FINAL.pdf) (Accessed on: 21-Sep-2014)
&nbsp;
Lianne Caetano (2014); Mobile Malware in 2014; Available on: (http://blogs.mcafee.com/consumer/mobile-malware-2014) (Accessed on: 21-Sep-2014)
&nbsp;
Judith Keeling (2014); The Future of Cyber Security; Available on: (http://www.oxfordtoday.ox.ac.uk/features/future-cyber-security) (Accessed on: 21-Sep-2014)
&nbsp;
Stefan Frei (2011); Cybercriminals do not need administrative users; Available on: (http://secunia.com/?action=fetch&amp;filename=secunia_cybercriminals_do_not_need_administrative_users.pdf) (Accessed on: 21-Sep-2014)
&nbsp;
Zahir Tari, RMIT University (2014); Security and Privacy in Cloud Computing; Available on: (http://www.computer.org/cms/Computer.org/ComputingNow/issues/2014/09//mcd2014010054.pdf) (Accessed on: 21-Sep-2014)

	              
	              Virtualisation, cloud and data ownership
&nbsp;
In 2011 it became clear that the US Patriot Act could be used to access for example European based cloud data when the provider is an US based company. It was Microsoft U.K. managing director Gordon Frazer who explained during the Office 360 release in that year, that &nbsp;cloud data, regardless of where the data is hosted, is not protected against the Patriot Act. This had a great impact on the discussion around virtualisation solutions with regard to US based cloud services in Europe and the already existing data protection concerns. In Jared Carstensen &nbsp;words: ''Any US-owned organisation that touches data on your behalf will make you subject to the Patriot Act''.
&nbsp;
&nbsp;In January 2012 an EU data protection proposal was proposed that requires companies to employ a data protection officer and notify users within 24 hours of a data breach. The proposal went further as it also contains ruling with relation to an individual's right “right to be forgotten“ and was a clear start to redefine the consumers rights in a global, mainly US dominated, cloud services arena. The regulation aimed at improving the transparency with relation to what happens with the data users provide to digital agents.
&nbsp;
The issue of data ownership and privacy in a worldwide networked infrastructure became even more prominent when it became clear in 2014 that the NSA is collecting vast amounts of data tapping traffic at the interconnection points, internet service providers gateways and even directly working together with US-based companies like Facebook. Questions like who is the owner of data, are you allowed to protect them, is the physical location of the server still relevant and what is the role of the infrastructure or solution provider in a virtualised word need further investigation. &nbsp;
&nbsp;
References: 
Whittaker, Zack, (2011), ZDNET, Accessed on 21 September 2014, http://www.zdnet.com/blog/igeneration/microsoft-admits-patriot-act-can-access-eu-based-cloud-data/11225.
Kronebeter, Andreas (2013), 'cloud Security and Privacy in the light of the 2012 EU Data protection regulation', Cloud Computing, Third Inernational CloucComp, Vienna 2012,l Mazin Yousif ed. and others. Accessed 21 September 2014.
Carstensen, Jared (2012), 'Cloud Computing', IT Governance publishing, p265.

	              
	              Hi Belinda
It was interesting to read your paper, with the case study that “…many T-Systems customers requested to run their communications solutions in a cloud environment” and your reference specifically to virtualisation.
Just wanted to share my experiences which may also help bring it to life.
One of the biggest challenges that I have faced over the last decade is constantly being challenged to deliver solutions in support of business and customers needs. 
Traditionally, we (IT) have tended to be reactionary to these needs, was very much behind the times and unfortunately we had taken too long to build capabilities (which was typically a big bang approach). There were many reasons, but consequently this did not enable the business to differentiate, it just enabled them to simply keep up – this however was the norm and was the expectation. "IT is expensive, and we take too long"!
Given the increased pace at which businesses were moving, and continue to move, there was a need for us (IT) think differently, to respond faster to meeting these needs. In terms of IT strategy &amp; technology roadmaps covering infrastructure and applications, we (IT) looked to close the gap by introducing incremental stages of upgrades that moved alongside the same timeline but this came at a much bigger cost and significant management overhead, as the amount of change was and is immense. This was seen as a positive step forward however, as we were trying to be better and responsive to the needs of the business. So, "IT is more expensive but at least we’re getting quicker"!
More recently, we (IT) have been looking to apply different rules, to move our strategic thinking forward in that we began to construct our investment plan to build capability in advance, and be ahead of the demand. This has required a different concept in driving much more engagement with business leaders, to help them define their longer term business strategies (we restructured and added a business architecture function, with more of a business and consulting mindset) and as a result, we then built our IT strategies (combination of business and enterprise architecture) which take us ahead of the demand curve and enable our business leaders to drive differentiation of thought, rather than be hindered through a lack of information technology. We were also able to build better investment plans that had clearer return on our investments, and provided a more detailed view of IT planning. We invested in strategic capability rather than tactical capability that initially cost more, however over the longer term (3-5 years) provided a better investment and ultimately delivered a better ITR rate. "IT are now driving down operating costs and long term investment plans, whilst also delivering ahead of the demand needs"!
Along with an enterprise wide application strategy, and a more focused thinking on longer term business needs, Cloud Computing and Virtualization have been crucial in enabling these type of strategic capabilities to be defined and built, and allowed us to move ahead of the demand curve. In fact the limitations are around legacy platform applications, which either need to be modernised or decommissioned.
Best Wishes, Craig.
	              
	              Hi Terry.
Your post caught my attention the moment I saw the words chess, Kasparov and Deep Blue.
I'd like to contribute to your comment in your conclusion, where you mention that we often solve problems by pattern matching. I recall a course video I watched (The Art of Critical Decision Making) from Professor Michael A Roberta, Trustee professor of management at Bryant University. In one of the chapters, Professor Roberta revisits 2 famous fires where the notion of pattern matching that was commonly used by the firefighters did not add up.
In one incident, the firefighters had been called to put out what they considered to be a routine kitchen fire. Upon entering the house they proceeded to attempt to put out the fire in the kitchen... only that the fire did not stop and seemed to actually increase. This did not fit into the "pattern matching" that they were used to applying in resolving fires. The team leader, then through his intuition figured out that the fire must be coming from somewhere else and called out his team just in time before the floor they were previously standing on collapsed into the basement where the fire was at its highest.
In a second incident, another set of firefighters had been called in to quell what they originally thought to be a routine forest fire. However, when their team leader assessed the situation on the ground, he had the sense that the fire was moving too fast and the traditional methods that they would have applied in this situation would not work. So seeing that they would not be able to outrun the fire, he instinctively ordered his team to drop their tools and went ahead to burn up a circular area so that the whole grass area was scorched black. He then lay down in that area and the fire came and jumped right over where he was lying and he escaped unharmed. According to Professor Roberta, that move was not known before and the firefighting team leader, had just invented it on the spot.
So these two cases demonstrate the use of Intuition,&nbsp;that the firemen used to resolve the problem. I imagine though, that computers are limited to pattern matching.
I guess for Gary Kaspoarov as well, use of intuition must have come into play at some point in his game with Deep Blue
Reference:
Art of Critical Decision Making;&nbsp;Professor Michael A. Roberto D.B.A., D.B.A.Bryant University
http://www.thegreatcourses.com/courses/art-of-critical-decision-making.html&nbsp;


	              
	              Hi Joseph, Hi Terry,
This is a really interesting discussion thread :-)
If I am following...
Joseph, whilst your example of pattern matching is a brilliant way of showing initiative and thinking out of the box, in the context of Terry's theme of playing chess wouldnt&nbsp;that be a case of changing the rules mid way through the game (i.e.&nbsp;if I can use the example of perhaps extending the board by an extra row in this explanation)?
Terry, is your conclusion based on the premise that there is so much more data available to then be able to determine a more complex set of different analytics of data and potential outcomes but still within the original ground rules of the game (i.e. without needing to extend the board by an extra row in this explanation?) and also along the lines of John Conway's Game of Life, with rules on cells and the application to the next generation?
Best Wishes, Craig
	              
	              Trends in Computing: Computer Security
Cyber Security and Countermeasure
Information systems (computers and networks) are increasingly the targets of attacks ranging from vandalism to serious crimes (Richardson, 2003). “Cyber-attacks can substantially hurt an organization's IT environment, leading to serious operational disruptions, from simply damaging the first layers of IT&nbsp;security&nbsp;up to identity theft, data leakage and breaking down networks.” (Neghina, D, &amp; Scarlat, E 2013)
It is therefore important to become familiar with the kinds of possible attackers, attacks, and countermeasures that&nbsp;organization's IT environment&nbsp;could encounter on their computer systems and computer networks (Boswoth &amp; Kabay, 2002; Schwartau, 2001).

In 1988
The Morris worm - one of the first recognized worms to affect the world's nascent cyber infrastructure - spread around computers largely in the US. The worm used weaknesses in the UNIX system Noun 1 and replicated itself regularly. It slowed down computers to the point of being unusable. The worm was the work of Robert Tapan Morris, who said he was just trying to gauge how big the Internet was. He subsequently became the first person to be convicted under the US' computer fraud and abuse act. He now works as a professor at MIT.&nbsp;
JUNE 2007
The US Secretary of Defense’s unclassified email account was hacked by unknown foreign intruders as part of a larger series of attacks to access and exploit the Pentagon's networks.
AUGUST 2008
Computer networks in&nbsp;Georgia were hacked by unknown foreign intruders around the time that the country was in conflict with Russia. Graffiti appeared on Georgian government websites.&nbsp;There was little or no disruption of services but the hacks did put political pressure on the Georgian government and appeared to be coordinated with Russian military actions.
JANUARY 2009
Hackers attacked&nbsp;Israel’s internet infrastructure during the January 2009 military offensive in the Gaza Strip. The attack, which focused on government websites, was executed by at least 5,000,000 computers.&nbsp;Israeli officials believed the attack was carried out by a criminal organization based in a former Soviet state, and paid for by Hamas or Hezbollah.
JANUARY 2010
A group named the "Iranian Cyber Army” disrupted the service of the popular Chinese search engine Baidu. Users were redirected to a page showing an Iranian political message.
The same “Iranian Cyber Army” had hacked into Twitter the previous December, with a similar message.
JANUARY 2011
The&nbsp;Canadian government reported a major cyber attack against its agencies, including Defense Research and Development Canada, a research agency for Canada's Department of National Defense.
The attack forced the Finance Department and Treasury Board, Canada’s main economic agencies, to disconnect from the Internet.
OCTOBER 2012
The Russian firm Kaspersky discovered a worldwide cyber-attack dubbed “Red October,” that had been operating since at least 2007.
Hackers gathered information through vulnerabilities in Microsoft’s Word and Excel programmes. The primary targets of the attack appear to be countries in Eastern Europe, the former USSR and Central Asia, although Western Europe and North America reported victims as well.
The virus collected information from&nbsp;government embassies,&nbsp;research firms,&nbsp;military installations,&nbsp;energy providers,&nbsp;nuclear and other critical infrastructures.

Many security companies approach are offering only the basis solution in other to keep organization's IT environment&nbsp;safe online hence concentrating in certain fields while other important areas are been&nbsp;&nbsp;overlooked. ‘Security methods applied are based mainly on high level information and tools, also, the technologies implemented are generally configured based on existing standards, and not customized on the features of the environments that would have the ability to promptly identify, enclose and restrict, evaluate and restore compromised characteristics’(Neghina, D, &amp; Scarlat, E 2013). Cyber criminals take advantages of this approach through all sorts of deception techniques to make their traces inaccessible.
However, Defenders of&nbsp;IT environment&nbsp;can use a variety of countermeasures depending on the kind of attack and their resources through means of Education, operational processes, patches, security tools and techniques
‘Organizations should transition from basis solution approach to a more risk assessment approach, thus addressing vulnerabilities within the risk management planning and methods. Entities should continuously increase their security awareness procedures, apply active monitoring procedures and complete periodical trainings for all operational and technical personnel to achieve an effective cyber security stance.’(Neghina, D, &amp; Scarlat, E 2013)&nbsp;
It is important to fix flaws or bugs in software as soon as they are discovered, since attacks are typically launched within days of the discovery of major flaws.&nbsp;&nbsp;Manufacturers provide "patches", "security updates", or "service packs" to fix flaws, in the form of modified software that you must go to their Web site to download.&nbsp;&nbsp;The Web site www.cert.org, among others, keeps a current listing of known flaws in important commercial software and their patches. (Neil C. Rowe, 2012)
&nbsp;
Technology already installed for security reasons must be enabled to log security events, to centralize them and as a basis must send alerts in case of incidents recorded or exceptions; signature-based controls such as anti-virus and intrusion-detection software must be implemented.
&nbsp;
Future Trends
Attacks on computer systems and networks will continue to increase in the future if powerful countermeasures are limited.&nbsp;&nbsp;A shift in attackers from amateurs to professionals will continue if basis solution approaches to a more risk assessment approach are still being used.&nbsp;&nbsp;Among the countermeasures currently available, education will remain important in the future.&nbsp;&nbsp;But security tools and techniques, patches, and simple active network defense will increase in importance as technical details of their implementation are worked out. Despite their weaknesses, countermeasures do help protect systems since they have raised the necessary level of sophistication required by an attacker to succeed.
&nbsp;
Conclusion
Diana-Elena Neghina, Emil Scarlat&nbsp;affected a significant number of core cyber threats risk management capabilities which are important enough to increase awareness efforts and to highlight the critical importance of using the full extent of resources provided.
In Nigeria, Cyber security is a big concern. Therefore these resources should be allocated to gather and process cyber threat analysis information, notifying the results and defining alerts for better security measures to be taken by the operational unit.
References
Laureate Education, (2014) Exploring New Avenues with Computing: Week 3 Lecture Notes [Video, Online], (accessed: 19 september 2014)
The history of cyber-attacks - a timeline, (2014) http://www.nato.int/docu/review/2013/Cyber/timeline/EN/index.htm [online], Viewed 19 September 2014)
Brookshear, G. (2011) Computer science: An overview. 11th ed. Boston: Addison Wesley/Pearson.
Neghina, D, &amp; Scarlat, E 2013, 'Managing Information Technology Security in the Context of Cyber Crime Trends', International Journal Of Computers, Communications &amp; Control, 8, 1, pp. 97-104, Computers &amp; Applied Sciences Complete, EBSCOhost, viewed 20 September 2014.
Neil C. Rowe, cyber-attacks http://faculty.nps.edu/ncrowe/edg_attacks.htm [online] (accessed: 19 september 2014)
Boswoth, S., &amp; Kabay, M., eds. (2002).&nbsp;&nbsp;The computer security handbook.
	              
	              Hello Craig, Chris and Terry,
Following your discussion with interest. A promise of the cloud is to deliver a higher degree of protection in a multi devices ecosystem. The security is 'centralised' in the cloud approach on the application side and one should think that companies providing these solutions have the necessary security expert skills as Chris also points out.
There is a concern though, as the iCloud personal picture leakage showed once more, that it is not clear who in this complex environment is in the end responsible for 'hackability' the data protection. The password strength is a users' contribution to security but the fact that procedures can be used to find access is clearly a problem for the provider. How is it possible that one of the best and well equipped IT-companies does not create safe digital content repositories? Many voices in the regulatory area think it is a lack of rules to protect the user. Removing the focus on data location did in this example not result in increased data security.
Craig's point of residence of the actual server has another catch. It is not only the location of the server that counts. There is also the location of the owner of the server. Due to the Patriot Act the US can access all content on servers of US-based companies, even if they are abroad. You can find some more info in my 'official' response to the week 3 discussion below.
Greetings from Bram
	              
	              Thanks to the F1, I had to stay home in the weekend, so I had enough time to complete my essay. Their exercises started already on Thursday. I was having a discussion about computer security with a friend from Symentec that day at a really nice American brewer. We had to prolong the discussion until very late when the traffic recovered.
	              
	              Hello Chris,
The good news is that we need not fwait for next 20-30 years as there are already many companies are working on implementing wearable health care devices and making it simple and cheap.&nbsp;I have come across one such company "nuviun"[1].
This is very exciting development and the key concept of "Internet of Things (IoT)" [2], which will help taking forward these trends.&nbsp;
References
[1]&nbsp;http://nuviun.com/digital-health/sensors-and-wearables[2]&nbsp;http://en.wikipedia.org/wiki/Internet_of_ThingsBest regards,
Kharavela
	              
	              Hello Craig,
Very rightly pointed, interestingly this type of early detection is already under testing. I have come across one such application "m-Health"[1], which detects&nbsp;Cardiovascular diseases (CVDs) with the help of wearable gadgets, mobile and cloud healthcare services [1].
This DQ is very interesting :-)
Reference:
[1]&nbsp;http://lifesciences.ieee.org/publications/newsletter/march-2014/520-cardiovascular-health-informatics-wearable-medical-device-and-flexible-biosensor-for-m-healthBest regards,Kharavela
	              
	              Hi&nbsp;Kharavela,
Thanks for pointing this trend out. I also think wearable devices are coming to us in every direction.
I also think your story reveals another challenge we are having with big data. Quite often we can mind the data to some degree, but we cannot interpret the result, or use it efficiently in our decision making. Like this 7% sound sleepers, we need to dig a lot deeper to make some sense out of the statistics.
br, Terry
	              
	              Hi Chris,
It won't take 20~30 years, even for EVERYONE.&nbsp;
br, Terry
	              
	              Hi Joseph,
I do agree computers are not good at "intuition" at this moment.
But what is "intuition"? What's the mechanism behind intuition? The two firemen stories are not convincing enough to say that the decisions they made were not from their experiences. Perhaps it matched some memory that is not from their firefighting experience. Maybe the so-called "intuition" of human is just fast pattern matching, and our brain is so good at using the data in our memory that we can do inductive reasoning across many forms. Is it possible intuition is just an illusion? (Seems I'm still obssessed with one of the discussion topics from last week:)
On the other hand, perhaps computers don't have to be good at what people have advantages. In fact, the world strongest chess team is a mix of two amateur human players and two off the market PCs. The advanced chess (Wikipedia, 2014),&nbsp;wherein each human player uses a computer chess program to help him explore the possible results of candidate moves, start to get a lot of attentions from people who study AI now.
br, Terry
References
Advanced Chess. (2014, June 9). In&nbsp;Wikipedia, The Free Encyclopedia. Retrieved 02:34, September 22, 2014, from&nbsp;http://en.wikipedia.org/w/index.php?title=Advanced_Chess&amp;oldid=612228741
	              
	              Hi Craig,
Very interesting point. This is also one of the concerns in the article Is chess the drosophila of artificial intelligence? A social history of an algorithm&nbsp;I cited previously.
Problems like chess playing have clear rules, and any state of the game has nothing to do with the playing history (correct me if I'm wrong), and it's transparent to every party in the game. But real life decision making is based on unclear and ever-changing rules, and the state and history be both unformatted, abundant and partial (not transparent). So to apply the same idea of chess playing directly on solving real life problem is far from ready.
But, just like my reply to Joseph, I believe that the computers may also have or borrow the ability of intuition in the future. So that they can use their data (that's their experience) extensively in solving problems in unknown circumstances.
br, Terry
	              
	              Hi Everyone,&nbsp;How much usage is made of cloud computing in your part of the world? Also, are there any legal or security issues affecting their use?Anthony
	              
	              Hi Chris
Feel free to jump in at any time :-)
That's a great angle you have there about "what data not to save". I im convinced that we are saving to much data. A down to earth example: How many people delete the pictures the don't need, that don't bring any value? And antoher example you probably can nod to is "file servers" and "intranets" where people tend to have many copies of the same files and material.
Then again, I'm sure that there is a lot data that we are just not aware of what value they would bring us if they were put into analysis, data mining etc.
Best regards
Bo
	              
	              Hello Chris, Robin and Bo,
Craig, I am following your foot steps..:-)
The concept of virtualisation also helps to achieve green computing [1], it helps in reducing the hardware and carbon footprin.It is the best way forward for BIG Data and it also provides a strong abstraction layer for the data [2].
References:[1]&nbsp;http://explainingcomputers.com/green.html[2]&nbsp;http://www.infoworld.com/article/2611579/big-data/big-data-needs-data-virtualization.html
Best regards,Kharavela
	              
	                 The concept of “Big Data” can be described as a series of things mainly surrounding: the collection of large amounts of data, the analysis of large amounts of data and handling different types of data, according to Computer Structures — Lecture Notes. The emerging need for information storage, analysis and information availability has contributed to this phenomenal. Additionally as more and more organisations deal with increasing amounts of data, they looking for data analysis systems, cloud computing and cheaper means of data storage and availability. This phenomenal as impacted many aspects of our society, including health, social, political and economic aspects. But how can 'Big Data' impact our lives?  “In 2009 a new flu virus was discovered. Combining elements of the virus that cause bird flu and swine flu, this new strain, dubbed H1N1, spread quickly.”, (Big Data, a revolution that will transform how we live work and think, 2013)  The authors went on to explain, that there was no vaccine to use against the H1N1 virus at the time and as such the Public Health Officials needed to prevent the virus from spreading. Thus, the Centre of Disease Control and Prevention (CDC) needed information of new instances of the virus. However, although this information was being provided, there was a major delay in the timeless of this information; persons took some time before they reported the matter to the hospitals, alerting CDC of the information took some time as well, additionally when CDC got the information it took sometime to process it as this was done once per week. The authors later on noted that Google, who had received “more than 3 billion search queries everyday and saved them all” because of it large database of information and statistical capabilities was able to be the to “predict” the spread of the flu. While both CDC and Google was able to know the areas where the flu had spread, Google was able to get this information much sooner than CDC.  The H1N1 virus reportedly took the lives of millions world-wide.  Becauseof the pandemicat hand there was a strong need for information and more importantly information in a timely manner, which was clearly lacking in this situation.There was also a need for some form of online storage and or a larger data warehouse for this type of information. As in the case described,if health officials could have inputed there information into one central database, it would have reduce the time involved in acquiring andlogging of information significantly.We can inferhere that data analysis was also critical.  This event can clearly been seen as one of those that may have significantly contributed to the rise in the era of 'Big Data', as not only health officials sawthe need for data availability but government officials, researchers and scientists as well.  If the necessary information was available in a timely manner perhaps the virus could have been better contained, thus resulting in fewer deaths.  Surprisingly, there was a major software giant, Google, that knowingly or unknowingly at the time possessed the capable of predicting the virus.  The scholarly article, What Next? A Half-Dozen Data Management Research Goals for Big Data and the Cloud, by Surajit Chaudhuri, examines “six data management researcher challenges relevant for Big Data and the Cloud.” In this article he provides clarity and organisation as he speaks to matters concerning, data privacy, approximated results, enterprise data enrichment with web and social media and query optimization, among others. He also provides a clear presentation of his ideas as he makes reference and citations, through his quotations, paraphrasing and referencing, to the ideas belonging to others. Thus providing clear evidence of his research. Additionally, the article also clearly states that it is accepted for scholarly work, thus being peer-reviewed.   Today organisations are using “Big Data” to track statistical, it is used for wether forcasting, politicans are using it to analyse predict elections results, it is used in our health care systems, more and more individuals and organisations are looking to cheaper and more economic ways of storing there data and search engines are using it to track consumer interests, among others. The “Big Data” phenomenal surrounds us and it will clearly continue to be an emerging factor.  References: Understanding responses to government health recommendations: Public perceptions of government advice for managing the H1N1 (swine flu) influenza pandemic. [Online] Available at: http://www.sciencedirect.com.ezproxy.liv.ac.uk/science/article/pii/S0738399111000073 DOI: 10.1016/j.pec.2010.12.026 (Accessed on September 21, 2014)  Computer Structures — Lecture Notes [Online] Available at: https://elearning.uol.ohecampus.com/bbcswebdav/institution/UKL1/201480_SEPTEMBER/MS_CKIT/CKIT_501/readings/UKL1_CKIT_501_Week03_LectureNotes.pdf (Accessed on September 21, 2014)  Viktor Mayer-Schonberger and Kenneth Cukier, Big Data, a revolution that will transform how we live work and think, page 1-2 (Copyright 2013)  Surajit Chaudhuri - What Next? A Half-Dozen Data Management Research Goals for Big Data and the Cloud [Online] Available at: http://delivery.acm.org.ezproxy.liv.ac.uk/10.1145/2220000/2213558/p1-chaudhuri.pdf?ip=138.253.100.121&amp;id=2213558&amp;acc=ACTIVE%20SERVICE&amp;key=BF07A2EE685417C5%2E0622C687C1A10C50%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;CFID=430746031&amp;CFTOKEN=93224243&amp;__acm__=1411334837_62914c3a03fa9c0a4abb8fa8661b6a15 (Accessed on September 21, 2014) 
	              
	              Hi Everyone,&nbsp;There is presently a great deal of concern about the erosion of civil liberties, caused by fairly extensive surveillance and monitoring of individuals, both at work and in their day-to-day lives outside of work. Significant advances in I.T. have made such monitoring relatively easy to carry out by those in authority.What are your views on this particular trend with I.T., and what would you like to see happen?Anthony
	              
	              In April 2014 the internet community was shocked by the discovery of a potentially devastating vulnerability known as the Heartbleed Bug.
"The Heartbleed bug is a defect in a web infrastructure program that can make it easier for bad guys to steal your logins and passwords at many websites, perhaps even your credit card or banking account information".&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (Goldsborough, 2014)
The heartbleed bug affected the open source OpenSSL web protocol used for encrypting information entered on a secured website. "Attackers could exploit the bug to access an application’s memory,
including sensitive data and private encryption keys. The latter could let them launch man-in-the-middle attacks and decode intercepted communications."( Garber, 2014)
&nbsp;
This vulnerability has cause many business and the computing society alike to react by carrying out measures such as security audits, IT policy reviews, system patching and updating were necessary, etc. One of the common effect this event has had, is prompting the use of two factor authentication by several online service provider to increase the security of customers information, making it harder for their online accounts to be compromised.
&nbsp;
This event is indeed significant as it contributes and enforces the need for internet users to be vigilant about securing their account information and implement the necessary personal and organizational safeguards to keep their credentials protected.
&nbsp;
This event brought about social change in many forms. It is well known that many people do not like to change their passwords and when&nbsp; they do not much effort is employed to ensuring that it's a strong password or meets any of the accepted security recommendations. Individual must now consciously update their passwords, keep it safe and use the recommended complexities available. The legal implication far outweigh the social as preaches in your online information can have catastrophic impacts on ones credibility. This event has also highlighted the ethical issues of computing and once a vulnerability is found it can be exploited rather quickly.
&nbsp;
"In Canada, 19 year-old Stephen Solis-Reyes was arrested for allegedly exploiting the Heartbleed bug
&nbsp;to hack the website of the Canada Revenue Agency. It's claimed he stole 900 social insurance
numbers. And in the UK, it's thought that Mumsnet's 1.5 million members may have had their account
details compromised."
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (Network Security, 2014)
&nbsp;
The Heartbleed vulnerability has further galvanized the need for internet users to have secure passwords and multi-factor authentication where available.
&nbsp;
&nbsp;
References
&nbsp;
"Heartbleed bug leads to forking and funding", 2014, Network Security, vol. 2014, no. 5, pp. 1-2. 
Carvalho, M., DeMott, J., Ford, R. &amp; Wheeler, D.A. 2014, "Heartbleed 101", IEEE Security &amp; Privacy Magazine, vol. 12, no. 4, pp. 63. 
Garber, L. 2014, "News Briefs", Computer, vol. 47, no. 6, pp. 12-17. 
Goldsborough, R. 2014, "Dealing with the Heartbleed Bug", Teacher Librarian, vol. 41, no. 5, pp. 68-68. 
LOHR, S. 2014, "Security Flaw Stresses Need to Change Passwords", New York Times, vol. 163, no. 56466, pp. B4. 

	              
	              Hi Ricardo,
Heartbleed bug, what a scary bug. I hadn't heard of it till now. When I see https on my adress bar when making payments I feel a sense of comfort and security, this bug makes me think twice.
It's scary what security vulnerabilities are exposed as we advance technology wise and some things are hard to foresee. As a test analyst, my job is to expose vulnerabilities and bugs in our software/systems and get then fixed before production roll out. Bugs like this truly scare me.
What you said about more secure password s true, people tend to use passwords that they can easily remember which are often easy to guess for hackers.

best regards,
Belinda&nbsp;
	              
	              Hi Craig,

Thank you for your response. You always give such constructive feedback.&nbsp;
You are absolutely right traditionally &nbsp;IT has been taking a more reactive role rather than a proactive role. I like Steve Jobs' thinking of making products that cutomers "didn't know they needed", we need to create the need. Like you said we must "move our strategic thinking forward" and "and be ahead of the demand".

best regards,
Belinda
	              
	              I agree Terry
Obviously without knowing the detail, I guess&nbsp;those 7% could have potentially been out of the area on business or holiday for example. Kharavela's summary only referenced where they lived, not necessarily where they where. Equally, they may have been inebriated on 'Napa valley wine' and&nbsp;just thought the 'trembler' was part of their tasting experience?&nbsp;The BBC (2014) reported that at least 87 people have been taken to hospital, with at least 3 in serious condition.
Detailed analysis and interpretation is critical as you point out - there are many applications in the use of 'big data' and that you can apply 'business specific' intelligence to the data itself to help with interpretation and decision making.&nbsp;
Best Wishes, Craig
Reference
BBC News. (2014). California eartquake strikes San Fransisco [Online]. BBC News USA and Canada. Available from:&nbsp;http://www.bbc.co.uk/news/world-us-canada-28918600 (Accessed 22 September 2014)
	              
	              Hi Dr Anthony,
In South Africa, cloud computing services are being pitched to companies and consumers with promises to change the way people engage with technology, access information and share content. Although there was some hesitation at first due to security concerns with regard to critical company data, more and more enterprises are embracing this trend.&nbsp;

However, there are some hurdles that need to overcome before cloud computing can fully take off, for instance &nbsp;there is the availability of broadband internet issue which has long been a problem &nbsp;for internantional businesses looking to invest in Africa as a whole: 
&nbsp;"More than 10 years ago to this day, most of the continent’s access to the Internet came from satellite based uplinks (with some of the infrastructure sponsored by a USAID project in the late 90s) and analog telecommunication solutions. It was only in 2001, when the South Atlantic 3/West Africa Submarine Cable went live that much of western Africa was given access to reliable digital connection. However, the cost of connections over the cable was high – with an E1 connection costing $300,000 a month. The ridiculously high pricing discouraged actual use of the internet in Africa then, for both residential and business use." &nbsp;-&nbsp;http://cloudtimes.org/2012/10/08/the-state-of-cloud-computing-around-the-world-south-africa/&nbsp;
However, the situation has since improved with several undersea cable networks being implemented in 2009 and "increasing South Africa's national bandwidth to 8 terabits per second". However there are other issues to consider such as the power grid, there are frequent outages as well as a lack of electrical capacity to sustain large companies with server farms.
However inspite of these challenges, quite a numbe rof enterprises have adopted cloud computing and many are seeing the benefits of migrating to the cloud. It seems it's even easier for smaller enterprises as there is less complexity, which is not the case for large organizations who also have to consider compliance issues which require information to be stored within the country.
Besr regards,
Belinda
References:

http://cloudtimes.org/2012/10/08/the-state-of-cloud-computing-around-the-world-south-africa/
http://deloitteblog.co.za/tag/cloud-computing/
	              
	              Hi Craig,
"Napa valley wine" sounds like something I should try in the future. I'm quite sure about the analogy I'm going to make. Just like some people believe that wine tasting is just a myth:
http://io9.com/wine-tasting-is-bullshit-heres-why-496098276
There are people believe big data is overrated:
http://www.slate.com/articles/technology/bitwise/2014/07/facebook_okcupid_user_experiments_ethics_aside_they_show_us_the_limitations.html
I don't believe my numb tongue can do any better with wine but hopefully I will learn how to taste the big data.
br, Terry&nbsp;
	              
	              Hello Craig, Chris and Bram,
I share Craig's idea about the importance of governance and regulation in computer security.
Online sales companies leak customer credit card information because, in the first place, they kept the information. This is rather a business choice rather than any technical decision. So the social part of the security issue seems more important even than technology.
Sorry, I didn't complete one sentence in my first reply to Chris post. I said I hadn't used any anti-virus software. That's not true. I meant to say since year 2006. Nobody could survive without extra protection in the time when operating systems were still not mature, but Internet had come to its age.
br, Terry
	              
	              Hi Craig,
As expressed in another post, I share your idea that social factors are playing a very important role in computer security.
The bright security technology and the dark attacking technology are just like the twins growing from the same root. They will always grow together. Seeking for protection only from technology is not enough.
br, Terry
	              
	              Dear Dr. Ayoola,

I use the service from Linode. I'm developing a new web site and deploy to linode at the moment.
I use the Google App Engine to host my open source project:&nbsp;http://www.lizard.ws.
When I was in China, I used Amazon cloud service to build my VPN. I don't know where the servers are, but they are out of China, because I can use it to by pass the infamous Great Firewall (the GFW) and update my Facebook page.
When I check-in my code into my github open source projects, a https://travis-ci.org service will notice my change. Then resource will be allocated from a cloud consist of volunteer computers. It will then fetch my code, get all the dependencies and do all the unit testing and integration testing on the cloud. When I, or somebody else make a mistake, it will send an email to inform us.&nbsp;
I only buy iOS devices with 16G storage. I kept my movies on the cloud -- at iTunes.
I kept my files on Google Drive and Dropbox.
The last project I was doing before I quit Nokia was to make radio network resource fluid.
I feel much more relax when working on Google App Engine than on Linode. With GAE, I only care about the feature I'm delivering, google will take care about the security, at least the technical security issues. But with Linode where we build our server from clean Linux system, I need to know every detail about the potential risk of such a system and deal with them before hackers found them.
br, Terry
	              
	              Anthony&nbsp;
Cloud Computing is a big theme here in the UK however within large corporations, it doesn’t appear to have been adopted as much or as quickly as perhaps expected. The opposite seems to be true for the smaller enterprises, who can be much more agile in their approach.
Cloud is very much seen as an opportunity to exploit and many have already embraced it however, one of the biggest challenges for IT functions in large corporations is being able to unpick existing services that have already received large financial investments, which in the majority of cases, there must be a realisation of those benefits.
That said, Cloud is very much a central part the strategic planning process, and combined with other key computing trends such as mobile, data analytics, virtualisation etc it is being implemented where there are little benefits to be realised in retaining existing legacy IT services and of course, those new developments whereby Cloud can be adopted immediately.
There are legal constraints to Cloud, and these need to be considered in the process of building your strategy – I referred to one of these one of my follow up discussions, relative to the UK Data Protection restrictions for transferring data outside of the EEA.
Security is also a key factor. There are various choices you can look at to reflect your organisational needs, such as public or private Cloud – for example, business critical services would be more appropriately situated on private Cloud services, rather than something that is perhaps less critical that could potentially reside on public Cloud services.
Some interesting statistics (Cloud Industry Forum, n.d.) that I came across for the UK for last year;

At the end of 2013, up to 75% of UK based organisations will be using at least one type of Cloud service
82% of people say that security is the biggest concern for Cloud software
89% of Cloud users prefer to keep their data in the UK
Security ranks higher than data storage compliance decision&nbsp;

Best Wishes, Craig
References
Cloud Industry Forum (n.d.). UK Cloud Adoption and Trends for 2013 [Online]. Available from: http://www.fasthosts.co.uk/downloads/white-papers/cif-whitepaper-8-uk-cloud-adoption.pdf
	              
	              ...it sounds like you were perhaps happy to prolong your discussions until the traffic abated? :-)
	              
	              Hi Terry,
I enjoyed reading (..a little of) the article your referred, and perhaps it has some truths - there is a well known UK wine taster called Jilly Goolden who is famous for her strange descriptions of wine tasting. Some of them are just ridiculous, and now that I know that our tasting is the weakest of our senses, I am comforted that I was not able to pick out the woody, charcoal and lemongrass with a hint of poppyseed and carpet fur aroma! I made that up, just to give appreciation of what Jilly is like!
I agree, I am looking forward to being part of the big data (I've seen references to Extreme Data and Fast Data recently too) revolution and trends over the coming years. This is one of the modules which interests me for later on in this programme, I'd like to get a deeper understanding of the subject matter.
Best Wishes, Craig
	              
	              Yep it's an area of the tech/computer field I really feel has room to grow into and is extremely exciting.

Author: Craig Thomas Date: Sunday, September 21, 2014 4:41:34 PM EDT Subject: RE: Trend in Computing - Big Data, Small Devices (Terry Yin)

Hi Chris,&nbsp;

That is a really great point, predictive technology before the potential health hazard occurs such as heart attack or stroke.&nbsp;

It is an exciting time.

Best Wishes, Craig.



	              
	              Hi,

Yes indeed.. there are several projects ongoing like the one you mentioned.. granted I was giving a good bit of time when I said 20-30 years... I didnt want to sound like it was arriving tomorrow. Much like when Richard Branson said people would be up in space on one of his Virgin Galactic flights by 2010 and now in 2014 he admits its still a few years away.
I think there will be very good health monitors available soon if not already to some extent with pedometers and pulse watches etc but i'm thinking of the all encompasing device like the project to create the tricorder from Star Trek.
It's a very exciting time.

	              
	              Hi Terry,

I hope your right. I was over estimating just to be sure but as I work for the NHS I also know how long it takes for us to adopt new technology into public healthcare at least. (*smile*).
Privately though i'm sure there will be many available soon.

ta
Chris

	              
	              Hi Kharavela,

Thats a great article you've pointed out and exactly what I was describing. It explains it really well and it's a field of development I think will really take off very soon to the benefit of all. I love it when technology gives back a noticable benefit to society and not just a new social network site or angry birds app.

ta
Chris

	              
	              Hi Craig and Colleagues,
Did you notice the definition of Computer Science in our text book Computer Science An Overview? It defines Computer Science as:




Today, computer science has established itself as the science of algorithms.(J.G. 2011)

I wonder if that is still true. Isn't computer science more about data? If a computer can deal with data, who care about algorithms?
br, Terry
References
Brookshear, G. G., &amp; Brookshear, J. G. (2011). Computer Science: An Overview.



	              
	              Hi Chris,
Oh, you might be right. I didn't notice that it's actually fall into the "healthcare" category.
I remeber someone said people tend to be optimistic about the near future like within 1 or 2 years, but be pessimistic about the longer future like over 10 years.
Tried to find who said that. But I cannot find it perhaps because the bad English I used.
br, Terry
	              
	              Hi Terry
Yes, I noticed that. Something to do with it evolving from mathematics I recall.
Relative to your point, Data cannot do anything without Algorithms. And Algorithms need something to use such as Data. The lecture notes cover some of this too, referring to algorithms being the processes and data being the entities. Isn't the computer itself built on forms of algorithm aswell, the transistors, and the different types of gates to pass the electric current?
Computing itself is all about processing information, so in essence it could be deemed to be all about data.
Best Wishes, Craig
	              
	              Hi Dr Anthony,
Cloud computing is a relatively new concept of computing in Nigeria and its use is not widely spread and has not been fully embraced, though users inadvertently subscribe to this technology through GSM companies that offer to back up storage of files for a subscription fee, as well as email service providers like Hotmail.
No borders within the cloud regarding data protection
The penetration and reception of cloud computing in Nigeria still have some challenges. In regards to data protection, cloud computing raises a number of interesting issues. Data protection law is based on the premise that it is always clear where personal data is located, by whom it is processed and who is responsible for data processing. Cloud computing appears to conflict fundamentally with this evidence. For example, if a customer uses an e-mail service based on cloud computing, the customer's data can be stored anywhere in the world, depending on where the servers on which the necessary storage capacity is available are located. Different services supplied by a wide range of providers are regularly bundled to produce an end-user proposal, for example if the mail service provider obtains the storage capacity required to store its customers' data from other providers. Therefore, with cloud computing it is no longer possible to say where the data is at a certain moment and by whom and how it is being processed.
&nbsp;
Regards,
Babatunde.
	              
	              Hi Kharavela.
The concept of "Internet of Things" has sure gotten currency lately, and the recent move by Angela Ahrendts former Burberry CEO to Apple as retail SVP is a pointer to the aspect that major companies are positioning themselves to cash in in this area.
References:
http://9to5mac.com/2014/09/18/apple-svp-angela-ahrendts-added-to-fortunes-most-powerful-women-in-business-list/
https://www.apple.com/pr/bios/angela-ahrendts.html



	              
	              Hi Babatunde,
The reference to the use of a risk assessment approach is a very practical approach to safeguarding information assets. However, the practice of proactively undertaking a risk management process is not one that is ingrained in many organisations in such a way that they conduct and plan for annual risk assessments.
It's often only after impact of a security breach that many organisations start to address more consciously,&nbsp; vulnerabilities within their systems and assess their level of risk exposure and tolerance. In his book Software Risk Management page 6, Barry Boehm displays a good outline of major steps and techniques in software risk management.
References
Software Risk.&nbsp;Management: BARRY&nbsp;W.&nbsp;BOEHM
	              
	              Hi Anthony,&nbsp;
This is an incredibly large and complex subject and extends across a whole host of different and diverse situations.
I think there is a place for it, and I also think there is no place for it. My view is dependent on the specifics.
For example, lets take something like border control. We have a passport and visa system that enables us to travel and move across the globe, with varying degrees of restrictions. Technology is enabling the authorities to monitor this much more closely and the principle of this, is something that I am very supportive of.
Then lets take your mobile phone. Most mobile operators need to know the location of the mobile phone within a cellular grid system to be able to quickly connect calls to you. This data is being collected and in the last half-decade, operators have been trying to figure out how this information can be monetised, relative to enabling other third party organisations get access to your movements for the purpose of marketing messages and sales opportunities. To some extent, this is one of those things that you can opt in/out of, similarly to dealing with those adverts that pop up on your computer screen when you are surfing the internet. I am generally relaxed about this type of monitoring and haven’t really worried too much about any consequential damage. I can however see why other people may not be as relaxed with this.
Then there is the intelligence type intercept surveillance, which I believe does open up a huge ‘can of worms’ but again, for me it is situation dependent. Shouldn’t we be expecting our authorities to be trustworthy and act with unwavering integrity to be ahead of the game when it comes to cracking down on criminal type activities? I want to be supportive of this type of monitoring and surveillance and to be protected in society, however my attitude towards this is somewhat affected when you see the various journalism reports and security leaks outlining underhand operations and spying activities by organisations like the NSA and GCHQ, often without due process.
It is this that undermines my confidence. I want to be confident about living in a global economy that operates appropriately, and serves to protect our civil liberties. The question is; how can this be done in parallel whilst so much criminal activity is happening around us?
Best Wishes, Craig


	              
	              
Hi Bo and others folks in the thread,
 
&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;A little delayed in your response...
 
Storage growth has not been a challenge for the big giants like Google, Facebook, Apple and Amazon when Hadoop, HDFS[i] and MapReduce[ii] technologies have been utilized for Big Data Analytics. &nbsp;
 
The challenge has been with reducing the latency and keeping the input/output operations per second (IOPS) acceptable for handle the Data Analytics in the back end of Big Data clusters
 
The largest big data practitioners (mentioned above) use &nbsp;hyperscale computing environments also known as HPCC (High-Performance Computing Cluster)[iii]. The architecture comprises of vast amounts of commodity servers with direct-attached storage (DAS)[iv] with Redundancy at entire compute/storage unit level. If server goes down, the whole unit is replaced. As the Data would have failed over to another server.
 
Other mainstream enterprise companies have adopted traditional scaled-out NAS[v] or clustered Network-attached storage - NAS[vi] (is a NAS that is using a distributed file system running simultaneously on multiple servers, vendors such as &nbsp;Isilon and BlueArc) which can be scaled out to 10,000+ of servers
 
Another storage technology (not a mature as Scale-out NAS) is the Object storage systems[vii] can scale to very high capacity. 
 
I do agree though there is a lot of data out there and not all of it totally relevant. One of the initial steps on a Big Data projects is understand your client data structures and governance. Then decide based on the Analytics business case what data sets you need to load into your Big Data cluster and then ultimately do your data analytics.
Robin&nbsp;&nbsp;

 

 
[i] http://en.wikipedia.org/wiki/Apache_Hadoop#HDFS 
 
 
 
[ii] http://en.wikipedia.org/wiki/MapReduce 
 
 
 
[iii] http://en.wikipedia.org/wiki/HPCC 
 
 
 
[iv] http://en.wikipedia.org/wiki/Direct-attached_storage 
 
 
 
[v] http://whatis.techtarget.com/definition/scale-out-storage 
 
 
 
[vi] http://en.wikipedia.org/wiki/Network-attached_storage 
 
 
 
[vii] http://en.wikipedia.org/wiki/Object_storage 
 
 
 
	              
	               
Hi Craig 
 
&nbsp; Yes Virtualization (VDI vendors, oracle, Microsoft etc) and in particular Data Virtualization[i] plays an important role in making Big Data Analytics cheaper, fluid, extensible and seamless interoperability&nbsp;
 
&nbsp;
 
Robin
 
 
 
[i] http://en.wikipedia.org/wiki/Data_virtualization 
 
 
 
	              
	              Hi Colleagues,
Craig I second that point and the lecture notes did make mention of algorithms being the processes and data the entity. But to further extend that point, hardware as currently design cannot operate on its own without a logic operator to instruct it. Algorithms are needed for the step by step instructions for processing data of any form and transforming it into information. Inturn "The algorithm represents our way of&nbsp;analysing the world, and as such, it is dependent on our ability and intelligence"(Laureate Online Education, 2014). The day algorithms are not needed is the day hardware can instruct themselves and become "Self-Aware" as depicted in "Terminator 3: Rise of the Machines".
The trend of Big Data is really exciting and the possibilities are endless, I do believe security and controls to the usage of data is a critical factor that scales as fast as data grows. Imagine having all your medical information collected by a wrist watch type device, transmitted through wireless technology to some storage repository, possibly in the cloud. This infomation can provide critical information on an individual physical, mental and other states, which could be extremely usefull to anyone with malicious or criminal intent. All I'm saying is that while the benfits are largely positive, we must ensure that extensive care is taken to protect the data produced.
	              
	              Hi Anthony,
Fom my side, in RDC there is almost nothing to say about the usage of cloud computing, it’s a new concept and we don’t get really experimented it. Essentially for two reasons, firstly because people (enterprises and particulars) are still hesitate (fear of the unknown).
And secondly, the access to broadband internet is a real problem in this part of the world. It was only last year in 2013 that we were connected to the optical fiber and is not yet fully accessible to everyone and is extremely expensive.
So, on this side we are still almost at zero point with the Cloud Computing. Only same ISPs are using it. The other experience with Cloud Computing is essentially with Smartphone; where we can see some people, mostly young; use Google’s Cloud for saving multimedia data.
Reference:
Radio Okapi (2013) Optical fiber: high-speed Internet is already in Kinshasa, Kin-Key Mulumba ensures [Online]. Kinshasa: Available from:http://radiookapi.net/regions/national/2013/07/07/fibre-optique-linternet-haut-debit-est-deja-kinshasa-assure-kin-key-mulumba/ (Accessed: 22 September 2014)

	              
	              Hi Belinda,
The Heartbleed bug was confined to vulnerabilities that existed in OpenSSL, so websites that used proprietary SSL services like Symantec were not compromised. But the very fact that this vulnerability could have allowed a would be attacker access to secure information collected by affected websites was frightening enough to cause a panic throughout several organizations online.
Since this event people (Well atleast in my communities) are more conscious about there online activities, especially banking and online shopping. Doing their everbest to ensure that they only use protected websites for online transactions.

&nbsp;
	              
	              Hello Peers,
I think algorithms for data deduplication are also playing important role while taking about handling Big Data and storage related concerns.
Regards,,,
Numan Arshad
	              
	              Hello Anthony,
Asia region is receiving good exposure to Cloud computing as it provides some really effective and scalable systems for small and medium business enterprises [1]. "For the 50 million or so start-ups and small businesses in India, cloud computing is set to be a game changer" as quoted on CNBC website [2]. &nbsp;This clearly shows the extent&nbsp;of usage and the potential of the cloud eco system.

As you can see in the figure, Indian Cyber Law has identified the above key legal issues pertaining to the usage of cloud computing [3].
 References:  [1]&nbsp;http://www.theguardian.com/media-network/partner-zone-microsoft/cloud-computing-perfect-small-businesses [2]&nbsp;http://www.cnbc.com/id/101626286#. [3] www.cyberlawconsulting.com
Best regards,Kharavela
	              
	              Hi Ricardo
Thanks for this - yes, Security is and will continue to be crucial for all types of computing, including big data.&nbsp;
Numan talks about Cyber security, and specific attacks targeting data. The potential impacts are huge, not just in relation to deleting data, but also (as you also reference Ricardo) any manipulation of that data and its potential use. Along with the new and exciting trends in computing such as Big Data, Cloud, Mobile etc, it is Information Security that is very much needing to be at the top of our CIO's priorities. I mentioned in a previous post about the role of CISO and that that role is getting more visbility in the boardroom.
I agree, an Information Security strategy will be just as important if not more, as adopting new technological trends in computing.&nbsp;
Best Wishes, Craig
	              
	              Hi Kharavela
Thanks’ for your link to “Nuviun.com”. I think the research and development in this area is fantastic – I have deep respect these people. On the page you linked to, they write “Some monitoring systems require the gathered sensor and wearbles data to be uploaded to a remote site such as a hospital server for further clinical analysis.” I have two people close to me who both have an apparatus in their chest registering if their hart gets regular impulses. An even though this is fantastic in itself, it’s still a long way from also reporting to a “hospital system” which can decide what kind of treatment they need and how fast. It would be fantastic if that would be on the fly, through any wifi or other connection type.
I see this kind of research and progress as one of the most fantastic things that can benefit humans.
The biggest worry I have about this is “Big brother” and companies misusing these data.
Best regards
Bo W. Mogensen
	              
	              Hi Numan,
Interesting to read your post, some insightful perspectives.&nbsp;
I completely agree that security trends need to be more focused on dealing with Cyber threats.&nbsp;
The potential impacts are huge, not just in relation to deleting data, but also (and as referenced by Ricardo) any manipulation of that data and its potential use. As I outlined in my reply to Ricardo, an&nbsp;Information Security strategy will be just as important if not more, as adopting new technological trends in computing.&nbsp;
It was also interesting to read that one of the many steps being taken by the Oil and Gas company in Kuwait to address cyber security is to hire "ethical hackers" to help identify loopholes.&nbsp;
Best Wishes, Craig
	              
	              COMPUTING TREND: BIG DATA
In 1998, the world meteorological organization recorded the highest level El-nino impact. This impact was as a result of the estimated land and sea temperatures going higher than normal and caused by series of climatic changes affecting the equatorial pacific regions. This was felt in most part of the world causing drought, flood, forest-fire and extreme rainfall in America, Mexico, Indonesia, Peru, and western pacific islands. El nino means ‘the child’ in Spanish and it refers to the Christ child because it occurs during Christmas period. El nino happens in every four to twelve years and causes unusual global weather patterns.
However, several years after an observation system has been established giving way for scientist to observe the data of the tropical pacific region in real time. Some of the tools the scientist use to detect these changes include satellites – which give data of ocean temperatures, rainfall and wind. Also, radiosondes give data of the state of the atmosphere and world weather patterns. These data received from all these equipments are stored in the observation systems and are processed by computer systems into information which is then used to monitor and forecast future occurrence of El nino.
This I feel is significant in big-data computing trends because it entails large data which is processed to information. The satellite transmits data to a secured computer system and the system translates this data into information. This process requires a lot of analysis, and space for data storage which is gotten from a super computer and which in turn will be used for strategic planning and forecasting of El nino in these regions. Example of this type of data processing is where I work where data gotten from the satellite is being processed via a computer system to get particular information ready which is used for several purposes.
CONLUSION:
Satellites transmission of real time data enables computer systems to translate these data into information for El nino monitoring in real time.
REFERENCES:
Lecture notes, (2013). [online] Available from: https://elearning.uol.ohecampus.com/bbcswebdav/institution/UKL1/201480_SEPTEMBER/MS_CKIT/CKIT_501/readings/UKL1_CKIT_501_weeks03_lecturenotes.pdf (Accessed: 20th September 2014)
Born, K. (2013) Big data, small world [online]. Available from: https://www.youtube.com/watch?v=Zr02fMBfuR (Accessed: 21st September 2014)
Yekaterina, G. (2004) The El Nino phenomenon: From understand to prediction [online]. Available from:http://www.csa.com/discoveryguides/prednino/overview.php (Accessed: 21st September 2014)
El nino (The freedictionary)[online]Available from:https://www.thefreedictionary.com/EL+nino (Accessed: 21st September 2014)

	              
	              Good evening
Firstly, since Terry asked, a (there are several, but the one’s I’ve seen are very much alike) definition: “Intuition is a process that gives us the ability to know something directly without analytic reasoning, bridging the gap between the conscious and nonconscious parts of our mind, and also between instinct and reason.” (Cholle 2011)
As I am a firm believer, and as I mentioned last week, of being open minded and seeing the need for sometimes to provoke, as do I believe that computers, or rather algorithms, should have some kind of artificial intuition – a kind of a wild card if you like. I’m not sure exactly how, but I’m thinking something like the ability to take in an unknown and simulate from that. I’m not thinking in the lines of “normal” simulations where everything is programmed, but the real think, or as close as it can get now a days – I hope you understand where I’m getting at.
Cholle, Francis P. 2011. [online]&nbsp;http://www.psychologytoday.com/blog/the-intuitive-compass/201108/what-is-intuition-and-how-do-we-use-it. (Accessed: 22-09-2014)
Best regards
Bo W. Mogensen
	              
	              hello Ricardo,
This is my first time of hearing of the heartbleed bug and reading through your post its scary as i have been doing evrything possible on the internet without minding what site.
Please can you share more light on the OpenSSL/ SSL? And how can one know which site are secure and which onces are not?
I know of the symantec site as we use their softwares at work but for OpenSSL sites, how can one secure its system?
Regards
martins&nbsp;
	              
	              Hi Colleagues
Big Data and the Environment...
I've been thinking quite alot about our "Big Data" discussion, and especially with the theme of small devices, nanotechnologies, miniaturisation etc.&nbsp;
When I started work in computing, we experienced the transition from reasonable sized computer rooms into becoming data centres. They housed enormous machines but also many many rows of IBM DASD storage arrays, StorageTek tape libraries and the traditional tape drives. Then with the emergence of Mega Centres, these rooms and buildings became colossal. Then, towards the late 1990's and early 2000's, they started to become somewhat obsoloete as technology was getting much smaller, with the needs for large computer rooms becoming less,&nbsp;for the housing of computer systems and storage.
Now that we're in very much in the era of "Big Data", storage needs are becoming more prevelant again, and datacentres are 're-finding' their purpose. I started to think about the impact on the environmet and came across the infographic (see reference).
- 90% of Data in the world was created in the last two years (Robin had also referenced this in his discussion post)
- 35 Zettabytes of Data will be generated by 2020
- On average the worlds Datacentres use 30 billion watts of energy, equal to the output of 30 nuclear power stations
What is really positive, is when you see companies like Facebook developing Datacentres that run entirely on Wind Power, or other companies with Datacentres across the world that are powered by Solar Arrays.
Some really insightful numbers in here, take a look...
Best Wishes, Craig
References
Van Rijmenam, M. (2014). How big datacentres impact the environment [Online].&nbsp;Smart Data Collective. Available from:&nbsp;http://smartdatacollective.com/bigdatastartups/195561/how-big-data-centers-impact-environment-infographic (Accessed 22 September 2014)
	              
	              Hi Nurman
&nbsp;&nbsp;&nbsp;&nbsp; There may some benefits. In my opinion getting reducing duplicate data and shorten the data transfer is a factor (def. of data deduplication)&nbsp;but not a major driver in Big Data clusters. Do you have specific findings you can share? Thanks.
robin
	              
	              Hi Robin,
Thanks for the very useful information. I did some study around the information you shared. I think it's essential to my big data study.
Yes, the storage capacity has not been a challenge, it's growing all the time. I have a colleague who has been following a "no deleting" rule for several years. He never deletes any email, photos (he would delete unwanted immediately) or any other data. And it seemed a larger storage would always come to mean stream when he needed it.
I think what's really challenging is how to split the big data and put them in the storage. In QCon 2010, I joined a discussion about this with engineers from Twitter, Facebook and the Chinese Weibo. They all have simple but enormous, fast-growing data tables. There has never been one solution that solve all the future problem, because everyday they are facing a problem in a different scale.
That was 4 years ago already, I wonder whether the exploration by Twitter, Facebook, Chinese Weibo and the other companies having very big table already made the others' life easier within a certain scale. Can anybody point a direction to where I should look at?
br, Terry
	              
	              
Hi Terry,

I noticed your comment, "Isn't computer science more about data? If a computer can deal with data, who care about algorithms?", and as such I would like to share this with you.

In 2009 when the H1N1 virus was discovered, one of the things that the Centre of Disease Control and Prevention (CDC) wanted to do was to contain its spread. As such they needed to know from the hospitals, all reported cases of the virus. Prior to this Google was able to predict the virus based on the queries they received from their search engine. Thus like the CDC, Google was able to establish which areas were affected with the virus. However, they could have only accomplished this through a series of mathematic algorithms, “In total, they processed a staggering 450 million different mathematical models in order to test the search terms...”, (Big Data, a revolution that will transform how we live work and think, 2013). Clearly data would be meaning less without some form of algorithms, to interpret it.

As you may have already read, “Informally, an algorithm is a set of steps that defines how a task is 
performed.”, (Computer Science: An Overview, 2011)

Going back to our history of computing, while the computer stores data in using binary (1s and 0s), we as humans need to build applications and programs to tell the computer what to do with this data and this can only do done through algorithm development.

Today as we see in the Universities around us, the structure of a Computer Science Program is often based on software engineering, algorithms and mathematics. 

Data would be nothing if we do no know what to do with it or if we cannot come up with interpretations of it and this interpretation can only be done through algorithm development.

The entire concept of computer science is based on algorithms because without it the computer may not have existed to begin with. Algorithms are the core of every application and or program that is built.

In closing I would like to ask you to do this, image yourself building a house with no idea of how it is built.

References:

Viktor Mayer-Schonberger and Kenneth Cukier, Big Data, a revolution that will transform how we live work and think, page 1-2 (Copyright 2013)

Brookshear, G. G., and Brookshear, J. G. (2011). Computer Science: An Overview.



	              
	              
Terry,
I knew I read something far more captivating on algorithms in my lecture notes but surprisingly I could not have found it at the time of my initial post. However, as I just did, let me share it with you, it should clear up your concerns as it relates to algorithms.

“Everything a computer does is expressed as an algorithm. The expression of an
algorithm is required for a simple task of adding just two numbers, managing the
inventory of a large factory, landing the Apollo space vehicles on the moon and
injecting intelligence into the computer. The algorithm represents our way of
analysing the world, and as such, it is dependent on our ability and intelligence. It is
the tool by which you can instruct the computer on what to do, and your proper
mastery of the algorithm is essential to the success of the task. “ Computer Structures — Lecture Notes

I do hope that statement clears your concerns.

References:
Computer Structures — Lecture Notes [Online]. Available at:
https://elearning.uol.ohecampus.com/bbcswebdav/institution/UKL1/201480_SEPTEMBER/MS_CKIT/CKIT_501/readings/UKL1_CKIT_501_Week03_LectureNotes.pdf (Accessed on September 22, 2014)

	              
	              Hi Craig, Ricardo and Colleagues,
I just feel that defining the Computer Science as the science of algorithms ignore the importance of data the computer is working on. After all, people's expectation of a computer is the output data by the given data. Probably it's the science of ... say, transforming data? I know I must be very naive now but I cannot stop thinking about this.
Yes, I'm a strong believer of the "rise of the machines". I'm thinking of forming a coalition of "International Association of Fighting Computer Slavery". Shall we build that community among the colleagues first?

br, Terry
	              
	              
Dr. Anthony,

There still may lie a bit of hesitance as it relates to this matter, as some organisations may have a lot of skepticism as to what gets stored online. This is mainly so, because of issues surrounding privacy and security. However, as more and more individuals and organisations are becoming aware of the capabilities of 'The Cloud' they are seeking out the services that best suit their needs and pockets. Many organsations have systems, which utilizes services such as virtualization, for example web services and the use of thin clients, which are offered through the organisations private cloud. These organisations are using these virtual servers as a means of reducing cost, maintenance and improving availability. They are also using Managed Service Providers (MSP) to manage Spam emails among others. I believe more and more uses of the Private Cloud is coming on stream as more and more data is needed to become accessible easily.

Additionally as smart phones and laptops are becoming increasingly popular, more individuals are choosing online storage services, such as Dropbox and Google Drive as a means of storing their data. We also see the use of Software as a Service (SAAS) among this group as many are using services such as Google Apps. Thus, cloud computing maybe gaining greater popularity here as well.

Software developers are also making use of this era as it offers Infrastructure as a service (IAAS), where software developers can build applications using the software provider infrastructure, which is hosted on the Internet. 

Based upon of the use of the cloud services and the sensitivity of the data in question, there may arise issues of security. However, I believe a prime legal issue that may arise, depending on which cloud service being used, is an issue of licensing.

References
Nuno Santos, Krishna P. Gummadi and Rodrigo Rodrigues - Towards Trusted Cloud Computing . [Online] Available at: https://www.usenix.org/legacy/event/hotcloud09/tech/full_papers/santos.pdf (Accessed on September 22, 2014)

Computer Structures — Lecture Notes [Online]. Available at:
https://elearning.uol.ohecampus.com/bbcswebdav/institution/UKL1/201480_SEPTEMBER/MS_CKIT/CKIT_501/readings/UKL1_CKIT_501_Week03_LectureNotes.pdf (Accessed on September 22, 2014)


	              
	              Hello Everyone,
Enterprise Content Management (ECM) systems have gained traction in recent years, and have been utilized in areas such as e-Health, for managing medical records from diverse sources/media.
Have you worked with, or encountered, any commercial ECM application(s)? &nbsp;Are there any drawbacks or cautionary issues with such schemes?
Anthony

	              
	              Hi Dr Anthony,

I have worked with and encountererd ECMs before. One of those is our South African Revenue Services &nbsp;aka SARS. &nbsp;This ECM system allows users to submit a variety of tax returns including VAT, PAYE, SDL,UIF, Income Tax, STC and Provisional Tax though their website and mobile app.
An ECM system such as this one has numerous benefits to users such as the zero cost, it is free, it is simple to use and secure. Users have a full history of all submissions.&nbsp; &nbsp;At my organization we also have an ECM system for all employees; which is used to store our employment information such as monthly payslips, leave applications etc. ECM sytems from a business/organization standpoint have benefits such as; they allow direct, efficent access to information, they allow for easy collation of data, and reduce operational costs through a sound infrastructure core that optimises resources and costs among many others. "
ECM provides a great set of tools and technologies which can help achieve great business results through increasing speed of marketing and other operations, mainly through faster access to relevant information."-http://www.streetdirectory.com/travel_guide/159359/enterprise_information_systems/benefits_of_enterprise_content_management_systems_.html&nbsp;
I'd say the drawbacks are not drawbacks per se, but rather factors to take into consideration when implementing an ECM. These include security, support and performance.&nbsp;
Best regards,
Belinda

	              
	              Hi Anthony,
Liberty of “normal” people is now constantly monitoring (Electronic Monitoring), especially in developed countries. Various technologies developed by computer science have significantly increased traceability of people worldwide. All our activities, our conversations, our tastes, hobbies leave traces in multiple computer systems that we use in everyday life. All these data are collected, centralized and stored by public or private organizations;they can know at any time the profile and location of each individual. 
The presence of surveillance cameras are increasing in most cities. Installed in the streets, train stations, subways or public buildings. To these are added cameras on roads, face recognition software. This software is able to simultaneously detect multiple faces in a crowd by comparing them with faces whose image stored in a database. Thus, the monitoring process can be fully automated, so systematized. 
In a sense this may be understandable in the sense that the state is now confronting various types of threats, crime, terrorism, illegal immigration etc. So to fight against these evils requires effective tools, independent, automated and powerful. 
And development of such monitoring software and hardware represents a significant advance for IT, and we still hope of many advanced in this area. This trend of IT is progressing well and will still grow very quickly, because of the market demand.
But then it also caused some serious problems of civil liberties and protection of privacy, because people feel violated in their intimacies with all these tools of surveillance in public places, the workplace, etc. 
Hence the eternal dilemma: people's safety or people’s privacy? Honestly I will sleep better knowing my children safe wherever they are.

	              
	              Hi Terry
&nbsp;&nbsp; Also look into two other techniques
"Small data": Think of all the digital tidbits consumers leave in their paths as they go through the day. Credit card payments, location fixes, newsletter signups, Facebook likes, tweets and Web searches. You can think of it as trail of breadcrumbs' behind us with our digital service providers, which together create our digital traces.
"Thin Slicing". Thin slice is a manageable data slice from the Big Data cluster so one can do focused and manageable analytics.

robin

	              
	              Hi Craig,
I guess the 35 Zettabytes of Data is mostly just redundent information. One of the reasons that we are create new data so fast is that we are duplicating very fast. We do not only duplicate existing data without adding or extracting any new meaning from them. We also duplicate the raw information from the physical world around us without digging any deeper. This is another challenge we are facing, too much and too redundent information.
br, terry
	              
	              Hi Tanisha,
Thanks for your brilliant reply. Great examples and very good points. Thanks you.
The assumption of solving problems with algorithm is based on the arithmetic nature of the problem. So if it can solve problem for 1 and 2, it can also solve problem for 3. But don't forget that the algorithm, or the arithmetics it's based on, is a generalization of the specific (sometimes infinite) situations. These situations are represented by data. So, I may define algorithm as an generalized solution to a set of problems.
Generalization may comes from two sources: theoretic deduction, or generalized from specific cases. I believe the truth is in the specific data, any computer algorithm is just a generalization of a certain data set. It's just today data still cannot speak for themselves.&nbsp;
I believe, in the long-long-term furture, algorithms will be very cheap and data will be very expensive. Do you agree?
br, Terry



&nbsp;



	              
	              Hi Craig

I second your example on the issue of border control and also the mobile phone part but the ease with which it is now possible to encroach on personal civil liberties is possibly the more relevant point here. Before, people were less willing to give up their information, but the feeling of liberation that comes from being able to is the trap – not an intentional one, but one that can be used. One might argue that with such freedoms, such visibility of information, there can be no more security. But why should we be so demure and allow those who are in power and those who would like such power to be so free with our information? We imagine some government official rubbing his hands with glee at the realization that we should begin giving up so much personal data to the public domain in the near future, and a whole new government department evolving to deal with it. This is evidently not true, but information sequestered in existing departments there might be. But then, have not governments be bent on this sort of activity long before the advent of computers and concerns over terrorism and so forth? If those who run our government are so feeble minded and wish to collect personal data, then let them burn for doing so. I can’t imagine government police rattling on my door in the middle of the night, I hope, nor the thought police watching me, but we have already seen this government trying to delete past speeches, almost trying to change history. ‘Transparency’ that word that so readily drips from conservative lips seems to have lost all meaning. We are allowing a government that has little love for its people to make decisions that we do not want. It is our fault; not one else’s.
	              
	              Hi Dr Anthony
I have not worked with any. The closest I have come to learning about is Enterprise Configuration Manager Repository view used in the company I work for. Which provide access to target, metric, and monitoring information stored in the Management repository.
Regards,
Babatunde
	              
	              Dr. Anthony,
Cloud computing is fast growing in my country and the Caribbean as a larger community. As a part of the western hemisphere we seem to try and keep pace with the new technological developments from our Northern neighbours (USA, Canada). 
There is still a lot of work to be done in order to facilitate better utilization of cloud services. Some of these areas includes internet penetration, bandwidth access, service reliability and cost. 
Cloud computing is big for small businesses and personal use; these categories of persons utilize hosted service like email, documents storage and backup facilities. Several small companies have migrated to Office365 which provides the flexibility of software as a service at an affordable price. These categories are not too concerned about legal and security issues, many simply ensured that local backup facilities are in place should they encounter challenges with the cloud service or access to it.&nbsp; 
On the other hand large companies are slowly engaging in cloud computing. Companies with international affiliation may have their own private cloud, or possible diversified infrastructure with a mix of public and private facilities. Some organizations still have large infrastructures and will slowly migrate aspect of that facility to the cloud to take advantage of the cost benefits. Security, reliability and legal arrangements are critical to these organization, especially for services hosted in other regions. From my experience when engaging service providers for hosted service the terms of agreement for a service contract will outline matters of liability, confidentiality, intellectual property, etc. This will provide the assurance to the customer in any event.
Cloud computing is a developing area, especially for large businesses and we'll see that continue to grow in the future and the possible implementation of &nbsp;specific legal structures to safeguard both the service provider and the customer.
&nbsp;

	              
	              Hello Craig,
The hacker group, Anonymous initiated #OpPetrol to launch cyber attacks on June 20, 2013 on the oil companies in countries involved in the global oil trade. After this threat, when our company KNPC invited ethical hackers, it was very interesting idea for most of us and it really helped to secure the network. I remember they were trying to sneak in and giving feedback to the security team to take necessary actions to close the loopholes. They stayed till the day when the hackers announced they will attack and continuously monitored the activities on network to detect the intrusion. End of the day, we were unharmed by this attack and the idea worked.
 
&nbsp;
References:
&nbsp;
http://www.youtube.com/watch?v=Ko_TQ_skYCY
&nbsp;
http://www.cyberoam.com/blog/anonymous-to-attack-oil-companies-on-june-20-with-oppetrol/
	              
	              Hi collegues
I believe that for many years onwards we will use algorithms (in lack of something better?).
And Terry, I agree with your last question, that data will be the more expensive part. I have worked in a bank for many years, and done my “small queries”. And looking at all these data, seeing how they increase and how they can be made more useful, I was long ago convinced that data was the most valuable thing we had. And I was even more convinced at a SAS institute seminar in Stockholm somewhere back in the 00’s. And I remember coming back, writing a recommendation to the management to hire people with knowledge and competencies within statistics and analysis and data improvement (they didn’t listen at that time – and the bank later went bankrupt).
Look at this citation by Bottles et al (2014) which I support:
One cannot simply combine multiple databases, crunch the numbers, and magically uncover actionable correlations that can automatically and unthinkingly be implemented. Human beings with domain expertise and knowledge of the problem being investigated have to oversee the collection of the data, the asking of the right questions, and the interpretation of the results in order to make the best use of this disruptive technology tool.
To comment on it, I do actually believe that “magic” can happen when you look at data, i.e. if you are open minded at look at data from different angles, also angles which didn’t plan, magic can happen, and you will find patters. But still, you need people to interpret and decide that the data can be used 
And I believe the above mentioned citation also supports Tanisha’s sentence witch starts with “data would be nothing….”.
Bottles, K., Begoli, E. &amp; Worley, B. 2014, "Understanding the Pros and Cons of Big Data Analytics",&nbsp;Physician Executive,&nbsp;vol. 40, no. 4, pp. 6-12. 
Best regards
Bo W. Mogensen

	              
	              Hi
Here is just a little note on the environment issue. If we believe my neighbours, the Icelanders, then “all” (perhaps my interpretation) data centres should be moved to Iceland because they have 100% green energy. They have enormous unused energy resources. And they are even planning to install a cable from Iceland to England to export the energy [2].
[1] http://www.invest.is/opportunities/data-centers-in-iceland/
[2] http://www.datacenterknowledge.com/archives/2013/10/15/icelands-renewable-power-play/
Best regards
Bo W. Mogensen

	              
	              Dear Robin,
What I found is data deduplication is an ideal technology to reduce storage footprint and improve operational efficiency upto 5 – 10x for Big Data environments and upto 5x capacity reduction that is enormous savings for Big Data storage. &nbsp;Also deduplication is very helpful and improve efficiency while taking and restoring backups Big Data environment.
Regards,,,
Numan
	              
	              Hi all,
&nbsp;
Have a nice day! There is some additional information to supplement my discuss that the security treat and its resolution strategies: 
&nbsp;
Malware:&nbsp; Malicious software can include viruses, worms, Trojan horse programs, etc. but most importantly websites that host malware, which has become the most prolific distribution method.&nbsp;  Resolution: URL Filtering, Patch Management and Other Protections. Proactively manage the sites where employees are allowed to surf by limiting them to safe, approved sites from reputable web publishers. Employ Patch Management and system AV &amp; spyware protection to combat the malware threat. 
&nbsp;
Exploited Vulnerabilities (Use the attack surface): Hackers try to find a weakness in a commonly used system or software product and exploit it for their gain.
Resolution: Implement Comprehensive Patch Management:&nbsp; Often some of the most sensitive data are on non-Microsoft systems such as Linux, UNIX or Macintosh.&nbsp; Invest in a patch management solution offering full visibility into your network and covering all operating systems and vendors, not just Microsoft.&nbsp; Consider host-based intrusion prevention (HIPS) which can monitor your system looking for anomalous behavior, applications attempting to be installed, user escalation, and other non-standard events. 
&nbsp;
Unstable 3rd Party:&nbsp;&nbsp;While there is an increase in IT security expenses required to keep up with the growing threats cape and regulatory environment, there is a decrease in revenues in the market.&nbsp;This may lead many providers to go out of business or cut corners that could lead to a security compromise.&nbsp;  Resolution: Consider Streamlining Your 3rd Party.&nbsp;Ensure that you are using providers that have been in business for a long time, have seen hard times before and have been regulatory focused for years.&nbsp; Ask for audited financials and ensure your provider is profitable. Choose a firm that can offer you multiple solutions via one integrated portal to gain the benefits of economies of scale and reduce the burden on existing IT staff resources.&nbsp; 
&nbsp;
BR,
&nbsp;
KingTan Yu
&nbsp;
References
&nbsp;
Abhishek P. Bhatt (2013); Computer &amp; Network Security Threats; online: http://www.academia.edu/4052668/Computer_and_Network_Security_Threats
Andrew Brandt (2009); 17 High-Risk Security Threats (And How to fix them); Online: http://www.pcworld.com/article/157106/security_threats.html 

	              
	              Hi Craig,

Thanks for the reply and comments.
I fight every day with the cynical/conspiracy devil on one of my shoulders and the logical nice angel on the other lol.
I guess it makes me question things more which can be good and search for answers but at the same time it can come across as me being negative as well.
Your absolutely right though with regards to governance and policy. I regularly spend time with our head of governance to talk about this exact thing in our areas of business and I feel the same that these lessons should be transfered to this avenue of computer security.
In business as you mentione there are rules around where the data can be held. For example we in the NHS are doing this right now as we are moving some streams of work to the cloud. However we have to make sure that the service provider is within the EEA like you said and that they abide by the terms and conditions needed for the data we will store with them and then make sure it is signed off by the government before we do it.
Socially however these rules don't exist as far as I know of, so for you and I having a cloud based service to save our personal data and pictures the avenue is open to lesser policies and thus more chances of interception and missuse.
Thanks
Chris


	              
	              Hi Martins, OpenSSL is the open source version of a Secure Scoket Layer protocol that is used by websites, especially ecommerce sites to encrypt data transmission. This is required for banks, online retail stores, etc. to ensure that credit card and other sensitive information are not communicated over the internet in plain text. Ecommerce, banking and other website that would collect personal/credit card information from users will do so through a secured webpage denoted by the "https://" URL in the address bar and some website display the seal of the security certificate at the bottom of the page. This is the official website "http://heartbleed.com",&nbsp;that can give you more information on the heartbleed bug. You can also read the journals I referenced in my initial DQ post and there is tons of information online just ensure the source is credible. 
	              
	              Hi Anthony and Everyone,
&nbsp;
Regarding the legal or security issues affecting the cloud use, as Ivana Deyrup &amp; Shane Matthews (2014) indicate, Due to the novelty of cloud computing, there are only a few laws that regulate this method of storing and sharing information. The situation of US is that although states have their own computer crimes laws, the most important law regulating cloud computing is the federal Computer Fraud &amp; Abuse Act (CFAA). While this law was not designed to target crime in the cloud—instead it was aimed at other kinds of criminal activity on computers—several of its provisions can be applied to harmful activity on the cloud. it is the same situation in Hong Kong that there is not any concerning statutes to guide use of cloud. Generally, we are adopting Privacy and Confidentiality law to protect the benefits of cloud users. Cloud provider will breach of privacy laws if:
&nbsp;

Using the personal data for unauthorized purposes
Selling the personal data to third parties
Not using adequate security measures

&nbsp;
However, cloud providers based in Hong Kong are usually not subject to Hong Kong’s Personal Data (Privacy) Ordinance that a data user means someone who controls the collection, holding, processing or use of data as well as does not include someone who holds, processes or uses personal data solely on behalf of its customers and not for its own purposes.
&nbsp;
References
&nbsp;
Ivana Deyrup &amp; Shane Matthews (2014); CLOUD COMPUTING &amp; NATIONAL SECURITY LAW; Available on: (http://www.academia.edu/526807/CLOUD_COMPUTING_and_NATIONAL_SECURITY_LAW) (Accessed on: 23-Sep-2014)
&nbsp;
Nicholas Blackmore (2014); Legal issues arising from cloud computing; Available on: (http://www.austcham.com.hk/wp-content/uploads/2012/03/Cloud-computing-Austcham-2014.pdf) (Accessed on: 23-Sep-2014)
&nbsp;
Paul Ticher (2012); Cloud Computing - Data Protection and Other Legal Issues; Available on: (http://www.ictknowledgebase.org.uk/cloudcomputingdataprotection/1) (Accessed on: 23-Sep-2014)

	              
	              Hello Ramin, 
Thank you for your analysis.
You do not mention the use of big data for personalisation services. Companies like Netflix, or almost any other content providers opening large libraries to audiences, work with big data pipelines to convert a variety of log files and user settings into useful data. In this case not for business intelligence per se (it is also used for that because you can track what is popular and improve the content proposition on basis of that information) but recommendations based on earlier use patterns. Metadata is mapped with the ontological assessments which are often enriched with other peoples use patterns, be it friends or unknown people with comparable preferences.
This field of big data opens a lot of questions on who can use which data and how far companies can go creating profiles. It has a great impact on society.
Greetings from Bram
&nbsp;
	              
	              Hi Anthony,
Here in the UK cloud storage and use is actually quite well taken and has been adopted by many businesses. For example the UK government G-Cloud plan sets out a drive to reduce government IT costs by £200m per year by calls for 50% of new government IT spending to move to cloud computing services by 2015.
A government "app store" called CloudStore was launched in February to offer such services to the public sector. According to some more than 75% of all UK businesses will use at least one type of cloud service by the end of the year.
Certainly the worrys are always about the security of the data it will then hold and the policies or legalities around what happens if anything happens. When you host and own your own servers you can hardly sue yourself for any breaches in security but when you pay for a service and the same happens with a 3rd party supplier then contract based policy making means companies can take legal action against providers if such things do occur.
ta
Chris





	              
	              Hello Anthony,
I am at present working on a similar project, where the application manages the contents and images and validates based on defined business rules.Our CMS application deals with enormous number of SGML and XML, as well as various types of images (JPEG, CGM, PNG,BMP,TIFF). But I have never used any of the e-Health ECM's.The simplest commercial ECM which comes to my mind is Microsoft's SharePoint server which can be customized depending on user requirements.
Best regards,Kharavela
	              
	              Hi Tresor
Very well put indeed, I especially like your last sentence.
Best Wishes, Craig
	              
	              Absolutely Chris, isn't that why we have 'two' shoulders?... :-)
We're on the same page. I have a challenging mindset myself. I try to encourage a culture within my teams of 'high challenge' 'high support'. It gives us the ability to talk openly and challenge our thinking, but equally we're then extremely supportive of each other.
I use Cloud myself, iTunes, iCloud, Dropbox, Acrobat.com etc to name just a few. I have no clue as to where the data is held. Its quite ironic really, that I pay no attention to personal items such as these and yet in business we're all over this sort of stuff.&nbsp;
Best Wishes, Craig
	              
	              Thanks Bram
I read something in the press about the whistleblower 'Snowden' saying that it is frightening what the NSA can see about anything and everyone.
What is the price of freedom?
I am not sure I have the answer but I refer to Tresors last post, in that I too sleep better at night knowing my family is safe.
Best Wishes, Craig
	              
	              Thanks Bram
I read something in the press about the whistleblower 'Snowden' saying that it is frightening what the NSA can see about anything and everyone.
What is the price of freedom?
I am not sure I have the answer but I refer to Tresors last post, in that I too sleep better at night knowing my family is safe.
Best Wishes, Craig
	              
	              Hi Dr. Anthony
To my knowledge there are no statistics on cloud usage on the Faroe Islands, so I will explain how I see it. Firstly, it is fair to divide the usage into 3 groups (could be more sub-groups):

Private
Google tools for e.g. mail and calendar are much used
Dropbox is also very much used
Then also the usage of other app’s on mobile devices

Industry
Dropbox is used to some extent
Otherwise it’s my impression that they have just slowly begun using cloud solutions, e.g. Office365 and Google tools

Public (government and municipalities)
I’m pretty sure they don’t use any cloud solutions, except perhaps some employees using their dropbox.


The main reasons, from my knowledge talking to colleagues, for not using cloud solution to a bigger extent in the industry and public sector is that they don’t know enough about it, and the uncertainty of law issues. The “Data protection agency” [1] described a case, in last year’s annual report [2], where a tax department wanted to use Dropbox. The law about personal information defines certain demands about how to store information, when to delete data, that data must be encrypted etc. Since it’s unlikely that Dropbox will change or live up to these demands, the department decided not to use Dropbox.
[1] Data protection homepage: http://dat.fo/
[2] http://dat.fo/documents/B8ADC84A-41AF-4C61-91BD-15F72F950D22.PDF
Best regards
Bo W. Mogensen

	              
	              hi Bram
&nbsp;Excellent point, Big data is huge with companies like Amazon, were they provide the personalisation analytics you mentioned. Thanks for highlighting.
Robin&nbsp;
	              
	              Hi Tanisha
Interesting read, thanks for this.
Another example of the role Google can play in quickly analysing the data from search results - identifying trends, search patterns, common themes at certain times and in certain places.&nbsp;
Best Wishes, Craig
	              
	              Hi Anthony
One’s in a while I have this naïve thought that I life in a so small country, that nobody bothers to monitor us (lol). But when that thought passes, then sadly I do believe that also we are being monitored, and the monitoring happens on many areas, e.g.:

Individually: primarily for commercials and sales
Companies: trying to catch their possible business intelligence
Politically: perhaps especially here, because even though we are a very small country, we have big ocean areas around us with huge fish resources, and also oil and gas potential.

I agree with Craig about the principle, and that it’s good to have in e.g. passport controls. Also I’m supportive in other kinds of monitoring like cameras on roads etc. if the data are used for criminal investigation, hence data not used for its good potential should be permanently deleted.
And then there is the IoT (Internet of Things), where I can see several areas where it would be beneficial for all humans, e.g. the monitoring of energy usage in the different tools – but again, only if it’s according to it’s original purpose, for example the energy usage data could be collected by Statistics agency of the contry.
My conclusion is that if data are used as intended and for good purposes, then I’m for it. But having said that I’m sure that:

Were already on a slippery slope where we accept increasingly more - voluntarily forced.
Those who monitor don’t care what we citizens say or do.

PS: I think we have to admit, that we have seen, that technology has been misused in again and again.
Best regards
Bo W. Mogensen
	              
	              Hi Anthony,&nbsp;
I havent worked with the enterprise content management system but am optimistic I will in the nearest future.
However, I have heared of other management systems like the enterprise document management system. &nbsp;
Are they the same?

Regards
Martins
	              
	              Hi Ricardo,
Thanks for your reply, explanation, and references it really help give me more insight into the heartbleed bug and openSSL.
I imagine how many people out there without this information and they just enter every site they come accross.

Regards
Martins
	              
	              Hi Anthony
To be honest it’s actually the first time I see the term Enterprise Content Management. We are using SharePoint 2013, though we have just started. So right now is’t “just” an intranet. But we have big plans. P.t we are approx. 1.600 employees and by the end of this year we will be over 2.000 employees, hence we need a tool to help us. But since the term is new to me, I’m not sure that SharePoint covers it all.
Am I supposed to understand this term as being one tool?
I’m asking because we also use a journalization software called 360 which has been implementet to make sure we have a system which meets all law requirements.
PS: I’ll have to research this more.
Best regards
Bo W. Mogensen
	              
	              Hi Anthony
Yes, I have come across the principle of ECM - my team have used something similar (if not ECM, although I dont think they were especially comprehensive) for policy control documentation relative to SOX compliance and workflow. We also used for group wide risk management tracking. This was useful to me as I was able to get MI reports on progress. Concerns were just about duplication of documentation, but that was because we'd not really fully integrated the process into our operating model. This was low on our priorities, primary concern at the time was compliance.&nbsp;
The other issues were that we had a complex set of multiple management platforms covering the enterprise, service and customer management, supply chain and resource planning, finance etc etc and we needed&nbsp;more of a holistic strategic approach moving forward. This very much formed the basis for building strategic platform capability, with the definition being crystalised earlier this year.&nbsp;
Best Wishes, Craig
	              
	              Hi Craig,
Thanks your comments and feedback.
Tanisha.
	              
	              
Hi Terry,

Thanks for your positive comments. An interesting question that you asked there, “I believe, in the long-long-term furture, algorithms will be very cheap and data will be very expensive. Do you agree?”.  However, it would have been nice if you could have supported your points with some form of referencing. Interestingly enough, at the first read of your reply my first though was “what is he talking about?”. Thus, as I read your reply, I was thinking basically, 
“Data is nothing without algorithms. You need algorithms to build applications, computers and everything surrounds some form of algorithm. So what was he asking?” 

I know, it may sound foolish to you, my thoughts that is, however I later proceeded start an idea outline for my assignment, which I had decided was going to be on 'Big Data' and I did a bit of research.

As many of us already know the two, data and algorithm, are interrelated thus they need each other. Algorithms need data to perform an operation on, data need algorithms in order to make sense. However, the bottom line, as it turned out, was that data may actually be the more significant in the near future, than algorithms. But, what is also funny, is that we may face a situation where there would actually be a need for far more Data Analysis than that which may be available. Which suggests some form of scarcity for these professionals. And once there is scarcity, cheapness is highly unlikely. 

Once again, thanks for your points, and thanks for making me dive a bit further. 

References
Viktor Mayer-Schonberger and Kenneth Cukier, Big Data, a revolution that will transform how we live work and think, page 1-2 (Copyright 2013)

Big Data, Small World: Kirk Borne at TEDxGeorgeMasonU [Online] Available at:
http://www.youtube.com/watch?v=Zr02fMBfuRA (Accessed on September 21, 2014)

	              
	              Dr. Anthony,
Craig has classified this correctly, it is a large and complex subject and my views will also be dependent on the specific aspect of monitoring. I'll like to focus on video surveillance. 
Civil liberties are provisions of a democratic society and protected by the constitution of most countries. It provides "freedom from arbitrary interference in one's pursuits by individuals or by government" (http://www.merriam-webster.com/dictionary/civil%20liberty, n.d.). 
I believe video surveillance of public, commercial and private business is imperative for the protection and security. This type information has proven to be extremely useful is cases of crimes and other incidents, providing valuable information to the relevant authorities.
In a large city, video surveillance is a huge component of the "big data" utilized to aid in providing security, managing traffic and controlling several other aspects of public activities. 
"The Rio Center of Operations is one example. &nbsp;By converging huge amounts of live data from a multitude of traffic and surveillance cameras, social networks and a variety of other data sources, the Center of Operations mobilizes the resources at its disposal to quickly remediate critical situations."
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;(http://reflectionsblog.emc.com/2014/03/big-events-big-data-redefine-rio, March 2014) 
I strongly believe that these benefits surely far outweigh the infringement on one's civil liberty and provide the essential service that aids the authorities in carrying out their duties.
I do also support the need for governments to have specific laws protecting this type of information and its usage. Ensuring that there is no personal or unauthorized use.
These trends in surveillance and monitoring support the development of cities and the possibilities for real time management, pro-active threat mitigation and many other spin off services. A small price to pay to aid in providing a safe community. Infact I believe if you have no criminal or malicious intent you will not be the subject of a targeted surveillance effort.
&nbsp;
References
Merriam-Webster (n.d.) Civil Liberty [online]. Available from http://www.merriam-webster.com/dictionary/civil%20liberty (Accessed: September 23, 2014).
Reflections (March 2014) Big events &amp; big data redefine&nbsp; Rio de Janeiro [online]. Available from: http://reflectionsblog.emc.com/2014/03/big-events-big-data-redefine-rio/ (Accessed: September 23, 2014).

	              
	              BIG DATA - MY OUTLINE
It is without a doubt that there is a lot going on in big data, and it is a trend that has come to stay considering how profound its effects have been on businesses and soceities in general. As simple as the term "Big Data" may sound, it is so often miscontrued. It means different things to different people, including organizations et al.&nbsp;Below are brief definitions of Big Data:
Wikipedia says: “Big Data is an all-encompassing term for any collection of data sets so large and complex that it becomes difficult to process using on-hand data management tools or traditional data processing applications.”
Microsoft says: “Big Data is the term increasingly used to describe the process of applying serious computing power – the latest in machine learning and artificial intelligence – to seriously massive and often highly complex sets of information.”
Mayer-Schönberger &amp; Cuckier say: “Big Data refers to our burgeoning ability to crunch vast collections of information, analyze it instantly, and draw sometimes profoundly surprising conclusions from it.”
IBM says: “Big Data is being generated by everything around us at all times. Every digital process and social media exchange produces it. Systems, sensors and mobile devices transmit it. Big data is arriving from multiple sources at an alarming velocity, volume and variety.”
To give completeness to what big data really means, most of us relate big date to volume, variety and velocity where volume is lots of date in tera-peta bytes, and variety is data in various formats and velocity is the speed in which the information or data is coming into the system.&nbsp;Similar to how the internet changed lives in the last 20/30 years, big data is seen in a similar light to change our lives in the next 20/30 years. Data has obviously been around for a while, but now it does not only relate to companies or organizations, but includes smartphones, pets which chips, smart meters etc.&nbsp;
References:
http://www.bigdata-startups.com/the-big-data-trends/


	              
	              Hello Class,As our standard way of rounding up the week's activity, please provide a short summary/commentary of the key 'Computer Structures' module lessons learnt in week 3, from your perspective.Anthony
	              
	              Hello Class,&nbsp;I.T. – driven Geographical Information Systems (GIS) have generally provided many benefits to society. However, can you think of negative aspects or issues that may arise from large-scale (global) adoption of GIS?Anthony
	              
	              

Hi Everyone,&nbsp;I.T. mediated rights management schemes such as DRM (Digital Rights Management) and EIRM (Enterprise Information Rights Management) are increasingly being used to enforce intellectual property rights and ensure copyright protection.&nbsp;Whilst these schemes have been mostly welcome in the corporate world, their over-enthusiastic and often pervasive use appears to be encroaching on the individual’s right to privacy, data protection and fair use of information.Do you have any views on this issue?Anthony



	              
	              Hello Class,&nbsp;Software vendors are increasingly adopting supplier-push, rather than customer-pull, approaches for distributing their applications and software updates. Do you find this I.T. trend a desirable one, or do you have reservations about it?Anthony
	              
	              Hi Dr. Anthony,
I do know what an ECM is, unfortunately I haven't work with many. What I have used in the past is Microsoft CRM and Sharepoint Server as document management system. I have also come across Knowledge Tree the open source version of a document management system. There are many tools available for document management which can be customized to meet the requirements of an organization.
I can't speak to any drawbacks at the moments, as for the systems I have used, I haven't come across any major issues once used for simple document management. I must say that with Microsoft CRM 3.0 I did have challenge with integration with the email application but as for managing the document repository, that was seemless.

	              
	              I have not personally been involved in any ECM project, but my company has.
The truth is, as there are benefits with technology, there are also challenges.
ECM for E-health records have the potential to transform the health care system from a mostly paper-based industry to one that utilizes clinical and other pieces of information to assist providers in delivering higher quality of care to their patients.&nbsp;In view of this, it is a no-brainer to agree to the importance of the need for ECM in the health sector.&nbsp;
Despite the growing literature on benefits of various EHR functionalities, some authors have identified potential disadvantages associated with this technology. These include financial issues, changes in workflow, temporary loss of productivity associated with ECM e-health adoption, privacy and security concerns, and several unintended consequences.
Financial issues would include:
Adoption and implementation costsongoing maintenance costs&nbsp;loss of revenue associated with temporary loss of productivity and declines in revenueSensitizing physicians to adopt e-health applications thus converting paper charts to electronic charts, and training end-users
Maintenance cost can be very expensive, considering hardware must be replaced periodically, and software must be upgraded on a regular basis.&nbsp;
Another disadvantage is the disruption of worklows for medical staff and providers which result in temporary losses in productivity. This loss of productivity stems from end-users learning the new system and may potentially lead to losses in revenue.&nbsp;
One study involving several internal medicine clinics estimated a productivity loss of 20% in the first month, 10% in the second month, and 5% in the third month, with productivity subsequently returning to its original levels. In that study, the loss in productivity resulted in lost revenue of US$11,200 per provider in the first year. In a study of solo and small-group primary care practices of one to six FTE providers, revenue losses from reduced visits during the initial stages of an e-health ECM averaged approximately US$7500 per FTE provider. This depended on whether physicians worked longer hours during this stage or reduced patient visits.
Lastly, researchers have estimated that end-users spent 134.2 hours on implementation activities associated with getting and learning a new system. These hours spent on nonclinical responsibilities had an estimated cost of US$10,325 per physician.
Another potential drawback is the risk of patient privacy violations, which is an increasing concern for patients due to the increasing amount of health information exchanged electronically.&nbsp;
References:
http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3270933/

	              
	              Hello Anthony,
In my view, Digital Rights Management (DRM) though a solution to a particular problem, it in itself creates other problems. In vast majority, the 20th century was viewed as a time where people consumed culture produced elsewhere. Leaving that age was an extraordinary deviance in the history of human culture. For all of human culture before the 20th century, culture was what people both consumed and created.
The problem with the DRM models being developed in order to guarantee that you can't pirate content. But as unintended consequence of that, it would also develop in a way that you cannot creatively remix that content. So we'd be killing piracy with such actions, but then destroying the ecology of creativity that digital technologies invite us to embrace.&nbsp;It may be preferred to develop much alternative ways to make sure that brain-child owners &nbsp;get compensated without having to necessarily locking down their contents so they can be used for other creative uses.&nbsp;

	              
	              Hi Craig,Thank you for reading my post, I appreciate that you like the last sentence. i like it too. :)
Best wishes,
	              
	              Hi,

Yes we use Sharepoint 2010(ECM) currently for our Intranet which I was the devlopment lead for and currently support ongoing. We have 2,800 staff members who use the system at present on a daily basis.
What I've learned so far is the yes they are good if used correctly and extremely powerful but talking about drawbacks and cautionary issues I have to say it really depends on how you use them and what you use them for.
I don't have any stats to back this up but I feel many companies that I have known about or delt with who use such tools as Sharepoint under use them. In other words they buy in this complex ECM and then only use it for a document repository or Intranet in our case. While not a direct issue it is like buying a helecopter as a mode of transport to get you to do your weekly food shopping less than a mile from your house.
Also the learning curve is very steep with these tools so they tend to require a good pot of money to train staff via consultancy, workshops and official training routes. Match this with the large sums they take to aquire, sometimes companies would be better off taking more time up front to really look at what they need vs what they want.

Ta
Chris


	              
	              Hi
When I was studying for my degree, part of my course was around GIS and how technology can be used.
We never really covered the issues as we always focused on the positives and what could be achieved.

However looking into this a little more I liked the response (Geofutures) lists on this.
To quote: "
The use of GI tools can create its own issues, however:

&nbsp;Its technical nature can make results appear more reliable than they are; poor operators can hide assumptions and errors in a composite results, while users can be ‘blinded with science’ and not apply their usual standards of questioning to what they are being told
&nbsp;The results of a GI analysis can only ever be as accurate as the data which underlies them, and should only ever be reported at the finest spatial scale of any dataset used – if one input is data collected at county level, for example, the results of the analysis cannot safely be used to make decisions at district or ward level
&nbsp;The availability of data at the required scale at a reasonable cost is a universal issue.

"

Also I read an interesting article on protecting personal privacy using GIS by (Harlan J. Onsrud, Jeff P. Johnson and Xavier Lopez, 1994)&nbsp; which talks about the social negatives with the use of GIS where targeted information and data could be used by marketing industries to target areas for their own gains. Granted this paper was some time ago but I believe the principles were still spot on when they described "GIS technology has the potential to be far more invasive of personal privacy than many other information technologies. "
An interesting part is where the line crosses the LAW in aspects of expected privacy from a person or company vs what is actually legally allowed. They highlight satalite imagery where a companies property had aerial images taken of its land and buildings and the companies complaints while vlaid in some ways were dismissed as what the LAW defines as socially benefitial.

Reference:

Geofutures [http://www.geofutures.com/what-is-gis/pros-and-cons-of-gis/] Accessed 24 Sept 2014.
Harlan J. Onsrud, Jeff P. Johnson and Xavier Lopez, 1994 - Protecting Personal Privacy in Using Geographic Information Systems&nbsp; [http://www.spatial.maine.edu/~onsrud/tempe/onsrud.html] Accessed 24 Sept 2014.






	              
	              Hi Bo, Tanisha and Colleagues,
I think the statement "data would be nothing without algorithm" might be wrong. There are first-class data that is already the information we need. Yes, to look up is also an algorithm. But how important is that?
And even if "data is nothing without algorithm", that doesn't mean Computer Science need to be the science of algorithm.
I know many of us are here to study the subject of Software Engineering. I'm deeply influenced by an article&nbsp;DeMarco, T. (2009),&nbsp;that I believe the time for "software engineering" is gone. The ideal scientific way to produce software consistently and predictably is neither realistic nor important. "Software development is and always will be somewhat experimental," as said by&nbsp;DeMarco, T. (2009).
I think there are two ways to generate an algorithm. One is by mathematical deduction; the other is by generalizing from the data. I think the experimental way would suggest that the algorithm should be generalized from (or in another very popular phrase, 'driven by') the data. Thus, I believe "algorithm is from data".
Unlike arithmetics, which is from the numbers in the nature but we now have the theories, computer science cannot be too far away from the field. A software user couldn't care less about the arithmetical meaning behind the software his is using. I think software practitioners should be aware of that as well. As Kevlin Henney suggested in (Monson-Haefel, R. 2009),&nbsp;"simplicity before generality, use before reuse".
I know I'm kind of confusing "software development" with "computer science". But I believe they are now pretty much the same.
Reference
DeMarco, T. (2009). Software engineering: an idea whose time has come and gone?.&nbsp;IEEE Software,&nbsp;26(4), 96-95.
Monson-Haefel, R. (Ed.). (2009).&nbsp;97 things every software architect should know: collective wisdom from the experts. O'Reilly.
	              
	              Hi Tanisha,
Sorry for having no references. Your previous reply inspired me, and I just had that&nbsp;hypothesis as I was replying you. So thank you so much.
And just like you, when I'm thinking about my individual assignment, I decided to use "Software Will Become Cheaper, And Data Will Become More Expensive" as the topic. I used some references:



Gould, P. (1981), ‘Letting the data speak for themselves∗’, Annals of the Association of American Geogra- phers 71(2), 166–176.Open Source Finds Its Way Into the Cloud, Big Data, Mobile (2012).Richards, N. M. &amp; King, J. H. (2014), ‘Big data ethics’, Wake Forest Law Review 23.



But I still haven't found any reference to support my idea that "data will become MORE expensive". The 3rd reference "big data ethics" is to back up my idea that "when things become expensive, there come ethic problems."
--------------
BTW, the&nbsp;Big Data, Small World TED talk... I was quite lost... What do you think?
br, Terry

	              
	              Hi Robin,
Thanks for the information. Are these already the detailed data mining techniques? Might encounter them again as we study further.
br, Terry
	              
	              Hi Dr Ayoola,
The school of Lean Thinking (Womack, 2010) and one of its practice Kanban, which are from the manufactory industry, is now also applied to software development (Ikonen, 2011). The lean thinking suggested a pull system.&nbsp;“Use ‘pull’ systems to avoid overproduction." (Liker, 2006)
In a pull system, a downstream process pulls the type and quantity it need from upstream. And the user is deep down at the bottom of the stream.
The type of software products that use "push technology" usually has massive customer base. So individual needs doesn't necessarily reflect the collective need of all or the majority of the customers any more. And when "push" is used as a marketing approach, it might create more business opportunity and the cost of "overproduction" as suggested in lean thinking becomes insignificant.
"Push" as a software deployment technology on the public network does upset me a bit. I would trust a hybrid approach more, where at least the customer is obliged to choose to accept or deny. But this is just like another high-tech drug. It's so covenient just to let the server push, well, unless something screwed up.
br, Terry
Reference
Womack, J. P., &amp; Jones, D. T. (2010).&nbsp;Lean thinking: banish waste and create wealth in your corporation. Simon and Schuster.
Ikonen, M., Pirinen, E., Fagerholm, F., Kettunen, P., &amp; Abrahamsson, P. (2011, April). On the impact of kanban on software project work: An empirical case study investigation. In&nbsp;Engineering of Complex Computer Systems (ICECCS), 2011 16th IEEE International Conference on&nbsp;(pp. 305-314). IEEE.
Liker, J. (2006).&nbsp;The Toyota way fieldbook. Esensi.
	              
	              Hi Colleagues,
Another week of quite interesting study. I start to love this way of learning.
This week I learned more on academic writing.

I learned more on how to cite, e.g. quoting, paraphrasing and summarizing.
I learned how to create an outline for a paper.
I started to use Grammarly (I don't know if it has human slavery behind it, but it really helped me)

I learned a lot on the trends of computing, virtualization, big data and security, especially big data.

I studied the big events and trend of big data
During the discussion, I realized many new areas that big data can be applied
I also had an interesting discussion with colleagues on computer security, especially on the future of commercial security product and company and the social part of security issues.

I also learned the definition of computer science and algorithm.

I challenged the definition of computer science on why it's "science of algorithm" instead of "science of data". And this allowed me and the colleagues to dive deeper in this topic.
I had a hypothesis during the discussion, and it's "software (or algorithms) will become very cheap, and data will become very expensive".
I don't know how to prove my hypothesis yet. Hopefully, I can in the near future.

Another big lesson learned... they were right, you cannot possibly follow every discussion threads.
br, Terry
	              
	              Hi Terry.
I found the Big Data presentation on TED talks quite interesting. First off there was a mention or hint of plagiarism when Kirk Borne suggested that an idea on the concept of "unknown, unknown about data" that he had presented to a security team, had been used in a subsequent press conference by the Secretary of Defence who was probably not at that meeting.
My understanding of his discussion was largely on 4 main areas.
1) That we are more connected than we think and that social media has reduced our degree of seperation from each other. Previously the theory had been that we are connected by about six degrees of separation from any other person. i.e. you know somebody who knows somebody who knows somebody,&nbsp; who knows somebody,&nbsp; who knows somebody&nbsp; who knows a person you may be seeking. For instance, each one of us ideally could be able to have a link with our course instructor by connection through six people
2) Data growth has been exponential giving rise to Big Data i.e. The same amount of data that has been produced since the beginning of time to the year 2002(when it was first benchmarked) is now being generated in 10 minutes. This data is approximately 5 exabytes
3) The more data that is collected, the more questions are asked leading to even more data being collected. Organizations are mining data from social media and using that to inform their plans. A good example given is Amazon.com which I have been also prey to. The "pattern matching" discussion we had the other day applies here and companies such as Amozon, Netflix are able to make recommendations for book/video purchases/rentals based on your history and suggesting that people who bought the book/movie you are interested in also bought another one that the system is recommending. Quite profound. So we have more in common with people whom we do not know
4) Job demand for Big Data specialists is growing as big data has fast become the major driver for Government, business and industry in general

Reference:
http://en.wikipedia.org/wiki/Frigyes_Karinthy - (Six degrees of separation)

	              
	              hi DR. Anthony,
As babatunde stated, cloud computing is a new concept in my country Nigeria but its gradually gaining grounds. However, it will be a relatively good concept if some certain parameters or policies are checked and enforced by government for this to work effectively.
One of such parameters is that which has to do with the protection of data. The enforcement of laws as regards data protection here are weak and needs to be strenghtened.

Regards
martins
&nbsp;
	              
	              Hi Joseph,
Thanks for the summary. It saved my time of watching it again:-)
I felt lost is because I had a hard time connecting all these things togehter, like how six degree of separation is related to big data, and how the recommendation system related to a small world.
br, Terry
	              
	              Hi Terry
&nbsp;&nbsp; They are the current trends in Big Data Analytics.
robin
	              
	              Hi Joseph
&nbsp;&nbsp; Thanks for you post. I relate to the topic. I wonder how the new Apple Pay will play a role in the consumer authentication in years to come.
robin
	              
	              Hi Bo
A very good read - thanks for this.
I especially liked the way you structured your paper - your perspective of the virtualisation trend which is enabling new IT technologies such as Big Data and Cloud Computing to work successfully.
I agree, the consequence of Cloud Computing places a critical emphasis and requirement on stable, efficient and reliable communications and internet connectivity.&nbsp;
In relation to Virtualisation, I couldn't help to think back to the late 80's when I worked on DEC VAX systems and IBM Mainframe systems. We essentially took a number of systems (actual machines), and clustered them to make one virtual system, called a VMS Cluster for&nbsp;DEC VAX systems&nbsp;and a Parallel Sysplex for IBM mainframe systems.
Isn't it quite funny how times have moved on, whereby now we are taking one machine, running virtualisation software and creating many systems (virtual machines) within that machine.
Just linking back to Big Data, a couple of years ago my organisation developed a strategy to build on the new theme of Big Data, and centralised all DBA and MIS type functions into a new Business unit called Business Intelligence. As part of this, I transferred a number of my technical team and processes into this function. The function built new capabilities around Business Engagement, which then enabled dialogue to take place back into the business areas in developing intelligence, analytics and interpretation from the data that was being collected and available. Combined with the different types of analytical tools becoming available, this driven a very different level of dialogue compared to previous years.
As the 'data' was interpreted, aswell as focusing on specific areas of demand, it also opened up new market opportunities based on consumer behaviour. In addition, the my organisation also looked at ways it could monetise the data it was collecting, and develop propositions for other external businesses.
This was so much more than simply just running DBA type queries or business reporting against the database(s).
Best Wishes, Craig
	              
	              Hi Anthony.
What I have come across in the healthcare sector in Kenya are hybrid adaptations of core functionalities of Enterprise Content Management (ECM) systems in the implementations of Electonic Medical Records Systems (EMRs). For organizations implementing EMRs in Kenya, one of the major challenges they have have had to deal with is the issue of legacy data (Information stored in an old or obsolete format or computer system that is, therefore, difficult to access or process:, www.businessdictionary.com/definition/legacy-data.html&nbsp;)
Because most of the legacy data exists in paper form, part of the workaround has been the considered use and application of document management technology, to scan, archive and index that data and reference it from the EMR system. This saves the herculean task of mobilizing a data entry exercise for the capture of medical data that may not necessarily be required at present. However being in the medical field, clinicians would like to be able to view a patients past history including lab reports, xray information and other medical related records. The fact that this data is in paper form and that the structure of the data may have changed over time as data collection tools may have been modified or made obsolete presents a major hurdle. The EMR systems would ordinarily be required to provide support for legacy data through data entry of backlog data. However a solution most preferred solution would be the use of document management systems that could be linked to the EMRs.
Joseph
References:
Legacy data definition: www.businessdictionary.com/definition/legacy-data.html
Standards &amp; Guidelines for EMR Systems in Kenya, Pg 53,54; Implementation Planning, Requirements -http://www.nascop.or.ke/library/3d/Standards_and_Guidelines_for_EMR_Systems.pdf&nbsp;

	              
	              Hi Anthony,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Being US based, we see the most use of cloud technologies by nearly all major retail vendors and internet giants. Companies are adopting Private Cloud vs the public cloud especially Financial institutions&nbsp;as they have strong PII (Personal identifiable Information) and private data on the their clients. The largest and most successful of them is Amazon, Saleforce.com Oracle, Microsoft, Adobe, SAP, Google, LinkedIn and Akamai. Security is a major factor when deciding to move from on-premise to Cloud. Many companies have hybrid data centers. They use Virtulization in their on-premise data centers for sensitive.Mission critical data and cloud for other data. Data resiliency/Disaster Recovery is also a deciding factor.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; There is also a major legal constraints especially when data from different countries are stored on the cloud. Think of the potential legal ramifications of sensitive data leaving a stable country and residing in a potential politically volatile country!
Robin&nbsp;&nbsp;

	              
	              Terry,
Just a few pointers, somewhere in the video that I posted, “Big Data, Small World” by Kirk Borne in June 10, 2013, Kirk hinted at the fact that in the near future there will be a need for far more data analyst than there will be available, simply because of the need for data analysis.
The revolution of data in the next decade to me will be less limitless. Kenneth Cukier in his you tube video, "BIG DATA: A Revolution That Will Transform...” talks about application “anti-theft tools” that will analysis the way we sit in our cars and use this information to determine if our car was stolen. He talks about predicting where or not someone is likely to serve over the next decade, to name a few of his insights. But these videos talk about making discoveries, making correlations and predictions through data and data analysis.
&nbsp;
I think the focus here is not really on software engineering, but on data analysis. I am not going to ask you to picture yourself going through tons of paperwork, or tons of information generally, on your own without any form of software or anything to help you with this information. This is because, I know this can be time consuming and nobody will want to do this. All the CEO wants to know is: what is the likely hood of our new company succeeding in this market? Clearly, analysis is how we can find the meaning of data, somebody has to know how we are going to make sense of data, and thus some form of analysis has to be done on it.
Whether or not they are going to build millions of software to do this analysis or they are going to come up with millions of mathematical algorithms to do it, I don’t know. The bottom line is; data is a big concept, there is no doubt about this, thus data analysis is and will be just as big as well in the next decade. Will data be more expensive? It just might be. People may want to pay for it? People may want to safeguard it? All because of what it could mean and what can be disclosed from it.
Reference:
Kirk Borne (2013), ‘Big Data, Small World’ [Online] Available at:
http://www.youtube.com/watch?v=Zr02fMBfuRA (Accessed on September 24, 2014)
Kenneth Cukier (2013), ‘BIG DATA: A Revolution That Will Transform...,’ [Online] Available at:
http://www.youtube.com/watch?v=bYS_4CWu3y8 (Accessed on September 24, 2014)

	              
	              Virtualization plays very important while taking steps towards cloud computing. Our IT management has vision for cloud computing and they have already initiated step s by launching “Go Green” project to promote virtualization. They are encouraging to use virtual machines and also application administrators are working to check the support and compatibility issues related to their applications to proceed with migration of existing physical servers to virtual. Considering security factor management is hesitant to go for Public or Hybrid clouds but of course Private cloud is considered a feasible option at the moment.
References:
Michael Armbrust. et al. (2010) ‘A views of cloud computing’. Communications of the ACM [Online], 53:4, 50-58, Available online from http://dl.acm.org/citation.cfm?id=1721672 (Accessed: 24 September 2014). DOI: 10.1145/1721654.1721672

	              
	              Hi Dr Anthony


Firstly, definitions are very important to create context for this question. &nbsp;A supply chain is a minimum of a network of a business, its suppliers and customers. Thus, supply chain management by a particular business is the management of human capital, processes, materials and information between that business, its suppliers and its customers that ensures maximum customer service at maximum margin to that business. (Dr Keith McNeil) Importantly, while all participants in a supply chain can benefit from improvements to the functioning of the supply chain as a whole, rarely do they benefit equally.


Software vendors are increasingly adopting supplier-push, rather than customer-pull, approaches for distributing their applications and software updates because demand can be predicted reliably enough to define the procedures required to deliver resources to pre-specified locations before the demand actually materializes. Therefore Supplier-Push requires accurate forecasts to function effectively.


Push Programs


Push programs represent a top down approach to dictating activities. These programs tend to specify activities or procedures in detail. The core assumption of push programs is that demand can be anticipated and that it is more efficient and reliable to mobilize resources in pre-specified ways to serve this demand. These activities or procedures may be organized into modules (for example, semesters in a curriculum), but that is only for the convenience of the provider. The modules are usually tightly coupled –deployed in a pre-specified sequence.

Because of the work required to specify, monitor and enforce detailed activities, push programs tend to be restricted in terms of the number and diversity of participants. This is especially true beyond the boundaries of a single institution where the complexity overhead increases exponentially as the number and diversity of participants grows. This is a key reason why most large companies have worked so hard to reduce the number of suppliers in their supply chains. Even within a single institution, push programs specify the type of participants, their roles and the sequence of their involvement in the activities covered by the program.

Push programs tend to treat all relevant resources as a fixed and scarce quantity –after all, that is one of the rationales for a push program to begin with: to ensure that scarce resources are deployed to the highest priority needs. If one participant gets the resources or the rewards, other participants must do without. In this sense, push programs operate with zero sum reward systems for their participants. Often there is intense political maneuvering to gain privileged access to resources. Since the availability and movement of resources are dictated from above, political maneuvering focuses on influencing the center. The key planning instruments of push programs are budgets (for financial resources) and materials requirement plans (MRPs –for physical resources) –these become the focus of intense political rivalry. Reward systems tend to concentrate on extrinsic rewards –for example, money or grades. Participants in push programs are generally treated as instruments to ensure that activities are performed as dictated –their own individual needs and interests are purely secondary, if relevant at all. As a result, these programs generally tend to default to extrinsic rewards as a way to motivate participants. Push programs adopt a standard meta-design pattern where construction and creation are clearly separated from use or consumption:

Design –define specific procedures and specify people that must execute the procedures

Deploy –build dedicated facilities, train the people, and secure the resources 

Execute

Monitor

Refine –address specific performance gaps or introduce enhancements on schedules determined by the program designer



Reference:


(Dr Keith McNeil) Push vs Pull Supply Chain Models http://www.advantageinternational.com/www/content/default.aspx?cid=921&amp; [online] (Accessed 24 September 2014)

(John Seely Brown)From Push To Pull: Emerging Models for Mobilizing Resources http://www.johnseelybrown.com/ [online] (Accessed 24 September 2014)
	              
	              Firstly, definitions are very important to create context for this question. A Geographic Information System is a computer-based tool for mapping and analyzing events and places on the Earth's surface.&nbsp;A GIS is able to&nbsp;capture all types of geographical reference data as well as digitally&nbsp;manipulating images from the Earth's surface and presenting the data in 3-D. example Google Map.

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Negative Aspects or Issues that may arise from large-scale (global) adoption of GIS:


Requires enormous amount of date: makes it prone for error
Geographical error increases with larger scale
Relative loss of resolution
Violation of privacy

Regards,
Babatunde.
	              
	              Hi Anthony
There are various opinions on this, and I have had many conversations within my organisation about this type of software upadate.

What it does do is give me certainty. I am particularly supportive of this approach as it provides the ability to do really effective planning relative to product rollout and product roadmap.&nbsp;
As an example, Salesforce.com adopt this approach I have personally found it to be extremely effective.&nbsp;Given that the service is in the Cloud, Salesforce.com minimise the disruption to service also by adopting a modular approach to software upgrades, which means that you can still get access but with some levels of restriction. The impact is low relative to being able to maintain business operations.&nbsp;
I have also found that given the platform itself is multi-tenanted, whereby they're serving many high profile and global customers simultaneously, the quality and standards of rollout are extremely high. Terry has also referenced this too in his response.&nbsp;It is also easier to plan your own incremental changes to the platform, which can be slotted in between the known supplier 'push' releases.
There are of course other products and services which may be better suited to 'pull' upgrades. For example, if you adopt a client server model which is used through different time zones, you may want to schedule 'pull' upgrades at time when it is least impacting the timezone your in.
Best Wishes, Craig
	              
	              I was already aware of most of the current trends in computing e.g. Big Data, Data Security and Cloud computing but during this week I got the opportunity to go though in depth details of these computing disciplines and the critical analysis from peers really helped to clearly understand the concept. Feedback from my colleagues was really constructive and gave my encouragement to do more and improve my performance. Assignment for this week was also a good start to get familiar with standard formats of writing professional papers. 
Regards,,,
Numan Arshad
	              
	              Hi Anthony,
I &nbsp;am relaxed about the principle of this, and I also respect the Copyright needs of the owners. I use Adobe Digital editions for protected versions of PDF's such as manuals and books i.e. ITIL v3, MSP, TOGAF etc. It prevents you sending these books to others, and when the books are approaching £100 each, I can understand why the authors want to employ DRM software to these.&nbsp;
Also, as a consumer you want to be confident that if your paying, then everyone else is paying too ...notwithstanding the process of strong negotiation and any discounts that you may get in the deal :-)&nbsp;
Best Wishes, Craig

	              
	              Hi Anthony,
A very interesting week looking at computing topics such as virtualisation, big data, security, and algorithms.
When exploring Virtualisation, I was suddenly struck with the realisation that we have seen a complete shift from the early years when I started working in IT, whereby we have moved from virtualisation of many machines into one, to virtualisation of one machine into many. Something that was inconceivable 20 years ago…
We had some really interesting dialogue amongst class colleagues on big data and virtualisation, and especially in relation to security &amp; what could be done to govern technology companies more closely in relation to addressing security weaknesses that may not necessarily be at the top of their list of priorities.
We also talked a lot about algorithms and the relationship with data. 
We also reviewed automation models – Von Neumann’s cellular automata. I thought this was really compelling, with the re creation of new cells from a fairly straightforward set of rules (algorithm). I was really challenged in understanding this concept at first and wonder whether this is the foundation for advanced developments of things like artificial intelligence.
My discussion question centered around the issue of security, more specifically personal privacy, which is becoming a real concern following the exponential growth in social networking. I was also building on discussions from the previous week around data protection. 
I continued with the theme of personal privacy for my hand-in assignment too, but in relation to new technological trends that are bringing more intelligence to our homes through technologies such as smart metering, and the potential impact on personal privacy.
We have also continued our learning on academic needs such as citing, referencing and producing an outline for assignment.
I am thoroughly enjoying the programme and the collaboration and interaction with class colleagues. I can’t resist saying that in conclusion, algorithms and data are a science of computing J
Best Wishes, Craig
References
Laureate Education. 2014. Exploring new avenues with computing: Week 3 Lecture Notes and Video [Online]. Available through the programme classroom (Accessed 18 September 2014]
	              
	              Hi Anthony,

I've really enjoyed this week and I feel i'm starting to get to grips with the routine and timescales for the assignments we have for the course. It's certainly a different yet exciting way to learn and do academic work.
What i've learned:

Further experience and understanding about turnitin and assignment writing.
Big data and the correlation with virtualisation.
Creating outlines for thesis papers
Further learning around citing and referencing.
Future computer trends and relation to social issues.
Algorithms and connection to data and their definitions.

Overall it's been a really enjoyable week for me.

ta
Chris

	              
	              Hello Tresor,
&nbsp;
Thank you for your expose on the topic.
&nbsp;
Content routing in Network Virtualisation models heavily rely on data-driven protocol events. There are potential technical and security drawbacks like recourse exhausting, state decorrelation, path and name infiltration and cache pollution as pointed out by Wählisch, Schmidt and Vahlenkamp, M.(2012) in an interesting read. They claim these problems are introduced by a strong coupling of the control to the data plane in the underlying routing infrastructure.
&nbsp;
Greetings from Bram 
&nbsp;
Reference
Wählisch, M., Schmidt, C. &amp; Vahlenkamp, M. (2012) 'Backscatter from the Data Plane; Threats to Stability and Security in Information-Centric Networking' [online]. Computer Networks, Vol. 57, No. 16, pp. 3192-3206, Elsevier, Nov. 2013. Available from: &nbsp;http://arxiv.org/abs/1205.4778 (accessed: 24 September 2014).

	              
	              Hi Anthony,&nbsp;
Another week of lots of learning. This week I learnt alot on computing trends which include virtualization, big data, computer security, miniaturised devices, algorithms, data, cloud computer etc. I also learnt more on referencing, paraphrasing and quotation. However, I learnt how to outine a document .

Cheers
Martin
	              
	              Hi all
This week I learned that:

It takes extremely much time to try to read every discussion thread, and that I in the future must chose and focus.
I must read the assignments (DQ and IA) more carefully. This week I somehow misunderstood what I was supposed to do, and it was late Saturday before I realized that I more or less had to start over.
I should probably start by reading everything straight away, including the individual assignment – not waiting until after initial DQ has been turned in - to make sure my mind is aware of what lies ahead.
I must be better at searching in the UoL Library and Google Scholar.
I must be better at using RefWorks. Currently I don’t seem to get the RefGrab-it to work well – it often fails Grab.
I must be better at arranging al my links/findings, because at this pace I will get extremely many.

Also I learned a lot about trends and many different aspects of each trend, several which I’ve never thought about before, e.g. I have thought about the potential of Big data, but this week has made me aware of many more, but also that there are a lot of security and legal issues.
All in all, it was very interesting and educational.
Best regards
Bo W. Mogensen

	              
	              Hi Bo,
How's your experience with RefWorks? Are you using a full verions? It's not free, right?
So far I've been copying bibTex citation from Google Scholar or write it manually, and I have to use "APA" style to keep things simple. It's not exactly the same as harvard style though. It will be great if there's some software can do the job.
br, Terry
	              
	              
Hi Dr. Anthony, 
I work at a Sugar Corporation in my country which has several sugar estates in different sections of our country. Because of the large amount of data that is handled at the different estates and the huge inventory of stores items among others needs, we use Oracle E-Business Suite. We use the application to manage large chunks of data, financial purposes and Human Resource Management Purposes. Staff can track items in stock at the different estates, they can make requests for items, they can generate reports and much more. 

In terms of the selling of sugar we use the application sales functionality there as well. We also applications that we build, take for instance our in-house Payroll application. Other web-based applications are used for collaborative purposes, workflows, storing user manuals printable forms etc.

Our Manage Engine Service Desk application is to used log user related queries as they arise, this is used to track users requests, and also to track the work done by the department among others. There is also an asset management feature of this application which is not fully implemented. Reports are also generate with this application as well.

One of the drawback we often face is licensing. Our Information Systems Department tries to be compliant with all the applications that we use and as such, we are often faced with a challenge of high licensing costs, as these applications are often licensed per the amount of users. Since it is a very large organisation, which is currently under financial strain, licensing continues to be a challenge.

References
Noreen Izza Arshad, Mazlina Mehat2 , (2014), Exploring the Use of Enterprise Content Management Systems in Unification Types of Organizations. [Online] Available at:
http://eds.a.ebscohost.com.ezproxy.liv.ac.uk/eds/pdfviewer/pdfviewer?sid=14d6105f-512c-448b-94f2-5271fdc9bb2b%40sessionmgr4002&amp;vid=1&amp;hid=4102 DOI: 10.1051/epjconf /201 46800019 

Jan vom Brocke, Alexander Simons &amp; Anne Cleven, (2011), Towards a business process-oriented approach to enterprise content management: the ECM-blueprinting framework. [Online] Available at: http://eds.a.ebscohost.com.ezproxy.liv.ac.uk/eds/pdfviewer/pdfviewer?sid=3421177b-937a-4339-8cbe-7c75109c3907%40sessionmgr4001&amp;vid=1&amp;hid=4102 DOI 10.1007/s10257-009-0124-6

	              
	              
Hi Dr. Anthony,

I believe as long as information is placed out there we need to have it protected, depending on what type of information it is. In an article title, 'First, Get Data Classification Right ', (Jaclyn Jaeger, 2013) the author talks about information classification basically into four categories, that is Restricted, Confidential, Internal Use and Public. If we can categorize information appropriately, along these categories, then we can take if from there, that is the decision as to what is to me made available to the public, what is to be made available to certain levels of staff, what is to be available to all levels of staff and what needs to have the highest level of security, as the author hinted in his classification of the above mentioned terms.

Individuals are going to be concerned, about their information being available out there and who will have access to it. But once we can categorise our information and then take it from their in terms of policy implementation and security we can have something to work with. Information such as medical records should be highly protected. Additionally nobody want to know that their information will be used for unintended purposes, thus when assigning rights and implementing policies we have to be sure and confident about the persons that we are give the access to and that these individuals are responsible. 

References:
Jaclyn Jaeger , (2013), First, Get Data Classification Right , Weighing which controls should apply to what information; enter the data steward [Online] Available at:
http://eds.a.ebscohost.com.ezproxy.liv.ac.uk/eds/pdfviewer/pdfviewer?sid=57f98e47-7629-4a1a-a5f5-81b9baefc1d7%40sessionmgr4001&amp;vid=1&amp;hid=4102 (Accessed on September 24, 2014)
Integrated method and system for controlling information access and distribution, US 6141754 A [Online] Available at: http://www.google.com/patents/US6141754 (Accessed on September 24, 2014)



	              
	                Hi Dr. Anthony,  I think this strategy is a somewhat good one because in the field of Information Technology and Innovation the majority of the customers may not actually know what they what. While the developer or the engineer on the other hand can possibly be both the consumer and the customer at the same time. Even if they do not know what the customer want, in the gather of information at the 'customer requirements' phase which they should be well trained, if this phase is conducted well that is, they can infer what the customer want. Additionally, they can know whether or not they can meet the customer's expectations or exceed it. They are also other ways of knowing what the customer may wants, such as data analysis.  To me its also about the competition, what the other suppliers are capable of supply and meeting or exceeding their capabilities. I believe in the world today the innovators and or engineers are doing what we as consumers never thought or imagined could happen. They can also predict and discover what we want just by the things that we do. Google has that capability, to make predictions through data, using search queries. Thus the change in the traditional way of doing things.   Reference: Kirk Borne (2013), ‘Big Data, Small World’ [Online] Available at: http://www.youtube.com/watch?v=Zr02fMBfuRA&nbsp;(Accessed on September 24, 2014) Kenneth Cukier (2013), ‘BIG DATA: A Revolution That Will Transform...,’&nbsp;[Online] Available at: http://www.youtube.com/watch?v=bYS_4CWu3y8&nbsp;(Accessed on September 24, 2014)   
	              
	                  Dr. Anthony, I think a negative impact would be the violation of our privacy; if individuals knew every move that you made and if this information got into the wrong hands or if it is used for unintended purposes. There would be some form of negative impact of the other. Tanisha 
	              
	              Hi Anthony,
Another good and interesting week, Learning and exploring new trends in computing such as big data, virtualization, security, algorithms. Knowing how computing has changed in the recent past till present of 3D technology and still believing to see more coming in the nearest future. In addition I’ve gathered further learning around citing and referencing, creating outlines for thesis papers and future computer trends in relation to social issues.
I am really enjoying the programme and the collaboration and interaction with class colleagues.
Regards, Babatunde
Laureate Education, (2014) Exploring New Avenues with Computing: Week 3 Lecture Notes and Audio [Online], (Accessed: 17 September 2014) 
	              
	              Push versus Pull – Contrasting the Two Models
Push approaches to resource mobilization require fundamentally different ways of organizing resources and&nbsp;management techniques relative to push approaches. Push approaches are typified by “programs” – tightly scripted&nbsp;specifications of activities designed to be invoked by known parties in pre-determined contexts. Of course, we don’t&nbsp;mean that all push approaches are software programs – we are using this as a broader metaphor to describe one way&nbsp;of organizing activities and resources. Think of thick process manuals in most enterprises or standardized curricula&nbsp;in most primary and secondary educational institutions, not to mention the programming of network television, and&nbsp;you will see that institutions heavily rely on programs of many types to deliver resources in pre-determined contexts.&nbsp;
Pull approaches, in contrast, tend to be implemented on “platforms” designed to flexibly accommodate&nbsp;diverse providers and consumers of resources. These platforms are much more open-ended and designed to evolve&nbsp;based on the learning and changing needs of the participants. Once again, we do not mean to use platforms in the&nbsp;literal sense of a tangible foundation, but in a broader, metaphorical sense to describe frameworks for orchestrating a&nbsp;set of resources that can be configured quickly and easily to serve a broad range of needs. Think of Expedia’s travel&nbsp;service or the emergency ward of a hospital and you will see the contrast with the hard-wired push programs. Let’s&nbsp;looks at push and pull models in a little more detail.
What is Best?
The advantages of operating to a pull model are very compelling for businesses that are able to do so, principally because the planned level of production and/or service delivery is not dependent on forecasts with their inherent inaccuracies. &nbsp;The inevitable over-compensation for forecasting inaccuracy is inventory. &nbsp;One of the five basic tenets of the “Lean Thinking” business model, founded on the Toyota manufacturing model, is to “Only make what is pulled from the customer at the rate of their requirement”. &nbsp;
The closer a business can move to real-time customer demand fulfilment, the greater is the reduction in risk of producing or locating in the wrong place what is not wanted by the customer. &nbsp;In addition, a company operating in a demand pull mode can be expected to be better positioned to innovatively respond to changes in customer tastes and expectations. &nbsp;This theme was one noted in a recent article published in the McKinsey Quarterly (October 2005) that was titled “From Push to Pull; The Next Frontier of Innovation”.&nbsp;
However, as suggested above, not all participants can rationally or easily apply a demand pull model. &nbsp;A business operating in demand pull mode will strive to move the inventory decoupling point further upstream to its supplier or its supplier’s supplier. &nbsp;For example, consider a manufacturing company (A) in Australia that exercises a demand pull model and uses components manufactured off-shore by another company (B). &nbsp;This company, in turn, acquires its raw materials from another Australian bulk-exporter (C). &nbsp;Company A will be rightly concerned at its risk of supplies if they are procured direct from overseas and may insist that B maintains inventory in Australia to manage that risk. &nbsp;
Company B because of concerns with respect to shipping lead times is likely to supply to forecast and becomes in respect to its Australian customer, a push supplier although, for its own domestic customers, it may too be able to operate a pull model. &nbsp;Company C, because of even longer lead times will likely operate as a push model either because of long lead times to the ultimate end customer or because of a prime business imperative to capture economies of scale in its processing infrastructure.
In conclusion for me, we need to consider what the primary business driver is for the software vendors adopting the supplier-push model....which is to maximize the utilization of their resources to keep cost as low as possible, becuase adopting the supplier-pull model requires a high level of customer service through responsiveness and flexibility to meet uncertain customer demands.
REFERENCES
http://www.advantageinternational.com/www/content/default.aspx?cid=921&amp;
http://edgeperspectives.typepad.com/edge_perspectives/2005/10/from_push_to_pu.html
Anderson, Chris (2006), The Long Tail. (New York: Hyperion)
Hagel, John III and Brown, John Seely (2005), The Only Sustainable Edge. (Boston: Harvard Business&nbsp;School Press)
Hagel, John III and Brown, John Seely (2006), Creation Nets: Harnessing the power of open innovation,unpublished working paper.
Hagel, John III and Brown, John Seely (2002), Service Grids: The Missing Layer in Web Services,Release 1.0, Volume 20, Number 11, 1-32

	              
	              Hello Dr. Ayoola,
On a global stage, GIS may be percieved by many as invasion to privacy.&nbsp;The vast collection, maintenance and dissemination of personal information by government and industry has increased public suspicion that their personal information privacy is eroding. Personal privacy is an issue that will continue to grow in importance as the potential for invasive information handling grows and the public becomes more aware of the threats to their personal privacy.
The privacy regulations of individual countries reflect national differences in culture, politics, and the expected roles of their institutions. However, as trade in information commodities becomes increasingly important internationally, the need for international data protection standards increases dramatically.
Although most GIS applications are viewed by the public as socially beneficial, many current and future applications may be considered as highly intrusive. '...Failure to reassure a skeptical public about the civil liberties implications of new information technologies may make it impossible to put promising technological solutions to work' (Flaherty 1989, 309). The GIS community has a substantial interest in maintaining citizen trust in geographic information technology. To maintain and earn that trust, reasonable privacy policies need to be established and implemented in developing spatial databases and in networking them with other databases. Awareness by the GIS community of privacy protection issues will promote fair information practices generally and prepare the GIS community to have a voice in the drafting of future privacy legislation. Adoption and promotion of privacy protection guidelines such as those set forth in this article will contribute to the long term health and growth of the GIS industry.
Reference
http://www.spatial.maine.edu/~onsrud/tempe/onsrud.html
	              
	              Hello Dr, Anthony,
It is without a doubt these days that you can pretty much obtain whatever information you will about someone so long as they have workstations, smart devices (or iphone lovers), facebook accounts, google accounts etc. I &nbsp;believe the keyword here is "Privacy". No matter how the Internet age has helped our soceities advance, Privacy is still very important for a lot of reasons.
Some have to do with the consequences of not having privacy. People can be harmed or debilitated if there is no restriction on the public's access to and use of personal information. Other reasons are more fundamental, touching the essence of human personhood. Reverence for the human person as an end in itself and as an autonomous being requires respect for personal privacy. To lose control of one's personal information is in some measure to lose control of one's life and one's dignity. Therefore, even if privacy is not in itself a fundamental right, it is necessary to protect other fundamental rights. &nbsp;&nbsp;
Protection from the Misuse of Personal Information
There are many ways a person can be harmed by the revelation of sensitive personal information. Medical records, psychological tests and interviews, court records, financial records--whether from banks, credit bureaus or the IRS--welfare records, sites visited on the Internet and a variety of other sources hold many intimate details of a person's life. The revelation of such information can leave the subjects vulnerable to many abuses.
Privacy and Relationship
Privacy is also needed in the ordinary conduct of human affairs, to facilitate social interchange. James Rachels, for example, argues that privacy is an essential prerequisite for forming relationships.&nbsp;The degree of intimacy in a relationship is determined in part by how much personal information is revealed. One reveals things to a friend that one would not disclose to a casual acquaintance. What one tells one's spouse is quite different from what one would discuss with one's employer. This is true of more functional relationships as well.&nbsp;
Autonomy
As Deborah Johnson has observed, "To recognize an individual as an autonomous being, an end in himself, entails letting that individual live his life as he chooses. Of course, there are limits to this, but one of the critical ways that an individual controls his life is by choosing with whom he will have relationships and what kind of relationships these will be.... Information mediates relationships. Thus when one cannot control who has information about one, one loses considerable autonomy.
Human Dignity
Autonomy is part of the broader issue of human dignity, that is, the obligation to treat people not merely as means, to be bought and sold and used, but as valuable and worthy of respect in themselves. As the foregoing has made clear, personal information is an extension of the person. To have access to that information is to have access to the person in a particularly intimate way. When some personal information is taken and sold or distributed, especially against the person's will, whether it is a diary or personal letters, a record of buying habits, grades in school, a list of friends and associates or a psychological history, it is as if some part of the person has been alienated and turned into a commodity. In that way the person is treated merely as a thing, a means to be used for some other end.
Privacy and Power
Privacy is even more necessary as a safeguard of freedom in the relationships between individuals and groups. As Alan Westin has pointed out, surveillance and publicity are powerful instruments of social control.&nbsp;If individuals know that their actions and dispositions are constantly being observed, commented on and criticized, they find it much harder to do anything that deviates from accepted social behavior. There does not even have to be an explicit threat of retaliation. "Visibility itself provides a powerful method of enforcing norms."&nbsp;Most people are afraid to stand apart, to be different, if it means being subject to piercing scrutiny. The "deliberate penetration of the individual's protective shell, his psychological armor, would leave him naked to ridicule and shame and would put him under the control of those who know his secrets."&nbsp;Under these circumstances they find it better simply to conform. This is the situation characterized in George Orwell's&nbsp;1984&nbsp;where the pervasive surveillance of "Big Brother" was enough to keep most citizens under rigid control.
In conclusion, it is also important to note that Privacy is not absolute.&nbsp; When we speak of privacy, particularly as a right, we focus on the individual. The individual must be shielded from the prying curiosity of others and from prejudice and discrimination. The individual's autonomy and control over his or her person must be preserved. The individual must be protected from intimidation and coercion by government.&nbsp; These are important considerations; but not the whole story. For the human person does not exist purely as an individual. People live their lives as members of society. In fact they are members of many societies, which may include families, circles of friends, work organizations, churches, voluntary associations, civic organizations, city, state and nation.&nbsp;These associations are not merely preferences or matters of convenience. To be human is to be in relationship. Therefore social obligations, that is, all that is required to maintain the complex Web of relationships in which each person lives, are fundamental human obligations. Moreover each individual has an obligation to contribute to the good of society, the so-called "common good." 
Reference
http://www.scu.edu/ethics/practicing/focusareas/technology/internet/privacy/why-care-about-privacy.html&nbsp;



	              
	              Hello Dr. Ayoola,
Cloud computing is still buzzing in the West African market, and more specifically to me; Nigeria. I work for one of the biggest ICT firms in Africa off their West African office in Nigeria. My company owns two of the 3 Tier IV Datacenters that have been certified by the Uptime Institute, so it is safe to say this is one our major source of revenue as a Group.&nbsp;
We had recently launched our cloud business in Nigeria, becuase you have a lot of customers extremely skeptical about moving their data offshore, so issues around trust and security and availability (Business Continuity to be precise) tend to arise way too often. Has it been daunting thus far? Yes, it has. Becuase of the cultural background of business owners in Nigeria, a lot of large enterprises feel more comfortable owning their infrastructure; in other words, not minding the premium they get to pay to expend so much on IT which predominantly does not form the core of their business.&nbsp;
Even after you jump the hurdle of proving their are no legal and security issues, you still have another set of internal stakeholders who believe you are a threat to their only source of livelihood because cloud business to a CEO/CFO means reduction in operational expenses....and without mincing words, this translates to entrenchment to i.e. the head of IT who happens to be responsible for a whole lot of technical ground soldiers.
It's a race that has begun, and some parties are already boarding the band-wagon....slowly...but surely though. In Nigerian Market, most investors wait for scapegoats to try before they buy or walk away. But the trend may take a completely different paradigm shift considering other factors that are being rectified in Nigeria to enhance productivity and guarantee i.e. Business Continuity.&nbsp;
	              
	              Good points guys, particularly when discussing the downside to Cloud computing.As a suggestion, for follow-on posts/replies, it is a good idea to try to include, whenever possible, fuller reference information for the references &nbsp;cited e.g. Title of paper/book, publisher, date accessed if from a web source etc.&nbsp;We generally favor the use of Harvard referencing notation on the Liverpool Master's program, and when I grade the weekly submissions I tend to emphasize this. There is a guide for referencing provided in the week 2 reading list.Anthony
	              
	              The discussions dealt with current trends in Computing/IT that were impacting the global community.There were posts on the impact of trends and new developments in cloud computing, software development frameworks (e.g. SOA, Spring, Struts), Health-Care Informatics/e-Health, e-Banking, Open Source Software, Mobile Computing/Communications, Computer-based Surveillance, GIS Systems, Virtualization, DRM and Push Application updates.The week’s discussions and debates have been again very lively and informative – keep up the good work!Anthony
	              
	              Dr. Anthony

This week was quite interesting. Below are highlights of the majority of the areas that were covered.


I learned a little about the various trends in Computing, which included:


A. Virtualisation,

B. Big data,

C. Computer security,

D. Universal constructor, cellular automata (CA), the Game of Life, and

E. Algorithms

&nbsp; &nbsp; &nbsp; II. &nbsp;&nbsp;Intelligence and Machines
III.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Modeling

IV.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Fractals

V.&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;Role of Computer Hardware and Software &nbsp;

&nbsp; &nbsp; &nbsp;VI.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Developing thesis statements
VII. &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Writing Outlines , Paraphrase, Quoting and summarizing&nbsp;
VIII. &nbsp; &nbsp; &nbsp; &nbsp;I participated in quite a few discussions on ‘big data’ and algorithms in the near future. Through this participation and my initial discussion which was focused on ‘big data’ quite a few things were learned, which included: use of big data to make predictions, discovery and correlations.
IX. &nbsp; &nbsp; &nbsp; &nbsp; Other discussion questions posted by you, challenged me to look briefly into areas such as:&nbsp; IT facilitating monitoring of individuals, Push applications and software updates, IT mitigated rights management and Enterprise Content Management Systems.
Once again, it was quite an interesting and I believe that I have learned quit a bit.

&nbsp;

	              
	              Hi Tanisha,
Between data and data analysis, I still think data is more important than data analysis. In general I'm pessimistic about having too many layers of indirections. Interpreting data can be hard and misleading. I believe like many problems, the best solution to the problem of data analysis is to eliminate the need of data analysis, instead of advancing data analysis. I'm not saying we should not advance the data analyzing technology. It's just the priority of solutions.&nbsp;That's another reasoning to my&nbsp;conjecture.
br, Terry
	              
	              Hi Tanisha, Hi Terry, Hi Joseph, 
It has been fascinating following your discussion thread and I have also made some time to review your references too. 
I am convinced that I had actually heard ‘known-knowns’, ‘known-unknowns’ and ‘unknown-unknowns’ well before 2001 albeit I cant honestly recall where or when. It does however seem that Rumsfeld is being credited with this phraseology. 
That aside, ‘Information’ has always been the key component to almost anything that we do, from having a discussion in the local pub about our favourite football teams, their matches, goals scored, points won and lost, to discussions in business about performance, customers, revenues and balance sheets. Did you hear this? Or did you hear that?… Everything is about ‘Information’.
I believe that ‘Big Data’ is providing us with the ability to crystalise this ‘Information’, to be able to present it in a multitude of ways that then opens up new discussions, new thinking and new opportunities. Recently, I have also heard the terms ‘Extreme Data’ and also ‘Fast Data’ – Terry, this is already showing stages of evolution along the lines of your 'Big Fish eats Little Fish' to 'Fast Fish eats Slow Fish' analogy. 
We have seen various debates this last week or so on whether computing is a science of algorithms or a science of data. I don’t have a specific reference, but can’t resist thinking that algorithms and data are actually a science of computing, albeit in my opinion these two may have different positions of importance, depending on the scenarios in question.
Let me use sport as a way to explain where I am coming from; Let’s take Formula 1. Formula 1 is a sport that is based entirely on Data. Don’t get me wrong, we watch it because we want to see the excitement of the race, and hope that our favourite driver wins (come on Lewis!). However, behind the scenes, it isn’t just engineers or mechanics, there are probably ten times the amount of people back at headquarters running algorithms and analysing data, to then provide guidance back to race control, based on quantitative, qualitative and predictive information, to help make decisions which then could then enable their drivers to be better placed, or even win the race. 
In the old days, this was done with a stopwatch, pen &amp; paper, and some quick ‘gut-feeling’ type analysis. Nowadays, the teams back at headquarters are running extremely complex algorithms to determine the best possible outcome. The interesting factor here for me, is that when these algorithms first came into use, it was probably the algorithm that cost the most to develop. Now however, these algorithms need different types of data at different times, and it is along these lines that I support what you have been suggesting, in that data has become the most important attribute. What amazes me about the Formula 1 analysts, is that they’re not only analysing their own positions, but also the other teams, in real time; for example, comparing the reduction in tyre grip of each of their own four tyres, with the tyre wear on many of the other teams. Knowing that there is a sweet spot for when the tyres go off on other cars (reaching the point whereby they start losing time), and if they can analyse the data correctly, make decisions on this to then put their own cars in the best positions possible for that time. Equally, things change each lap, such as factoring when other teams may do their own pit stops, so they need to be fluid in their analysis too. 
Now, I’ve gone on a bit here but ultimately, what I am trying to say is that once the algorithm was built, it’s the data that becomes the most important attribute, and the faster they’re able to analyse that data, the more valuable it will be relative to winning the race. 
I am absolutely certain, as you have suggested, that Big Data or its successor, is becoming the primary attribute for success ...albeit after the algorithm has been built :-)
Best Wishes, Craig
	              
	              Hi Anthony.
Week 3 for me has been a lot about discovery based on the theme in the course outline on Exploring New Avenues with Computing. It would appear to me that with the level of innovation and vast trends in computing as shared in the discussions, the best is yet to come and we are at the centre of yet another set of disruptive and transformative technology. One of the key lessons I have learned has been towards critical thinking by attempting to look at the current trends in computing today and making a prediction of what is to come tomorrow and better still, be a part of that innovation.
Touching also on the area of Entreprise Content Management was also an eye opener as to the vast amount of data that some organizations store over time and what Big data makes available
Joseph
	              
	              Hi Terry
I've used the free browser version, though it does install a part on my machine, and it does work just not as good as I had hoped. After installation, a new item shows in the word &nbsp;menu which allows me to syncronize from the internet locally, but I rearly use this. Instead I just use the browser version, and normally I create the reference my self, put them into folders, drag over to Export where I can choose Harvard, release it, and the bibliografy shows up in a otherwise blank browser window. I copy it, and paste it into word, and manually add some correction since it's never perfect, for exampel I always have to write [Online] and (Accessed yyyy-mm-dd).
My conclusion is that I've had more trouble than joy, but hopefully it will get better.

Best regards
Bo W. Mogensen
	              
	              Thanks for the summary list/comments, Terry.&nbsp; Yes, there were some interesting discussion threads over the course of the week, with many new insights gleaned.Regards,Anthony
	              
	              Thanks for the comments, Numan. &nbsp;Andragogical learning, via your peers, really does indeed make studying more interesting :-)..&nbsp;Regards,Anthony
	              
	              Hi Bo,
Thanks for the information. I decide to continue to do it manually, considering the value added by this tool and the trouble.
br, Terry
	              
	              Thanks for the summary of topics covered, and comments, Craig. 
True, algorithms and data do form the bedrock of the science of computing. Regards,Anthony
	              
	              Thanks for the comments, Kharavela - this is a very good cohort, and it is quite productive interacting with everyone in the weekly DQ forums.&nbsp;Regards,Anthony
	              
	              Hi Anthony,
I must say, it was indeed a fruitful week, as I gained in-depth knowledge of current trends in diffrent perspevtives. Also realized that in one way or another, directly or indirectly we are using BIG data, cloud computing, virtualisation and mobile computing concepts in our day-to-day life.Really appreciate the way my fellow classmates came-up with the insightful questions and various detailed facts&nbsp;they have shared.
Best regards,Kharavela
	              
	              Hi Bo.
You have a point about the time to read every discussion thread, but I'd say as well that the varied perspectives and depth of research done by other classmates helps a great deal in cutting to the chase over a wide subject area. There's just so much information on the internet on any one topic, so the summarised perspectives and references help in homing in and appreciating the broad scope of content there is to cover.
There's a saying in my country that "If you want to travel fast, go alone, but if you want to travel far, go with others".
Another reference I'd like to point out is from the TV program "Who wants to be a millionaire". In his book, The Wisdom of Crowds (2004)&nbsp;, &nbsp;Author James Surowiecki &nbsp;suggests that large groups of people are smarter than an elite few, no matter how brilliant–better at solving problems, fostering innovation, coming to wise decisions, even predicting the future. The aggregation of information in groups, resulting in decisions that, he argues, are often better than could have been made by any single member of the group. He found that the answers given when sought from the group proved to be more much more accurate than when given by individuals.&nbsp;

References
1) Wikipaedia The Wisdom of Crowds, Available at&nbsp;http://en.wikipedia.org/wiki/The_Wisdom_of_Crowds
2) James Surowiecki&nbsp;(2004),&nbsp;The Wisdom of Crowds
Joseph
	              
	              Hi again
I'm usually using MS Excel for a lot of things, and I have been thinking about using Excel instead of Refworks. In that way I could add the same issue, but I could also add columns like e.g.:

Course
Week
Assignment
Keywords
Links
etc.

Then it will be easy to find the same information and just copy-and-paste.
Well, just a thought for now.
Regards
Bo W. Mogensen
	              
	              Hi Joseph
First, that's a great saying :-)
Second, you are right about all you mentioned. The only reason for me to perhaps focus on fewer is the advice from UoL webinars from other colegues. Future will tell which one I'll choose. Normally I'm the kind of guy who reads it all.
Regards
Bo
	              
	              Hi Craig
Thanks for the feedback.
So, when it comes to data then there are endless possibilities - just look at this wideo that was shared by one of our collegues (don't recall right now who&nbsp;&nbsp;)
https://www.youtube.com/watch?v=bYS_4CWu3y8

Bo
	              
	              Thanks for the summary and comments, Chris. The topics for week 3 were selected to allow a smooth transition into week 4 and beyond, of the module..Regards,Anthony
	              
	              Thanks for the comments, Bo. Yes, some of your listed suggestions are quite useful, and help with time-management and personal efficiency, for the Master's program.Regards,Anthony
	              
	              Thanks for the comments, Martin. Yes, there were indeed a varied selexction of &nbsp;topics for week 3 - I am glad that you found them useful.Regards,Anthony
	              
	              Thanks for the summary and comments, Babatunde.&nbsp; Yes, a useful feature of the program is the interaction with peers via the weekly discussion threads.Regards,Anthony
	              
	              What if you could store thousands of your favourite films and television shows on a small tablet or your mobile phone, and still have plenty of memory left over for your own data? With current developments in the computer technology industry, there may soon be methods to produce huge arrays of non-volatile memory. Hundreds of terabytes or even petabytes of data may one day fit in your pocket or purse.
In this Focused Discussion, you will discuss the potential impacts of these major changes in data storage on computer architecture, data processing and everyday life.
To prepare for this Discussion:

Review the required Learning Resources for this Week.
Conduct a literature review regarding the impacts of the rapid increase in the capacity of non-volatile memory on computer architecture and data processing and storage.
Formulate predictions about the most significant of these impacts.
Find at least one peer-reviewed article supporting the predictions you wish to make in your initial post.

To complete this Discussion:
Post: Create an initial post in which address the following:

Identify the potential impacts of the rapid increase in the capacity of non-volatile memory on:


The architecture of computer systems
Data processing and storage techniques
User experience, as results of the above impacts


Critically analyse these impacts and make predictions about which impact will be the most significant, and why.
Fully state and justify any choices, assumptions, or claims that you make using the suggested Learning Resources for this Week and/or your own research. Support your arguments with at least one external peer-reviewed article.

Respond: Respond to your colleagues. Address the following:

Support or refute your colleague’s predictions about which of their noted impacts will be the most significant.
Alternatively, you may identify impacts that your colleague overlooked and explain why these will be just as significant, if not more so, than the impacts your colleague analysed.
Be sure to support any claims you make.

For all Discussions (unless stated otherwise):

Create a single document with your initial post. Your document should be 350-500 words, though you will be marked based on the quality of your writing, not on the number of words.
By Sunday, post the text of your document to the Discussion Board for this Week, and upload the document using the Turnitin submission link for this Discussion.
By Wednesday, make 3–5 substantial follow-up responses to your colleagues. These can include responses to your colleagues’ initial posts, as well as responses to colleagues who responded to your own initial post. Your total Discussion Board participation must occur on at least 3 individual days during each week. Follow-up responses should be significant contributions to the Discussion. Do not submit your follow-up responses to Turnitin.
In general, online discussion is best when you:


Ask insightful questions.
Extend the discussion into new but relevant areas.
Model or promote critical reflection.
Support your arguments with citations and references from the assigned Learning Resources and other literature, using Harvard Liverpool Referencing Style.



Ensure that you spread your discussion posts across at least three separate days of each week. This will help maximise the value of your discussion with colleagues and serve to meet the learning objectives for each activity.
Click on the Reply button below to reveal the textbox for entering your message. Then click on the Submit button to post your message.
	              
	                 The Opportunity Of A Simplified Memory Hierarchy By Non-Volatile Memory Terry Yin September 27, 2014 1 Challenge from the traditional computer memory hierarchy About 30 years ago in one cold morning, I saw my father making fire under his truck to unfrozen the engine and then use the starting handle to start the vehicle. The whole process took like an hour, and it looked scary to me. The basic memory hierarchy of the modern computer system has two levels, the main memory and the secondary memory. The main memory is volatile, and the secondary memory is non-volatile.     &nbsp; Main memory Secondary memory   speed fast slow   persistency volatile non-volatile   price expensive cheap   address byte bulk   energy inefficient efficient        &nbsp;But the memory hierarchy in a real computer architecture is rarely just two levels. Because we couldn’t get all the good things in one level, we had to resort to the good old tool of computer science, “another level of indirection”,&nbsp;Spinellis&nbsp;(2007). So a typical memory hierarchy is like (picture from wikipedia.org):       And these are only the indirections of the hardware. On the software side, there are more levels of indirections, optimizing compilers, segmentation, memory paging, virtual memory, file system, hot spare,&nbsp;just some examples. These are all the indirections between the processor and where the data is finally located.       The beautiful part of this story is, from a programmer’s view there are only two levels, main memory and secondary memory. The ugly part of this story is, too many levels in the memory hierarchy made the computer architecture over complicated. Some bugs become very tricky, and the compatibility becomes lower. Sub-optimization is very common, which mean the optimization in a part of the system lower the overall performance of the whole system. As&nbsp;Drepper&nbsp;(2007) pointed out: “Hardware designers have come up with ever more sophisticated memory handling and acceleration techniques–such as CPU caches–but these cannot work optimally without some help from the programmer. Unfortunately, neither the structure nor the cost of using the memory subsystem of a computer or the caches on CPUs is well understood by most programmers.” Another problem of having only the volatile memory as main memory is just like the engine starter story I shared in the beginning, starting a computer is very slow. Designing a high availability system has been very challenging and introduced too much extra complexity. The volatile memory also needs energy all the time to maintain the data. Given the demand for high availability services and fast developing data centers, the current memory hierarchy is becoming a big problem. 2 The impacts of non-volatile memory These days in the winter, I just press a button, and my car will start immediately (Well, the fact that I live in Singapore and I don’t actually own a car shouldn’t matter). It’s not because my car has a strong heater, or I have somebody helping me to spin the handle. It’s because the liquid doesn’t freeze, and the battery doesn’t run out that fast now. 2.1 The ideal future As the improvement in the cars, The emerging fast and cheap non-volatile memory may stop us from adding more levels in the memory hierarchy and eventually simplify the hierarchy. “Simplicity is the ultimate sophistication.” Leonardo da Vinci Having the fast and massive non-volatile memory as the main memory, or the only level of memory, as in&nbsp;Perez &amp; De Rose&nbsp;(2010), we can expect: •&nbsp;Lower energy consumption  •&nbsp;improve current memory hierarchies  •&nbsp;allow memory systems to continue scaling up  •&nbsp;Devices that has higher availability and longer duration, e.g. normally-off computer,&nbsp;Ando et al.&nbsp;(2014) Among all these opportunities brought by non-volatile memory, I think the most impacts will come from the simplification of the memory hierarchies. With a simple structure, computer systems will become more reliable and it allows us to build more complicated system based on it.       2.2 Some potential challenge when non-volatile memory goes upward in the memory hierarchy The ideal one level memory hierarchy with cheap, massive, energy efficient, fast and non-volatile memory will not happen very easy nor very soon. Some concerns also showed like in&nbsp;Perez &amp; De Rose&nbsp;(2010),“Actually, the introduction of these new technologies will probably result in&nbsp;more complex hierarchies&nbsp;instead of simpler ones.” In some situation, volatile memory is an important security feature. With only non-volatile memory, to replicate that feature will add more complexity to the system. 3 Conclusion and what user can expect The non-volatile memory will go up in the memory hierarchy and eventually simplify the memory hierarchy. Because of the usage of non-volatile memory as the main memory (maybe the only memory), user could expect more available computers. Just like the availability of cars today compares to 30 years in the cold winter. The computers will use a lot less energy than today. The mobile devices will have longer duration. Because of the simplification of the memory hierarchy, user will experience higher quality and more reliable computer systems. References Ando, K., Fujita, S., Ito, J., Yuasa, S., Suzuki, Y., Nakatani, Y., Miyazaki, T. &amp; Yoda, H. (2014), ‘Spin-transfer torque magnetoresistive random-access memory technologies for normally off computing (invited).’,&nbsp;Journal of Applied Physics&nbsp;115(17), 172607–1 – 172607–6.  URL:&nbsp;http: // search. ebscohost. com. ezproxy. liv. ac. uk/ login. aspx? direct= true&amp;db= a9h&amp;AN= 95982836&amp;site= eds-live&amp;scope= site Drepper, U. (2007), ‘What every programmer should know about memory’,&nbsp;Red Hat, Inc&nbsp;11.  Perez, T. &amp; De Rose, C. A. (2010), Non-volatile memory: Emerging technologies and their impacts on&nbsp;memory systems, Technical report. Spinellis, D. (2007), Another level of indirection,&nbsp;in&nbsp;A. Oram &amp; G. Wilson, eds, ‘Beautiful Code: Leading Programmers Explain How They Think’, O’Reilly and Associates, Sebastopol, CA, chapter 17, pp. 279– 291.  URL:&nbsp;http: // www. dmst. aueb. gr/ dds/ pubs/ inbook/ beautiful_ code/ html/ Spi07g. html &nbsp;       
	              
	              Week 4 Discussion: Computer Memory
For the past 50 years we have assumed a two-level store i.e. a fast primary memory and a slow secondary memory. This impacts the entire system structure including I/O system, the protection system, the scheduler, the way in which processes and programs are managed and initiated as well as the virtual memory. The rapid increase in the capacity of non-volatile memory could change all that and result in only one level of persistent and uniform memory, that has no paging of disks or memory, no booting on restart. This rapid increase could also influence the design of major operating system components program and operating system execution models, as well as enhance the performance and reliability traits of the overall system, which would lead to faster , cheaper and more agile systems than we build today.
NVMs could also impact the way applications are installed and launched; instead of currently where applications exist in their packaged state before installation, in the file system after installation and finally their launch into processes. However, non-volatile storage has the potential to change this by enabling applications to be checkpoints in an already executing process thus changing the application shipment game. Non-volatile technologies are invaluable wherever there is a bottleneck in terms of storage and I/O and where downtime costs money such as in searching, data mining, business analytics, digital media creation and or transmission and financial modelling.
Non-volatile memory is especially advantageous in a power failure as they allow servers to almost instantaneously recover in their memory state without extra load on the storage backend. This is very beneficial in business critical applications such as online transaction applications which can be up and running again in a short period of time e.g. minutes instead of hours, they provide memory storage that is not lost when power is not supplied to the electronics basically providing an instant-on capability. NVMs also mean better scalability and lower power leakages.
However, in spite of these positive impacts of NVMs, there are a few negative predictions to consider, such as the risk of data corruption, should there be some data corruption as a result of a hardware or software fault then that corrupted data is also non-volatile especially if there is one level of memory unlike with traditional memory storage architecture of two levels, where data is scrubbed as it is transferred between the two levels. There is a potential risk with data portability; hard drives and SSDs can be physically moved and rearranged whichever way, however with future NVMs that might be impossible to do as the NVMs might be built on non-portable architecture. &nbsp;
References:
http://www.vikingtechnology.com/uploads/nv_whitepaper.pdf Accessed 26 September 2014
http://www.aicit.org/ijact/ppl/vol4no9main_part22.pdf Accessed 27 September
http://www.zdnet.com/intel-non-volatile-memory-shift-means-chips-need-an-overhaul-7000004221/ &nbsp;Accessed 26 September
http://www.forbes.com/sites/tomcoughlin/2013/11/19/changing-roles-of-enterprise-flash-memory-and-hdds/ Accessed 26 September
http://www.tomcoughlin.com/Techpapers/Nonvolatile%20Memory%20Report%20Brochure,%20040914.pdf Accessed 26 September
http://www.cse.psu.edu/~yuanxie/nvm.html Accessed 26 September
http://books.google.co.za/books?id=JRr1AgAAQBAJ&amp;pg=PA370&amp;dq=what+is+memristor+technology&amp;hl=en&amp;sa=X&amp;ei=0LsmVJO9C-607Qa_5ICYDQ&amp;ved=0CCUQ6AEwAQ#v=onepage&amp;q=what%20is%20memristor%20technology&amp;f=false Accessed 27 September 2014

	              
	              
DQ1, Week 4, Computer memory

Introduction
I think it’s safe to say that also this week’s subject is extensive, and almost every part in computer science can be draw into the discussion, i.e. I’m not sure which part I can’t draw into this discussion. 
New knowledge, skills, intuition, predictions etc. are emerging every day catalysing this process to repeat itself again and again. Expressing itself in new possibilities, demands and solutions. Researchers are on an eternal hunt for continues progress. And in this we are also faced with new issues on moral, ethics, and other issues.
Non-volatile memory is the type where you store data for a long time and don’t lose it when you turn off the power, e.g. hard. &nbsp;Volatile memory like the internal memory (RAM, registers and cache) on the other hand is memory that will lose data when shut off. (Lecture notes).
Abbreviations are shown in the appendix.
Thesis
The currently know technical solutions to the rapidly increasing capacity (Borne 2014) will not be enough. Developers will have to come up with new innovative solutions, though the current technology will hold up for a foreseeable future by just adding more storage of the same kind. But at some time this has to change to a situation where we can store more data on less space.
Content
Potential impacts of the rapid increase in the capacity of non-volatile memory
The amount of data is rapidly increasing, and this of course affects both hardware and software, in such a way, that they will have to be re-innovated to be able to handle and manipulate the data. There are a lot of potential impacts, and below I’ll outline, and shortly comment, some of them. But there is also a lot of emerging NVM technologies like the MRAM, RRAM and PCRA, the last “is argued to be a scalable technology” (Perez, Rosa, p 18) but then again all these technologies have drawbacks.
1. Impacts on the architecture of computer systems
1.a) CPU
The CPU has to be optimized; it has to get faster at handling the instructions which in turn also have to get better, e.g. the “data transfer group”. This again demands the bus (and wires) and the controllers to be more efficient allowing the CPU to delegate certain tasks, hence running in parallel. Another plausible solution is that there will be more processors in every computer.
Also the volatile memory, i.e. internal memory, must increase in capacity and speed.
1.b) Memory
Speed: Memory has to get faster, and one way of doing this is using the SSD since its faster than HDD (Baxter).
Capacity: The vast amount of data demands huge memory devices. If we compare SDD and HDD, the HDD currently has a huge advantage. But I’m sure that the SSD’s at some time will be able to contain as much data as HDD’s and even more. The process of increasing capacity has already started increasing bits pr. cell from 1 (SLC) to 3 (MLC), but MLC has read/write disadvantages compared the SLC (ExplainingComputers and OCS).
Endurable: Many say that the SSD are not as endurable as the HDD when it comes to saving data. But according to Baxter it’s not true. The SSD technology has improved, and “the data storage integrity will be maintained for well over 200 years” (Baxter).
Price: The SSD’s are currently very expensive and according to Ask.com the price (February 2014) was for HDD 8 cents/GB, and for SSD 60 cents/GB.
1.c) Energy / power
Increasing processors etc. could also increase the energy usage. Developers will have to find ways to minimize the energy usage. “Main memory energy/power is a key system design concern“(Meza 2013).
1.d) Operating system
Depending on the chosen technology it can have impact on the OS which could require improvements (Perez, Rosa, p 37)
2.&nbsp;Impacts on data processing and storage techniques
2.1) Database designs must be improved: In the footsteps of Google, Amazon and Hadoop, database design must be continuously re-innovated to cope with the ever increasing data amounts.
2.b)&nbsp;Techniques must be improved for saving, fetching and storing data, e.g. by new variants of de-duplication.
2.c) Compression algorithms must also continuously be optimized according to the type of data available.
2.d) Re-storage of old data: Until now, as far as I know, there has not developed any analogue or digital media which can save data for all eternity. Further we know that systems are changing, so even if there is an eternal media, we normally shift to new systems which cannot read the original media; hence if we want to use the old data, we probably need to develop some technique which “converts” the data to the new media design semi-automatically (probably some human or algorithm has to decide if the data are worth saving). Today, data are often saved to tapes “with the hope that it will never need to be restored” (Lecture Notes, week 4, p. 3)
3. On user experience, as results of the above impacts
3.a) Wearables: These will be able to collect, analyse and send more data.
3.b) GPS: Systems will be even more “intelligent”.
3.c) Cars: People’s posture will be registered and will hopefully save lives and prevent theft (Mayer-Schonberger, Cukier2013)
3.d) IoT: All these devices will have more data and be more intelligent.
3.e) Misc. data: Every person will have more data (documents, music, etc.), and these must at all times be accessible and easy to find.
Critical analysis of the potential impacts
From my readings, especially from Perez and Rosa, it’s not likely that there will be any simple solution in the nearest future, hence we will have to implement hybrid and/or other advanced (e.g. hierarchical) solutions. This could have negative impacts in the sense of manufacturers having to modify both hardware and software.
Assuming that the developers will find the “universal memory” then this will have a big impact since it will positively increase the attributes of cost, speed and density etc. On the other hand, it’s not developed, hence we don’t know its impact on other areas (Perez, Rosa 2010).
Conclusion
It’s clear to me that we have to do something and we cannot wait for the “universal memory”.
Having researched a great deal of material (relative to the time), I’m complied to agree whit the conclusion (Perez, Rosa. chap. 5) in the report by Perez and Rosa, i.e. since the current emerging technologies do not have all the necessary attributes (see appendix) for a single “universal memory” we’ll have to use some kind of memory hierarchy or at least some hybrid designs.
Appendix
References
ASK.Com 2014. [Online] http://www.ask.com/wiki/Solid-state_drive#Quality_and_performance. [Accessed 2014-09-27]
Baxter, Storagereview. [Online] http://www.storagereview.com/ssd_vs_hdd. [Accessed 2014-09-27]
Borne, K. 2013, 2013-06-10-last update, Big data, small world [Homepage of TEDx talks], [Online]. Available: https://www.youtube.com/watch?v=Zr02fMBfuRA [Accessed: 2014, 09-29].
Brookshear, J. Glenn, Computer Science, 11th edition.
ExplainingComputers, “Explaining SSDs 2014 update”, 2014. [Online]. https://www.youtube.com/watch?v=TFoOyPXYJ-E. [Accessed 2014-09-27]
Lecture Notes, week 4. [Online] https://elearning.uol.ohecampus.com/bbcswebdav/institution/UKL1/201480_SEPTEMBER/MS_CKIT/CKIT_501/readings/UKL1_CKIT_501_Week04_LectureNotes.pdf. [Accessed 2014-09-25]
Mayer-Schonberger, Viktor and Cukier, Kenneth, 2013-03-05. "BIG DATA: A Revolution That Will Transform”. [Online] https://www.youtube.com/watch?v=bYS_4CWu3y8. [Accessed 2014-09-27]
Meza, Justin. 2014. “18-447: Computer Architecture Lecture 34: Emerging Memory Technologies”.&nbsp; [Online] http://www.ece.cmu.edu/~ece447/s13/lib/exe/fetch.php?media=meza-447-spring13-lecture34-emergingmemory.pdf. [Accessed 2014-09-27]
OCS, OCS storage solutions. [Online] http://ocz.com/consumer/ssd-guide/ssd-vs-hdd. [Accessed 2014-09-27]
PCMag, Price 201. [Online] http://www.pcmag.com/article2/0%2c2817%2c2404258%2c00.asp.&nbsp; [Accessed 2014-09-27]
Perez, Rosa 2010. “Non-Volatile Memory: Emerging technologies and their impacts on memory systems”. [Online] http://www3.pucrs.br/pucrs/files/uni/poa/facin/pos/relatoriostec/tr060.pdf. [Accessed 2014-09-27]
Singel “universal memory”
Referencing Perez and Rosa a single “universal memory” should have the following attributes:

Non-volatile
Low-cost
Higly dense
Energy-efficient
Fast
High endurance

Abbrevations
HDD &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Hard Disk Drive SSD &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Solid State Disk SLC &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Single Level Cell, storing 1 bit pr. cell MLC &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Multi-Level Cell, storing 2 or more bits pr. cell CPU &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Central Processing Unit IoT &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Internet of Things, refrigirators, etc. Wearables &nbsp; &nbsp;Wathces, medico devices, gps etc. NVM &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Non-volatile memory PCRAM &nbsp; &nbsp; &nbsp; &nbsp;Phase-Change RAM MRAM &nbsp; &nbsp; &nbsp; &nbsp; Magneto resistive RAM RRAM &nbsp; &nbsp; &nbsp; &nbsp; Resistive RAM
&nbsp;
Best wishes
Bo W. Mogensen

	              
	              
	                Attachment: &nbsp;Craig Thomas - Week 4 Computer Memory 28 Sept 2014.docx (568.61 KB)
	              
	              Week 4 Discussion Question – Computer Memory
Craig Thomas – 28 September 2014
Computer memory is a form of storage for data.
I’d like to start by summarising the types and the hierarchy of computer memory within a computer system. This will also help to position my questions and predictions throughout this discussion. 
Firstly, there are two distinct types of computer memory. These are volatile, and non-volatile memory. Volatile memory requires a constant electric charge to be able to retain its storage. Non-volatile memory does not, in that it is able to save its data even when there is no power being provided.
Secondly, computer memory is typically constructed around a hierarchy within a computer system and is classified as either primary or secondary memory. This hierarchy generally consists of different levels of memory, which are positioned relative to the Central Processing Unit (CPU). 
Computer memory development has moved on over the years however to help explain, in summary;

The memory closest to the CPU is known as processor registers (level 0) and is extremely fast and extremely expensive. Registers typically include critical operational or program instructions and data that is immediately applicable (Brookshear, 2012) and being manipulated by the CPU. This memory is on the same die of silicon as the processor. Typically, the speed of this memory is sub nanosecond, less than 1 CPU cycle, and is accessed directly by the processor itself (Franklin and Garcia, 2011). This is volatile memory.
As you move away from the CPU, you then have cache (level 1) which is still on the same chip as the processor, again it is sub nanosecond and extremely fast, but not as fast as the registers. Level 1 cache typically contains a portion of main memory that is of current interest relative to the program being executed (Brookshear, 2012). This is volatile memory.
Then you have cache level 2, sometimes referred to as SRAM. This is Static Random Access Memory. This is volatile memory.
Moving even further down the memory hierarchy, you then have the other cache (levels 3 and above) that still, are relatively fast, but are hundreds of times slower than the memory positioned earlier in the hierarchy. These are typically a form of SRAM and are volatile memory.
Then you reach the computers main memory, often referred to as Random Access Memory, or DRAM, which is dynamic random access memory. This type of memory is still quite quick, but in relative terms, it is thousands of times slower than the memory higher up the hierarchy. This is managed by the Operating System (OS). This is also volatile memory. This type of memory slowly loses its charge, and requires refreshing periodically.
As you reach the lower end of the memory hierarchy, you will come across secondary memory. This is typically disk, flash or even tape. This memory is, in comparison, massively slower than the other memory types (tape is even slower). This is managed by the OS. This is the lowest cost of memory however, this memory is non-volatile and as such, retains its data content. 

Please see Fig. 1 (Franklin and Garcia, 2011), which helps to outline the memory hierarchy.
Fig.1 (Franklin and Garcia, 2011)

We have seen constant developments and evolution of computer memory over the last 25 years or so that has directly impacted the design and architecture of a computer system. 
As described by Oscar Villellas (2013), computer architecture no longer consists of a single layer of non-persistent (volatile) storage and a single layer of persistent storage (non-volatile). Computers now have multiple layers of volatile storage, such CPU registers, various levels of cache and RAM, as well as multiple layers of non-volatile storage, such as hard disk, NAS drives, solid state drives, flash drives, and although not so much nowadays, tape drives. Also see Fig. 2 (Oscar Villellas, 2013).
Fig. 2 (Oscar Villellas, 2013).

Through each evolutionary period, we have seen an increase in the memory that is positioned closer to the CPU towards the top of the memory hierarchy (shown at the bottom of the picture in Fig. 2), as well as an increase in the use of solid-state type technologies at the other end of the hierarchy (top of picture in Fig.2). Both of these have seen a significant reduction in size in the order of magnitude that implies that we will continue to see further reductions over the next decade. 
I would like to pose a question that if we continue to see this level of development and memory continues to be reduced in actual size, and at the same rate it also continues to be made faster, with even deeper levels of compression logic invented, is it feasible that all memory could be positioned on the same silicon as the CPU, extending the processor down the left hand side of Fig. 3 illustration (Leng, 2007)?
Fig. 3 (Leng, 2007) 

I acknowledge that this would also need to be combined with scientific advancements in the construction of memory itself (i.e. materials, electrons, nanoscale).
As outlined by Lankhorst et al (2005), the ideal semiconductor memory for future integrated silicon circuits should look to unify the inherent qualities of the different technologies. It should have the high speed of static random access memory (SRAM), the non-volatile nature of flash and the density of dynamic random access memory (DRAM). However, the physics, the material properties and the programming voltages mean that scaling down to nanometre cell sizes will be a challenge. 
If successful however, is it then feasible to suggest that the need for any external volatile random access memory (main memory) no longer exists, as we know it today? 
Wouldn’t it be great if the hardware just intuitively handled all types of memory requests, meaning it was automatically fully adopting the best, quickest and most appropriate way for calling on memory, without the need for programmers to worry about that in their coding?
Lastly, is it also feasible to suggest that external memory of a non-volatile nature, continues only to be used (in this same time period) for longer-term archive type storage requirements? 
Continuing with this theme, perhaps eventually, all types of memory become non-volatile and we see the emergence of ‘always on computing’? Imagine the positive user experience then? 
We have also see the arrival of solid-state disks in recent years. These include Enterprise grade, which are used across various technologies supporting business operations. Also, we have solid-state for the Prosumer, which we see in high-end personal computers, smart phones and tablets, and then there are solid-state for Consumer class products, which typically include flash-type drives for connection to USB ports on computers. The latter enable you to store hundreds of GB of data, and carry this around with you on your key chain. Something that was unimaginable 20 years ago!
When you look at the evolution of smart phones, you are able to store video and music, and have this integrated with the smart phone technology to then watch or listen to your music whenever you want and wherever you are, even on the go.
However, in addition to the developments in this type of non-volatile storage which has enabled this type of technology to thrive and deliver a first class user experience, we are now also seeing disruptive services such as cloud storage, which are to some extent potentially negating the need for you to keep the storage with you at all times. Subject to bandwidth and connectivity constraints, this is now enabling content to be stored offline, albeit can be streamed to your device as if it is online. Combining a great user experience, with a solution that is perhaps more robust and cost effective.
Could we see further emergence of computing power in the cloud also? What I mean here is that, similar to the legacy type client server environments in the past across enterprise operations, whereby processing is done at the server end, is it possible that our devices become more of a viewing device? One of the things that we are restricted by is power, the retention of power, and subsequent need to re-charge the power. What if the majority of processing that draws the majority of the power is all done in the cloud? That, combined with any technological advances in quantum mechanics that mean less power is needed on the device anyway, could mean a device stays charged for a week, or a month, or indefinitely? 
Given the emergence of cloud storage services, and given the continued development of connectivity services such as 4G for mobile and high speed Fibre Optic cable for business premises and home (such that latency is lessened), is it feasible to suggest that for Consumer class flash storage, these needs are migrated to cloud based services in the medium term, with the only question left being around the reliability of the connection? Will flash storage exist in 3 years time, in the sense we know it today for consumers?
In contrast, and given both the cost decline and capacity increases in terms of Moore’s law, according to Caulfield et. al (2009, cited in Deng &amp; Zhou, 2010 pg. 224) using NAND flash memory to build large-scale storage systems is more and more practical. 
In conclusion, we will continue to exploit new technologies such as solid-state flash storage, particularly at the higher enterprise end of the scale. We could however see the opposite at the consumer end of the scale, in that it becomes obsolete (or massively reduced in terms of usage) with the full migration to cloud based services, but this is dependent on key connectivity technology becoming ultra reliable and continues to be cost effective. 
My biggest prediction, which is dependent on the laws of economics (p.s. I didn’t even come close to getting the winner at the Grand National this year!), is that we will see the full integration of non-volatile memory onto the same silicon die as the CPU, with technological and quantum mechanical developments that enable reliable and sustained ‘charge’ for operation, and sufficient capacity to support all critical level data processing needs. 
Best Wishes, Craig
References
Laureate Online Education. (2014). Computer Structures – Lecture Notes [Online]. Available from: https://elearning.uol.ohecampus.com/bbcswebdav/institution/UKL1/201480_SEPTEMBER/MS_CKIT/CKIT_501/readings/UKL1_CKIT_501_Week04_LectureNotes.pdf (Accessed 25 September 2014)
Oscar Villellas, F. A. (2013). A data format leveraging the hierarchical memory model [Online]. Continuum Analytics. Available from: http://continuum.io/blog/blz-format (Accessed 26 September 2014)
Leng, R. J. (2007). The Secrets of PC memory: Part 1 – Simplified Computer Memory Hierarchy illustration [Online]. Available from: http://www.bit-tech.net/hardware/memory/2007/11/15/the_secrets_of_pc_memory_part_1/3 (Accessed 27 September 2014)
Deng, Y., Zhou, J. (2010). Architectures and optimisation methods of flash memory based storage systems. Journal of systems Architecture 57 pp. 214-227 [Online]. Bit-Tech.net. Available from: http://www.sciencedirect.com.ezproxy.liv.ac.uk/science/article/pii/S1383762110001657 (Accessed 25 September 2014)
Nishi, Y. (2011). Challenges and opportunities for future non-volatile memory technology. Current Applied Physics 11 pp. 101-103 [Online]. Available from: http://ac.els-cdn.com.ezproxy.liv.ac.uk/S1567173911000435/1-s2.0-S1567173911000435-main.pdf?_tid=1cb73ddc-4695-11e4-b4e4-00000aacb361&amp;acdnat=1411856877_31c55540b4cfd365f2b5a24e4373f623 (Accessed 27 September 2014)
Lankhorst, M. H. R. et al. (2005). Low-cost and nanoscale non-volatile memory concept for future silicon chips. Vol. 4 issue 4, pp. 347-352 [Online]. Available from: http://content.ebscohost.com.ezproxy.liv.ac.uk/ContentServer.asp?T=P&amp;P=AN&amp;K=18892784&amp;S=R&amp;D=a9h&amp;EbscoContent=dGJyMNHX8kSeqLU40dvuOLCmr0yep7ZSsau4SrOWxWXS&amp;ContentCustomer=dGJyMOzprkmvqLJPuePfgeyx43zx (Accessed 27 September 2014)
	              
	              Hello Belinda,
I would like to point out that its not just the NVM's or SSD's which will improve the performance, but also how memory will be addressed in an OS and CPU chipset. As we know, all memory is addressed by the OS's File Allocation Tables, the change in the storage will enforce architectural changes in the CPU and thereby changing the way memory is addressed; as well as change the way FATs are designed by the OS. In modern devices, memory is addressed and data is stored in the Advanced Format (512e)[1].
Reference
[1] http://en.wikipedia.org/wiki/Advanced_Format
Best regards,Kharavela
	              
	              Hello Bo,
I completely agree with your introductory statement, as the overall system performance depends on the individual performance of storage devices.
For example, even when you have 50 Mbps internet connection, but your hard drive is relatively slow, the content that is getting downloaded will not be written on to the HDD with the same speed; and hence your system will be losing out on the available bandwidth.
Best regards,Kharavela
	              
	              


Hi Craig,







I read you prediction “we will see the full integration of non-volatile memory onto the same silicon die as the CPU,” and the question you asked. My first thought was that this is not desirable and not likely to happen. But when I think more about this idea, I start to feel it might be a good idea. 
Let’s don’t confuse this with the already existing idea of “microcontroller”, Wikipedia (2014), which is simply put the CPU and memory on one physical chip and has limited areas of using. I guess you are proposing a deeply integrated chip with the emerging technology and will be general-purposed, like it will be used to build both small phones and large data center. 
Nowadays CPU slowed down to increase in frequency but start to have more cores. Because of the need for parallel processing, functional programming languages will perhaps dominate the future, Hughes (1989) (yes, the reference is a bit too old). Unlike the structured or object-oriented programming paradigms, the functional programming does not share mutable states among the processes, and information is pass in the form of closures. So let’s be very open minded, having each core of the CPU having its own non-volatile memory as a highly cohesive unit might be a superb idea. Maybe even the system software on the same chip as well. The concept of closure could be extended to “you can unplug the physical unit at any time and ship it oversea and continues to execute from there.” We could build highly distributed, functional style system using these chips. 
Things that belong together should be kept together (Cohesive Software Design, by Tim Ottinger, Jeff Langr). Nobody keeps the two pieces of a pair of scissors separately. This is called cohesion. What things should be kept together? I think they should be the things that is used together and changed together. In the traditional computer architectures, CPU and memory are used together, but often changed separately. I think that is why they are usually decoupled. Will the emerging functional programming style of design change this situation and make them a cohesive unit? 
br, Terry 
References 
Hughes, J. (1989), ‘Why functional programming matters’, Computer Journal 32(2), 98–107. cited By (since 1996)139. URL: http: // www. scopus. com/ inward/ record. url? eid= 2-s2. 0-0024648265&amp;partnerID= 40&amp;md5= 326e561e4b4e4897a29292ccfd9dbde5 
Wikipedia (2014), ‘Microcontroller — wikipedia, the free encyclopedia’. [Online; accessed 28-September- 2014]. 
URL: http: // en. wikipedia. org/ w/ index. php? title= Microcontroller&amp;oldid= 627227088 1 




	              
	              Computer memory &nbsp; Introduction     &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Back in early 2000, when I used desktop computers, primary memory or RAM was limited to just 256MB. It would not be an exaggeration to state that today’s desktops, laptops, tablets and mobile devices, or any smart device for that matter, have taken a quantum leap by being able to address more primary memory than ever before. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; On the other hand, non-volatile memory or HDDs are able to store significantly high amounts of data, but with some latency. The “Hard” in the HDD is nothing but the rapidly rotating magnetic platter which will store the information with the help of actuator [1]. The existing mechanism was good enough until the demand in data mining started to increase rapidly and it made way to new problems and prospects. HDD’s have evolved over the past so many years, based on their architecture, storage capacity and performances as listed below [2]: &nbsp; &nbsp;&nbsp;&nbsp; 1.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; PATA / IDE / EIDE The labels relate to the type of interface that is employed to connect the disk drive to the CPU board. These drives utilize either a 40 or an 80 wire cable with a broad 40-pin connector. 40 wire cables are utilized in older and slower hard disks, whereas 80 wire cables are used in faster ones. Nowadays, these types of hard disks are being substituted by SATA hard disks. EIDE hard drives were introduced after some advancement in IDE hard disks, however, the term IDE refers to both IDE and EIDE disk drives.     &nbsp;&nbsp;&nbsp; 2.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; SATA These hard disks are faster data interface than the latter. SATA drives are more efficient, and use less power than the PATA ones.     &nbsp;&nbsp;&nbsp; 3.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; SCSI These hard disks are similar to IDE hard drives. They also spin at a higher rate in comparison to IDE and SATA ones. IDE and SATA drives generally spin at 7,200 rpm, whereas SCSI ones spin at 10,000 to 15,000 rpm. Today, SATA drives, featuring a speed of 10,000 rpm, are also manufactured. The higher the rpm, faster is the data access, but it may also lead to a faster breakdown.     &nbsp;&nbsp;&nbsp; 4.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; SSD SSDs, unlike the other types, don't consist of moving components, but they use semiconductors for this purpose. SSDs are much faster and less likely to break down than other drives. &nbsp; Even though SSDs are available in the market, they haven’t yet replaced the SATA drives due to higher costs. The storage architecture community [3] is exploring the prospects of faster and cheaper non-volatile memory; which also includes the error or checksum correction mechanism built-in to the NVRAMs [3]. The implementation of the NVRAM will also bring about changes in the way the Operating Systems address the memory while accessing data as shown in the figure [3].    &nbsp; With all these improvements, users will be able to observe quicker data transfer rates between systems and lightning fast data retrieval in the system, which in turn improves the overall experience of the system response. We can see the detailed speed test done by (Lucas Mearian, in his article on Computer world website) comparing the HDD and SSD [4]. &nbsp;      &nbsp;Properties   Intel 520 Series SSD   Western Digital WD Black HD   Seagate Momentus XT Solid State Hybrid Drive     Capacity   240GB   500GB   750GB     Price   $229   $72   $134 ($89 for 500GB)     First/third boot-up time   12/9 sec.   20/21 sec.   20/12 sec.     Max. read speed (4K blocks)   456MB/sec.   122MB/sec.   106MB/sec.     Max. write speed   241MB/sec.   119MB/sec.   114MB/sec.     1.19GB file transfer   15 sec.   34 sec.   29 sec.     First/third time opening a 372-page Word doc.   57/10 sec.   48/9 sec.   58/10 sec.     Opening a 10MB PowerPoint document   2 sec.   2 sec.   2 sec.         Even though this is just a comparison between 3 specific products, we can clearly see the difference in the speeds and the cost.     Conclusion     &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; All the memory that is available for the CPU to perform any task will reduce the paging, which in turn will enable the OS to multi-task faster. As of today data mining is the major and most crucial task for any organization, which requires reliable and faster data storage devices available at their disposal for quicker turn-around-time in order to provide support and resolve their customer issues. &nbsp; &nbsp; Abbreviations     PATA&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; - &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Parallel Advanced Technology Attachment   IDE &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;- &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Integrated Drive Electronics   EIDE&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; - &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Enhanced Integrated Drive Electronics   SATA&nbsp;&nbsp;&nbsp; &nbsp; - &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Serial ATA   SCSI&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; - &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Small Computer System Interface   SSD&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; - &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Solid State Drives   NVRM&nbsp;&nbsp; &nbsp; - &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Non Volatile Random Access Memory   OS&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; - &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Operating System      References:     [1] http://en.wikipedia.org/wiki/Hard_disk_drive [2] http://www.buzzle.com/articles/hard-drive-types.html   [3] Operating System Implications of Fast, Cheap, Non-Volatile Memory - https://www.usenix.org/legacy/event/hotos11/tech/final_files/Bailey.pdf   [4] http://www.computerworld.com/article/2492024/solid-state-drives/hard-disk-drives-vs-solid-state-drives-are-ssds-finally-worth-the-money.html &nbsp;&nbsp; Best regards,   Kharavela
	              
	              Hi Bo,
Though I would also like to see the "universal memory" in the future, but still, let me challenge this idea a bit:-)
Last week we argued over the idea of computer science is the science of "algorithm" or "data". This week, as I read all the related materials, I got a wonderful new idea: "maybe computer science is the science of indirections (abstractions)." Or, put them all together, "computer science is the science of the abstractions between the algorithm and the data".
Let's take the TCP/IP protocol stack as an example. The IP (Internet Protocol) is unreliable. What you sent via IP protocol is not promised to reach the target. So people build the TCP protocol upon IP and the TCP protocol is reliable. Please find the whole story in (Spolsky 2004). The same story happened in the memory hierarchy as well. Although these abstraction layers are now and again "leaky", as Spolsky as pointed out, but they allow us enjoy the feature we don't directly have, or don't have yet. Some of the levels are also used to decouple the solution for different concerns. So there's quite some motivation for having a memory hierarchy.
Another issue is there will always be new technology that is better than the old ones, and is often more expensive. Because of this, perhaps given any time in the future, a hybrid solution might always be more desirable than using one "best" solution.
br, Terry
Reference
Spolsky, J. (2004). The law of leaky abstractions. In&nbsp;Joel on Software&nbsp;(pp. 197-202). Apress.
	              
	              Hi Kharavela
Thanks for taking time to read my initial DQ.&nbsp;
Yes, we agree on the memory performance issue.&nbsp;But imagine putting a state-of-the-art memory unit in your "computer" but you don't get the expected performance boost.&nbsp;Hence&nbsp;since everything is interconnected, the performance also relies on e.g. the programmers to adjust OS&nbsp;(Perez, Rosa 2010)&nbsp;to make use of the memory features.&nbsp;
Perez, Rosa 2010. “Non-Volatile Memory: Emerging technologies and their impacts on memory systems”. [Online]http://www3.pucrs.br/pucrs/files/uni/poa/facin/pos/relatoriostec/tr060.pdf. [Accessed 2014-09-27]
Best wishes
Bo W. Mogensen
	              
	              Hello Everyone,How reliable are Solid-State-Disks (SSDs)?Can you think of instances where you would advise against their use?Anthony
	              
	              Hi terry
Both of your challenges are relevant and justified. The life cyklus of technology has repeatedly shown us, that new technology gets invented prior to the marked being ready, hence your thought about always using a hybrid solution is probably the most likely and practical approach.
Perez, Rosa 2010. “Non-Volatile Memory: Emerging technologies and their impacts on memory systems”. [Online]http://www3.pucrs.br/pucrs/files/uni/poa/facin/pos/relatoriostec/tr060.pdf. [Accessed 2014-09-27]
Best wishes
Bo W. Mogensen
	              
	              Hi Craig
I hope you remembered to also use the "Discussion" menu to access the Turnitin.
Best wishes
Bo W. Mogensen
	              
	              Hi Bo,
Yes, I did remember, thanks for the pointer though :-)
I inserted this, as for some reason I wasn't able to cut and paste and include my pictures? I am not sure how the others do that. So, aswell as pasting it, I also attached so everyone can see Figs 1, 2 and 3.
Are you/anyone able to advise how that is done please?
Cheers, Craig

	              
	              Hi Terry,
Thanks very much for your feedback. 
I actually found this discussion to be extremely challenging, both in terms of pushing my technical knowledge and understanding (I’ve forgotten much more than I have remembered!) as well as trying to think about how we could look to build on the advances made to date. I’m sure my prediction is nothing new with the chip manufacturers.
I did look at the microcontroller as part of my research, and as you have highlighted, the use is somewhat limited and typically for low-end items, household, remote controls etc.
My thinking is similar but for was for the high end use, whether it be smart phones or in large data centre server environments.
My thinking was guided by the evolution that we are making advances in reducing the size, increasing the speed, and making things cheaper. As part of my research, I recall seeing that in early 1990’s, 1Gb of solid state storage cost ~ $80k, yet now 1Gb solid-state is ~ $10 (Sorry, I didn’t keep the reference as wasn’t intending on using this specific information).
The main area that focused my mind was around the introduction of cache memory and that we have seen advances over the years of moving memory closer to the CPU. Also, the fact that other main memory was accessed via a bus, which introduced delays to the process. Whilst this main memory is still super quick, in relative terms, and in relation to CPU cycles, it is still massively slow compared to the speed for accessing registers and cache. 
The fact that we have evolved from magnetic tape, to magnetic disk platters, to optical lasers and then to charged electrons and now we’re talking about 3D memory stacking in the development of memory and storage, the obvious area for the future was on the integration (as you say, deeply integrated) of CPU and non-volatile memory, assuming those really bright physicists can make this happen. 
If this ‘deep’ integration can also be done with multiple cores having dedicated memory, and introducing 3D stacking on the memory itself, we could see advances even more significant than we have seen to-date.
Best Wishes, Craig
	              
	              I actually read your docx attachment, for I know it should have better format:-) You have to upload the figures manually using this button if you want to see it in your post:  
	              
	              Hi Dr. Ayoola,
I personally have mostly good experience with SSDs, without knowing any of it's potential problems.
Some quick study from wikipedia, I found most of the problems of SSD are either already better than what we have with HDD or can be avoided with special arrangement. I found three situations that don't fall into either situation:
1. SSD cannot survive too many sudden power outage
2. HDD can be stored longer offline.
3. Data recovery in desastrous situation will be a lot harder with SSD
So my conclusion is that with the current SSD technology, it may not be suitable in a situation that the environement is very coarse while the data in the system is very precious.
br, Terry
Reference
Solid-state drive. (2014, September 26). In&nbsp;Wikipedia, The Free Encyclopedia. Retrieved 15:51, September 28, 2014, from&nbsp;http://en.wikipedia.org/w/index.php?title=Solid-state_drive&amp;oldid=627169003
	              
	              Hi Anthony

I’d say Solid-State-Disks are magnificent devices that could make you love your computer more than you ever thought possible, especially if you’re the impatient type. Solid-state drives not only significantly reduce boot times and make your system feel much quicker, they also make app launching and file copying lightning fast.

&nbsp;

Reliability


Installing a solid-state drive (SSD) into your machine can radically strengthen its speed and reliability. What makes a SSD superior to a regular HDD is its memory. In an HDD, there are constantly spinning discs that read and write data magnetically.&nbsp;In an SSD, however, the memory doesn’t move.&nbsp;SSDs, instead, use a motionless technology called&nbsp;NAND flash [1]&nbsp;memory to read and write.&nbsp;Notably, a computer takes a lot less time to hunt and gather data from an SSD because it’s able to find data just as quickly, no matter where it is in the memory. Meanwhile, a machine must search everywhere in an HDD to find a specific block of information, as the data block’s fragments may be spread across different locations. In fact, an SSD purposefully stores data in different spots to cleverly avoid wear and tear – but this never affects efficiency.


On the downside, an SSD can become slower as time progresses, but a recently manufactured SSD should last as long as (if not longer than) an HDD.

Things you shouldn’t do with SSD


Don’t use old Operating System i.e.(Windows XP, Windows Vista)
Don’t store large, infrequently accessed files
Don’t write constantly to them.
Don’t fill them to capacity
Don’t defragment
Don’t wipe



Abbreviations  HDD&nbsp;&nbsp;&nbsp;&nbsp; -&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Hard Disk Drive SSD&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Solid State Drives

Regards, Babatunde


References

[1] http://whatis.techtarget.com/definition/NAND-flash-memory, and http://en.wikipedia.org/wiki/Flash_memory#NAND_flash (online) (Accessed 28/09/14)
Things you shouldn’t do with your SSD (online). Available from: http://www.howtogeek.com/165472/6-things-you-shouldnt-do-with-solid-state-drives/ (Accessed 28/09/14)

http://www.digitaltrends.com/computing/solid-state-drives-vs-hard-disk-drives/ (online) (Accessed 28/09/14)
	              
	              Thanks Terry Just testing it - it seems we both reviewed similar memory hierarchy illustrations :-) I'll respond to your thread seperately - I can't help to think that my predictions somehow contribute to alleviating some of the challenges presented to programmers, that you referred to.  
	              
	              Hi Terry,
Seems we both referenced similar memory hierarchies. I can't help think that our minds were working in similar directions.
Just building on your comments about the memory sub system of a computer or the caches on CPU's not being well understood by programmers, I wonder whether my prediction, however far out of the box it may seem, could potentially remove the need for programmers to need to understand the way memory works, in that this is built into the CPU itself, that sufficient memory store is simply available and managed entirely by the hardware?&nbsp;Also, to your point on not adding more levels to the memory hierarchy, perhaps there is no need to add levels as this is simply already there and fully integrated (deeply integrated as you state in your response) with the CPU.
So, and also referring to Belinda's&nbsp;DQ response of assuming a two-level store, we could continue to have a two-level store however it becomes much more efficient and effective, whereby the primary store is non-volatile and embedded with the CPU (and adopts a size of nanometre proportions, continues to sufficiently quick, and is cost effective) and a secondary store available for the majority of the longer term and archival type storage needs, which is of course non-volatile and continues to builds on solid-state technologies.
Just some blue sky thinking... :-)
Best Wishes, Craig
	              
	              Week 4 DQ - Computer memory
 
This topic is both interesting and topical, indeed; since the invention of non-volatile memory until today, do we reach the limits in this area? What progress can be expected? What path has already been traveled?
Whether internal hard drives (HDD), external hard drive, and more recently, flash memories. Non-volatile memories experienced a turbulent history. Today, many of us can walk around with 32 or 64 GB on his keys in the form of a flash disk, in his Smartphone, etc. But it was not so long ago that we were still charged with a whole set of CD-R, 3.5-inch disks, and even, for some of us, punch cards.
We can easily notice that the evolution of non-volatile storage have practically followed the evolution of the computing power and the architectural evolution of computers. Flash memory storage, or hard disk drive or perpendicular holographic, or SSDs (Solid State Drives) currently allow storage of large amounts of data on small surfaces. And it'll continue to evolve, because the evolution of data storage devices is firstly eliminating the complexity and on the other hand allow a better and family use by users.
The potential impacts of this evolution on the recent and future computer architectures are numerous. Especially as we have data storage devices that are increasingly large in terms of storage capacity (HDD), and I/O performance (Flash Memory), and on the other hand prices are more and more low. The 3 key factors in this industry are capacity, performance and price (N. Fisher et al, 2011). Indeed; computer technology and other small mobile devices are evolving with ever more powerful performance.
with data storage devices increasingly small and powerful, we will also build computers increasingly small and powerful, the current Smartphone are great example of this, we have currently computers and mobile phones (which are real computers) more small and powerful in terms of data processing, I/O performance and data storage.
Thus users can now store any type of data; pictures, videos or music in a small device. In a more professional use, it is also used to store documents, archive files on local servers and / or remote.
The evolution of data storage devices allowed the invention of new technologies. Indeed, one could say that the rapid growth of operating systems, software, PC (laptop), Internet, cloud Computing, Smartphone, tablets would not have been possible without data storage devices (mainly HDD).
And about users experiences, I would say that the evolution of this technology follow user’s request, people want more and more small devices, reliable, with a very high capacity, high performance, and has a very low price. This evolution responds perfectly to the user’s requests and with this demand, the technology will still going to evolve further.
In conclusion, I think that in the coming years, the evolution of non-volatile memories will be a key element in

Miniaturization of computers,
The construction of computers more powerful, faster and reliable
The construction of Smartphone more powerful, faster and reliable
The construction of operating systems that will be launched more quickly and be more efficient.
More robust software and optimize
The evolution of cloud computing etc.

And regarding the storage devices themselves, I think the flash is going to completely replace traditional hard drives in future years with more capacity and performance.
Reference:
N. Fisher et al. (2011) ‘A hybrid filesystem for hard disk drives in tandem with flash memory’. Computing (2012), Vol. 94, pp.2 1-68.
R. Wood (2009) ‘Future hard disk drive systems’. Journal of Magnetism and Magnetic Materials, Vol. 321, pp. 555-561.
Wikipedia Inc. (2014) Data storage device [Online]: Wikipedia Inc. Available from: http://en.wikipedia.org/wiki/Data_storage_device (Accessed: 27 September 2014)
Lecture Notes, (2014). [Online]. Available: https://elearning.uol.ohecampus.com/bbcswebdav/institution/UKL1/201480_SEPTEMBER/MS_CKIT/CKIT_501/readings/UKL1_CKIT_501_Week04_LectureNotes.pdf (Accessed: 27 September 2014)
&nbsp;
&nbsp;
&nbsp;
 
	              
	              Hi Anthony,
&nbsp;
There is a threshold limit on the flash memory cells and when that limit is reached, an SSD will not be able to write any new data (SSDs can’t overwrite like HDDs). An SSD becomes read-only once the threshold limit is reached. The advantage here is that usually we can calculate the life remaining, before the SSD runs out of disk space, so that we are not faced with a sudden unforeseen failures [1]. But then again, another drawback is that SSDs could crash during power outages. Researchers from University of Ohio tested 15 SSDs and found that the results are not promising for durability of the device [2]. They have tested based on the six criteria and established results as follows [2] :


Bit errors: Random, incorrectly written bits of data


Flying writes: Writes that were correctly written, ended up in the wrong location


Shorn writes: Writes that were below the expected size, due to the power failure


Metadata corruption: Corruption to the Flash Translation Layer (FTL) that sits between the SSD hardware and the operating system


Bricked device: Self explanatory


Unserializability: The storage blocks that are written were not written in the proper operation order



Reference:
[1] http://www.itworld.com/answers/topic/hardware/question/ssds-longevity-and-reliability-how-do-they-compare-hdds 
[2] http://www.extremetech.com/computing/169124-the-mysteriously-disappearing-drive-are-power-outages-killing-your-ssds
Best regards,Kharavela
	              
	              &nbsp;
Non-volatile memory technologies will change the architecture of computer systems.
&nbsp;
Today the standard computer architecture is being challenged by new technologies in the non-volatile memory. Traditionally, this type of memory has used magnetic technology for mass storage systems, which have the major advantage of large capacities but with the disadvantage of being relatively slow due to mechanical motion required by the disks. Today we are assisting to the rise of Flash memory technology and Phase-change memory (PCM). These systems rely only on circuitry and electricity to store and erase data; this operating methodology is not so different from the main memory used by the CPU (Brookshear 2011). 
While Flash and PCM technologies have their limitations, it is clear that technological advances are making these types of storage devices attractive as well as increasing the size and lifetime of them. Flash technology is particularly commercially viable and will continue to develop in the next years as it has for the past decade (YunSeung 2005).
Because all the above, there have been already proposals to replace main memory with these types of technologies. Roberts mentions on his Ph.D. thesis (2011, pp 9) that combining main memory and data storage in the same device will minimize inter-device data transfer and maximize communication bandwidth. He proposes that this be replicated at the datacenter level by co-locating CPU’s with non-volatile storage, eliminating storage hierarchy (Roberts 2011). We may still be relatively far from this scenario, but its implications and future repercussions are worth considering: 

First: overall performance gain, as evidenced by Roberts’s thesis. In such architecture, because the main memory and data storage are the same entity, many file copy/move operations can be eliminated. For example, today system architecture forces us to read data from a hard disk (via the Bus) and place a copy of it in main memory. The CPU then will perform the machine cycle to manipulate the data and store the results again in main memory. Finally, we store the results back into the external disk (via the Bus). However, with a single level memory/data design we no longer need the Bus to transfer data, nor do we need to copy/write data to and from main memory. The copy operation is reduced then to a simple LOAD and STORE CPU operation. When we scale this to terabytes (or more) of data, we can clearly see that the speed increase could be considerable. This architectural change requires developments on data processing and storage techniques: file systems will need to be changed to account for a different way of manipulating data/files. Applications and operating systems will need to be adapted to support this design properly.  
Second is the miniaturization of the storage and savings into space and power, considering the amount of space and power needed today in a datacenter for storage, these savings could be substantial.

While we can see how this can impact system architecture up to a whole datacenter what about the end users? What would be the impact of such changes to the day to day, average usage? Faster processing in terms of computational capacity or data manipulation will be critical in the future. Particularly as we face an impressive amount of data that need to be processed. With faster computing and data processing, we can do more things faster, from having the whole earth topography on a single smartphone to calculating and predicting sand storms on mars in real time. From watching 3D movies to processing a species DNA genome. The possibilities are endless.
While having more storage availability is good, if only for the sake of storing data, from the research I have found I can only conclude that the most significant impact due to technological advances of non-volatile memory will be changes in the architecture of computer systems. These changes will improve computing and data processing speeds enabling us to leverage the enormous amount of data that we will store in the future. Without such advancements no matter how much data we can store, we will not be able to process is in a timely matter, and data is meaningful only if we can process it. 
References
Brookshear, J. G. (2011) 'Data Storage' in Computer Science: An Overview, 11th edn, Addison-Wesley, pp. 26-33.
Caulfield, A., Coburn, J., Mollov, T., De, A., Akel, A., He, J., Gupta, R., Swanson, S., Jagatheesan, A. &amp; Snavely, A. (2010) 'Understanding the Impact of Emerging Non-Volatile Memories on High-Performance, IO-Intensive Computing', 2010 International Conference for&nbsp;High Performance Computing, Networking, Storage and Analysis (SC), New Orleans, LA, USA, 13-19 November. IEEE, pp. 1-11. doi: 10.1109/SC.2010.56.
Roberts, D.A. (2011) Efficient Data Center Architectures Using&nbsp;Non-Volatile Memory and Reliability Techniques. PhD thesis. The University of Michigan. Available at: http://web.eecs.umich.edu/~tnm/trev_test/dissertationsPDF/daveR-ref-fixed.pdf (Accessed: 27 September 2014).
YunSeung, S. (2005) 'Non-volatile memory technologies for beyond 2010', 2005 Symposium on&nbsp;VLSI Circuits, 2005. Digest of Technical Papers, pp. 156-159. doi: 10.1109/VLSIC.2005.1469355.
&nbsp;

	              
	              DQ1 - Week 4: Impacts of the rapid increase in the capacity of non-volatile memory
Introduction

For many years, fundamentals of computer memory system have been the same, high speed volatile memory technologies like DRAM and SRAM are being used for main memory and cache. Magnetic disks for high end data storage and for mobile devices the low capacity, low energy and low speed flash memory. Today memory system is being scale up to produce non-volatile high capacity memory chips that are low power and better in speed than current volatile memory chips. However, these high capacity NV storage devices have a significant impact on system architecture, data processing and storage techniques (Katelin, n.d).

Impacts on Computer Architecture


The new NVRAM technology and rapid increase in the capacity and speed might have huge impact on architecture of computer system. If the existing two level memory system is replaced by the one level of memory it will impact input output system, how the processes are managed and the programs are initiated, virtual memory system and the protection system. The following Figure 1 will explain the possible architectural changes (B.C. Lee, 2010).



From hardware implementation viewpoint Option (C) in the figure is to be expected the most flexible option as it allows DRAM and NVRAM to have diverse performance and power characteristics and potentially provides many areas of research into the interaction of DRAM and NVRAM.

Impacts on Data Processing and Storage Techniques

Digital information is increasing enormously with evolution of computing, networking and internet technologies. According to experts by 2020, 35 zettabytes of digital information will require storage space that is reliable, quickly accessible and safe. Storage impacts the performance and capability of a computing system.

Existing data storage systems have been reviewed by experts and they have suggested techniques to use next generation NVM as RAM and for NV storage. These next generation devices will be fast, affordable and scalable. SSDs (Solid state drives) are already being used by different data centers instead of hard disks. However SSDs are still slower than RAM and expensive.

In future, more intelligent algorithms will be required, which will priorities data tasks according to user demand in NVM. 

User Experience

As a result of above impacts, user will experience significant benefits related to performance of computer. Some of these benefits are listed below


Advancements in non-volatile memory will allow users to do multitasking e.g. 
Better video streaming
Advance photo editing
3D Gaming
Video editing simultaneously. 

Down time is minimized by non-volatile memory devices, so businesses where performance is critical NVM are used. For example data centers use NVM to perform efficient data mining, searching and analytical activities.


References:

Katelin, Bailey. et al. (n.d). Operating System Implications of Fast, Cheap, Non-Volatile Memory [Online]. Washington: University of Washington. Available from https://www.usenix.org/legacy/event/hotos11/tech/final_files/Bailey.pdf (Accessed 27 September 2014).

B.C. Lee. et al. (2010). Phase-change technology and the future of main memory. Micro, IEEE, 30(1), 2010.

Adrian M Caulfield. et al. (2010). A High-performance Storage Array Architecture for Next-generation, Non-volatile Memories. MICRO, 2010.

	              
	              The potential impacts to the architecture of computer systems, data processing and storage techniques due to the rapid increase in the capacity of non-volatile memory.&nbsp;&nbsp;&nbsp; To understand the impacts that non-volatile memory can cause due to the rapid increases of capacity we must first explain what non-volatile memory is. As defined by Wikipedia (Sept 2014), non-volatile memory is “memory that can get back stored information even when not powered. Examples of non-volatile memory include read-only memory, flash memory, ferroelectric RAM (F-RAM), most types of magnetic computer storage devices (e.g. hard disks, floppy disks, and magnetic tape), optical discs, and early computer storage methods such as paper tape and punched cards.”
Recent developments with (NVMs) Non-volatile, solid-state memory is going to help revolutionize the current storage in computer systems as we currently know it. One such emerging technology is phase-change memory (PCM), another is known as spin-torque transfer memory or (STTM). These will provide far better performance, lower energy usage and increased capacity than any of the current hard drive technologies currently out there. The full impact is hard to predict but some known areas can be discussed.The effects on architecture
&nbsp;&nbsp;&nbsp; The way current non-volatile memory is slow in comparison, led architects to design systems that purposely mitigate this by introducing complex I/O schedulers to reduce the impact on I/O and to optimise any bottlenecks with the input and output. With the advancement of these new technologies the need for such I/O management and scheduling will not be needed in the same way and thus the way systems are designed now will have to change to fully feel all of the positives that this new advancement will give such as increased speed, greater capacity and lower energy consumption.Physically, machines that use this new technology will become smaller, give off less heat and require less energy to use. This will inherently mean far quicker operation thus for example current battery life for mobile devices and laptops will last longer and other applications where this new advancement could be used would also be possible.
The effects of data processing and storage&nbsp;&nbsp;&nbsp; The effect these new developments will have on data processing and storage will be allowing larger amounts of data to be stored. The storage predicted to increase from terabytes to petabytes and the increased speed to which data can be saved and retrieved by an estimated relative factor of 4 times with tested average performance gains of up to 35 times according to Caulfield, A. Et al (2010). However programmers will have to change the way operating systems and software is programmed to use this new tech so that it is more efficient and fully uses the possibilities available without inherent bottlenecks that remain from the older designed software.
There are challenges as well to overcome with regards to fragility of the non-volatile memory around data corruption being a single level store instead of dual levelled. Also data portability could be an issue because currently we have portable drives which can be swapped with ease. This new tech may not be so easily removed as it is foreseen that new memory could be combined with CPU on one chip for example or statically built onto motherboards for convenience, size and cost.
The impact on user experience&nbsp;&nbsp;&nbsp; In the whole aspect the user experience should be a positive one. The ability to have smaller devices, larger storage capacities, quicker responding technology and less energy consumption. If we simply talk about the current tech however it gets complicated. Currently SATA and SSD hard disks are increasing in capacity yet the technology behind them has not changed. If we look at those technologies then costs will go up, energy consumption would be the same overall and there would be no changes to the need for operating systems of software to handle them. So it really does depend on the view you take. The most significant impact in my mind will be the inherited changes that will be needed in the OS and in software to handle this new advancement. Technology will grow as it always has but the way we used them and design systems to get the full benefits is where the challenge will be.Conclusions&nbsp;&nbsp;&nbsp; If you look at the emerging new technologies which are predicted to be in our homes within the next 4 years such as PCM and STTM then these new non-volatile memory designs will radically change the way we look at and use computer systems. It will open up more doors to exciting and new technology development possibilities and drive down prices and energy usage while increasing storage and speed. Caulfield ET AL (2010) summarise very well when they say “We find that NVMs offer large gains in latency and band-width and can significantly accelerate database applications, reducing query execution time from days to hours in some cases”If we look at the current non-volatile memory option available right now and the progression of larger capacities on existing architectures then we will be going down an avenue which will be harder to unpick at a later date, with rising costs, the same speeds and designed architecture can only take so much before it becomes an issue which we are already seeing in larger applications and discussions around Big Data. Taciano Perez, César A. F. De Rose (Sept 2010) concluded that if we don’t embrace this new emergence of memory design we would hit a “power wall” because of the current know growth due to “Moore’s law”. To continue sustainability into the next decade we need to change the way we work with NVM’s.My own personal opinion is that I agree with the above statement, we need to really embrace these new advances sooner rather than later. A lot of change is coming and for the better, if we don’t do it now then we will only have to create more work for ourselves later, as the change is unavoidable due to the masses of information we are now storing. AbbreviationsHD – Hard DiskSSD-Solid State Disk&nbsp;&nbsp;&nbsp; SATA-Serial ATAPCM-Phase-Change MemorySTTM-Spin Torque Transfer MemoryCPU-Central Processing UnitI/O- Input OutputOS- Operating SystemNVM- Non-Volatile MemoryReferences:Brookshear, G. (2011) Computer science: An overview. 11th ed. Boston: Addison Wesley/Pearson. •&nbsp;&nbsp;&nbsp; Sections 1.1, 1.2, 1.3, 1.4, 1.8 and 1.9 in Chapter 1 •&nbsp;&nbsp;&nbsp; Sections 2.1, 2.2, 2.3 and 2.5 in Chapter 2
Caulfield, A, Et Al, (2010), 'Understanding the impact of emerging non-volatile memories on high-performance, IO-intensive computing', [http://mesl.ucsd.edu/site/pubs/Caulfield_SC10.pdf] Accessed 27 Sept 2014.
Dong, Li. Et Al (May 2012), 'Identifying Opportunities for Byte-Addressable Non-Volatile Memory in Extreme-Scale Scientific Applications’,&nbsp; [http://ft.ornl.gov/~dol/papers/ipdps12_app_nvram.pdf] Accessed 27 Sept 2014.
Katelin Bailey, Et Al, (May 2011) 'Operating System Implications of Fast, Cheap, Non-Volatile Memory', [https://www.usenix.org/legacy/event/hotos11/tech/final_files/Bailey.pdf] Accessed 27 Sept 2014.
Laureate Education, (2014) Machine Architecture: Week 4 Lecture Notes [Video, Online], (accessed: 26/09/14)
Taciano Perez, César A. F. De Rose (Sept 2010), ‘Non-Volatile Memory: Emerging Technologies And Their Impacts on Memory Systems’ [http://www3.pucrs.br/pucrs/files/uni/poa/facin/pos/relatoriostec/tr060.pdf] Accessed Sept 2014.&nbsp;
Wikipedia (Sept 2014), Non-volatile memory: [http://en.wikipedia.org/wiki/Non-volatile_memory] Accessed 27 Sept 2014.
	              
	              Hi Craig,

It was weird seeing you describe that because that was exactly the way I was thinking as well. I feel future trends will move to combined single layer CPU and non-volatile memory such as the new developments in PCM especially for the initial heirarchy level to eliminate the transfer between levels currently and as you said have SSD still available for backups, and other less intensely needed storage. This arrangement would also help where these new NVM's are slightly concerned with their fragility. During a PC low latency/usage idle or scheduled durations i'm sure a duplicate backup verification layer could be introduced. Seen as we will have masses of new space i'm sure this could be incorporated which could help with data verification as well.
Great read guys
&nbsp;
	              
	              Moving forward with Non-volatile memory (NVM)&nbsp;
Prices per Terabyte for Non-volatile memory (NVM) are dropping. Developments seem to follow Moores law as computational power (consisting of speed x density x storage capacity) &nbsp;that can be bought for a thousand dollar has doubled every two years (De Salvo interpreting Kurzweil, 2012). This forecasts an end for volatile memory at the point when prices per Terabyte get in the same range. At least for the generic appliances as the use in mobile devices. &nbsp;&nbsp;
The advances made in NFV have a great impact on the architecture of computer systems. Due to their limited size they can be integrated in smaller form factor devices. Unlike volatile memory they do not have rotating parts which makes them silent and relatively cool in temperature. The absence of moving parts makes them also shock resistant improving the applicability in mobile devices. And last but not least they excel in data access times resulting in faster processing power (Brookshear, 2012).
There are limitations to the current NVM techniques used. Physical boundaries are reached when lithographic production processes have to reach a micro-level beyond the 20nm. Even if this is possible production-wise it is expected that electrons will start behaving differently and cannot be stored in their silicone cells anymore. Alternative techniques are investigated and could lead to the a real universal memory that consumes less power while being highly resilient and generate fast date access (De Salvo, 2012).
The upcoming 10 years though the progress of NVM will be guaranteed and mobile devices will have much more solid state memory on board. If we combine this trend with that of the cloud this could lead to new architectural appliances. For example continues connection with the cloud, exchanging small packages, takes a big toll on the battery capacity the device uses. Short and small data transfers have also a negative impact on cellular networks that are typically bothered with high link latencies. By using the vast amounts of storage on mobile devices one could push more data in one go, therefore not further jamming the link latency bottle neck. &nbsp;
The idea is to extend the cloud functionality to hand held device, resulting in fast responsive applications as user experience that do not stop functioning when connection is lost for a moment. These applications can be maintained centrally by the content provider as he uses generic capabilities of the device using the remote cloud access (Koukoumidis, Lymberopoulos, Strauss, Liu and Burger, 2012). &nbsp;&nbsp;
Brookshear, G. (2012), Computer Science: An Overview, Perasoon Education Inc. publishing as Addison-Wesley, Boston, United States, Version 11, pp 33.&nbsp;
De Salvo, B (2009) Silicon&nbsp;non-volatile&nbsp;memories: paths of innovation. Iste [Online], Available from https://www.dawsonera.com/abstract/9780470610398 (Accessed: 28 September 2014).&nbsp;
Koukoumidis, Lymberopoulos, Strauss, Liu and Burger (2012) 'Pocket Cloudlets', ACM Sigplan Notices, 47(4), April, pp. 171-183, &nbsp;ACM Digital Library [Online]. Available from: http://10.1145/1961295.1950387 (Accessed: 28 September 2014).

	              
	              Hi Anthony,
I was in the process of researching the issues with SSD’s, specifically looking to provide some insight to the reliability concerns relating to the write-erase cycles, however I came across an article which provides a case for reliability to not be considered such an issue, when comparing with the lifespan of HDD’s.
The article by Graefe (2008) suggests that current NAND flash (most SSD’s use NAND flash technology) reliability suffers somewhere between 100,000 to 1,000,000 erase-write cycles and that the assumption is that there is some mechanism for “wear-leveling”, ensuring that the pages or blocks are written with the about the same frequency. Furthermore, it is then suggested that traditional HDD’s do not necessarily support more write operations, for example taking 6 years of continuous and sustained writing at 100MB per second overwrites an entire 250GB disk fewer than 80,000 times. Similarly, a 32GB flash disk 100,000 times at 30MB per second takes about 3½ years.
In addition, according to Baxter (n.d.) the MTBF for SSD is 2 million hours, compared to 1½ million hours for HDD.
Therefore, is the reliability comparable between SSD and HDD, and should it really be considered an issue as such?
Best Wishes, Craig
References
Graefe, G. (2008). The Five Minute Rule Twenty Years Later and how Flash Memory changes the rules pp.44 [Online]. Available from: http://delivery.acm.org/10.1145/1420000/1413264/p40-graefe.pdf?ip=82.40.0.78&amp;id=1413264&amp;acc=OPEN&amp;key=4D4702B0C3E38B35%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35%2E6D218144511F3437&amp;CFID=576867361&amp;CFTOKEN=24865389&amp;__acm__=1411944142_b66df51ac78e9f5432ec029b6b893786 or http://queue.acm.org/detail.cfm?id=1413264  (Accessed 28 September 2014)
Baxter, A. ed. (2014). SSD v HDD [Online]. StorageReview.com. Available from: http://www.storagereview.com/ssd_vs_hdd (Accessed 28 September 2014)
	              
	              Hello Terry,
Following your discussion with Craig and also Belinda's position with interest. &nbsp;
Correct me if I am wrong but your assumption is that Universal Memory can be used for all memory layers from cache, RAM and Storage memory and this would reduce complexity as programmers only have to differentiate between primary and secondary level. 
&nbsp;Is the cache not a layer outside the reach of programmers and a closed pre-defined environment of the CPU (forgive my ignorance if this is not the case)? I thought the cache is totally controlled by the CPU as a sort of buffer. Of course programmes should understand its functioning as commanded processes should not create loops or other conflicts in this domain. I just expect more problems if this should be defined by programmes when the cache is not pre-allocated for the CPU.&nbsp;
In lesser extend this also exists for the RAM as this is the 'buffer' for the Operating System. I think my worry is systems are inherently complex and become more complex over time. Optimising the CPU - Cache exchange demand very skilled low level programmers and this domain should perhaps not be opened for the &nbsp;application layer programming.&nbsp;
Reading the paper I think you refer to of Perez &amp; Rose (from http://www3.pucrs.br/pucrs/files/uni/poa/facin/pos/relatoriostec/tr060.pdf) they seem to point out that Universal Memory is still an abstract idea as there are no candidates who fulfil all needs. Furthermore as the CPU capacity progresses this will reflect on the requirements of the caches. Even though they also think the overall performance of the memory hierarchy can be improved, one can wonder if the Universal Memory is technical and economical a viable candidate for all layers.
Greetings from Bram

	              
	              Hi Craig,
Remeber the T-1000 from Terminator 2? The robot man compose by liquid alloy?
With small but smart particles that having it's own CPU &amp; memory we can get very close to something like that. Because each small unit can make its own decision, and the the interdependency between the units are minimize.
Another similarity to T-1000 is, it sleeps when no engery, but wake up and contiune to run when energy comes back (thanks to NVM). Just like T-1000's liquid alloy start to flow in the heat.
br, Terry
	              
	               Non-Nolatile Memory usages and applications     &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  &nbsp;  [i]Non-volatile memory is the type of computer memory used to store data without power, in other words the NVM does not need to connect to a power source to access the data stored in the device. In an event the computer is interrupted or shut down in mid-usage, data stored will not be lost.  &nbsp;  Non-volatile data storage can be classified into two types[ii]:    Mechanically accessed   Electrically accessed Mechanically accessed devices utilize a contact structure to write and read the device medium. The data stored is much larger than electrically accessed devices. Examples of mechanically accessed systems are optical disks, hard disks and holographic memory.     Electrically accessed devices are based on the write mechanism. These devices are getting more affordable with technology advances. A few examples of electrically accessed devices are flash memory, FRAM and MRAM.     Some examples of NVM include:      All types of read-only memory (ROM)   Flash memory, also known as Flash Drives (or Thumb Drives)   Hard disks   Optical &nbsp;The biggest advantages to NVM storage devices are convenience and ease of storage, while costs and the potential for data to be erased or written over can be drawback. Ever changing technology changes can also make certain NVM devices or ways of accessing data obsolete, though in most cases the core information still exists on the devices but getting access to the data might be a challenge. [iii]Non-volatile, secondary memory advantages are generally less costly to produce than volatile memory devices and data can be stored for a long time. In many cases these devices enable computer hardware to easily be upgradeable. &nbsp; Computers typically have different ways of storing data and known as memory. Non-volatile memory one method which is permanent, generally the data is a second copy, which means the data exists somewhere else. This is not usually intended as a backup but could potentially be used as a backup. Flash memory is the most popular used non-volatile memory device nowadays. In order to keep up with the demand for higher memory capacities and smaller devices. &nbsp;Manufactures have been working to reduce production cost and increase performance. Charge trapping memory (PCM/PRAM See below) is regarded as one of the most promising flash memory technologies as further down-scaling continues. In addition, more and more exploration is investigated with high-k dielectrics implemented in the charge trapping memory. The utility of non-volatile devices when it comes to long-term storage may also mean that they might not work as well as those that handle immediate memory, at least when it comes to delivering quick and consistent results. Volatile devices (such as RAM see below) are usually better suited to memory that needs to be read and written over. It is generally more expensive than non-volatile memory, but is also much faster. Random-access memory (RAM), is volatile memory in a computer which is used real time. If the computer loses power or turned off, this information disappears.   Typical ROM Read-Only Memory (ROM), is one of the most popular types of non-volatile storage. It stores information permanently and can’t be erased from the chip. There are a number of other non-volatile types of memory in the same family as "ROM” that can actually be rewritten. These are known as Erasable Programmable ROM (EPROM), another type is the Electrically Erasable Programmable ROM (EEPROM), and flash memory, also sometimes called flash ROM. Most EPROM are erased using Ultra Violet light, which erases all data on the chip, while data on an EEPROM chip can be erased and rewritten as required. Flash memory originated from and quite similar to, EEPROM ROM based   Mask ROM Chip      Mask ROM   [iv]Mask ROM (MROM) is a type of read-only memory (ROM) in which contents are programmed by the integrated circuit manufacturer, as appose to the user.         PROM Chip    PROM   [v]A type of ROM (read-only memory) where data is permanent and cannot be changed. PROMs are typically used to store &nbsp;permanent data, and utilized &nbsp;low level programs such as firmware (machine level code)      &nbsp;     EPROM Chip    EPROM   [vi]An EPROM, or erasable programmable read only memory, that &nbsp;retains its data when its power supply is switched off         EEPROM Chip    EEPROM   [vii]EEPROM (Electrically Erasable Programmable Read-Only Memory), much like EPROM but generally used in &nbsp;devices to store small amounts of data that must be saved when power is removed      NVRAM  Currently available    [viii]Flash memory is an electronic non-volatile computer device that can be electrically erased and reprogrammed. Two typical types Flash NAND/NOR named after NAND and NOR gates. The NAND type is generally used in main memory, memory cards, USB flash drives, solid-state drives... The NOR type, allows true random access thus best used in direct code execution. Also used as a replacement for the older EPROM/ROM       NVSRAM Chip    NVSRAM   [ix]nvSRAM is a type of non-volatile random-access memory (NVRAM). It is similar to static random-access memory (SRAM). It is battery-backed so power is not lost.      &nbsp;  &nbsp;  &nbsp;  &nbsp;&nbsp; Emerging Technologies     FeRAM technology    FeRAM   [x]Ferroelectric RAM (FeRAM, F-RAM or FRAM) is a random-access memory similar to &nbsp;DRAM, it utilizes a dielectric layer to achieve non-volatility. FeRAM is a growing alternative non-volatile random-access memory technologies that offer the same functionality as flash memory. FeRAM advantages over flash include: lower power usage, faster write and a much greater maximum number of write-erase cycles. &nbsp;Disadvantages of FeRAM are much lower storage densities than flash devices, storage capacity limitations, and higher cost      &nbsp;   &nbsp;   PRAM Architecture    PCM/PRAM   Phase-change memory another non-volatile random-access memory. Utilizes unique behavior of chalcogenide glass. .         MRAM    MRAM   [xi]Magnetoresistive random-access memory (MRAM) is a non-volatile random-access memory technology been in development since the 1990s. Its continued advances will make it most &nbsp;&nbsp;dominant for all types of memory in the industry know as &nbsp;universal memory      &nbsp;     TYPICAL HDD  HDD (Hard Disk Drives) have had a number developments from speed (I/O) and cost over the years. There primary architectures are outlined below:  &nbsp;     SATA  SATA - One of the fastest data interface in the HDD devices, they are efficient with respect to power consumption     IDE  IDE (Integrated Drive Electronics)/PATA (Parallel Advanced Technology Attachment) /EIDE (Enhanced Integrated Drive Electronics) - Older HDD technologies, slowest in the HDD devices. These devices are being replaced by the SATA devices  &nbsp;     SCSI  SCSI (Small Computer System Interface) - Similar to IDE hard drives.  &nbsp;   &nbsp; &nbsp;   Examples of SDD  SSD (Solid State Drives)- These devices do not have any moving components. They have a fast I/O and have a low likelihood of break down compared to other drives.  Optical Technology     [xii]CDs and DVDs are also a form of non-volatile memory, Data stored on the surface of the disks. Optical technology is utilized as opposed to magnetic based access. This gives them an advantage over other media where data loss could occur in close proximity access to magnets. Optical disks, do have their own challenges such as scratching.Non-Volatile memory technologies such as CDs and DVDs, are expected to be accessed through the near future however SSD technologies are becoming faster, cheaper and easier accessible and will most likely be the preferred NVM of the future.  References            [i] URL http://www.techopedia.com/definition/2793/non-volatile-memory-nvm         [ii]URL Posted by Cory Janssen &nbsp;http://www.techopedia.com/definition/2793/non-volatile-memory-nvm        [iii]URL http://eds.b.ebscohost.com.ezproxy.liv.ac.uk/eds/detail/detail?vid=2&amp;sid=24f54311-e3ff-4d9b-8e09-5a732a55b67e%40sessionmgr115&amp;hid=115&amp;bdata=JnNpdGU9ZWRzLWxpdmUmc2NvcGU9c2l0ZQ%3d%3d#db=a9h&amp;AN=97228769&amp;anchor=GoToAllQVI        [iv] URL http://en.wikipedia.org/wiki/Mask_ROM         [v] URL http://en.wikipedia.org/wiki/Programmable_read-only_memory         [vi] URL http://en.wikipedia.org/wiki/EPROM         [vii] URL http://en.wikipedia.org/wiki/EEPROM         [viii] URL http://en.wikipedia.org/wiki/Flash_memory#NAND_memories         [ix] URL http://en.wikipedia.org/wiki/NvSRAM         [x] UR http://en.wikipedia.org/wiki/Ferroelectric_RAM         [xi] URL http://en.wikipedia.org/wiki/Magnetoresistive_random-access_memory         [xii]URL http://www.wisegeek.com/what-is-non-volatile-memory.htm    &nbsp;     
	              
	              Hi Terry,
Yes, that is one of the things I do remember!
We just need those super smart particle physicists to get to it!
BTW, I've heard that Anthony is setting next weeks hand-in assignment on the science of indirect abstraction ;-)
Best Wishes, Craig



	              
	              Week 4 Discussion Question – Computer Memory

For almost 30 years, computer memory systems have been essentially the same: volatile, high speed memory technologies like SRAM and DRAM used for cache and main memory; magnetic disks for high-end data storage; and persistent, low speed flash memory for storage with low capacity/low energy consumption requirements such as embedded/mobile devices. [1]

Currently, non-volatile memory (NVM) technologies are emerging and may substantially change the landscape of memory systems. Non-volatile memory (NVM) technologies such as phase-change RAM (PCRAM), magnetic RAM (MRAM) and Memristor promise to enable memory chips that are non-volatile, require low energy and have density and latency closer to current DRAM chips. The creation of byte-addressable, non-volatile solid state memory could make a significant amount of persistent main memory available to computer systems, allowing for consolidating these two different levels of the storage hierarchy – main memory and secondary storage – into a single level. [2]

The painfully slow performance of non-volatile storage has been an unfortunate reality for system designers for several decades. Systems designers have gone to great lengths to try to mitigate this poor performance: Operating systems employ complex schedulers for IO, and most of the complexity in database management systems is in buffer management and query optimizations designed, in large part, to minimize IO

Slow disks have also had a large impact on how we build supercomputers. Many large-scale scientific applications benefit as much (or more) from the terabytes of DRAM that high-end systems provide as they do from the number of FLOPS. Using DRAM to provide support for large working sets has been the only practical solution, but it is expensive and energy-intensive. [3]


Solid-state non-volatile memories will potentially find use in many different types of applications, and their impact will vary depending on how systems use them. There are at least three large categories of applications that may benefit significantly:

1) Raw device and file access in these applications, NVMs replace disks as the primary storage medium. Applications access the data via normal file operations (open (), close (), read (), write () etc.) or by accessing the raw block device directly. [3]

2) Database applications Databases are playing a growing role in many scientific applications. They provide sophisticated buffer management systems meant to hide the latency of slow disks. Buffer management and file system efficiency both impact performance. [3]

3) Paging using non-volatile storage to virtualize DRAM can increase effective memory capacity. The impact of paging on application performance is potentially quite large, especially for hard drive-based paging systems. Solid-state storage technologies, however, may be fast enough to make paging a useful alternative to increasing DRAM capacity in high-performance systems. [3]

The hybrid nature of these NVM technologies such as phase-change memory (PCM) makes it difficult to use them to best advantage in the memory-storage hierarchy. These NVMs lack the fast write latency required of DRAM and are thus not suitable as DRAM equivalent on the memory bus, yet their low latency even in random access patterns is not easily exploited over an I/O bus.

It is noted that NVMs offer large gains in latency and band-width and can significantly accelerate database applications, reducing query execution time from days to hours in some cases. Their usefulness as backing store for paged virtual memory varies between applications depending on paging frequency. I’m complied to agree whit the conclusion of Perez, T, Calazans, N, &amp; De Rose, C in their report by Perez and Rosa having researched a great deal of material.

However, in spite of these positive impacts of NVMs, there are a few negative predictions to consider,

Things you shouldn’t do with SSD


Don’t use old Operating System i.e.(Windows XP, Windows Vista): SSD is a modern tech and should always run a modern Operating System.
Don’t store large, infrequently accessed files: they are smaller and more expensive per-gigabyte than HDD, hence, works with reduced power consumption, less noise but increased speed
Don’t write constantly to them.
Don’t fill them to capacity: always leave a sensible amount of space else it will slow performance down dramatically.
Don’t defragment: The storage sector on an SSD has a limited number of writes. Defragmentation will result to many more writes as your defragmenter moves files around.
Don’t wipe


&nbsp;

References

[1] Perez, T, Calazans, N, &amp; De Rose, C (2012). A preliminary study on system-level impact of persistent main memory, Thirteenth International Symposium on Quality Electronic Design (ISQED) 12 pp. 84-90 [online], Available from: http://ieeexplore.ieee.org.ezproxy.liv.ac.uk/stamp/stamp.jsp?tp=&amp;arnumber=6187478 (viewed 27 September 2014)

[2] Perez, T. (2013). System-level impacts of persistent main memory using a search engine. Microelectronics Journal 14 pp. 211-216 [online]. Available from: http://eds.b.ebscohost.com.ezproxy.liv.ac.uk/eds/command/detail?sid=d7438b7b-fa36-4880-bd4f-742184f6b249%40sessionmgr115&amp;vid=1&amp;hid=110 (Viewed 27 September 2014)

[3] Adam, M, Joel C, Todor, Arup D, Ameen A, Jiahua, A. (2010). Understanding the Impact of Emerging Non-Volatile Memories on High-Performance,IO-Intensive Computing [online]. http://cseweb.ucsd.edu/~swanson/papers/SC2010HASTE.pdf (Viewed 27 September 2014)

http://www.howtogeek.com/165472/6-things-you-shouldnt-do-with-solid-state-drives/ (Viewed 28/09/14)
	              
	              Hi Craig,
Programmers will not need to understand a memory layer only when that layer doesn't exist. The encapsulation and abstraction will make their life easier in the normal situations, but they will still now and again face the ugly detail. The idea "managed entirely by the hardware" in real implementation often becomes "managed by the hardware most of the time". So, to clean up all the ugly detail is probably a better solution then hiding it somewhere.
Your idea about having the secondary store only for longer term and archival type storage isn't blue sky at all, because that's exactly how data storage works in Twitter and Facebook. How often do you browse the content of a month ago on your homepage on twitter? Rarely, right? So, their database is for writing only, and they "almost never" read from it, because it's optimized for writing and reading is too slow. Reading is mostly from the cache.
br, Terry
Reference
http://www.infoq.com/news/2009/06/Twitter-Architecture
	              
	              Hi Anothny
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; SDD devices are faster (they do not have moving disk arm and require to locate the data on disk sector, enabling low read latency), don't get as hot as typical Hard Disk (as they don't have moving parts in them), they are lighter (not as much Hardware required with SDD). With less mechanical components the SDD will be more reliable, less noisy&nbsp;and less likely to require failure in part, not to mention accidental breakage benefits on SDD over HDD. An excellent use of SDD is in Laptops. With SDD windows OS can boot up many times faster using the FAST BOOT feature and make the laptop very thin and light.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; With SDD technology the probably of read or write errors happen when system is WRITING to the SDD. With HDD technology the probably of read and Write error will likely happen when system is READING from the HDD. The advantages of HDD over SDD is lower cost of HDD and much larger capacity capability in NDD over SDD.
robin

	              
	              Introduction
&nbsp;
Non-volatile memory (NVM) is a type of device which is able to maintain saved data without power supplying. NVM can be divided into two main categories which are mechanically addressed systems and electrically addressed systems. The mechanically addressed systems primarily indicate secondary storage, such as hard disks; holographic memory; optical disc and so on. While electrically addressed systems mainly comprise Flash memory, random-access memory, etc. One of the latest NVM technologies is RRAM (Resistive random-access memory).
&nbsp;
Non-volatile memory
&nbsp;
In recent years, The memory hierarchy adopted during the computer design has been virtually unchanged that high-speed and volatile memory technologies such as DRAM and SRAM are applied for cache or main memory; magnetic disks (hard disks and removable diskettes) are applied for high-end data storage; as well as low speed crash and persistent memory are applied for storage with low energy consumption or low-capacity demands, like mobile and embedded devices. Throughout the development of new memory technologies, NVM technologies like MRAM (Magnetic RAM), PCRAM (Phase-Change RAM), RRAM (Resistive RAM) and FRAM (Ferroelectric RAM) will probably make memory chips which are low-energy demand, non-volatile as well as high-density and latency similar to present DRAM (Dynamic random-access memory) chips. Although many of those technologies are also in preliminary phases, which prospective attributes in terms of energy, density, endurance and latency are already appraised. According to those evaluations, certain articles have been posted putting forward concrete applications for those new NVM technologies. Those articles supply major view on what the influences on memory systems should be supposing such technologies advance to be competitive during the market.
&nbsp;
The impacts
&nbsp;
The perfect memory technology would be low-cost, non-volatile, energy-efficient, high-density, high-speed and high-endurance. This ideal technology would be a universal memory without the necessary for intricate hierarchies during the memory subsystem. However, the currently presented technologies have no all the essential properties to achieve that. For examples, RRAM has extremely finite endurance. STT-MRAM is unable to realize high-densities. PCRAM is not durable enough (low-endurance) and is not fast enough (low-speed). &nbsp;There will not have the single universal memory unless the technical obstacles are resolved.&nbsp; Supposing RRAM is able to be made to possess high-endurance, and then it will be advantages enough to supplant both STT-MRAM and PCRAM for all memory hierarchies that comprise the cache, the main memory as well as the hard disks, and being a really universal memory. However, it is still a difficulty and there is no evident direction evincing in case this will come true and when. In fact, the application of those new technologies is going to possibly lead to more intricate hierarchies rather than more uncomplicated ones. While MRAM is capable of bettering on-chip and off-chip caches, it will claim perhaps hybrid designs as well as more intricate cache management algorithms. PCRAM is capable of adopted to produce a high-density, low-power main memory, but it will possibly require hybrid designs which relate DRAM buyers. RRAM is able to be applied for write-a-little read-a-lot devices (e.g. FPGAs) and else as main memory in hybrid designs relating DRAM buyers. All these three types of memories are able to potentially be adopted as constant storage supposing their price reduce as much as possible, that could arise supposing they turn into diffusely utilized and manufactured in broad-scale. Those new technologies are able to enable the structure of memory chips which are low-energy, non-volatile, high-density and with latency near or better than present DRAM chips. This can be sufficient to avert hitting a power-wall concerned with the present limitations of DRAM. In case their price turns into low as much as possible, they are also capable of supplanting crash memory and hard disks as constant storage, reducing the latency disparity between main memory and constant storage. Concisely, in spite of their lacking strength to fulfill all Universal Memory demands, those new technologies are expected to better present memory hierarchies as well as enable memory systems to last scaling up.
&nbsp;
Conclusion 
&nbsp;
Due to NVM has very low static power consumption, energy-proportionality should be reached for the memory and/or storage subsystem supposing it is applied in caches, main memory and constant storage. The application of each of these NVM technologies will come into being important and evident variations to memory systems. Certain of those variations will be explicit to operating systems and software applications, but some will possibly not. 
&nbsp;
References
&nbsp;
Akerman, Johan (2005); Toward a universal memory; Available on: (http://www.highbeam.com/doc/1G1-132192934.html) (Accessed on: 28-Sep-2014)
Bruce Jacob, Spencer W. Ng &amp; David T. Wang (2007); Memory Systems: Cache, DRAM, Disk; Available on: (http://www.e-reading.me/bookreader.php/138837/Jacob,_Ng,_Wang_-_Memory_systems._Cache,_DRAM,_Disk.pdf) (Accessed on: 28-Sep-2014)
David Andrew Roberts (2011); Efficient Data Center Architectures Using Non-Volatile Memory and Reliability Techniques; Available on: (http://web.eecs.umich.edu/~tnm/trev_test/dissertationsPDF/daveR-ref-fixed.pdf) &nbsp;(Accessed on: 28-sep-2014)
G. W. Burr , B. N. Kurdi , J. C. Scott , C. H. Lam , K. Gopalakrishnan , R. S. Shenoy (2008); Overview of candidate device technologies for storage-class memory; Available on: (http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.160.4525&amp;rep=rep1&amp;type=pdf) (Accessed on: 28-Sep-2014)
&nbsp;
Mark H. Kryder and Chang Soo Kim (2009); After Hard Drives—What Comes Next? Available on: (http://web.iaincirebon.ac.id/ebook/luke/ieeeexplore/Magnetics_IEEE_Transactions_o/After_Hard_Drivesamp%3B_What_Comes_Next_-b4B.pdf) (Accessed on: 28-Sep-2014)
&nbsp;
Taciano Perez, César A. F. De Rose (2010); Non-Volatile Memory: Emerging Technologies And Their Impacts on Memory Systems; Available on: (http://www3.pucrs.br/pucrs/files/uni/poa/facin/pos/relatoriostec/tr060.pdf) (Accessed on: 28-Sep-2014)
&nbsp;

	              
	              Impacts of the rapid increase in the capacity of non-volatile memory
Preamble:
“The transition to flash memory for performance enterprise applications will have enormous consequences for customers as well as system designers” – Forbes (Nov 2013) http://www.forbes.com/sites/tomcoughlin/2013/11/19/changing-roles-of-enterprise-flash-memory-and-hdds/&nbsp; &nbsp;
Discussion
The above opening statement is a reflection of the current drive towards improvement in memory technology driven by greater demand in a large variety of computing resources. The slow performance of non-volatile storage and slow disks has had an impact on how systems have been designed. The use of Non-volatile, solid-state storage technologies alleviates these limitations and lead to higher performance, improved efficiency, smaller footprint and lower power consumption of devices.

Potential Impact
The world has undergone tremendous transformation in the concept of computing, from the early days of the mainframe to the current computing trend of “Internet of things”. This rapid growth and development has resulted in major shift in the architecture of computer systems as a multiplicity of functions become possible. Many more devices are continually being connected to and applied on the internet. Having achieved a level of miniaturization that makes it possible to pack computing power in extremely small devices, the increase in capacity of non-volatile memory would result in smaller and faster devices being built with huge performance improvement relative to the disk.
Non-Volatile RAM may also potentially bring significant power savings in future exascale systems, drastically reduce latency and be a viable alternative to &nbsp;support memory paging in high performance computing systems (Identifying Opportunities for Byte-Addressable Non-Volatile Memory in Extreme-Scale Scientific Applications – Caulfield, M; Coburn, J; et al). Such exascale systems typically require huge data processing and storage capabilities as depicted in the graphic above.
&nbsp;User experience, as a result of this would be greatly enhanced as the computer becomes ubiquitous in day to day life and operations. The growth in the number of channels that people can use to access all types of content will continue to drive demand for digital storage as more and more content is generated. &nbsp;&nbsp;
&nbsp;Conclusion
The rapid increase in the capacity of non-volatile memory is resulting in application of new architectures with huge performance improvements from the traditional disk based systems providing users with access to greater technological applications.
References

Coughlin, T. (2004), Digital Storage in Consumer Electronics: The Essential Guide (Embedded Technology), 1st Edition, Newnes; &nbsp;
Coughlin, T. and Grochowski, E. (2014). &nbsp;2014 Emerging NV Memory &amp; Storage Technologies and Manufacturing Report , (Peer reviewed article) Available at

http://www.tomcoughlin.com/Techpapers/Nonvolatile%20Memory%20Report%20Brochure,%20040914.pdf. (Accessed 28/09/2014)

Caulfield, A. Coburn, J.&nbsp; Mollov, T.&nbsp; Arup De.&nbsp; Akel,A. &nbsp;Jiahua H. Jagatheesan, A. Gupta, R. Snavely, A. Swanson, S. Understanding the Impact of Emerging Non-Volatile Memories on High-Performance, IO-Intensive Computing , Department of Computer Science and Engineering, University of California, San Diego.` (Accessed 28/09/2014)

&nbsp;Available at http://mesl.ucsd.edu/site/pubs/Caulfield_SC10.pdf . (Accessed 28/09/2014)

Brennan B, (2013) New directions in memory architecture, &nbsp;&nbsp;&nbsp;– Available at 

http://www.cs.utah.edu/events/thememoryforum/brennan.pdf, &nbsp;Memory Solutions Lab, (Accessed 28/09/2014)

Data centres: The future is both consolidated and dispersed.

&nbsp;Available at -&nbsp; http://www.forbes.com/sites/tomcoughlin/2014/09/05/data-centers-the-future-is-both-consolidated-and-dispersed/ &nbsp;(Accessed Sep 2014)

Coughlin,T. (2013), Flash Memory Is Changing How Companies Buy Storage, Forbes 11/19/2013, &nbsp;Available at -

http://www.forbes.com/sites/tomcoughlin/2013/11/19/changing-roles-of-enterprise-flash-memory-and-hdds/. (27/09/2014)

New DRAM and flash memory architectures are needed – Available at

http://www.cadence.com/Community/blogs/ii/archive/2013/08/08/memcon-samsung-keynote-new-dram-and-flash-memory-architectures-are-needed.aspx. (27/09/2014)

	              
	              Hi Bo.
In the area of database design, I have come across the application of an "In-Memory" database that has been applied by SAP who are one of the large entreprise vendors, to improve performance of their SAP system.&nbsp;SAP HANA has one copy of the database that runs in memory and&nbsp;combines database, data processing, and application platform capabilities in-memory. This I guess is one of the new architectures designed to work around the slow disk based operations. The application would therefore run much faster given the huge volumes of data that these entreprise software manage.


Reference
- SAP, About HANA (2014), Available at: http://www.saphana.com/community/about-hana (Accessed 29/09/2014)
Joseph
	              
	              Hi Craig, On the battery life issue, I have read about an emerging technology in&nbsp;silicon-anode batteries which are expected to ship in smartphones in the coming years as a replacement for the current&nbsp;lithium-ion batteries. These offer much greater power capacity, increased battery life, are able to charge more quickly and produce 30-40% more electricity. In the area of computing power on the cloud, I would say that the computing power is already there going by the capabilities that data centres have to offer including scale up of required resources from cloud providers. A computing device that accesses the cloud would still need to perform basic I/O operations and due to miniaturization, still ends up packing a lot of power in that little device in order to harness the efficiencies of speed and performance. The idea of a very "thin client" though is most ideal. Regards Joseph Reference Afeyan, N. 2014, Top 10 emerging technologies for 2014 (Accessed 29/09/2014) Avaliable at&nbsp;http://forumblog.org/2014/09/top-ten-emerging-technologies-2014/#nanowire (29/09/2014)
	              
	              Hi Bram,
As I replied to Bo, I like the universal memory idea, but I don't think that's desired in all the situation and possible to achieve as a general-purposed solution. Instead I would like to see a simplified memory hierarchy.
Regarding your statement about cache, I think it's yes and no. Yes, cache is encapsulated in the CPU and the optimization of cache using is mostly done by compilers automatically. But the behavoir of cache is still quite often visiable from outside to the programmers and users. One example is the inconsistent performance of memory in different situation, because in the end of the day we don't really have all the memory so fast as the cache. So, no, programmers still need to be aware of it.
I agree revealing these "ugly details" to the programmers will make the system more complicated over time. But I think a better solution than hiding them is to remove them. Because we usually cannot hide it that well. That's why I prefer a simplified memory hierarchy.
BTW, cache and buffer are two different concepts:&nbsp;http://superuser.com/questions/433948/what-is-the-difference-between-a-cache-and-a-buffer
br, Terry
ps. I know a good joke about optimizing compiler: anybody use gcc here? The gcc option -O will set the optimization level of the compiling. "-O0" means no optimization, and "-O2" is the default optimization. Do you know what "-O5" will do?
	              
	              Hi Craig,
Yes. Just like to make the laptops and phones thiner, eventually the challenge is not from computer sicence but from the physical materials.
How do you know the next week's assignment? Or you mean virtualization?
br, Terry
	              
	              Hi King,
When I think about he perfect memory technology you listed, "low-cost, non-volatile, energy-efficient, high-density, high-speed and high-endurance". It seems they are all desirable in all the situations, probably except the volatility.
Is there a situation that volatile as a feature is desired?
br, Terry
	              
	              Hi Anthony.
Solid-State-Disks (SSD) are generally more reliable and have higher level of endurance than normal hard-disks, because they do not have moving parts and as such are not vulnerable to mechanical failure and crashes. Though Hard-disks though have been around much longer and are much more tested, SSDs have become much more popular and significantly also because of the reduction in their overall price.
A study done in&nbsp;December of 2010 by Hardware.fr a French company, released SSD failure rate data gleaned from it’s parent company LDLC, one of the top French tech retailers. From the data, it appears that the return rate for SSD were much lower than those for typical hard drives. Although due to their mechanical nature, handling and shipping challenges could contibute towards damage in the hard drives. (http://www.tomshardware.com/reviews/ssd-reliability-failure-rate,2923-3.html)
With regards to advise against their use, flash memory technology is not suitable for general main memory applications where its contents might be altered many times a second (Brookshear 33)&nbsp;. However the more likely instances where I would advise against their use is where the deployment costs are high and this is especially so for servers. (http://www.techradar.com/news/world-of-tech/roundup/solid-state-drives-in-a-server-1131483)

Joseph

Reference
1) Brookshear, J. G. Computer Science: An Overview XML Vital Source ebook for Laureate Education, 11th Edition. Pearson Learning Solutions. VitalBook file.
2)&nbsp;Toms Hardware (2011) &nbsp;‘Investigation: Is Your SSD More Reliable Than A Hard Drive?’, Andrew KuJULY 28, 2011&nbsp;.&nbsp;Available at: http://www.tomshardware.com/reviews/ssd-reliability-failure-rate,2923-3.html&nbsp;[Accessed 29/09/2014].&nbsp;
3) Computer World (2012), 'Hard disk drives vs. solid-state drives: Are SSDs finally worth the money?',&nbsp;Lucas Mearian Sep 17, 2012 ,&nbsp;Available at&nbsp;http://www.computerworld.com/article/2492024/solid-state-drives/hard-disk-drives-vs-solid-state-drives-are-ssds-finally-worth-the-money.html&nbsp; [Accessed 29/09/2014].
4) Tech Radar&nbsp;(2012), 'Solid state drives in a server, Is this the memory platform for future server storage?',&nbsp;David Howell &nbsp;March 4th 2013 ,&nbsp;Available at&nbsp;http://www.techradar.com/news/world-of-tech/roundup/solid-state-drives-in-a-server-1131483&nbsp;[Accessed 29/09/2014]. 

 


	              
	              
“The main memory of a computer is referred to as short-term memory and the moment the power is turned off, information is erased. In order to preserve the information, we store information in long-term memory so that information is stored even after the power is turned off.”, (Computer Structures — Lecture Notes, Page 2).
The hard drive is usually used to stored data, for 'long-term' storage while the computer memory is for short term storage. Today the non-volatile memory of our computer has increased its capacity to hold temporary information; now being able to hold many gigabytes of data. This evolution is causing us to to see changes in the computer architecture, the way we store and process information and the user experience. Below I will explore briefly these emerging changes and its potential impact.
Potential impacts of the rapid increase in the capacity of non-volatile memory on:
Architecture of computer systems
This increase in storage capacity, has now influence the architecture of computer systems, in that computers can now be much smaller devices, thus, it can also be cheaper to produce. 
Data processing and storage techniques
On the side of data processing and storage techniques, there is the possibility of Non-volitile memory replacing hard drives as the primary storage media, according to Adrian M. Caulfield &amp; Joel Coburn, (2010). The authors also highlight that the capacity of the non-volatile memory will cause high improvements in read and write times in data processing, and increase memory capacity in high performance computer. This will also lead to improved latency and bandwidth.
User experience
The user will see huge improvements in the time to process data, and the overall more efficient, durable and faster performing computers and devices utilizing this type of memory. Because of the capability to produce cheaper equipment we may also see more users owning a desktop computer or some other device whether it is a laptop, tablet, iPod or smart phone. 

I believe that the ability to produce faster and more efficient devices at a cheaper cost may be the most significant impact in the near future. 


References:
Computer Structures — Lecture Notes, [Online] Available at: https://elearning.uol.ohecampus.com/bbcswebdav/institution/UKL1/201480_SEPTEMBER/MS_CKIT/CKIT_501/readings/UKL1_CKIT_501_Week04_LectureNotes.pdf (Page 2) (Accessed on September 28, 2014)

Adrian M. Caulfield, Joel Coburn, Todor I. Mollov, (2010), 'Understanding the Impact of Emerging Non-Volatile Memories on High-Performance, IO-Intensive Computing.' [Online] Available at: http://ieeexplore.ieee.org.ezproxy.liv.ac.uk/stamp/stamp.jsp?tp=&amp;arnumber=5645465 (Accessed on September 28, 2014)

Sanghyuk Jung, Jin Hyuk Kim and Yong Ho Song (2009), 'Hierarchical Architecture of Flash-based Storage Systems for High Performance and Durability.' [Online] Available at: http://eds.a.ebscohost.com.ezproxy.liv.ac.uk/eds/pdfviewer/pdfviewer?sid=0a5a53bf-f84d-48e8-8e86-86f0fb406dfc%40sessionmgr4003&amp;vid=1&amp;hid=4108 (Accessed on September 28, 2014)



	              
	              The rapid increase in the capacity of non-volatile memory will have a significant impact on computer systems architecture, data processing and storage techniques. In this discourse we will identify some these impacts and the result they have on the users experience. We'll then analyse the impact we believe will be the most significant.
In the current computer systems architecture memory bandwidth is one of the most important factors in system design. With the extensive development in processor technology the demand for memory bandwidth increases. Non-volatile memory technologies will be used to re-architect the current memory hierarchy using a high performance persistent memory design, allowing for higher throughput, greater efficiency and reliable data movements. Adopting Non-volatile memory in the CPU as cache will improve performance at large capacities and higher system performance.
"For decades, computer systems adopt a two-level storage model to manipulate data access: a fast, volatile memory updated by loads and stores, with data being lost when system halts or reboots; a slow, non-volatile storage device managed by databases or file systems, while data can survive across system boots. With&nbsp; the game-changing feature, NVRAMs can enrich such two-level memory or storage stack with the capability of accommodating fast accesses to permanent data storage in a unified non-volatile memory. This feature brings new opportunities to address the massive online data storage and processing requirements of&nbsp; "big data" applications, allowing them to directly access permanent data storage in memory without the performance and energy overheads of transferring data from or to storage."
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (Zhao, 2014)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
This architecture change will required application and operating systems manufacturers to redesign their programs interact with the new computer architecture to take advantage of the efficiencies and higher throughput gains. transforming from the two-level memory stack to a unified stack.
&nbsp;
Data processing and storage techniques will be able to take advantage of the new gains in processing capabilities and the higher throughput allow big data to be processed more efficiently in less time. DRAM memory modules has scalability and energy challenges when it comes on to handling "big data" and is known to "incur excessive cost to provide large capacity to run big data applications" (Chen, n.d.)
Storage techniques using non-volatile memory architecture will eliminate the need for paging file as in the two-level memory architecture. A computer will now have one level of storage, hence no need to page. 
The impacts identified will have positive effect on users. larger amounts of data can be access, stored and processed on mobile devices at faster rates allowing for greater productivity and access to information whenever required. Users will have capacities in mobile devices that was previously limited to workstations and advance computer systems.
I believe one of the most appealing benefit from non-volatile memory implementation is the energy efficiency brought about the low power consumption property. The is will extend far into lower operational cost to data centers and by add meaningful contribution to the efforts to lower the carbon footprint of the computing community. "Peak power and energy usage have become key drivers for all aspects of the system. Energy concerns are driving an accelerating pace in the increase in parallelism as we continue into the multi-/many-core era. System balance is also becoming harder to maintain as the cost of data movement begins to dominate energy usage." (Hemmert, 2011)
&nbsp;
&nbsp;
References:
Exascale Hardware Architectures Working Group [online]. Available from: https://e-reports-ext.llnl.gov/pdf/474589.pdf (Accessed: September 28, 2014).
Understanding the Impact of Emerging Non-Volatile Memories on High-Performance IO-Intensive Computing [online]. Available from: http://mesl.ucsd.edu/site/pubs/Caulfield_SC10.pdf (Accessed: September 28, 2014).
Exploring Opportunities for Non-Volatile Memories in Big Data Applications [online]. Available from: http://prof.ict.ac.cn/bpoe_4_asplos/wp-content/uploads/2013/11/Exploring-Opportunities-for-Non-Volatile-Memories-in-Big-Data-Applications.pdf (Accessed: September 28, 2014).
JINGTONG, H.U., CHUN JASON, X.U.E., ZHUGE, Q., TSENG, W. &amp; SHA, E.H.-. 2013, "Write Activity Reduction on Non-Volatile Main Memories for Embedded Chip Multiprocessors", ACM Transactions on Embedded Computing Systems, vol. 12, no. 3, pp. 1-27.
Perez, T., Calazans, N. &amp; De Rose, C. 2014, System-level impacts of persistent main memory using a search engine.

	              
	              Hi Craig,
I found the following diagrams illustrating two scenarios where we could have NVM only architecture and NVM and DRAM acrchitecture, this was in line with an experiment on an NVM emulator to evaluate the benchmarks on two different &nbsp;NVM latencies;2X DRAM and 8XDRAM, where the base DRAM latency was approximately 90ns&nbsp;&nbsp;:

&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; NVM Only Acrhitecture &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;NVM + DRAM ARchitecture

The findings were that memory-oriented systems are better-suited to take advantage of NVM and outperform their disk-oriented counterparts. However, what they also found was that in both architectures, the throughput of the memory-oriented system decreases as workload skew is increased while the the throughput of the disk-oriented systems increases as workload skew is decreased. Based on these findings the conclusion drawn was that a new system for NVM need features of both disk-oriented and memory-oriented systems.
Refereences:
http://istc-bigdata.org/index.php/oltp-database-systems-for-non-volatile-memory/ &nbsp;Accessed on 29 September 2014

	              
	              Hi Kharavela,

Thanks for your comment and your referrence. It was quite an interesting read!

Best regards,
Belinda&nbsp;
	              
	              Hi Terry,
Thanks for putting it to a relatable perspective. Terminator is an awesome example!
kind regards,
Belinda
	              
	              The Impact of Emerging Non-Volatile Memories on Computing Systems
Introduction
New memory technologies promise game-changing features whose impact felt broadly, from embedded computers to mobile devices to datacenters. For example, phase-change and memristor memories can provide fast, inexpensive, non-volatile, highly dense (denser than DRAM), byte-addressable storage systems. Think of the impact of a terabyte byte-addressable persistent storage chip on your mobile phone.
The architecture research community is actively exploring the implications of fast, cheap non-volatile memory, including error correction mechanisms and memory hierarchy organization. Software research has focused primarily on NVRAM-based file systems that maintain current file system semantics and on programming interfaces for persistent objects. These efforts are evolutionary: they integrate NVRAM into existing architectures and programming structures.
Instead, we believe that NVRAM could be revolutionary rather than evolutionary; in this paper, we discuss the ways that cheap byte-addressable NVRAM could substantially affect OS design. For example, new NVRAM technology might change the basic premise of a two-level store (a fast primary memory and a slow secondary memory) that we have assumed for over 50 years. The structure of moving-head disks impacts the entire system structure, including the I/O system, virtual memory, the protection system, the scheduler, the way that processes are managed, and the way that programs are initiated. What if future systems contained only one level of memory that was persistent and uniform?
What if there were no disk pages, no memory pages, no buffer cache, no page faults, no swapping, and no booting on restart? How would we choose to organize the OS and persistent storage? How would we structure, share, and protect persistent storage? What parts of the OS could we simplify or remove, and what features or capabilities would be enabled by this technology?
Current Landscape
In order to discuss the emergent NVM technologies, we need to start by summarizing the current state of memory systems. 
An ideal memory system would be fast, cheap, persistent and big (highly dense). Until now, all known memory/storage technologies address only some of these characteristics.
Static RAM (SRAM) is very fast, but it's expensive, has low density and is not persistent.
Dynamic RAM (DRAM) has better densities and is cheaper (but still expensive) at the cost of being a little slower, and it's not persistent as well. Disks are cheap, highly dense and persistent, but very slow. Flash memory is between DRAM and disk; it is a persistent solid-state memory with higher densities than DRAM, but its write latency is much higher than the latter.
Fortunately, it is possible to design a memory system that incorporates all these different technologies in a single memory hierarchy. Such hierarchy allows the creation of a system that approaches the performance of the fastest component, the cost of the cheapest component and energy consumption of the most power efficient component. This is possible due to a phenomenon known as locality of reference, so named because memory references tend to be localized in time and space. This can be so summarized:

If you use something once, you are likely to use it again (temporal locality).
If you use something once, you are likely to use its neighbour (spatial locality).

This phenomenon can be exploited by creating different memory levels; the first levels are smaller and more expensive, but have fastest access, and the other layers are progressively bigger and cheaper, but have slower access times. Appropriate heuristics are then applied to decide which data will be accessed more often and place them at the first levels, and move the data down on the hierarchy as it ceases to be used frequently. Figure 2.1 depicts the concept of memory hierarchy.
For at least the last 30 years, we have been building memory hierarchies that rely on
SRAM for caches, DRAM for main memory and disk or flash as persistent storage.
&nbsp;
Figure 2.1: Diagram illustrating the concept of memory hierarchy.
Emerging Memory Technologies
There are several new Non-Volatile Memory (NVM) technologies under research. One study lists such technologies: FRAM, MRAM, STTRAM, PCRAM, NRAM, RRAM, CBRAM, SEM, Polymer, Molecular, Racetrack, Holographic and Probe. Most of these technologies are in different stages of maturity. Some of them are still in early research stages, others have working prototypes, and some of them are already entering into commercial manufacturing.
In the present work, I have limited my study to three of these technologies: Phase-Change&nbsp;RAM, Resistive RAM (including Memristors) and Magnetoresistive RAM (including Spin-Torque Transfer RAM). All these fall into the category of the most actively researched today, are backed by solid companies of the technology industry, and considered most promising of being commercially feasible.
Impacts on Memory Systems
No single memory technology of today can satisfy all requirements of low cost, high density, low energy, low latency and high endurance. For this reason, we use memory hierarchies to obtain a performance similar to the faster technology and cost similar to the cheaper technology. But one of the drawbacks of a sophisticated memory hierarchy is complexity.
Today's memory systems evolved to extremely complex hierarchies, with many specialized layers, which often interact in unexpected ways. There are some known cases of techniques that bring performance improvements to specific layers but result in overall system degradation.
With the current memory technologies approaching physical limits that will prevent them from scaling at the current pace (predicted by Moore's law), it will be necessary to develop new technologies. The ideal future memory technology would have most (or all) of the benefits of cost, density, energy and latency, thus avoiding the need of too complex hierarchies. Recently, an article called such ideal technology _Universal Memory_, and pointed MRAM as its most promising example. Unfortunately, as we have seen, none of the current technologies under development has the complete set of attributes to radically simplify memory systems.
Despite their inability to meet all _Universal Memory_ requirements, these technologies promise to improve current memory hierarchies, and allow memory systems to continue scaling up. In the last few years several papers have been published exploring the potential uses of new non-volatile memories.
Conclusion
For almost 30 years, computer memory systems have been composed of SRAM for caches,
DRAM for main memory and magnetic disks and flash memory for persistent storage. So far, this design has enabled growth according to Moore's Law. However, limitations of these technologies threaten the ability to sustain this growth rate during the next decade, creating a power wall. In order to allow continuing growth, it will be necessary to develop new memory technologies.
As consequence, today we watch the emergence of new memory technologies that promise to change radically the landscape of computer memory systems. In this paper,
I have presented three of the most promising new memory technologies under development:
Phase-Change RAM (PCRAM), Magnetoresistive RAM (MRAM) and Resistive RAM (RRAM). The ideal future memory technology should be non-volatile, low-cost, highly dense, energy-efficient, fast and with high endurance. Such ideal technology would be a universal memory, eliminating the need for complex hierarchies in the memory subsystem.
Unfortunately, as we've seen, none of the presented technologies has today all the necessary attributes to accomplish that. PCRAM is not fast enough and does not have the necessary endurance. STT-MRAM cannot achieve high densities. RRAM has very limited endurance. Until these issues are addressed, there will be no single universal memory.
If RRAM can be made to have high endurance, it can replace (with advantages) both
PCRAM and STT-MRAM for all levels: cache, main memory and disks, becoming a truly universal memory. But this is still a challenge and there is no clear roadmap indicating if this will happen and when.
Actually, the introduction of these new technologies will probably result in more complex hierarchies instead of simpler ones. MRAM can improve on-chip and off-chip caches, but will require more complex cache management algorithms and maybe hybrid designs. PCRAM can be used to create a low-power, highly dense main memory, but will probably demand hybrid designs that involve DRAM buffers. RRAM can be used for write-a-little-read-a-lot devices such as FPGAs and also as main memory in hybrid designs involving 40
DRAM buffers. All the three can potentially be used as persistent storage if their cost/bit goes down enough, which might happen if they become widely adopted and manufactured in large scale.
These technologies can allow the construction of memory chips that are non-volatile, low-energy, highly dense and with latency close to or better than current DRAM chips. This would be enough to avoid hitting a power wall associated with DRAM's current limitations. If their cost becomes low enough, they can also replace disks and flash memory as persistent storage, closing the latency gap between main memory and persistent storage.
In summary, despite their inability to meet all Universal Memory requirements, these technologies promise to improve current memory hierarchies, and allow memory systems to continue scaling up.
The adoption of one or more of these NVM technologies will bring significant changes to memory systems. Some of these changes will be transparent to software applications and operating systems, but others will probably not. We still need to understand carefully the impacts of such changes and how to prepare for them. This work tries to make a contribution in this direction.
Reference
Taciano Perez, César A. F. De Rose “Non-Volatile Memory: Emerging Technologies and Their Impacts on Memory Systems” Port Alegre September 2010
Katelin Bailey Luis Ceze Steven D. Gribble Henry M. Levy University of Washington&nbsp;Department of Computer Science &amp; Engineering: Operating System Implications of Fast, Cheap, Non-Volatile Memory





	              
	              Thanks for this  Kharavela. Quite insightful.&nbsp;
	              
	              
Hi Dr Anthony,

SSDs are most effective for server applications and server systems, where I/O response time is crucial, therefore data stored on SDs should include anything that creates bottlenecks such as databases, library and index files, authorization and login information etc..
While SSDs offer much better performance than hard disk drives and are considered more reliable for mobile devices because there are no mobility issues. However there are some snags especially with writes whereby SSDs eventually slow down after initial use because once a sufficient amount of data has been written to them, the read-modify-erase-write cycle function kicks in&nbsp;where the drive begins to move data around.&nbsp; So each time new data is written to the SSD, data must first be marked for deletion before new data can be written. Over time, the cells or transistors in NAND flash wear out due to the erase-write cycle.
The reality is that although they are still immature and may fail &nbsp;http://lkml.iu.edu//hypermail/linux/kernel/1309.1/01669.html they are still faster and more reliable than hard disks. Also the referrenced failure was a few years ago and SSDs have improved immensely since.

I'd advise against using SSDs in the following instances

 when applications are write-intensive
dont use ssd cache when data access is highly random
deploying consumer-grade SSDs for enterprise applications
using Tier O for solving network problems-if data delivery is hindered by the network, inceasing storage capacity behind the network won't help much, however server-side ssd may reduce the need to access the storage system and ultimately reduce the network demand

References:
http://searchsolidstatestorage.techtarget.com/tip/When-using-SSD-is-a-bad-idea Accessed 29 September 2014
http://www.webopedia.com/TERM/S/solid_state_disk.html&nbsp; Accessed 29 September 2014 
 &nbsp;http://lkml.iu.edu//hypermail/linux/kernel/1309.1/01669.html Accessed 29 September 2014 

Best regards,
Belinda&nbsp;


	              
	              The Advantages and Disadvantages of Solid State Drives
The advent of solid state drive (SSD) in the last few years as a viable alternative to the traditional hard drive (HDD) providing a much higher speed than the latter and other considerable advantages, has built up the reputation of this flash-based storage technology so quickly to the point that so many IT professionals were impelled to deeming it “the best performance upgrade” and recommending it to everyone looking forward to take their computer to the next level. That’s because, unlike solid state drives, no single hardware upgrade is capable of offering the kind of instantaneous and observable boost in system responsiveness, compared to a system based on traditional hard drive spinning media.
Yes, it’s true that a faster processor, additional memory, and a faster graphics card may notably better the overall system performance, but compared to what an SSD can do, all those upgrades fall behind.
But all the stated above traits don’t make SSDs free of disadvantages, they already undergo their own. Below are highlights of the advantages and disadvantages of SSD
Advantages of Solid State Storage Device

Faster startup because there is no spin-up required.
Far faster than conventional disks.
Faster boot and application launch time when hard disk seeks are the limiting factor.
Somewhat longer lifetime – Flash storages typically has a data lifetime of 10 years before it stops working.
Security – allowing a very quick scan of all data stored.
Some of the storage devices are waterproof.

Disadvantages of Solid State Storage Devices

Price – Flash memory prices are still considerably higher per gigabyte than those of comparable conventional hard drives.
Vulnerability to certain types of effects, including abrupt power loss, magnetic fields and electric/static charges compared to normal hard disk drives.
Limited writes cycles. Typical Flash storage will typically wear out after 100,000-300,000 write cycles, while high endurance Flash storage is often marketed with endurance of 1–5 million write cycles
Slow random write speeds – as erase blocks on SSDs generally are quite large, they're far slower than conventional disks for random writes.


	              
	              The Advantages and Disadvantages of Solid State Drives References https://www.champsbi.com/the-advantages-and-disadvantages-of-solid-state-drives/ http://bmisigcseit.wikispaces.com/Advantages+and+Disadvantages+of+Solid+State+Storage+Device 
	              
	              Hi Terry - it was just a little joke about computer science being about indirect abstraction ;-)
	              
	              Thanks Joseph
Yes, thin client sounds a worthwhile option to explore. The key is the connectivity relaibility, speed and cost, otherwise it will not be a positive experience and therefore unlikely to be adopted on large scale. Cheers!
Best Wishes, Craig
	              
	              Hi Craig,
The question in the joke does have an answer:
"gcc -O4 emails your code to Jeff Dean for a rewrite."
Jeff dean is the legendary engineer from Google who is an expert in performance optimization.
I bet colleagues will be rolling on the ground loughing after reading all the Jeff Dean facts:
http://www.quora.com/What-are-all-the-Jeff-Dean-facts
br, Terry
	              
	              Hi Bram

Here are some little findings about database buffer cache

The database buffer cache is the portion of the system global area (SGA) that holds copies of data blocks read from datafiles. All users concurrently connected to the instance share access to the database buffer cache.

The buffers in the cache are organized in two lists: the write list and the least recently used (LRU) list. The write list holds dirty buffers, which contain data that has been modified but has not yet been written to disk. The LRU list holds free buffers, pinned buffers, and dirty buffers that have not yet been moved to the write list. Free buffers do not contain any useful data and are available for use. Pinned buffers are currently being accessed.

When an Oracle Database process accesses a buffer, the process moves the buffer to the most recently used (MRU) end of the LRU list. As more buffers are continually moved to the MRU end of the LRU list, dirty buffers age toward the LRU end of the LRU list.

The first time an Oracle Database user process requires a particular piece of data, it searches for the data in the database buffer cache. If the process finds the data already in the cache (a cache hit), it can read the data directly from memory. If the process cannot find the data in the cache (a cache miss), it must copy the data block from a datafile on disk into a buffer in the cache before accessing the data. Accessing data through a cache hit is faster than data access through a cache miss.

Before reading a data block into the cache, the process must first find a free buffer. The process searches the LRU list, starting at the least recently used end of the list. The process searches either until it finds a free buffer or until it has searched the threshold limit of buffers.

If the user process finds a dirty buffer as it searches the LRU list, it moves that buffer to the write list and continues to search. When the process finds a free buffer, it reads the data block from disk into the buffer and moves the buffer to the MRU end of the LRU list.

Reference

Memory Architecture – Oracle Documentation [online] http://docs.oracle.com/cd/B28359_01/server.111/b28318/memory.htm#CNCPT1222 
	              
	              Hi Bo

Just a hint on storage for a long time part


In the area of age symmetry, in relation to Age out of the system i.e. the behavior of the behavior of the SSD after spending some time in an unpowered state. Suppose you preload data onto an SSD (it may be an OS with apps for a notebook - or a backup data set).

If you go back after a few days, a few months, 6 months or 2 years - you will see very large differences in the data integrity on that SSD. SSDs which have high data integrity in the always or mostly powered state may be terrible if they are left for a long time unpowered. For flash SSDs - the cause is the difference in remanence between SLC and various flavors of MLC. These weaknesses can be resolved in the powered up state by healing processes - but when the power has been off for a long time - the intrinsic defects can be significant.

SLC&nbsp;&nbsp;&nbsp;&nbsp; - single-level cell

MLC&nbsp;&nbsp;&nbsp; - multi-level cell

&nbsp;

Regards, Babatunde
	              
	              Hi Belinda.
Thanks for your useful reference from techtarget site. Looking through some of the material there and referencing the case study of a hospital that made a choice to apply a hybrid flash implementation for their server, it would appear that there may still be some sticky points regarding the reliability of SSDs. One question that came to my mind was why the choice of hybrid flash (storage systems (block or file-based) that contain both hard disk drives and NAND flash with a caching or tiering software layer that's charged with keeping the appropriate data on the flash tier)? Cost containment at best compared to a disk array?
The above reference from TechTarget, 2014, 'Hospital-prescribes-hybrid-flash-array-to-speed-storage-performance', &nbsp;talks about the implementation of a hybrid flash in which data that needs minimal writes but fast read speeds, is stored in the middle tier of multilevel cell (MLC) NAND (type of non-volatile storage technology that does not require power to retain data), and Data that requires long-term retention storage gets pushed to the third tier of hard disk drives.&nbsp;
In addition there is mention in TechTarget, 2014,&nbsp;' Overcoming the unpredictable nature of the hybrid flash array  ',&nbsp;of the ocassional flash misses (where the required data is not in the cache or on the flash tier when requested leading to unpredictable storage performance), whose overall impact&nbsp;can actually be worse than the consequences of that data being on a traditional disk array with no flash.
Data integrity is a key aspect especially in database systems that read and write large interdependent transactions. The ACID &nbsp;principle (Atomicity, Consistency, Isolation, Durability)&nbsp; ought to apply in all cases and I was beginning to wonder what exactly happens in cases of these 'flash misses'. So what does this mean in terms of reliability? Performance gain through the NAND at the expense of having reliable data in place is a trade off one may have to think twice about.
References:
1)&nbsp;Kranz, G. 'Hospital prescribes hybrid flash array to speed storage performance',&nbsp;TechTarget&nbsp;(04 Sep 2014) ,&nbsp;Available at http://searchsolidstatestorage.techtarget.com/news/2240228198/Hospital-prescribes-hybrid-flash-array-to-speed-storage-performance; (Accessed at 29 Sep 2014)
2) Slack, E. 'Overcoming the unpredictable nature of the hybrid flash array', TechTarget&nbsp;(04 Sep 2014),&nbsp;Available at http://searchsolidstatestorage.techtarget.com/tip/Overcoming-the-unpredictable-nature-of-the-hybrid-flash-array,&nbsp; (Accessed at 29 Sep 2014)
3) Chapple, M. 'The ACID Model', About Technology (no date), Available at&nbsp;http://databases.about.com/od/specificproducts/a/acid.htm , &nbsp;(Accessed at 29 Sep 2014)


	              
	              Hi Joseph,

That surely is something to think about. Performance means nothing if you can't trust the device that's holding the data. However, I'd like to think that things are improving on that front,based partly on the following statements:
 "From the data I've seen, client SSD annual failure rates under warranty tend to be around 1.5%, while HDDs are near 5%" Ryan Chien, an SSD and storage analyst with IHS's Electronics &amp; Media division. 
"I think the best way to describe SSD reliability is that thanks to controller maturation, average product endurance is improving and the standard deviation is falling," &nbsp;Ryan Chien, an SSD and storage analyst with IHS's Electronics &amp; Media division. 
I also believe a lot of it has to do with maturity, SSDs are still immature compared to HDDs that have been around for so long. As we learn more and more about their limitations and best uses we will use the findings to out advantage and who knows?

Best regards,
Belinda
http://www.tomshardware.com/reviews/ssd-reliability-failure-rate,2923.html
	              
	              Hi Terry
You make me laugh, alot.
I hadn't heard of Jeff Dean before, but enjoyed reading the facts, especially the ones I understood!
I especially like the simple ones, like Jeff Dean can beat you at connect four in three moves, and that he failed the turing test :-)
Best Wishes, Craig

	              
	              Hi Robin
You make some great points.
I notice that Apple are now doing a fusion type disk for their iMac's, essentially combining 128GB of super fast flash storage with a traditional HDD. Aparently, the OS automatically moves your most used files to the flash storage for quicker access. It also enables faster booting, as this is done from the flash too.
A good example of a technology business trying to deliver the benefits of flash to the consumer, in the most cost efficient way.
Best Wishes, Craig

	              
	              Hi Babatunde.

That's an interesting note regarding data integrity of the SSD, which still begs the question on the reliability of the SSD. It would appear to be somewhat a matter of level of risk tolerance by an organization as well as critical need for performance
Regards
Joseph
	              
	              Hi Belinda
This is a brilliant example of experimenting with the two scenarios. Thank you for this.
I have read it a couple of times and whilst&nbsp;I can certainly see that the conclusion drawn is a combination of both memory-oriented and disk-oriented systems, I still feel that there may be an opportunity for the 'DRAM' (and other) layers to still become non-volatile.&nbsp;
I am not sure yet how to articulate this, I probably need to re-read a third (and perhaps a fourth) time but to help explain my current thinking, if you take a 'hybrid' car, the reason for the 'hybrid' bit (combination of multiple sources of power, for example electric and petrol) is that the technology has not yet reached the 'turning-point' at which the 'electric' bit becomes the best of both worlds.
We know that really clever people are working on this type of stuff, so that the ultimate solution fully addresses the issues and challenges of things like reliability, efficiency, speed, horsepower, torque, power retention, and of course the economics of it. Eventually, and I guess contrary to what the Gas and Oil industries want, we are led to believe that electric may well become the only source of power for the motor vehicle over the next decade or so.
Just to build on this, and to help further as an analogy, the hybrid car may suffer performance wise if it is also expected to tow a caravan. Therefore, that is when the engine management system uses the 'combustable engine' bit to help propel the car. So, the conclusion is that a combination of both is the most appropriate approach currently.&nbsp;
My perspective is that at some 'turning-point' the hybrid eventually becomes fully capable using electric power only.
My memory prediction follows a similar path, in that at a 'turning-point', non volatile memory potentially replaces volatile memory (DRAM, Cache etc) and the technical advances enable this to be the best solution, rather than a combination of two or more solutions.
Of course, I am&nbsp;still trying to get my head&nbsp;fully&nbsp;around all of this, so I am currently very easilly persuaded either way as my understanding and knowledge is not yet fully formed.
Thanks for the response Belinda. Your reference was a really good read and you have certainly helped me with my learning this week.&nbsp;It actually worries me how little I actually know about computing, especially having worked all of my life in this industry!&nbsp;To one of Terry's points, there is so much abstraction that I have applied to the science of computing than I had consciously realised.
Best Wishes, Craig.
	              
	              Hi Joseph
Thanks for the info. Since SAP is one of the big players, I expect this solution to work well for them. I just wonder what the downsides are, there always are, but I can't see them right now - execpt it being proprietary. Can you?
Bo
	              
	              Hi Babatunde That is very interesting.&nbsp; I have not, in my research, found anything about that issue. Do you have some references that you could share?  Bo
	              
	              Hi Craig
You had a question about if it's feasible to extend the processor further down the left side. Yes, I think it's feasible but I would probably depend on the cost. I could also ask "why do we have to have RAM?". I think as Terry has also written, that the wish must be to get a simper hierarchy.
Bo

	              
	              Thanks for this Joseph. Quite insightful
&nbsp;
Though, I found something I fell could be more advanced than SAP HANA. -Oracle Database: see how Oracle Database In-memory beats the competition below;


Best Regards, Babatunde

Reference
Oracle Database In-Memory Versus SAP HANA [online] Available from: http://www.oracle.com/technetwork/database/options/dbim-vs-sap-hana-2215625.pdf?ssSourceSiteId=ocomen 

	              
	              Hi Everyone,Large-scale In-Memory Database (IMDB) schemes are now sometimes used for time-critical and/or resource-limited database storage systems. The prime benefit of such a scheme is faster database access. However, can you envisage possible problems or issues with IMDBs.Anthony
	              
	              Hi Dr Ayoola,
How reliable are Solid-State-Disks (SSDs) depends on the context of usage of them. For the home and retail usage I would say that they are fairly reliable, their advantages on mass commercial devices (such as smartphones) outweigh their limitations. However, if we look at the enterprise and datacenter levels, we should thoughtfully evaluate where we are going to use them.&nbsp;
In a very good white paper by Hutsell, Bowen and Ekker (2008) we can identify the problems with the SSD technology. Perhaps the most important one is when data is erased then the endurance of the chips wears down. In most scenarios in the consumer market, the life of the chips are of no concern because writes are infrequent (W. Hutsell, J. Bowen and N. Ekker. 2008).
Today there has been quite some advancements in remediating some of the issues SSDs have, and we will certainly see more. However, even so, there are places in a datacenter where I would need to think twice where to use this type of disks. For instance where I work we have been trying SSD solutions and FusionIO cards. Our best results are for desktop virtualization (XenDesktop pooled machines): since we don’t care about losing data (we have several type of storage types that provide us with redundancy) we have the Operating System disk on SDD storage for fast user experience.
On the other hand we have tried some SSD solutions for high I/O databases and, while the performance gains is extremely good, we found that with our I/O the disks would theoretically wear out in 1 year (with a 5-6 year lifetime estimate from the vendor of average usage). The FusionIO cards are a better fit for our intensive I/O databases, but we are still “testing” them.
References
Hutsell, W., Bowen, J. and Ekker, N. (2008) Flash Solid-State Disk Reliability. Available at: http://www.dynamicsolutions.com/assets/pdfs/whitepapers/flash_solid_state_disk_reliablity.pdf (Accessed: 9 September 2014).
&nbsp;
	              
	              Hi Craig,

I feel thats a fair assumption and good way to describe it using the hybrid car analogy. Like you said early days while changes are being adopted its pretty much a given in my mind to go about it in this approach. Also like what has been discussed it really does depend on the platform and technology this memory is embedded into as well. EG what memory is needed for certain hand held devices and phones will be different to the overall solution for a home PC which in turn will be different to a large complex data farm solution.

ta
Chris

	              
	              Hi Chris, yes exactly, thanks for helping to crystalise a little further.&nbsp;I'd like to see that this is perhaps the catalyst to also drive change in other areas, and to see cohesive and integrated technologies that are simplified and intuitive.
Best Wishes, Craig
	              
	              Yes, definitely.
Thanks Bo and Terry
Best Wishes, Craig
	              
	              Hi Dr Ayoola,
I can only envision one possible problem with In-Memory Databases systems (IMDBs): there is some data that we can’t store in memory (at least not yet). For example Hubble telescope or satellite imagery.
Today technology is reasonably reliable, albeit very expensive, when it comes to IMDBs. With transaction logs, database replication, battery RAM and hybrid systems, I think that given the money IMDBs are feasible on any scenario that requires very fast database access.
Regards,
Augusto.
&nbsp;
References
Garcia-Molina, H. &amp; Salem, K. (1992) 'Main memory database systems: an overview', IEEE Transactions on Knowledge and Data Engineering 4(6), pp. 509-516 doi: 10.1109/69.180602.
Januschowski, T., Kolassa, S., Lorenz, M. and Schwarz, C. (2013) 'Forecasting with In-Memory Technology', Foresight: The International Journal of Applied Forecasting Fall2013(31), pp. 14-20.
&nbsp;

	              
	              Hi Terry,
Very good question. The only possible place I could imagine where volatile memory might be used for security reasons is military applications. But I couldn't find any article that actually talks about such usage, to the contrary most relate to the use of fast solid state non-volatile memory in military areas.
Regards,
Augusto.
	              
	              Hi Terry and Bo,
I have been following your discussion about universal memory and judging by how the others also mention this memory it’s certainly a hot topic. I do believe that we are not that far from getting our technology to be able to provide the characteristics of universal memory. What captivated me was reading the PH.D thesis of Roberts D.A (2011), it’s a lot of pages to read but totally worth! One of the ideas that he puts forward is merging data storage with main memory (Roberts 2011), this is the closest thing to the universal memory concept.
I believe we have no choice but to move towards this concept. As we advance, the data that we gather keeps increasing and the only meaningful data is the data that we can process and understand. What use will be for us to be able to store all the possible data we can gather in the Solar System if we are not be able to use it to understand how it came to be? A perfect example of this today is the Kepler NASA mission: it produces 1 snapshot of the equivalent of 95 megapixels (MP) every 6 seconds, if the image is relevant it creates a 30 minute “snapshot”. That’s 300 images of 95 MPs (Ames Research Center NASA. 2013). Data is sent to earth for 100’000 stars, and then we are the bottleneck: we just cannot compute or process all that data so we “discover” planets couple of years later. Today the implications of finding a planet couple of years later are minimum, even if it’s an habitable planet with intelligent life (which Kepler cannot detect) the best we could say is: “wow” and “it is ridiculously far!”. But what about data gathered in real-time from the tectonic plates? I bet that data would have saved hundreds of thousands of lives in the 2004 Indian Ocean tsunami.
Inevitability we need fast memory and data technology for any kind of data that needs processing, be it real-time or not. Because data is rapidly growing then we need a faster way to process it and “simply” changing the system architecture to have data and main memory merged is a good thesis, at least worth of investigation.
Best Regards,
Augusto
&nbsp;
References
Ames Research Center NASA. (2013) Frequently Asked Questions from the Public. Available at: http://kepler.nasa.gov/Mission/faq/#c8 (Accessed: 29 September 2014).
Roberts, D.A. (2011) Efficient Data Center Architectures Using&nbsp;Non-Volatile Memory and Reliability Techniques. PhD thesis. The University of Michigan. Available at: http://web.eecs.umich.edu/~tnm/trev_test/dissertationsPDF/daveR-ref-fixed.pdf (Accessed: 27 September 2014).
&nbsp;

	              
	              Greetings Dr Ayoola, My experience so far has been that i've never had any issues with SSDs only the benefits of speed but that doesn't mean there arn't any. To talk about how reliable they are you can't really just talk about them on their own but compare the reliability of SSDs against traditional HDDs. Also there is the difference in the SSDs themselves. For example&nbsp; Andrew Ku (2011).:     So you can see from the table above there are many noticeable differences in SSDs themselves from 18.7 years to 62.6 years for write exhaustion on certain drives. Sometimes drive exhaustion isn't the issue however as many reported cases on line talk about the firmware being a bigger issue with SSDs rather than the drive themselves. InterServer’s experience with SuperTalent's SSDs is shown below over a 2 year period using SSDs.   You can see from this with their experience that 5% of HDDs had an average failure rate of 5% whereas with SSD's it was as low as 1.6%. This is a difficult statistic though as they used HDDs for 6 years and SSDs for just 2. You never know when during the duration the drives fail. EG if the SSD's were rated over 6 years the same they my see more failures and higher percentages as they may be more susceptible between 3-6 years rather then the first 2 for example. This is backed up by one of the largest hosting companies reports 'Softlayer' reported they found higher failure rates for SSDs over time so the longer they had the SSD the higher the failure rate.   Also it shows a difference in numbers with 150 SSDs in operation against 2800 is a bit of a difference. So take those results with a pinch of salt but its still interesting all the same.     It's very hard to compare the two drive types as HDD reports tend to look at seek times and rotational latency which is specific to HDDs. Also they have been around for far longer so there are more statistics on them where as SSDs concentrate more on returned rates on the drives themselves form manufacturers.   Failure rates tend to be better on SSDs over HDDs however SSDs are far more susceptible to sudden power interruptions and less robust so it really does depend on what you physically do with the machines.   Instances where I'd advise on not using them would be simply where cost is needed to be kept low or where speed wasn't a huge issue and where robust situations require more hardy solutions for the technology. Currently HDDs have larger storage as well so for data warehousing or storing of large amounts of data is needed and backups. I'm sure SSD's will soon compete with the space requirements but for now they don't.     References:   Andrew Ku, (2011) 'Is Your SSD More Reliable Than A Hard Drive?' , [http://www.tomshardware.co.uk/ssd-reliability-failure-rate,review-32241.html] Accessed 29 Sept 2014. 
	              
	              Hi Kharavela
First I would like to reference what Babatunde wrote further up, and especially the part regarding SSDs without power: “If you go back after a few days, a few months, 6 months or 2 years - you will see very large differences in the data integrity on that SSD. SSDs which have high data integrity in the always or mostly powered state may be terrible if they are left for a long time unpowered. For flash SSDs - the cause is the difference in remanence between SLC and various flavors of MLC. These weaknesses can be resolved in the powered up state by healing processes - but when the power has been off for a long time - the intrinsic defects can be significant.”
Though I haven’t seen his reference yet, I trust he has documentation for this off-power statement.

You wrote that “SSDs are much faster and less likely to break down than other drives”.

I guess you are writing under the assumption that the SSD it powered on? Under that assumption, and from my research and Babatunde’s finding, then your are right.

But it also shows that you have to know the pros and cons, and why you buy the SSD instead of some other technology. (see Baxter)

Baxter. http://www.storagereview.com/ssd_vs_hdd

Bo
	              
	              Thank you for the reply Terry,
I am not for hiding inconsequent behaviour but still wonder if bringing back the architecture from 3 to 2 layers will bring the intended simplification. 3 layers describe 3 different categories of memory use. When bringing it back to 2 layers one could lose that inherent difference. But following your reasoning you do not find that a problem but see it as an opportunity to get rid of often arising discrepancies.
The difference between buffer and cache is subtle but more clear not. The cache is intended to be a direct copy-able source for running processes while the buffer is only intended to create a direct accessible non-interrupted dataflow to run processes that need to process large data sets fluently.
Greetings from Bram
p.s. Can you tell how the joke ends? 
	              
	              Hello Babatunde,
Good explanation for how the cache can be used for buffer functions in a db environment. The proces on how new information can get stored is clear thanks to you.

Greetings from Bram
	              
	              Hi Craig and Bo,
I think another important aspect of consideration for chip design is the development cycle. Given the current technology, to build a product line and manufacture the processor probably still take very long time. It's crucial to get early feedback when designing complicated system. So probably keeping everything in one chip cannot be the mainstream yet.
But another fast developing technology, 3D printing, will change this in a revolutionary way. With 3D printing, a designer can change a little bit in his processor design, then use the 3D printer to print out the physical processor in a few minutes, and then almost immediately test it to see if his design change works. With the 3D printing technology, the fact that everything is build in one die doesn't imply hard dependencies anymore.
Long live the 3D printers!
br, Terry&nbsp;
	              
	              Hi Augusto,
If you search "anti-reverse-engineering", probably there's something related to our assumption. Yes, that's also the only area I can imagine where volatile memeory is prefered.
br, Terry
	              
	              Hi Bram,
I would agree if the layered structure is to separate concerns (different categories of memory use). But the cache layer is only concerning accelerating the access to memory by predicting the behavoir of use. When the main memory is also fast, it might be removed or simplified. Something simple is better than something complicated.
I'm a big fan of simplicity. One example is the editor written by another Bram:&nbsp;http://en.wikipedia.org/wiki/Bram_Moolenaar
The joke:http://www.quora.com/What-are-all-the-Jeff-Dean-facts
"gcc -O4 emails your code to Jeff Dean for rewrite." (sorry, it should be O4. Jeff Dean is an optimization expert from Google.)
br, Terry
	              
	              
Hi Dr. Anthony,
Solid-State-Disks (SSDs) are generally cheaper, faster and are more durable because they do not require using read write heads as do the traditional hard disks. They also consume less power and are generally suited for large enterprise systems. 
However I would advise against their use in instances where there may be large overhead involved.



References:

What is the Future of Disk Drives, Death or Rebirth? YUHUI DENG . [Online] Available at:
http://eds.a.ebscohost.com.ezproxy.liv.ac.uk/eds/pdfviewer/pdfviewer?sid=dfdb733e-f4a6-4396-8b92-4e7e4db39e58%40sessionmgr4004&amp;vid=1&amp;hid=4205

	              
	              Hi Colleagues,
I just wanted to endorse the point being made and also add my bit. A simplified memory hierarchy well provide outstanding performance to computer systems and improve the efficiency by eliminating process of moving data from disk based storage to memory then to CPU all via buses which also reduce the speed moving this data around. With the consolidated memory architecture using non-volatile memory the data wil be readily available to the CPU.&nbsp;
As for the hybrid approach, many companies are far ahead with this development and may already be in the marketplace solutions offerring the benefits of NVM. One such application for hybrid system being examined is in datacenters. By integrating next generation NVM into the current storage system architecture, the new system is able to achieve better performance in terms of cost, capacity an power consumption.(Leong, 2012).
I believe further development of NVM implementation will be larger dictated by the applications that demand the benefits that NVM offer. And as such a mix mode might be employed in delivering suitable solutions to the market.

Reference:
Yong Khai Leong (2012) Future of Data Centres: Next Generation NVM and Hybrid Integration [online]. Available from http://www.dsi.a-star.edu.sg/e-newsletter/Apr2012/T1.html&nbsp;(Accessed: September 29, 2014).


	              
	              Why IMDBs have not taken over the world!
In this age of big data, businesses are demanding faster and easier access to information in order to make reliable and smart decisions. In-Memory is an emerging technology that is gaining attention as it enables users to have immediate access to the right information which results in more informed decisions
Firstly, In-Memory is not a new concept.&nbsp; Hasso Plattner and Alexander Zeier have mentioned in their study&nbsp;‘In-Memory Data Management: An Inflection Point for Enterprise Applications’&nbsp;that in-memory databases have been around since the 1980s. The problem has always been the cost and the limited amount of RAM database servers could use. For example, 2 megabytes of RAM in 1985&nbsp;would have cost&nbsp;around £600.&nbsp; This is in direct contrast to current pricing, where you can pick up 8 gigabytes of RAM for well under £100.
Secondly, Modern operating systems such as Windows are now based on 64-bit architecture and are able to handle 1 terabyte of RAM with ease, allowing enterprise applications to be delivered on standard, off the shelf servers, without a mainframe in sight.
So how does it work?
Simply, In-Memory databases reside in the RAM of the server they are installed on.&nbsp; This can provide performance improvements of up 10x – 100x over traditional databases systems that use disks to store their data.&nbsp; They are able to handle an incredibly high throughput of data due to this improved performance, and therefore answer queries in a matter of seconds that would have taken minutes or hours on standard tools.
Why everyone is not using it
Despite the drop in the price of RAM and the improvements in the processing capabilities of the server running the appliance, some things are still, indeed, finite.&nbsp; The biggest issue for the in-memory challenge is the amount of memory a server can physically have installed.&nbsp; In-memory vendors have resorted to several ‘workarounds’ to this problem:

Use a compression based algorithm on the data to shrink the size of the stored data
Combine the in-memory processing with another complimentary technology
Develop a specific hardware solution, an ‘Appliance’, that can combine all of the above
Use the in-memory Database as an accelerator only, dealing with smaller data volumes for specific analytical purposes

In fact a number of vendors actually combine some or all of the above into their solutions, addressing the scalability issues found within in-memory architecture.
What is still holding people back…..
There are 2 key issues – Cost and Ease of Use…
When dealing with the world of Big Data – and by that I mean billions of rows of data from multiple systems, you require ‘big hardware and big ticket software’ to match.&nbsp; The leading lights (and by that I mean Oracle, SAP, IBM etc.) in the database world often hold their cards close to their chests on this, but hardware and software alone can easily reach several hundred thousand dollars, if not more – and that doesn’t include installation, support or training.&nbsp; It is also worth remembering that the database is only one piece of the bigger puzzle, with other technologies required for processes such as Extract, Transform and Load (ETL), data cleansing, manipulation and data mining.
Which brings us onto the second point – ease of use.&nbsp; While the business is certainly demanding faster access to data, they still have to ask their IT department to help get to that data.&nbsp; This is because the Business User (who has the domain expertise, and therefore should have control of the data) does not have the programming expertise to use the tools.&nbsp; This means despite the technological advances made, the process for data led solutions remains the same.
Business requirements gathering is a lengthy and arduous task, the requirements change often and the results don’t always hit the mark.&nbsp; As In-Memory technology is often forced to look at aggregated data (for the reasons stated above) – we can see that it still comes with traditional problems associated with any analytical implementation.
So what is the answer?
Solutions are coming to the market that will combine the advances in technology with an approach that gives them access to their raw data, and create applications that meet the business needs first time.&nbsp; This ‘Data Driven’ approach will allow the business to get more from their data in a shorter timeframe.
These solutions also encompass new ways of storing and analysing data, using new algorithms and architectures (such as a ‘Pattern Based architecture’) to shrink the data by over 30 times – allowing for huge datasets to be analysed within the confines of a single server.
However, Relational databases will continue to dominate the marketplace until the perception of In-Memory (which can be difficult to deploy and expensive to maintain) changes to be one of speed, ease of use and low cost.&nbsp; This can only be achieved by integrating new tools for delivery, ones which enable the business user to reap the rewards of improved access to mission critical data.

Reference
http://kdrrecruitment.com/news-blog/why-in-memory-databases-havent-taken-over-the-world/

	              
	              The reliability of SSD's are still not comparable to HDD's and varies widely depending on the manufacturer. The major challenging is the failure of SSD's which is due to power loss, this have been address in the latest models by incorporating " Power Loss Protection" mechanisms that eliminate failures due to power loss during writes to the drive.
when compared against HDD's, SSD's are still not as reliable and can prove to be difficult if data recovery is ever necessary. SSD data recovery process is very extensive and requires a series of highly complex steps such as navigating encryption and file system format.
&nbsp;I would advise that SSD's not be used for continuous data or media file repository which will be subject to numerous read/write activity. SSD's performance advantages over HDD's will be best served in application execution and operations.
I have employed SSD's in specialize systems like NLE (non-linear editing) video production systems where the application processes can take advantage of the systems high performance gained from using SSD as the system drive.
While SSD's are quite, have no moving parts and offer great performance, their usage should be for specific operations where the advantages will be well suited.
&nbsp;
Reference
Extreme Tech (2013) SSD Stress testing finds Intel might be the only reliable drive manufacturer [online]. Available from http://www.extremetech.com/computing/173887-ssd-stress-testing-finds-intel-might-be-the-only-reliable-drive-manufacturer (Accessed: September 30, 2014).

	              
	              Hi Tanisha,

I'm not sure your quote on SSDs being " generally cheaper" is correct.
Even searching on personal computers for example if I wanted to buy a 1TB ordinary HDD they cost around £42-£45 and if I wanted a 1TB SSD your talking at least £330.
Granted in the future they probably will be cheaper and on some levels they may be cheaper for manufacturers to produce but certainly as a consumer they are far more expensive to buy at the moment.
Even the reference you have used confirms what I mention in the conclusions, Deng (2011) states:
"Traditional disk drives are continuously challeneged by their competitor SSDs on the market at present. Fortunately due to their advantages in capacity and cost, they still have a few years to go"

References:
Yuhui, Deng (Apr 2011) 'What is the Future of Disk Drives, Death or Rebirth?' [http://eds.a.ebscohost.com.ezproxy.liv.ac.uk/eds/pdfviewer/pdfviewer?sid=dfdb733e-f4a6-4396-8b92-4e7e4db39e58%40sessionmgr4004&amp;vid=1&amp;hid=4205] Accessed 30 Sept 2014.


	              
	              Hi Everyone,Can you describe, using suitable illustrative examples, the basic features of Object-based Storage Devices (OSDs). What advantages do OSDs offer, in large commercial I.T/network installations?Anthony
	              
	              Hi Craig,
I absolutely agree with Chris, the hybrid analogy is a great one. While this study was very informative, I agree with your concept, it is early days yet, conclusions shouldn't be drawn at this time. While there are still some challenges with NVMs at the moment, we will learn and eventually reach our "turning point".
Best regards,
Belinda
	              
	              Hi Anthony,

As you probably already know (IMDB) In-Memory Database systems are sometimes also called (MMDB) Main-Memory Databases, have surprisingly been around for a while and to date have been mainly used in the telecoms industry where fast access is the driving factor. Reading about this I always assumed it was a relatively modern thing but apparently it has been around since the early 1980's or certainly types of IMDB's at least according to Napleton, M (June 2013).
I didn't know a large amount about this area so researching the potential problems and issues I found out that it was very hard to find because many papers talk about IMDBs in the positive way on how they are used rather than the issues and where not to use them.

What I have found out is that IMDBs are normally volatile rather than non-volatile and that they are more vunerable to software errors.
Hector Garcia-Molina(1992) states that applications that use satelite image data would never be used in such a way.
Tech Trends (Feb 2014) are real supporters of IMDBs and revolutionary for Cloud and Big Data solutions. They quote "CIOs can reduce total cost of ownership because the shift from physical to logical reduces the hardware footprint, allowing more than 40 times the data to be stored in the same finite space. That means racks and spindles can be retired, data center costs can be reduced, and energy use can be slashed by up to 80 percent."

So it's really hard to see where the issues are for this. The only thing I can think of is applications where certain restrictions on the data would be needed around policy and regulation, where speed is in fact not an issue but more importantly the database and data reliability might come into question.
ta
Chris

References:
Hector Garcia-Molina (1992) 'Main memory database systems: An Overview' [http://pages.cs.wisc.edu/~jhuang/qual/main-memory-db-overview.pdf] Accessed 30 Sept 2014.
Napleton, M (June 2013) 'Why in-memory databases haven't taken over the world' [http://kdrrecruitment.com/news-blog/why-in-memory-databases-havent-taken-over-the-world/] Accessed 30 Sept 2014.
Tech Trends, Mike Brown &amp; Doug Krauss (Feb 2014) 'In-Memory Revolution' [http://dupress.com/articles/2014-tech-trends-in-memory-revolution/] Accessed 30 Sept 2014.
	              
	              Hi Bram
Just following your thread and how you have described cache, do you feel that the cache should/must continue to be volatile memory.
One of the predictions I have made is that this type of memory could eventually become non volatile and fully integrated with the CPU die itself. My thinking is based purely on the trend of moving memory closer to the CPU over the years (also linking to Terry's point of it being fast too) and that non volatile memory would potentially reduce/remove the need for I/O with the external memory at the time of the execution. The data&nbsp;could then be archived once the process has ended, at a time which is less critical and isn't time dependent. All theory, but keen to get your perspective on whether cache (or in fact any of the volatile memory) should/must remain volatile and if so, the thinking behind that?
Thanks Bram
Best Wishes, Craig
	              
	              Hi Craig,
Given the definition of cache, its volatility should be irrelavent. It's volatile by nature. Because the cache only work as when the processor is running, and the content of cache should be from the other memory that it's caching, not from it's own persistent state.
br, Terry
	              
	              Hi Terry
Yes indeed, the 3D printing opens a whole lot of possibilities, atleast when it comes to testing.
3D printing, "IBM's 3D printer to revolutionize chip prototypin".&nbsp;http://www.eetimes.com/document.asp?doc_id=1322091
Bo
	              
	              Performance isn’t everything. When it comes to your data, all of the speed in the world means little if you can't trust the device holding that important information.
An SSD does much the same job functionally (saving your data while the system is off, booting your system, etc.) as an HDD, but instead of a magnetic coating on top of platters, the data is stored on interconnected flash memory chips that retain the data even when there's no power present. In SSD there are not moving parts involved which make SDD more reliable over HDD.
Although technology used in SSD is very efficient and give it edge over HDD but I believe its high cost is the main factor which is holding the users back otherwise my laptop might be having SSD. &nbsp;I would like to have feedback from my colleagues on this particular point.
Regards,,,
Numan
References:
http://www.pcmag.com/article2/0,2817,2404258,00.asp [Online]. (Accessed on 30 Sept 2014)

	              
	              
Thanks Christopher,
You stand correct. The cost of a regular HDD is cheaper than a Solid-State-Disks (SSD). But what if I challenge you to look beyond the cost factor or to look at cheaper from a different perspective. Look at cheaper in terms of performance. Look at cheaper in terms of manufacturing cost and power saving options.

Performance
We cannot escape the know fact the HDDs have read write heads, which ultimately means that there will be delays in terms of reading and writing data which proves as an added advantage of SSDs which do not have this functionality. Thus, SSDs reduces latency and provides way better performance as compared to HDDs. What many Information Technology (I.T) departments do, is when there is an issue in terms of performance, they basically throw more disks at the problem, which was hinted in the article by Eric Savitz, and James Candelaria, 2010. This in the long term may not only be counter productive but also costly to replace or to keep adding HDDs to a server perhaps. While you can pay the cost for a good SDD and experience way better performance. Thus proving to be more economical and cost effective in the long term.

But lets turn our eyes to another view. How about the 'down-time' of the department when a server slows up, causing users are spend minutes in front of their desktops waiting to get some actual work done. Lets look at the delay in each individual performance which may also lead to that user or users working overtime, which is an added cost for the company. Sounds cheaper?

Energy reduction
In cases where we may have many HDDs together, issues of energy consumption comes into play, in such cases it may be cheaper to reduce energy consumption buy using a SSD instead. Which was noted in the article by YUHUI DENG (2011), “SSDs provide a very good opportunity to build a cool storage system by leveraging their exceptional high performance.” Thus, SSDs can aid in the reduction in energy associated costs.




Architecture
Because of its architecture, SSD are usually smaller than HDDs, it can lead to the production or smaller devices which may ultimately be cheaper to produce. Additionally, many devices such as flash drives, cell phones, cameras and tablets use solid state technology, (Computer Structures — Lecture Notes, page 3), thus we can conclude that this technology can be one of the driving factors behind many of our devices today. 

While it may be cheaper to purchase a HDD which may offer more capacity, benefits of SSD such as performance, energy reduction and architecture, may prove SSD to be cheaper in the long term.

References:

Eric Savitz, and James Candelaria , (2010), Rebuttal: Yes, Solid-State Drives Will Replace Many Hard Drives&nbsp;[Online] Available at:
http://eds.b.ebscohost.com.ezproxy.liv.ac.uk/eds/detail/detail?sid=86fa44b2-ba9b-4499-9d01-50a371eca51a%40sessionmgr113&amp;vid=0&amp;hid=104&amp;bdata=JnNpdGU9ZWRzLWxpdmUmc2NvcGU9c2l0ZQ%3d%3d#db=edb&amp;AN=79541668 (Accessed on September 30, 2014)

YUHUI DENG , (2011), What is the Future of Disk Drives, Death or Rebirth? Pages 22-24 [Online] Available at:
http://eds.a.ebscohost.com.ezproxy.liv.ac.uk/eds/pdfviewer/pdfviewer?sid=dfdb733e-f4a6-4396-8b92-4e7e4db39e58%40sessionmgr4004&amp;vid=1&amp;hid=4205 (Accessed on September 30, 2014)

Computer Structures — Lecture Notes, (page 3) [Online] Available at: https://elearning.uol.ohecampus.com/bbcswebdav/institution/UKL1/201480_SEPTEMBER/MS_CKIT/CKIT_501/readings/UKL1_CKIT_501_Week04_LectureNotes.pdf (Accessed on September 30, 2014) 


	              
	              Hi Terry - thanks for this.&nbsp;
Best Wishes, Craig
	              
	              Thanks Bo, Thanks Terry
Best Wishes, Craig
	              
	              Object-Based Storage Devices

OSD: The Basic Concept

An OSD is analogous to a logical unit. Unlike a traditional block-oriented device providing access to data organized as an array of unrelated blocks, an object store allows access to data by means of storage objects. A storage object is a virtual entity that groups data together that has been determined by the user to be logically related. Space for a storage object is allocated internally by the OSD itself instead of by a host-based file system. OSDs manage all necessary low-level storage, space management, and security functions. Because there is no host-based metadata for an object (such as inode information), the only way for an application to retrieve an object is by using its object identifier (OID). The following figure contrasts the data structure of a traditional block-based disk with that of an object-based disk.


The collection of objects in an OSD forms a flat space of unique OIDs. Virtual file hierarchies can be emulated by rearranging pointers to objects, as shown here.

The object is the fundamental unit of data storage in an OSD. Each object is self-contained, consisting of user data, an OID, metadata (the physical location of blocks that constitute an object), and attributes, as shown here.


Advantages

Object storage systems are becoming a viable alternative to scale-out network-attached storage (scale-out NAS) because of their unlimited scalability, lower emphasis on processing and high-capacity networks, access via Internet protocols rather than storage commands, custom metadata and off-the-shelf component-compatibility.

Abbreviations:



OSD&nbsp;&nbsp;&nbsp; -&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Object-Based Storage Devices

OID&nbsp;&nbsp;&nbsp;&nbsp; -&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Object Identifier



Best Regards, Babatunde


References

Christian. B, (2007). Object-Based Storage Devices [online] http://www.oracle.com/technetwork/server-storage/solaris10/osd-142183.html#Concept (Accessed 30/08/14)

http://searchcloudstorage.techtarget.com/tip/Advantages-of-using-an-object-storage-system [online] (Accessed 30/08/14)

	              
	              Hi Babatunde.
Thanks for this feedback. Did I read correctly that the source of this information is Oracle?...
Well,&nbsp;I am not one to start a war and neither am I holding brief for SAP, but there's a complete rejoinder on the all the above 'facts' from Oracle by SAP. Look up&nbsp;http://www.saphana.com/community/get-the-facts-straight.
I sometimes try to stay away from vendor specific analysis because their interpretation may tend to be somewhat skewed in favour of their range of products.&nbsp;

Reference
Hinkel,K. 'Get the Facts Straight. Database In-Memory ≠ In-Memory Database',&nbsp;SAP HANA Jun 16, 2014 3:36 PM,&nbsp;Available at
(&nbsp;http://www.saphana.com/community/get-the-facts-straight&nbsp; &amp;&nbsp;http://www.saphana.com/docs/DOC-4701) , Accessed 30/09/2014

	                Attachment: &nbsp;SAP - Get the Facts Straight - Final.pdf (1.978 MB)
	              
	              Hi Dr Ayoola,

I can’t actually envision a possible problem with In-Memory Databases systems (IMDBs) but a question may arise: Isn’t the database just lost if there’s a system crash?

Answer: It needn’t be. Most in-memory database systems offer features for adding persistence, or the ability survive disruption of their hardware or software environment.  One important tool is transaction logging, in which periodic snapshots of the in-memory database (called “savepoints”) are written to non-volatile media. If the system fails and must be restarted, the database either “rolls back” to the last completed transaction, or “rolls forward” to complete any transaction that was in progress when the system went down (depending on the particular IMDS’s implementation of transaction logging).   In-memory database systems can also gain durability by maintaining one or more copies of the database. In this solution – called database replication – fail-over procedures allow the system to continue using a standby database. The “master” and replica databases can be maintained by multiple processes or threads within the same hardware instance. They can also reside on two or more boards in a chassis with a high-speed bus for communication, run on separate computers on a LAN, or exist in other configurations.  Non-volatile RAM or NVRAM provides another means of in-memory database persistence. One type of NVRAM, called battery-RAM, is backed up by a battery so that even if a device is turned off or loses its power source, the memory content—including the database—remains. Newer types of NVRAM, including ferroelectric RAM (FeRAM), magnetoresistive RAM (MRAM) and phase change RAM (PRAM) are designed to maintain information when power is turned off, and offer similar persistence options.  Finally, new hybrid database system technology adds the ability to apply disk-based storage selectively, within the broader context of an in-memory database. For example, with McObject’s hybrid eXtremeDB Fusion, a notation in the database design or "schema" causes certain record types to be written to disk, while all others are managed entirely in memory. On-disk functions such as cache management are applied only to those records stored on disk, minimizing these activities’ performance impact and CPU demands.

Best Regards, Babatunde


Reference

http://www.mcobject.com/in_memory_database 
	              
	              Hi Bo.
Probably the downside of things like with most new releases are felt within a short time of deployment and gauging on customer feedback. I looked this up and found reference to this article (Wailgum,T) 2012 that looks at some of the pros and cons and makes additional reference to an independent Gartner report that seems not to be available on their site any more.
SAP have also tried to counter any negative feedback given by their competitors in an update on their side (Get the Facts Straight Database In-Memory ≠ In-Memory Database).
My thoughts however are that being a first mover always has its risks, but it appears that SAP have moved forward with their new architecture with the view to addressing issues as they come. I believe being a dominant vendor in this space, SAP would not want to compromise customer satisfaction especially in the picky Enterprise software area. Competitors nontheless seem to also have picked up on the trend with their variation of the same, hence Babatunde's reference to the Oracle solution.&nbsp;
References:
Wailgum, T. 'Gartner: The Pros and Cons of SAP’s Core Strategies', asug news, Oct 2012, Available at http://www.asugnews.com/article/gartner-the-pros-and-cons-of-saps-core-strategies,&nbsp; Accessed 30/09/2014
Hinkel,K.&nbsp;&nbsp;'Get the Facts Straight Database In-Memory ≠ In-Memory Database', SAP HANA, Available at&nbsp;http://www.saphana.com/community/get-the-facts-straight, Accessed 30/09/2014

	              
	              Hi All
&nbsp;&nbsp;&nbsp;&nbsp; In memory Databases are growing in the Big Data analytics space. IMDB has&nbsp;some of the &nbsp;following advantages:



Real time processing of Big Data, typically Big data analytics done in batch
Business analytics run more efficiently and &nbsp;offer productivity gains
Data more accessible by Line of business staff rather than DBAs
Enterprises gain competitive advantages. They can keep&nbsp;real-time dashboard
New generations can process IMDB more effectively
Decrease the total cost of ownership



Reference
URL http://www.eweek.com/database/slideshows/in-memory-databases-driving-big-data-efficiency-10-reasons-why/

Robin

	              
	              Hi Anthony,
Personally since I started using the SSD, I do not see any disadvantages, it’s almost perfectly suits me, the only problems that I can raise are 
The price, which is still a bit more raise than traditional HDD
The capacity, &nbsp;which is generally inferior to traditional HDD. 
I’ve also found some others, in internet,
Disadvantages to Using a Solid State Hard Disk 

The Solid State Disk price per gigabyte is much higher than hard drives, so an upgrade to the same GB capacity can incur some considerable costs. 
While they are able to withstand movement, they are vulnerable to power loss and electrical/magnetic currents much in the same way as flash cards. 
Currently there are very few large capacity SSD models, though this is expected to change drastically over the course of the next few years. 
Flash SSD have limited write cycles. It is estimated that these write cycles will last until long after the computer is still being used, it is possible that some files could use write cycles often enough that it affects the owner/user. 
Despite requiring less power, many SSD still use more power than the standard hard drive, especially when idle. This can cause laptop batteries to use up more quickly. 

So basically, I don't have much against the use of SSD, instead I recommend its use.
Tresor Lungu
Reference:
 
solidstatestorage Inc. (2014) [Online]: solidstatestorage Inc. Available from: http://www.solidstatestorage.co.uk/adssd.html(Accessed: 30 September 2014)
 

	              
	              Hi Anthony
I don’t have that much experience with SSD’s, though I have one in my laptop, and apparently in my iPad.
Here are a few disadvanges I have found in my research of reliability:

From the article “Enterprise SSD’s” by Moshayedi and Wilison, 2008:

In an enterprise environment, you should obviously use SSD’s with an architecture designed for the purpose, i.e. not a notepad SSD. (paragraph “The performance and reliability challenge”)
Performance loss is a known issue and comes from “Extensive full data-path error detection/correction” (paragraph “Extensive full data-path error detection/correction”)
There is a potential loss of data in the process of bad-bloc management (paragraph “Bad-bloc management”


Other issues found

The disks have a feature called “Wear levelling” which is to prevent that no single block of memory is used to intensively, but over time it can cause the disk performance to decrease (ExplainingComputers and Chang et al 2013, 2.2).
“Wear levelling” can also cause problems for some encryption techniques (ExplainingComputers and Acmqueve).


When being without power they are said to have a lesser longevity than HDD’s, And yes, the disk has features to rebuild itself, but of power to long will make this process very difficult, and as I understand, perhaps even not possible.

All though the article “Anatomy of solid state” (Cornwell, Michael. 2012, p. 2) states “The NAND cell stores electrons on a capacitor indefinitely in a no-power state.”



Another issue is e.g.:

If you have a primary need for writing, like I understand that e.g. Facebook and Twitter use, then you&nbsp;should probably by SSD’s with SLC which are much faster at writhing than MLC or TLC.

References
Solidstatestorage. “Advantages and Disadvantages of the Solid State Drive vs” [Online] http://www.solidstatestorage.co.uk/adssd.html (Available 2014-09-30)
ExplainingComputers. “Explaining Solid State Disks”. [Online]. http://explainingcomputers.com/ssd_video.html (Accessed 2014-09-30)
ExplainingComputers. “Computer Storage”. [Online]. http://explainingcomputers.com/storage.html. &nbsp;(Accessed 2014-09-30)
Cornwell, Michael. 2012. “Anatomy of a Solid-State”. doi:10.1145/2380656.2380672. [Online] http://eds.b.ebscohost.com.ezproxy.liv.ac.uk/eds/pdfviewer/pdfviewer?vid=2&amp;sid=a4612a4c-9e77-4fed-b8b1-74ec37fb1df9%40sessionmgr112&amp;hid=110 (Accessed 2014-09-30) 
Moshayedi, Mark. Wilkison, Patrick. 2008 , “Enterprise SSD’s”. 2008. Volume 6, issue 4. [Online] http://queue.acm.org/detail.cfm?id=1413263 (Accessed 2014-09-30)
Chang et al. Chang, Li-Pin et al. 2013. "An Adaptive, Low-Cost Wear-Leveling Algorithm for Multichannel Solid-State Disks",&nbsp;ACM Transactions on Embedded Computing Systems,&nbsp;vol. 13, no. 3, pp. 55:1-55:26. [Online] http://eds.b.ebscohost.com.ezproxy.liv.ac.uk/eds/pdfviewer/pdfviewer?vid=5&amp;sid=a4612a4c-9e77-4fed-b8b1-74ec37fb1df9%40sessionmgr112&amp;hid=105. (Accessed 2014-09-30)
	              
	              Thank you Terry

For the further explanaition of your position, the other Dutch Bram quote and the joke:)

Greetings from Bram
	              
	              Hi Tanisha,

That's a much better answer , thank you! I did think your initial answer was a bit too general which is why I fired the question back. I was hoping you had not simply misread the referenced link.

I completely agree with you the Gross potential cost across the broad spectrum of the industry it would be place could be anywhere from hundreds to tens of thousands in a cost difference based on downtimes and repairs and taking in staffing wage costs etc. The only thing I'd say is that this could be thrown at any HDD solution whether SSD or HDD if the correct disaster recovery protocol is not in place and provisions put aside for such events then the costs could spiral in any direction. For example you could have a medium size business with a couple of offices which uses simple client server architecture and all machines are backed up but no disaster recover in place and they use standard HDDs . The cost would pretty much be the same whether they lost SSDs or HDDs through corruption or failure.
However its hard to compare in such situations because you could have a large SSD farm with adequate plans in place. These SSDs could fail more often than HDDs but because you know they do you could have processes and staff in place with warm DR recovery where costs are kept at a minimum compared to the medium business with HDDs because of future forward thinking and planning was ignored.
It's one key thing in business that many companies don't do enough of to protect the data and cost.
If you live on a boat, you'd be silly not to learn how to swim and keep SOS flairs and a life raft.
It doesn't matter if the boat is worth 1million or 1thousand, drowning is drowning.

Same with IT. You can buy quick SSDs and get the gains. but make sure you put in place the worst case situation plans in case something happens.
Your right when you sum up though, that right now this is the case but in a few years time.. SSDs will be cheaper than HDDs and more reliable so the choice will be an automatic one to make, the benefits will all be there.

ta
Chris





	              
	              Hello Craig,
That the cache is moving more close to the CPU and in future would be integrated in it is indeed a promissing thought. That the cache is voatile has also advantages as it is emptied totally and for example every reboot starts with a zero state. Very handy when the system freezes. Otherwise you will have to create processes that can flush that cache which can also complicate things more.
I have also other (theoretical) problems when the cache memory is not a fixed allocation of optimised cache memory. This could give rise to problems with (inherently) different processing using the same memory. This needs to be co-ordinated and if that not happens in the architecture it only opens an extra oppertunity for human mistakes. Making things not possible in architecture can also protect the integrity of the system.
The already in this discussion thread introduced hybrid concept is perhaps more compelling. It could allow for example a core hard embedded volatile memory to extend capacity by allocating extra cache on the non-volatile memory, for example for processes involving large databases.
Hope this helps,
greetings from Bram

	              
	              Hi Anthony
My findings for your question on “possible problems or issues with IMDBs” for now are:

The cost of main memory is very expensive, perhaps too expensive for most. Though for some it’s affordable. “A one-terabyte..large; 10TB..huge. A terabyte..today costs less than $ 30.000. (Savage 2014. p16)
The growth of data is increasing faster than “main memory is getting cheaper”, hence the companies will probably not achieve a 100% IMDB, but more likely a combination/hybrid solution. “For instance, the gaming company Zynga has amassed around five petabytes in its data warehouse.” (Savage 2014, p16)
Main memory (DRAM) is volatile; hence everything in the database in memory will be lost in case of a power outage. This can helped by some backup system, but that in turn can slow the system down. Another possible solution can be newer non-volatile solutions like e.g. PFRAM, and of course systems running in parallel (Plattner). The consequences from a breakdown could be very expensive, e.g. for at transaction based system that’s dependent (faster than the competitor) on fast data response. (Savage 2014)
And apparently there is no sure way to handle distributed databases. (Savage 2014. p17)

Ressources:
Plattner, Hasso. “SAP’s Hasso Plattner on Databases and Oracle’s Larry Elison”. [Online]. http://www.youtube.com/watch?v=W6S5hrPNr1E (Accessed 2014-09-30)
Savage, Neil. &nbsp;Sept. 2014. “The power of memory”. DOI:10.1145/2641229. Vol. 57. No. 9. P. 15-17 [Online] http://eds.b.ebscohost.com.ezproxy.liv.ac.uk/eds/pdfviewer/pdfviewer?vid=1&amp;sid=f9cbbd9b-a2f2-45d9-8aa3-1272a815f00c%40sessionmgr113&amp;hid=109 (Accessed 2014-10-01)
Best wishes
Bo W. Mogensen
	              
	              Thanks Bram
Ok, interesting - and this is now really testing my understanding. My question would be, in the future world of a non volatile memory hierarchy, why would you actually need to flush the cache?&nbsp;
Also, following Anthony's question on large scale in-memory database schemes (IMDB), I have been trying to research and get my head around it. It appears that there is something called NVDIMM, which is in-line non volatile DRAM. I cant seem to find many issues highlighted apart from concerns over skills and resourcing. However, are you aware of NVDIMM and is this where you would add your extra cache for processes involving large database?
I hope I am not confusing the issue and if I'm not on the same page, am I in the same book :-)
Best Wishes, Craig
	              
	              Hello Class,As our usual way of rounding up the week's activity, please provide a short summary of the key 'Computer Structures' lessons learnt in week 4, from your perspective.Anthony
	              
	              Hello Class,Phase-Change RAM (PCRAM), Resistive RAM (RRAM) and Magnetic RAM (MRAM) are some of the emergent SCM (Storage Class Memory) technologies used for NVM (Non-Volatile Memory). How do these devices actually store binary data, and how do they compare, performance-wise, with standard DRAM/SDRAM devices?&nbsp;Anthony
	              
	              Hi Anthony
In memory database schemes certainly appear to be a good step forward for fast database access and retrieval. 
Nowadays, everyone is looking for information much faster. It is noted by Bloor (n.d., cited in Vizard, 2012) “If you can make the database independent of disk, we’re talking about making applications a thousand times faster than they are today,” he said. “Just about every IT organization is going to take notice of that.” 
I have been unable to research anything that means this potentially represents a problem – in fact, according the Campbell (n.d., cited in Hall, 2012), IMDB’s are reaching a tipping point whereby companies are needing to deal with their increasing crush of data. This could however present challenges around skills shortages. &nbsp;Furthermore, Hallenbeck (n.d., cited in Hall, 2012) who is SAP’s senior director of HANA (High Performance Analytics Appliance) innovation suggests this is opening up a new world of opportunities, and is surely representing an increase in job opportunities. 
Best Wishes, Craig
References
Vizard, M. (2012). The Rise of In-Memory Databases [Online]. Available from: http://news.dice.com/2012/07/13/the-rise-of-in-memory-databases/ &nbsp;(Accessed 30 September 2014)
Hall, S. (2012). Rise of In-Memory Databases Impacts Wide Range of Jobs [Online]. Available from: http://news.dice.com/2012/05/04/in-memory-database-jobs/ (Accessed 30 September 2014)
	              
	              Hi Anthony,
I would like to comparing Object-based Storage Devices to two traditional technologies in database, transaction and stored procedure.
Database Transaction and Object Storage
A traditional database transaction can make sure the data to be atomic, consistent, isolated and durable (ACID). This is quite close to the storage of objects. But a transaction doesn't have any knowledge about the behaviour of the data it's protecting. And the atomic granularity level is often on database hierarchy rather than meaningful objects.
Stored Procedure and Object Storage
With traditional database, we can also define behaviours for the data. This can make the data quite close to the object concept we use in the object storage. But Stored procedure has many problems like hard to maintain and bad compatibility. Especally, the unclear definition of the role of stored procedure quite often lead to the comfusing of the data logic with the business logic, which often lead to a maintenance nightmare. With object storage, the boundary become must clearer.
br, Terry


br, Terry
	              
	              Hi Anthony
According to Crump (2011), Storage Class memory is a new form of storage created out of flash-based NAND that can provide an intermediate step between high-performance DRAM and cost-effective HDDs – providing read performance similar to DRAM and write performance that is significantly faster than HDD’s.
Crump (2012) goes on the state that most flash-based devices connect to a server via a traditional storage protocol (such as SATA or SAS) however, the most ideal connectivity would be using PCIe which connects the flash-based storage in direct connection with the CPU. As a result, this removes the traditional bottlenecks whilst providing similar speeds to that of DRAM memory but without the need for legacy protocol translation.
It appears that certain types of Storage Class memory work similar to that of Flash, in that they modulate the electron charges however, if you take Phase-Change RAM (PCRAM) for example, it exploits the unique behaviour of their materials, namely chalcogenide glass. Phase-change memory relies on materials that change from an amorphous structure to a crystalline structure when an electrical pulse is applied. The material has high electrical resistance in its amorphous state and low resistance in its crystalline state thus changing their properties and allowing the storage of information to that of binary 1’s and 0’s (Kurzweil News, 2013).
Best Wishes, Craig
References
Crump, G. (2011). What is Storage Class Memory [Online]. Available from: http://www.storage-switzerland.com/articles/entries/2011/12/13_What_Is_Storage_Class_Memory.html (Accessed 1 October 2014)
Kurzweil News. (2013). Will phase-change memory replace flash memory? [Online]. Available from: http://www.kurzweilai.net/will-phase-change-memory-replace-flash-memory (Accessed 1 October 2014)
Wikipedia. (2014). Phase Change Memory [Online]. Available from: http://en.wikipedia.org/wiki/Phase-change_memory (Accessed 1 October 2014)
	              
	              Hello Craig,
We are defenitely in the same book. Probably the problem is that the last chapters are not written yet or we did not finish reading it:)
NVDIMM looks like its addressing the use case of the databased or any other process that would benefit from extendable cache allocations. Coming back to 'our' discussiont this seems to introduce an extra hyarchy in the storage domain again.&nbsp;
I need to study this more to get an idea if this is related to the concerns related to skills and recources you mention. Also to see if these concerns would also apply to the simplier hyarchy where one also needs to implement the flexible allocation of memory.
Greetings from Bram
	              
	              Hi Dr. Ayoola and Colleagues,
This week has been quite challenging for me. Partly because I'm traveling in Bangkok Thailand since last Sunday, partly because the assignments cover broad areas, and I needed to do a lot of studies. So I have to give up touring around the city but to stay at hotel room doing my studies. I, again, feels it's a fruitful week, without any regrets.
This week, we have been one step closer to the core topic of computer science, abstraction. But before abstraction, I studied something very detail, the data storage. The thinking and discussion triggered by Dr. Ayoola about NVM reveal a whole new world of computer architecture. I learned:

The current and expected features of NVM
The possible impacts in the future by NVM
Simple is better
At the end of the discussion, I realize the great potential of 3D printing in processor and memory design.

Another topic I dived into is virtualization:

studied the virtualization on several different levels, hardware, OS, network.
spent several hours installing and playing with Docker and "container"
Summarized the horizontal and vertical abstractions in the computer system

In retrospect, I found the assignments usually cover a big range. The study might just spread to a large area and end up being very shallow. So I would try to focus on several points and dive deeper in my assignments in the future.
br, Terry
	              
	              Hi Ricardo,
Yes, I also think a simplified memory hierarchy will provide outstanding performance. Simple struture might not result in fast system directly, but a simper structure is easier to optimize.
br, Terry
	              
	              Dr. Anthony, &nbsp; Object-based Storage Device (OSD) basically stores data as objects. It uses address space to store data and there is no hierarchy of directories and files involved. With this storage system, objects are identified by a unique ID. OSD usually comprise of private network, nodes and storage. The nodes are the server running the OSD operating environment. This node provides storage management and retrieval services. They are usually have two services, metadata and storage service; metadata service usually being responsible creating object ID from a file. These nodes may also have other attributes and are connected to the storage through the internal network. While “the application server accesses the node to store and retrieve data over the external network.” John Wiley (2012).&nbsp;     The OSD was originally intended to replace storage interface such as SCSI, ATA and IDE, thus OSD the storage media remains the same, however the interface is different. &nbsp; &nbsp;The prime advantage of this system includes:  Improvement in Data sharing - this is due to the high-level interface and data attributes Scalability – reduce processing requirements and overhead, management of more devices etc. Clarence J M Tauro, (2012) Increase Security- it offers improved security such as encryption, network security among others Storage Management – e.g. it manages free space and does automatic backups Intelligence – objects can learn and become aware of their environment and as such resources are allocated better, Clarence J M Tauro, (2012)  &nbsp; References John Wiley &amp; Sons,&nbsp; 2012, ‘Information Storage and Management: Storing, Managing, and Protecting’, [Online] Available at: http://books.google.gy/books?id=PU7gkW9ArxIC&amp;pg=SA8-PA1&amp;dq=Object-based+Storage+Devices&amp;hl=en&amp;sa=X&amp;ei=G2MrVLqCEbiPsQSr6oKIBg&amp;ved=0CDMQ6AEwBQ#v=onepage&amp;q=Object-based%20Storage%20Devices&amp;f=false (Accessed on September 30, 2014) &nbsp; Dingshan He, (2006), ‘Data Management in Intelligent Storage Systems’, [Online] Available at: http://books.google.gy/books?id=BnBwdf_3aPcC&amp;pg=PA5&amp;dq=Object-based+Storage+Devices&amp;hl=en&amp;sa=X&amp;ei=G2MrVLqCEbiPsQSr6oKIBg&amp;ved=0CC4Q6AEwBA#v=onepage&amp;q=Object-based%20Storage%20Devices&amp;f=false (Accessed on September 30, 2014) &nbsp; Clarence J M Tauro, N Ganesan &amp; Anjan Kumar RS, (2012), A Study of Benefits in Object Based Storage Systems, [Online] Available at: http://research.ijcaonline.org/volume42/number8/pxc3877773.pdf (Accessed on September 30, 2014) 
	              
	              According to Oracle TimesTen (In Memory Database) documentation “Oracle TimesTen In-Memory Database operates on databases that fit entirely in physical memory”
So what we understand is with an IMDB, the whole dataset fits entirely in physical memory. If we take this sentence and call it the first of our fundamental statements about IMDBs: but with this statement we come dangerously close to the conclusion that an IMDB is simply a normal DB which cannot grow beyond the size of the chunk of memory it has been allocated.
Back to Oracle TimesTen (In Memory Database) documentation “Let takeTimesTen is designed with the knowledge that data resides in main memory and can take more direct routes to data, reducing the length of the code path and simplifying algorithms and structure.”
This is more like it. So an IMDB is faster than a non-IMDB because there is less code necessary to manipulate data.
What are the challenges for databases running in DRAM? 
1)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Scalability: If you impose a restriction that all data must be located in DRAM then the amount of DRAM available is clearly going to be important. Adding more DRAM to a server is far more intrusive than adding more storage, plus servers only have a limited number of locations on the system bus where additional memory can be attached.
2)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Price: cost is important, because DRAM is far more expensive than storage media such as disk or flash.
3)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; High Availability: is also a key consideration, because data stored in memory will be lost when the power goes off. Since DRAM cannot be shared amongst servers in the same way as networked storage, any multiple-node high availability solution has to have some sort of cache&nbsp;coherence&nbsp;software in place, which increases the complexity and moves the IMDB away from the goal of reducing the length of the code path and simplifying algorithms and structure.
References:
http://flashdba.com/2012/10/10/in-memory-databases-part2/
http://docs.oracle.com/cd/E21901_01/doc/timesten.1122/e21631/overview.htm#TTCIN119
	              
	              According to Oracle TimesTen (In Memory Database) documentation “Oracle TimesTen In-Memory Database operates on databases that fit entirely in physical memory”
So what we understand is with an IMDB, the whole dataset fits entirely in physical memory. If we take this sentence and call it the first of our fundamental statements about IMDBs: but with this statement we come dangerously close to the conclusion that an IMDB is simply a normal DB which cannot grow beyond the size of the chunk of memory it has been allocated.
Back to Oracle TimesTen (In Memory Database) documentation “Let takeTimesTen is designed with the knowledge that data resides in main memory and can take more direct routes to data, reducing the length of the code path and simplifying algorithms and structure.”
This is more like it. So an IMDB is faster than a non-IMDB because there is less code necessary to manipulate data.
What are the challenges for databases running in DRAM? 
1)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Scalability: If you impose a restriction that all data must be located in DRAM then the amount of DRAM available is clearly going to be important. Adding more DRAM to a server is far more intrusive than adding more storage, plus servers only have a limited number of locations on the system bus where additional memory can be attached.
2)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Price: cost is important, because DRAM is far more expensive than storage media such as disk or flash.
3)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; High Availability: is also a key consideration, because data stored in memory will be lost when the power goes off. Since DRAM cannot be shared amongst servers in the same way as networked storage, any multiple-node high availability solution has to have some sort of cache&nbsp;coherence&nbsp;software in place, which increases the complexity and moves the IMDB away from the goal of reducing the length of the code path and simplifying algorithms and structure.
References:
http://flashdba.com/2012/10/10/in-memory-databases-part2/
http://docs.oracle.com/cd/E21901_01/doc/timesten.1122/e21631/overview.htm#TTCIN119
	              
	              Hi Dr Anthony,
This week was intereting. It was great to learn and delve deeper into NVMs technology; cuurent and future. As well as understand the impacts of such technology.
It was also great to go back and look at what we use everyday; our current HDDs and SSDs we use &nbsp;without giving much thought. &nbsp;For instance &nbsp;i was unaware of the write limitations on SDDs probably because I haven't kept one for that long where I start to experience issues.
It was also interesting to look at virtualisation, in comparison with the traditional structure. We use VMs at my organization but I had no idea how it works.
Thak you for an interesting week.&nbsp;
kind regards,
Belinda
	              
	              Hi Anthony,
This week has been brilliant. Extremely challenging, intellectually. But brilliant.
We have learned about computer architecture, with a particular emphasis on storage during the weekly discussions, focused on volatile and non-volatile memory and memory hierarchies. In addition to this, the follow on questions have really opened up so much more about computer architecture.
Our hand in assignment was structured around Client Server and Virtualisation Architectures. 
Another great week with some fantastic discussion posts, which have been really insightful as well as educational.&nbsp;I have really enjoyed thinking beyond the boundaries of possibilities – although I haven’t specifically referenced this during any discussions, when researching memory, I have been amazed at the collaboration between quantum physics and computer science, particularly when you consider the evolution of how memory works, using electrons of nanometre proportions. The fact that we are developing transistors that are 22 nanometres (nm) is quite unbelievable. It is expected that this will be at 5nm by 2020, with some developments reaching 7nm already.
Just to help put this into perspective, during my research I found that a nanometre is a billionth of a meter. The article said that it was one millionth of a millimeter. A human hair is around 75,000nm in diameter.&nbsp;The relationship between a nanometer and that hair is similar to the relationship between one mile and an inch – one mile is 63,360 inches (Handy, 2011).&nbsp;
Best Wishes, Craig
References
Laureate Education. 2014. Computer Structures – Lecture Notes and Online Video – Week 4: Machine Architecture [Online]. Available through the programme classroom (Accessed 25 September 2014]&nbsp;
Brookshear, J. G. (2012). Computer Science An Overview. 11th ed. Boston Massachusetts: Pearson. Addison Wesley
Handy, J. (2011). How Big is a Nanometer? [Online]. Available from: http://www.forbes.com/sites/jimhandy/2011/12/14/how-big-is-a-nanometer/ (Accessed 27 September 2014)
	              
	              Hi Chris,
&nbsp;
You were right; my initial response was a bit general and short. However in my second response, the scenarios were given with the assumption, that conditions are the same, with the exception of the drives being used. That is, both conditions have disaster recovery plans in place. Given such cases, I believe that the Solid-State Drive (SDD) implementation may, in the long term, be a cheaper and more effective&nbsp;strategy. 
&nbsp;
Thanks
Tanisha

	              
	              
Hi Numan,
 I do agree that if you cannot trust the device holding your information then the rest has less importance. However, I would not say that performance is not everything. I guess it really depends on what you are doing with the data. In some cases losing data may not be a big issue. If you look at the lossy data compression technique (Bookshear 2011), you could imagine that there are scenarios, even business ones, where losing data is not necessary a bad thing and where performance is more important.
As I work for a software company we have QA labs. These labs data is unimportant because it can be quickly replicated or rebuild (we have enough automations for such events), however the performance is the most critical factor. The QA labs perform performance tests of our software, these tests are used to calculate the requirements and benchmark our applications, as many of them are CAD applications such information is critical to our customers.
Best Regards,
Augusto.
&nbsp;
References
Bookshear, J. G. (2011) '1.8 Data Compression' in Computer Science: An Overview, 11th edn, Addison-Wesley, pp. 58-63.
&nbsp;

	              
	              Hi Terry,
As a funny closing note, I guess we don't really need volatile memory even for military applications if we can have self-destructing memory:&nbsp;http://gizmodo.com/self-destructing-ssds-will-nuke-themselves-if-you-text-1640733628?utm_campaign=socialflow_gizmodo_facebook&amp;utm_source=gizmodo_facebook&amp;utm_medium=socialflow
Regards,
Augusto.
	              
	              Well, I must say this was a very hectic week in office and also the study related tasks but for learning point of view it was great week. There were new concepts which were new to me like IMDB (In Memory Database) and upcoming trends which are going to change the computer architecture. Also this week assignment for writing paper was helpful to practice and follow the standards of professional writing and also the subject to compare client/server &amp; virtualization architectures improved my knowledge and understanding of the subject.
Regards,,,
Numan
	              
	              Hi all
Indeed this week has been interesting and informative, both the research but certainly also following the discussions.
In this week we have been through many exciting issues, some more intensively than other. Here are a few of the learning points.

Development is racing on many fronts, and perhaps the fast increasing amounts of data are pulling on even more rapid development
We dug into the effects of memory, and this was clearly a much bigger issue than I have previously thought about.
It’s probably naïve to wish for a “single” level of memory, and rather we should just wish for faster memory as close to CPU speed as possible, and a as simple memory hierarchy as possible.
Pros and cons about the SSDs
The need for in-memory databases is growing.
Pros and cons for Virtualization
Refreshing and new knowledge about bits, bytes, hard drives etc.

Best wishes
Bo W. Mogensen
	              
	              Hi Dr Ayoola,
As Terry this week has been quite challenging for me too. Since last week I was on vacation and could not participate to the class discussion I felt I missed something, so I had to catch up with that, plus coming back from vacation I have more to do at my work than usual. Plus I always do the exercises in the textbook and easily get swayed by them into other details as I google more information.
The topics were very broad, but as always interesting, I am intellectually challenged and that’s feels extremely good.
For me this week was a good refresh into computer architecture and CPU memory details, the last time I touched those subjects was around 1998. I learnt that I do not remember everything and there is always something new to learn and re-learn. I learnt details about new non-volatile memory technologies like SSD and PCM. It is amazing how little we have gone in the computer world, we are storing data with crystals and electrons! No wonder there’s a buzz around quantum computing. Thinking how far we can still go it is overwhelmingly impressive.
And I bookmarked Docker, that’s a start.
Best Regards,
Augusto

	              
	              Hi Craig, If I'm not wrong an atom (hydrogen) is around 0.1 nanometer! So they are making transitors that can hold 220 atoms. Getting to hold 50 atoms it's going to ridiculously small.   Best Regards, Augusto
	              
	              Hi All,

Woaw what a week this has been. I feel like it has been none stop busy busy this week. I feel I'm really starting to get to grips with the routine better now. It's a little more intense with the double assignments each week than I first thought it would be simply because I like to really read as much as possible and make sure of my work as much as time permits.

I learned alot about future of NVM products and the potential such as PCM.
I liked the range of conversation between the students it can be both enlightening and funny.
I learned more about SSDs above and beyond what I already knew and the failure rates which are interesting.

I'm looking forward to the rest of the course.
Chris

	              
	              That's unbeleavable when you think about it in that way...

Surely things can't get any smaller than that... we're talking about atomic level transistors..... crazy stuff

It's amazing that things have gone this far but there must be a physical ceiling at some point were it's just not possible to get any smaller.....

	              
	              Hi all,Have a nice day!Equating “database management system” with “big” is justified, generally speaking. Even some embedded DBMSs are megabytes in code size. This is true largely because traditional on-disk databases – including some that have now been adapted for use in memory, and are pitched as IMDSs—were not written with the goal of minimizing code size (or CPU cycles). As described above, as on-disk database systems, their overriding design goal was amelioration of disk I/O.In contrast, a database system designed from first principles for in-memory use can be much smaller, requiring less than 100K of memory, compared to many 100s of kilobytes up to many megabytes for other database architectures. This reduction in code size results from:1. Elimination of on-disk database capabilities that become redundant for in-memory use, such as all processes surrounding caching and file I/O2. Elimination of many features that are unnecessary in the types of application that use in-memory databases. An IP router does not need separate client and server software modules to manage routing data. And a persistent Web cache doesn’t need user access rights or stored procedures3. Hundreds of other development decisions that are guided by the design philosophy that memory equals storage space, so efficient use of that memory is paramountBR,KingTan YuReferenceberdeen Group (2012);In-memory Computing: Lifting the Burden of Big Data; Available on: (http://spotfire.tibco.com/assets/bltaeee45a3d9ccd777/aberdeen-in-memory-analytics-for-big-data.pdf) (Accessed on: 01-Oct-2014)
	              
	              
	              
	              
	              
	              HI all,
&nbsp;
The simple reorganization for NVM banks can selectively sense and buffer only a small portion of data from NVM array. Since reads are non-destructive in NVM, we swap the column multiplexer (part of the I/O gating circuitry) and the row buffers in the data path. This allows us to reduce the row buffer size by sharing each basic building block of the new, smaller buffer among multiple columns in the same row. The net result is reduced dynamic energy waste compared to DRAM systems in which we must dedicate buffer resources to all columns in a row with no sharing, even if only a fraction of that data is actually accessed. Unlike LPDDR2, our proposed architecture employs a single data path for both reads and writes, and allows efficient bank operation to achieve parallelism for high bandwidth. We describe a new memory access protocol for NVM devices to accommodate our new architecture, while retaining a standard pint out and signaling interface (JEDEC DDRx type, identical to DRAM). To enable system-level evaluation, we develop energy and timing models for NVM devices based on their underlying technology parameters. We evaluate main memory designs with two representative NVM technologies, PCM and STT-RAM, to cover a wide range of NVM characteristics. Our results indicate that small row buffer sizes can provide significant improvements in terms of dynamic main memory energy. Similar to other studies, we find that the contention present on multi-core CMP systems greatly reduces the effect of memory mapping schemes designed to improve locality, reducing the performance benefits of large row buffers on such systems. In addition, all evaluated systems can support reasonable five year operational lifetimes. We also find that the ability for NVM devices to reduce or eliminate several DDR timings constraints in our new protocol can enable some technologies (such as STT-RAM) to achieve better performance than DRAM.
&nbsp;
Using energy and timing models, the system-level effects of our architecture in NVM main memories for many-core CMPs in terms of energy, performance, and durability and found that:
1. NVM main memories can achieve significant energy gains with reduced row buffer size (up to 67% less energy compared to DRAM).
2. These energy improvements can be achieved without significantly affecting performance. In fact, an STT-RAM-based main memory with small row buffers can achieve slightly better performance and much better energy-efficiency than DRAM due to the relaxed timing constraints of the NVM access protocol.
3. Even on a many-core CMP system with small row buffers, an NVM main memory is able to achieve a reasonable five year lifetime, with over ten years achievable with the addition of a small, 32MB e-DRAM cache.
&nbsp;
BR,
&nbsp;
KingTan Yu
&nbsp;
Reference
&nbsp;
&nbsp;
Taciano Perez, César A. F. De Rose (2010); Non-Volatile Memory: Emerging Technologies And Their Impacts on Memory Systems; Available on: (http://www3.pucrs.br/pucrs/files/uni/poa/facin/pos/relatoriostec/tr060.pdf) (Accessed on: 01-Oct-2014)

	              
	              Hi all,For IT professionals used to working with traditional file systems for their entire careers, object-based storage is like a whole new dimension they never knew existed. The concept of object storage can require a rewiring of the thought process concerning how data can be indexed and accessed in storage environments.That’s because object-based storage systems throw out the existing file system approach. Forget hierarchies, which become ever more complex and non-performing as they grow. The object approach dispenses with these and instead uses a structurally flat data environment.Object storage devices offer the ability to aggregate storage into disparate grid storage structures that undertake work traditionally performed by single subsystems while providing load distribution capabilities and resilience far in excess of that available in a traditional SAN environment.In other words, object storage devices operate as modular units that can become components of a larger storage pool and can be aggregated across locations. The advantages of this are considerable as distributed storage nodes can provide options to increase data resilience and enable disaster recovery strategies.To better understand the object structure, think of the objects as the cells in a beehive. Each cell is a self-contained repository for an object ID number, metadata, data attributes and the stored data itself.&nbsp; Each cell is also a separate object within the usable disk space pool.Physically, object-based storage arrays are comprised of stackable, self-contained disk units like any other SAN. Unlike traditional storage arrays, object-based systems are accessed via HTTP. That access method plus their ability to scale to petabytes of data make object-based systems a good choice for public cloud storage. An entire object storage cluster of disparate nodes can be easily combined to become an online, scalable file repository.Mike Mesnier, Carnegie Mellon and Intel,Gregory R. Ganger, Carnegie Mellon Erik Riedel, Seagate ResearcObject-Based Storage; (online : http://www.storagevisions.com/White%20Papers/MesnierIEEE03.pdf) (Accessed on: 01-Oct-2014)
	              
	              Hi Augusto,
LOL, I hope it's not true. Otherwise security will scan everybody's computer before the get in the bus.
Imagine that one day, your grand kid runs to you and says "grandpa, I finally invent a memory chip that automatically remove content when no power supply!"
br, Terry
	              
	              
OSD storage as an object can be flatted compared to traditional file system as outlined above. Enabling data optimization. A good example is in Big data usage for unstructured data. (Eric Slack 2011)
Reference
Eric Slack 2009 URL http://searchitchannel.techtarget.com/tip/Object-storage-Object-based-storage-devices-challenge-file-systems-for-unstructured-data-sets



	              
	              This week was challenging with the two papers Client Server/Virtualization Architectures and NVM
I did like the review of&nbsp; bits, bytes and&nbsp;basic hard drives technology lessons, been a while since I went back to the basics
I did like the&nbsp;in-memory databases and OBS discussion
robin&nbsp;
	              
	              Hi Dr. Anthony,

The very design of IMDB schemes are to enhance and optimize the processing of critical applications that requires fast and large memory resources for keeping transactions current and delivering the best performance possible.
I cannot foresee a scenario where IMDB's will where this type of mechanism will be problematic to applications that utilize these schemes. Apart from the current implementations that uses volatile DRAM technology that will lose data in the event of power loss.&nbsp; Contrarily I believe and it is well documented that implementation of a simpler memory architecture using non-volatile memory technology will provide greater capabilities for IMDB's and bring all the benefits this technology offers.
So in concluding I cannot envision any problems with IMDB's, however I look forward to the capabilities possible with new emerging technology.
&nbsp;
Regards&nbsp;

	              
	              ...and as they get smaller, my eyes are getting worse!&nbsp;

	              
	              Thanks for this KingTan
A good explanation and example of needless code to support the process for on-disk, rather than just what is needed for the executable.
Best Wishes
Craig
	              
	              Thanks for this King Tan I am reading the same paper you have referenced Another really good explanation of how they work.&nbsp;I especially like the ‘bee-hive’ analogy – I have also come across this slide, which relates to your analogy too.    Best Wishes, Craig Iren, S. and Schlosser, S. (2005). Database storage management with object-based storage devices - Seagate Research &amp; Intel Research [Online]. Available from: http://www.cs.cmu.edu/~damon2005/damonpdf/8%20database%20storage%20management%20with%20object-based%20storage%20devices.pdf (Accessed 1 October 2014)
	              
	              Hi Dr Ayoola,
This week was a good refresh into computer architecture, data processing, CPU memory details. I learnt details about new non-volatile memory technologies like SSD and PCM and their impacts. Learnt about database buffer cache, learnt more about the reliability of SSDs, also learnt more on object-based storage devices, its features and the advantages they offer in large networks. Broadened my knowledge in areas of SCM and how they store binary data. It was also interesting knowing more on virtualization, in comparison with the traditional structure.
It’s a good week and I’m looking forward to the rest of the course.
Best Regards, Babatunde
	              
	              Hi Dr. Anthony,
&nbsp;
I share the sentiments of a colleague in the discussion; this week was a bit hectic in deed. However it was quite informative. Below are some of the concepts which I learnt as it relates to machine architecture:
&nbsp;

Data storage: Analogue versus digital



Binary
Optional media
Volatile memory versus non-volatile memory
Magnetic hard disc drives
Compression
Object-based Storage Devices
Solid-State-Disks and Hard Drive Disks
Virtualisation
Bits and their storage
Main memory

&nbsp;

Data manipulation
Cache
Data flow
Stored program
Instruction format
Optional media


&nbsp;
	              
	              Thanks for the comments and summary, Terry.&nbsp;Shame about the disruption to your Bangkok holidays, by this module. &nbsp;Never mind - it is all for a good purpose!Regards,Anthony
	              
	              Thanks for the comments/summary of items studied this week, Belinda.&nbsp;Regards,Anthony
	              
	              Thanks for the insightful comments and summary, Craig, and good to hear that you found the topics in the study week enjoyable..&nbsp;Regards,Anthony
	              
	              This week’s posts dealt with issues pertaining to computer memory schemes, and their anticipated/actual impact on computer architectures and data processing.We also discussed the use of Solid-State-Disks (SSDs), In-Memory Databases (IMDBs) and Object-based Storage Devices (OSDs), as well as reviewing attributes of emerging NVM (non-volatile memory) technologies.Good discussions all round!Anthony
	              
	              Thanks for the comments and summary, Numan.&nbsp; I am glad to see that you had a productive study week.Regards,Anthony
	              
	              Thanks for the comments and summary, Bo.&nbsp;Yes, developments with computer memory have been a key driver of progress in IT.Regards,Anthony
	              
	              Thanks for the comments, Augusto.&nbsp;Yes, the Master's program is guaranteed to interfere with holiday enjoyment :-). &nbsp;I am glad to hear that you have now settled back into the "study-flow", after the vacation. &nbsp;The point you made about progress with computing is very true - we still have quite a ways to go!
Regards,Anthony
	              
	              Thanks for the comments and summary, Chris.&nbsp;Glad to hear that you are now getting used to the workload. &nbsp;I hate to upset you with this, but I have just finished teaching another Liverpool Master's module with four components each week ( Two DQs, one HIA and one Project) :-). &nbsp;Strangely, no one complained because they were already all used to the workload!Regards,Anthony
	              
	              You appear to have mis-posted without the necessary contents!
	              
	              This is again blank, Eddie - what was the problem?
	              
	              Thanks for the comments, Robin.&nbsp;Yes, we did have quite a bit of variation this week, around the main memory theme.Regards,Anthony
	              
	              Thanks for the comments/summary of items studied this week, Babatunde.&nbsp; Glad to hear that you found the topics interesting.Regards,Anthony
	              
	              Thanks for the comments and summary of items studied this week, Tanisha.&nbsp; I agree that the work is picking up pace, but this is still a little gentle compared to some of your future Master's modules :-).Regards,Anthony
	              
	              The Microsoft Windows Operating System
I currently run on Windows 8. The windows family have been a household now for almost 3 decades now. Microsoft Windows&nbsp;is a family of&nbsp;operating systems&nbsp;for personal computers. Windows dominates the personal computer world, running, by some estimates, on more than 90 percent of all personal computers&nbsp;– the remainder running&nbsp;Linux&nbsp;and&nbsp;Mac&nbsp;operating systems. Windows provides a&nbsp;graphical user interface&nbsp;(GUI), virtual memory management, multitasking, and support for many peripheral devices.
&nbsp;
The History
MS-DOS (Microsoft disk operating system)
Originally developed by Microsoft for IBM,&nbsp;MS-DOS&nbsp;was the standard operating system for&nbsp;IBM-compatible personal computers. The initial versions of DOS were very simple and resembled another operating system called CP/M. Subsequent versions have become increasingly sophisticated as they incorporated features of minicomputer operating systems.
&nbsp;
Windows 1.0 – 2.0 (1985-1992)
Introduced in 1985,&nbsp;Microsoft Windows&nbsp;1.0 was named due to the computing boxes, or "windows" that represented a fundamental aspect of the operating system. Instead of typing&nbsp;MS-DOS&nbsp;commands, windows 1.0 allowed users to point and click to access the windows.
&nbsp;
In 1987 Microsoft released Windows 2.0, which was designed for the designed for the Intel 286 processor. This version added desktop icons, keyboard shortcuts and improved graphics support.
&nbsp;
Windows 3.0 – 3.1 (1990–1994)
Microsoft released Windows 3.0 in May, 1900 offering better&nbsp;icons, performance and advanced graphics with 16 colours designed for Intel 386 processors. This version is the first release that provides the standard "look and feel" of Microsoft Windows for many years to come. Windows 3.0 included Program Manager, File Manager and Print Manager and games (Hearts, Minesweeper and Solitaire). Microsoft released Windows 3.1 in 1992.
&nbsp;
Windows 95 (August 1995)
A major release of the Microsoft Windows operating system released in 1995.&nbsp;Windows 95&nbsp;represents a significant advance over its precursor, Windows 3.1. In addition to sporting a new user interface, Windows 95 also includes a number of important internal improvements. Perhaps most important, it supports 32-bit applications, which means that applications written specifically for this operating system should run much faster.
&nbsp;
Although Windows 95 can run older Windows and DOS applications, it has essentially removed DOS as the underlying platform. This has meant removal of many of the old DOS limitations, such as 640K of main memory and 8-character filenames. Other important features in this operating system are the ability to automatically detect and configure installed hardware (plug and play).
&nbsp;
Windows 98 (June 1998)
Windows 98&nbsp;offers support for a number of new technologies, including FAT32, AGP, MMX, USB, DVD, and ACPI. Its most visible feature, though, is the Active Desktop, which integrates the Web browser (Internet Explorer) with the operating system. From the user's point of view, there is no difference between accessing a document residing locally on the user's hard disk or on a Web server halfway around the world.
&nbsp;
Windows ME - Millennium Edition (September 2000)
The Windows Millennium Edition, called "Windows Me" was an update to the Windows 98 core and included some features of the Windows 2000 operating system. This version also removed the "boot in DOS" option.
&nbsp;
Windows NT 31. - 4.0 (1993-1996)
A version of the Windows operating system. Windows NT (New Technology) is a 32-bit operating system that supports pre-emptive multitasking. There are actually two versions of Windows NT: Windows NT Server, designed to act as a server in networks, and Windows NT Workstation for stand-alone or client workstations.&nbsp;
&nbsp;
Windows 2000 (February 2000)
Often abbreviated as "W2K,"&nbsp;Windows 2000&nbsp;is an operating system for business desktop and laptop systems to run software applications, connect to Internet and intranet sites, and access files, printers, and network resources. Microsoft released four versions of Windows 2000: Professional (for business desktop and laptop systems), Server (both a Web server and an office server), Advanced Server (for line-of-business applications) and Datacenter Server (for high-traffic computer networks).
&nbsp;
Windows XP (October 2001)
Windows XP&nbsp;was first introduced in 2001. Along with a redesigned look and feel to the user interface, the new operating system is built on the Windows 2000 kernel, giving the user a more stable and reliable environment than previous versions of Windows. Windows XP comes in two versions, Home and Professional. &nbsp;Microsoft focused on mobility for both editions, including plug and play features for connecting to wireless networks. The operating system also utilizes the&nbsp;802.11x&nbsp;wireless security standard. Windows XP is one of Microsoft's best-selling products.
&nbsp;
Windows Vista (November 2006)
Windows Vista&nbsp;offered an advancement in reliability, security, ease of deployment, performance and manageability over Windows XP. New in this version was capabilities to detect hardware problems before they occur, security features to protect against the latest generation of threats, faster start-up time and low power consumption of the new sleep state. In many cases, Windows Vista is noticeably more responsive than Windows XP on identical hardware. Windows Vista simplifies and centralizes desktop configuration management, reducing the cost of keeping systems updated.
&nbsp;
Windows 7 (October, 2009)
Windows 7&nbsp;made its official debut to the public on October 22, 2009 as the latest in the 25 year-old line of Microsoft Windows operating systems and as the successor to Windows Vista (which itself had followed Windows XP). Windows 7 was released in conjunction with Windows Server 2008 R2, Windows 7's server counterpart. Enhancements and new features in Windows 7 include multi-touch support, Internet Explorer 8, improved performance and start-up time, Aero Snap, Aero Shake, support for virtual hard disks, a new and improved Windows Media Center, and improved security.
&nbsp;
Windows 8
Windows 8&nbsp;is a completely redesigned operating system that's been developed from the ground up with touchscreen use in mind as well as near-instant-on capabilities that enable a Windows 8 PC to load and start up in a matter of seconds rather than in minutes. Windows 8 will replace the more traditional Microsoft Windows OS look and feel with a new "Metro" design system interface that first debuted in the Windows Phone 7 mobile operating system. The Metro user interface primarily consists of a "Start screen" made up of "Live Tiles," which are links to applications and features that are dynamic and update in real time. &nbsp;Windows 8 supports both x86 PCs and ARM processors. 
Below are some of the reasons why I currently run on Windows 8. 
&nbsp;
Benefits of Windows 8

It is optimized for the touch devices.&nbsp;Windows 8 uses the ‘Metro’ interface which is improved for touch screen devices featuring a new&nbsp;‘Start screen’.
It supports the low-power ARM architecture.&nbsp;It has advanced security features such as antivirus capabilities and supports secure boot.
It has short boot time. Windows 8 boot time takes less than 8 seconds which is much shorter than its earlier version.
There is no need for the PC upgrade to run Windows 8.&nbsp;Any PC which is able to run Windows 7 on it can run Windows 8 and there is no need to upgrade PC.
One of the main features of Window 8 is the app platform.&nbsp;Windows Store has a number of apps that are built for Windows 8.
Windows 8 also supports Near Field Communications (NFC) printing.&nbsp;A technology which can aid in financial transactions digitally.

&nbsp;
Though there are a few drawbacks I would like to mention with my experience with Windows 8 thus far from both my personal experience and of others:
&nbsp;
Drawbacks with Windows 8

The main disadvantage of Windows 8 is overlapping of Metro and Aero User Interface.&nbsp; Switching between Metro applications and desktop applications is not user-friendly and creates confusion for the users and the developers.
There is no way to turn the home screen tiles into icons. When a number of apps are installed, then the Start screen looks garbled.
It is very difficult to swap between different screens.&nbsp;The absence of Alt-Tab function makes it difficult when working with many applications as there is no easy way to switch between programs.
Another disadvantage of Windows 8 is the Metro multitasking. In Windows 8, in the Metro interface for tablets, the screen display two applications are lined.
Metro interface works well on tablets but the User Interface is not very compatible on the desktop. There is a need of some kill-switch which can turn the Metro UI off.
Windows 8 doesn’t support any flash content on Tablet PC.&nbsp;Apparently, it has been done in order to save battery of the Tablet. It also protects our privacy and enhance the security of the Tablet.

&nbsp;
References
http://www.mswinupdate.com/windows-8-advantage-and-disadvantage/
http://www.webopedia.com/DidYouKnow/Hardware_Software/history_of_microsoft_windows_operating_system.html

	              
	              Hi Chika,
I hope you are not forgeting the Instructions given to us.
Please don't.

	              
	              Hi Dr. Ayoola, In your instruction for preparing the discussion you metioned the focus on developing and maintaining a blog site. Is this misplaced information? br, Terry   To prepare for this Discussion:   Review your weekly Learning Resources with a focus on developing and maintaining a blog site.     
	              
	              Week 5 Discussion Question – Functions of an Operating System
Craig Thomas – 3 October 2014
Currently, I am working with OS X v10.7.5 ‘Lion’ Operating System on Apple Mac hardware, a UNIX-based graphical user interface operating system developed by Apple Inc (Wikipedia, 2014).
OS X is designed to run on Apple Mac computers and has been preinstalled on all Macs since 2002 (Wikipedia, 2014). My decision to go with OS X was somewhat subconscious, in that I was actually choosing to purchase a MacBook over a traditional laptop PC. This decision was a result of the compelling nature of Apple products, which of course is largely based on their software driven approach to fantastic consumer electronic devices such as the Mac, iPhone and iPad etc, as well as the aesthetics and build qualities.
OS X ‘Lion’ runs on 64-bit Intel CPU technology and is based on Mach kernel that was developed at Carnegie Mellon University to support operating system research. The birth of the OS was from NeXTSTEP (then called OPENSTEP and a derivative from Berkeley Software Distribution family), which was the graphical, object-oriented and UNIX-based Operating System developed by NeXT (Wikipedia, 2014), a company that was founded by the late Steve Jobs, who was also the Co Founder and former ‘two-time’ CEO of Apple.
OS X ‘Lion’ was the natural upgrade path from my previous Operating System OS X v10.6 ‘Snow Leopard’. This was also a natural upgrade from the previous version, v10.5 ‘Leopard’, which was already preinstalled on the MacBook when I purchased it. 
Many of the previous versions of OS X have followed a naming convention in line with ‘Big Cats’, such as ‘Cheetah’, ‘Puma’, ‘Jaguar’, ‘Panther’, ‘Tiger’, ‘Leopard’, ‘Snow Leopard’, with v10.7 being ‘Lion’ and v10.8 being ‘Mountain Lion’. The naming conventions changed at v10.9, where Apple started to adopt famous locations in California with v10.9 being ‘Mavericks’ and the new up and coming release of v10.10 being ‘Yosemite’. 
I suppose Apple were not keen on keeping the ‘Big Cat’ theme going, and disappointingly did not go with either ‘Pink Panther’ or ‘Sabre-Tooth Tiger’ versions! I guess we look forward to perhaps seeing the ‘Golden Gate’ version, which may be the gateway to more openness and interoperability, or even the ‘Alcatraz’ version, which cannot be broken and completely Virus proof? J
OS X ‘Lion’ was the first Apple Mac Operating System that adopted similar features to those of the Apple iPhone and Apple iPad iOS Operating System, such as the use of the track-pad and gestures (Nick Meade, n.d.) and it also provides synchronization with those platforms, for example with Safari browser bookmarks. 
This functionality has opened up a whole new world to the friendly User Interface (UI) resulting in an even greater and positive User Experience (UX). This is a significant benefit, and has enabled the usage to be even more efficient and productive, especially with the virtual desktops whereby I can have many applications, documents, web pages and of course iTunes open and playing music simultaneously, and with various swipes easily be able to move from one desktop to another with complete ease for effective management of my workspace, whilst rocking it to Aerosmith J 
Although I only use my MacBook for office type applications, web browsing and listening to my music, it is suggested by Apple Inc (2014) that OS X is the desktop for open source. They go on to suggest on their website that since they first released the Darwin operating system in 1999, OS X has been closely identified with the open source community and they have worked with other pioneers to develop one of the first corporate open source licenses and built ongoing relationships with key projects such as Apache, Python, and Ruby. Today, Apple is the largest single vendor of open source software such as PHP (Apple Inc, 2014)
OS X has a layered Architecture with fundamental technologies within each layer. The lower layers provide the services on which all of the software relies, with the subsequent layers containing more sophisticated services and technologies that build on those layers (Apple Inc, 2014). See Fig.1 
Fig.1
Layers of OS X

Source: Apple Inc
As described by Apple Inc (2014), the various layers include:

Cocoa (Application) layer includes technologies such as API’s for building an application user interface, for responding to user events, and for managing application behaviour. 
The Media layer includes technologies audio-visual media including 2D and 3D graphics. 
The Core Services layer contains the services and technologies that are fundamental to Automatic Reference Counting, low-level network communication, string manipulation and data formatting.
The Core OS layer is defined for programming interfaces related to hardware and networking interfaces.
The Kernel and Device Drivers layer consists of the Mach kernel environment, device drivers, BSD library functions and other low-level components. The layer includes support for file systems, networking, security, interprocess communication, programming languages, device drivers, and extensions to the kernel. 

The Operating System provides a very strong array of usability and interoperability features for designers, and according to Dachis (2013), many designers still prefer Apple hardware and OS X for their workflow. Ultimately however, the choice may well be more of a personal preference, given the adaptability of many of the platforms available on the market today.
Personally, I have found the MacBook and OS X to be extremely adaptable, stable and reliable, and I have never needed to re-build or recover from backups during the 6 years I have owned it. 
In contrast, albeit in a time when personal computing technology was probably not as advanced, my previous experiences of PC’s and Window’s was the opposite, whereby I can recall many instances of losing my patience, as a result of the performance and operational degradation of the PC. I am not sure whether this was a result of cheap hardware components or the Windows operating system itself, but it was incredibly frustrating. The MacBook and OS X have been brilliant, from day 1.
Although less prevalent, Macs and OS X still continue to be less prone to bugs and viruses than PC’s, and most users get by without running any virus protection software. Although OS X is not immune, there is not so much of a need to stay on top of upgrades and patches for anti viruses.
Building on that theme, upgrades and maintenance tasks are also limited too, in that when they are needed, they are very straightforward to perform, over an Internet connection.
In addition, many companies are moving forward with trends such as BYOD and have developed various applications, configurations and security settings to allow you to use your OS X device in a work environment, that may traditionally be Window’s based.
If however, for other reasons you still need Microsoft Window’s, but want to keep the beautiful design aesthetics and the build quality, it is also possible to run Windows on an OS X Mac. According to Muchmore et al (2013), depending on your needs, you can do this by either setting up Apple’s Boot Camp or you can run Window’s as a virtual machine. The first option creates a partition on your hard drive that you can boot into and run Windows directly on the Apple hardware. The second option, virtualisation, is running Windows inside a container window and the underlying hardware is handled by the virtualisation software of which, the benefits are not needing a reboot and you can run Window’s side by side with OS X on the same screen. 
There is a downside to this. According to Bott (2011), there is performance degradation to this, as well as additional costs for Windows, which under license, Windows 7 cost is around $250, and VMware Virtualisation software (Fusion) is around $80. Please note; these prices may have fluctuated since 2011. 
In terms of drawbacks, although OS X does not operate the principle of a walled garden, Dachis (2013) argues that Macs don’t offer as much flexibility as a Windows PC and in particular, Apple tends to drop compatibility with older applications faster than the Windows platforms. In the same article, it is referenced that Windows is used more widely across the Globe and offers more flexibility and customisation capabilities. 
One of the biggest drawbacks for Mac’s and OS X is the cost. When reviewing the options, the lowest specification and lowest priced Mac’s are at least 25% higher in cost than the Window’s PC alternatives. This is based the comparison between a MacBook Air 11-inch 1.4 GHZ dual-core Intel Core i5 processor with 4GB memory and 128GB PCIe-based flash storage at £749, and a HP Pavillion 15-inch 1.7GHZ Intel Core i5 processor with 8GB memory and 1TB HDD at £550. I do recognise that these are not necessarily like-for-like products and are predominately based around the device itself, but worth seeing the comparison for illustrative purposes. That said, OS X upgrades tend to come either free, or for a minimal upgrade charge.
In conclusion, OS X has been great for me personally and if you are prepared to invest financially, but need additional functionality that can only run on Windows, it is possible to have OS X and Windows running side by side. 
However, I would still like to make a small number of recommendations on how OS X could be improved further, namely:

One of the first areas I would like to try to address would be the cost. As we all know, Apple Inc are making a stack of cash – I believe they could easily look to reduce their costs and in turn, this could also potentially drive more adoption of the base OS X and Mac products, bringing them much closer to mainstream users. 
Although we are beginning to see the blurring of lines between OS X and iOS, I would like to see OS X developed further to include full interoperability with iOS, to have identical features to those of the iOS for iPhone and iPad and to include touch screen technology. This would also include the seamless transfer of the comprehensive set of apps, which could be used fully on a Mac, and not just on iOS devices. Equally, to be able to use Microsoft Office for Mac products on the iOS devices too.
I would like to see better integration with cloud technology, especially for storing documents. I currently use Dropbox for this, as iCloud is restrictive (as far as I am aware) in allowing me to store documents. I understand that iCloud has been revamped in the next version of OS X ‘Yosemite’, with something called iDrive. I hope this comes with a free amount of storage space, and I’d be happy to consider using it moving forward.
Lastly, and this is more of a pet peeve for me, I’d like to be able to move the Hard Disk Drive icon off the desktop please, and be able to place it in to the dock. This is only minor I know, just a little nice to have, please?

Best Wishes, Craig
References
Laureate Online Education. (2014). Computer Structures – Lecture Notes Week 5: Operating Systems [Online]. Available from: https://elearning.uol.ohecampus.com/bbcswebdav/institution/UKL1/201480_SEPTEMBER/MS_CKIT/CKIT_501/readings/UKL1_CKIT_501_Week05_LectureNotes.pdf (Accessed 2 October 2014)
Meade, N. ed. (n.d.) OS X Lion [Online]. Softonic Review. Available from: http://os-x-lion.en.softonic.com/mac (Accessed 2 October 2014)&nbsp;
Brookshear, J. G. (2012). Computer Science An Overview. 11th ed. Boston Massachusetts: Pearson. Addison Wesley
Apple Inc. (2014). Mac Technology Overview [Online]. Available from: https://developer.apple.com/library/mac/documentation/MacOSX/Conceptual/OSX_Technology_Overview/OSX_Technology_Overview.pdf (Accessed 2 October 2014)
Apple Inc. (2011). OS X for UNIX Users – The power of UNIX. The simplicity of Mac [Online]. Technology brief. Available from: http://movies.apple.com/media/us/osx/2012/docs//OSX_for_UNIX_Users_TB_July2011.pdf (Accessed 3 October 2014)
Dachis, A. (2013). Mac vs. Windows: Your Best Arguments [Online]. Available from: http://lifehacker.com/mac-vs-windows-your-best-arguments-486125257 (Accessed 3 October 2014)
Muchmore, M. et al. (2013). How to Run Windows on a Mac [Online]. Available from: http://uk.pcmag.com/feature/15661/how-to-run-windows-on-a-mac (Accessed 3 October 2014)
Bott, E. (2011). The hidden costs of running Windows on a Mac [Online]. Available from: http://www.zdnet.com/blog/bott/the-hidden-costs-of-running-windows-on-a-mac/3673 (Accessed 3 October 2014)
	              
	              Hi Chika,
Just looking at the disadvantages that you have outlined, I believe you are able to turn off the live tiles. I haven’t done this myself (not a Window’s user) but the referenced web page below helps explain, although I am not entirely sure whether this will actually resolve your issue.
Also, according to the Microsoft website, you can also switch to your last app, by swiping from left edge of the screen. The Microsoft web page below gives access to some useful and common actions using touch. This may help? Sorry if you already know these – as I say, I am not a Window’s user, so not entirely sure whether these will actually help you?
Best Wishes, Craig
References
Pope, T. (2014). How to Turn Off Live Tiles in Windows 8.1 [Online]. Available from: http://www.gottabemobile.com/2014/04/06/how-to-turn-off-live-tiles-in-windows-8-1/ (Accessed 3 October 2014)
Microsoft (2014). Getting around your PC – Windows tutorial [Online]. Available from: http://windows.microsoft.com/en-gb/windows-8/getting-around-tutorial (Accessed 3 October 2014)
	              
	              


Apple OS X Is Unix 
Terry Yin October 4, 2014 
The main reason I chose to use OS X on this piece of over-priced Apple hardware is because it’s a Unix system. (Brookshear 2011, p.110) 
A popular opinion about Apple MacBooks is they are for the hipsters to showoff in Starbucks (GYEONG- MIN 2010, p.1-2). That is not all the facts. Apple Mac computer with the OS X running on it is a very friendly platform for most of the software developers. Well, unless you are developing Microsoft Windows desktop application. 
1 OS X as a software developer’s environment 
1.1 Unix-Like System 
The OS X operating system running on Apple Mac computers can find its root in the FreeBSD and NetBSD. The BSD (Wikipedia 2014a, Berkeley Software Distribution) systems are implementations of Unix. From 2007, OS X became a registered Unix System. The registered and not registered OSs that conform to the Single UNIX Specification are called Unix-like. None of the Linux implementations is registered because of the high registration fee, but they are also Unix-like system, Wikipedia (2014g). 
The advantage of being a Unix-like system is obvious, the consistent user experience and compatibility. More than just user habit and reusability, choosing a Unix system is a cultural thing for a developer. For&nbsp;example, The UNIX Philosophy by Gancarz (2003):
• Small is beautiful. • Make each program do one thing well. • Build a prototype as soon as possible. • Choose portability over efficiency. • Store data in flat text files. • Use software leverage to your advantage. • Use shell scripts to increase leverage and portability.• Avoid captive user interfaces.• Make every program a filter.
These are the values you can constantly see violated in other systems. I have been using the Unix Philosophy to guide my work as a software developer and would like to see them also in the OS I use. 
1.2 Clang and POSIX 
GCC (Wikipedia 2014d, GNU Compiler Collection)has been the default free compiler for C/C++ and some other programming language on Unix-like system for very long. But now it has grown very large and become extremely hard to maintain.
Clang, with the LLVM as the back end, is an open source tool set designed to replace GCC. The compilation by Clang is faster than GCC and the code generated by Clang are in general faster than by GCC. Even the error and warning messages by Clang is more friendly and useful than by GCC, Wikipedia (2014b).






In the development tools of OS X, Clang already replaced GCC. 
Another reason of choosing OS X is because it’s POSIX compatible. Many software products have been built based on the POSIX standard, and I often need to develop POSIX standardized software. 
1.2.1 My story with POSIX 
When I worked on a Windows system, I needed to install Cygwin Wikipedia (2014c)to simulate the Unix-like interface including POSIX. But Cygwin couldn’t work seamlessly on Windows. And the GCC compiler in Cygwin has been very slow. 
I used to work on an ancient proprietary OS of Nokia called DMX. DMX is the operating system for Nokia DX200 server platform, which started since 1973 and ended in 2013, served in Nokia exchanges, 2G GSM, 3G WCDMA and 4G LTE networks, Wikipedia (2014f). My job was to develop and maintain the POSIX component. The OS wasn’t POSIX compatible in the beginning. But later it became a problem, for example, it couldn’t take the advantage of the free resources, like the TCP/IP stack implementation from FreeBSD. So POSIX interface needed to be introduced to the OS. It had been a very painful move since the legacy system was too different from Unix. The late introducing of POSIX also caused a lot of confusion for the other developers. For example, when they called a function free() to free the memory, they were not sure whether they were calling a POSIX function or the DMX native function. 
So, my experience is native support for POSIX and other standard interfaces is much better than those adopted later, like Cygwin on Window and the POSIX component I used to maintain. 
1.3 Open source resources 
Because OS X is a Unix-like system, just like Linux, there are many open source resources available. I use homebrew to manage my software packages, Wikipedia (2014e). homebrew is just like the apt-get in a Ubuntu OS. It’s called “The missing package manager for OS X.” For example, 
brew install mysql

Will install the popular open source database MySQL Server on your OS X. 
The Python package management system PIP, Ruby package management system Gem/Bundler and Node.js package management system NPM all works smoothly on OS X. 
1.4 Drawbacks 
1.4.1 It’s not Linux 
Unfortunately, OS X is not Linux. Although both are Unix-like system, they still have quite different cores. Nowadays, many software development project is developing software products to be run on a Linux server. Because of the differences, developers still cannot verify everything locally within their OS X, without connecting to a server or using a virtual machine. 
1.4.2 Too expensive 
OS X “almost” only runs on Apple Mac computers. It’s possible to run OS X in a virtual machine on a PC, but it’s very slow and not officially supported. The Mac hardware is in general very expensive.






1.4.3 Addictive 
I’ve never heard anybody switch from OS X to other OS. . . 
2 I wish I can have ... 
2.1 Better AppleScript 
It will be great if OS X can have better desktop automation. The AppleScript Neuburg (2006) isn’t very easy to use. A friend of mine create an open source project named “Osaka” to wrap the Apple Script into usable Ruby APIs, Vodde (2014). We use Osaka to automate our document generation. 
2.2 More compatible with Linux 
Some open source tools like Docker depends on the Linux specific OS feature. When running Docker on OS X, we still need to run a lightweight virtual machine with VirtualBox. It will be great if Docker can also run on OS X directly. 
2.3 More open source OS X GUI application 
Most of the OS X open source software projects are command line tools. The open source applications with GUI are generally not as great as those commercial applications you can purchase from Apple’s App Store. It will be great if OS X can be more open to open source GUI applications. 
3 Conclusion 
Mac OS X is a nearly perfect environment for software development. It’s good for cross-platform development among Unix based systems, for example, a service to run on a Linux server. In the same time, software developers can also enjoy the advantages brought by OS X for all the users in general, which should be a lot, but not covered in this paper. 
References 
Brookshear, J. G. (2011), Computer science: an overview, Paul Muljadi. 
Gancarz, M. (2003), Linux and the Unix philosophy, Digital Press. 
GYEONGMIN, K. (2010), Exploring the culture of an online brand community: A study of a Korean Apple MacBook user community, PhD thesis. 
Neuburg, M. (2006), AppleScript: The Definitive Guide: Scripting and Automating Your Mac, ” O’Reilly Media, Inc.”. 
Vodde, B. (2014), ‘Osaka’, https://github.com/basvodde/osaka. 
Wikipedia (2014a), ‘Berkeley software distribution — wikipedia, the free encyclopedia’. [Online; accessed 3-October-2014]. URL: http: // en. wikipedia. org/ w/ index. php? title= Berkeley_ Software_ Distribution&amp;oldid= 626744005






Wikipedia (2014b), ‘Clang — wikipedia, the free encyclopedia’. [Online; accessed 3-October-2014]. URL: http: // en. wikipedia. org/ w/ index. php? title= Clang&amp;oldid= 625344646 
Wikipedia (2014c), ‘Cygwin — wikipedia, the free encyclopedia’. [Online; accessed 3-October-2014]. URL: http: // en. wikipedia. org/ w/ index. php? title= Cygwin&amp;oldid= 627786784 
Wikipedia (2014d), ‘Gnu compiler collection — wikipedia, the free encyclopedia’. [Online; accessed 3-October-2014]. URL: http: // en. wikipedia. org/ w/ index. php? title= GNU_ Compiler_ Collection&amp;oldid= 625182444 
Wikipedia (2014e), ‘Homebrew (package management software) — wikipedia, the free encyclopedia’. [Online; accessed 3-October-2014]. URL: http: // en. wikipedia. org/ w/ index. php? title= Homebrew_ ( package_ management_ software) &amp;oldid= 624628232 
Wikipedia (2014f), ‘Nokia dx 200 — wikipedia, the free encyclopedia’. [Online; accessed 3-October-2014]. URL: http: // en. wikipedia. org/ w/ index. php? title= Nokia_ DX_ 200&amp;oldid= 592268576 
Wikipedia (2014g), ‘Unix-like — wikipedia, the free encyclopedia’. [Online; accessed 3-October-2014]. URL: http: // en. wikipedia. org/ w/ index. php? title= Unix-like&amp;oldid= 626497410



	              
	              Hi Craig,
I would like to find a delete forward key on Mac. Can you find it on your Mac? :)
Never know that Mavericks is a place in Califonia! I would name the next OS X release "Route 1".
I would like to know when do you need to run the Windows in your virtual machine.
br, Terry
	              
	              Hi Terry
Sure.... delete forward is function and delete :-)
Yes, Mavericks is a surfing location, I guess on the Pacific coast somewhere... perhaps off v10.11 'Route 1' &nbsp;;-)
No, I dont need Windows, but if I did, then it can be done on a Mac.
Best Wishes, Craig

	              
	              Imagine that you found out about a software program that could revolutionise your productivity and you decide that it is worth the investment. When you read the product specifications, you learn that the software does not run on your Macintosh OS, and therefore you are unable to make the purchase. This is a major drawback to the OS and it might make you consider purchasing an OS that is compatible with the software you desire. The software compatibility of an OS is an important factor when determining which type of computer or device to purchase.
In this Discussion you will analyse the OS that you use most frequently and evaluate the advantages and disadvantages. Additionally, you will consider how you might improve the current OS (e.g., security, compatibility, communication, usability) to suit your professional needs.
To prepare for this Discussion:

Review the required Learning Resources for this Week.
Research online articles regarding your OS.
Reflect upon your own experiences with your OS and determine the benefits and drawbacks of this particular system.

To complete this Discussion: Post: Create an initial post in which you analyse a frequently used OS. Address the following:

Summarise the OS that you are currently working with.
Explain what led you to purchase the OS.
Analyse your experiences with your OS and summarise at least three benefits and three drawbacks that you have encountered.
Recommend at least three different ways to improve the OS you currently utilise.

Respond: Respond to your colleagues. Address the following:

Recommend additional ways in which your colleague can improve the OS they currently utilise.
Be sure to support any claims you make.

For all Discussions (unless stated otherwise):

Create a single document with your initial post. Your document should be 350-500 words, though you will be marked based on the quality of your writing, not on the number of words.
By Sunday, post the text of your document to the Discussion Board for this Week, and upload the document using the Turnitin submission link for this Discussion.
By Wednesday, make 3–5 substantial follow-up responses to your colleagues. These can include responses to your colleagues’ initial posts, as well as responses to colleagues who responded to your own initial post. Your total Discussion Board participation must occur on at least 3 individual days during each week. Follow-up responses should be significant contributions to the Discussion. Do not submit your follow-up responses to Turnitin.
In general, online discussion is best when you:


Ask insightful questions.
Extend the discussion into new but relevant areas.
Model or promote critical reflection.
Support your arguments with citations and references from the assigned Learning Resources and other literature, using Harvard Liverpool Referencing Style.



Ensure that you spread your discussion posts across at least three separate days of each week. This will help maximise the value of your discussion with colleagues and serve to meet the learning objectives for each activity.
Click on the Reply button below to reveal the textbox for entering your message. Then click on the Submit button to post your message.
	              
	               I have been a windows users since Win 3.11. In my house we have a mixed eco systems Windows and Apple. My wife and I utilize Windows desktop and laptops and the kids (three of the them) all have Apple Mac laptops.  The reason we decided to utilize Windows devices was due to previous knowledge and familiarity. Our kids requested Apple, due to their friends all utilizing Apple and it was the hip device to have.  I have outlined below the more than three Pros and Cons of Windows OS  The Windows OS we have is Windows 8.  Initially it took some time for my wife and I to get use to the Windows 8 GUI. However once we got use to the screens, switching from Mosaic to Desktop, we liked the Windows 8  A lessons learnt for Microsoft OS would be to not to change the OS QUI in a drastic way. Windows 8 lost the Windows 7 Start Program icon. Also Windows 8 was designed for primarily touch screen devices. Many users around the world still Keyboard and mouse.  &nbsp;  &nbsp;  Windows Pros:      User friendly and familiarity. Many people of the plant have user experience with more than one version of Windows, such as below:               &nbsp;      Plentiful software. There is a large number of software available for Windows.      Backwards software compatibility. There is a good chance that Old programs will work on newer version of Windows.      Support for new hardware. Nearly all hardware manufacturers provide support for a recent version of Windows.      Plug &amp; Play. Windows does a good job in recognizing new hardware. No manual intervention required      Games. As many games working on windows &nbsp;      Compatibility with MS driven websites. With Internet Explorer (IE) the world's most popular web browser. &nbsp;Microsoft have introduced more and more proprietary features into their web servers that can only be taken advantage of with Internet Explorer.        Windows Cons:      High resource utilization. Windows pushes for the faster processor and higher RAM      Closed Source. Windows does not have an open source. Users and developers don’t know what is going on internally.      Poor security. Windows has more security weakness than other OS      Virus susceptibility. Computer virus always target windows OS. It is important to have anti-virus installed to protect you      Outrageous license agreements. Microsoft has overpowering EULA (End User License Agreement)      Poor technical support. Microsoft not know for ease access to their support , and when one has access the support is expensive      Hostile treatment of legitimate users. Microsoft does not handle software piracy very well.      High license prices. Microsoft has maintained a high license costs.      Poor stability. Windows has regular issues which require a re-boot      Backwards file format &nbsp;incompatibility. A known drawback of using Microsoft applications such as Office products is that their file formats are not backwards compatible.     &nbsp; Reference URL http://www.rjsystems.nl/en/3200.php&nbsp; 
	              
	              Hi Ramin,
Just want to point out one thing. IE is not the most popular web browser. It's actually far from being the the most popular web browser. Chrome being the most popular web browser since 2012 has 2~4 times more usages now from different statistics source.
I think you just gave one of the reasons why people are leaving IE, They don't follow the standard.

"IE is Being Mean to Me" is an original song written and performed by Scott Ward.
br, Terry
References
http://en.wikipedia.org/wiki/Usage_share_of_web_browsers
	              
	              Hi Terry
We have 3-5 MAC's at work, and then appox 2000 Windows.
The MAC's have to run Windows to be able to run all the different applications/software that we use. The reason for having MAC at all in our environment is that they are better to handle graphical jobs.
br, Bo
	              
	              
DQ1, Week 5, Operating Systems

1. Introduction
Through my IT career I have worked with several OS’s, but primarily in the Windows family (Microsoft). Starting as a teenager with Commodore 64 with its 8 bit OS Basic licensed from Microsoft. Then I moved on to MS-DOS and Windows 1.0. In my working years, i.e. since 1988 in a savings bank, I shortly touched on Novell, moved back to DOS, and of course using terminal based systems like 3270 and 4700 connected to IBM Mainframe. Then the bank “played” with Windows 3.11 but moved on to IBM OS/2. After that we moved to Windows NT which we used for several years. Next in line was Windows XP with NT as server. Next jump was from XP to Windows 7 which I’m still using, though I’ve just bought a Surface 3 Pro for testing of a) a replacement for some of our laptops and b) Windows 8.1 to replace our Windows 7. On the server side I’m fighting for updating our servers from old Windows servers to Windows Server 2012 R2, or as a minimum to Server 2008 R2. The only Mac I have is an iPad, and I use Android on my smartphone.
2. Thesis
The only ways to ensure a good and sound business environment is by using one OS vendor.
3. Content
When I use a computer, it’s primarily for work or study. Hence I’ll be writing in a business context. I might add, that I started in my current job I December 2013, and I feel I have landed in an environment that really needs a loving but firm hand, i.e. it is an environment which has a lot of “self-inflicted” problems because of lack of maintenance and different working processes. In private, when I want to chill and just browse, then I take my iPad (in the lack of a similar Windows tool) because it’s neater in design and size.
3.1. OS summary and purchase reason
We use to OS’s:

Clients: Windows 7
Servers: Windows Server 2008 R2 and 2012 R2 (though we still have some older servers)

Clients all use Windows 7 except 3-5 MACs.
The reason for investing in Windows is that for our use there are most applications, there is a low learning curve since almost everybody at some time has used Windows, and last that it’s actually an ever improving platform both on server and client side.
3.2. OS analysis
a. My experience
I have generally been satisfied with the Windows versions that I have been using (Though I’m not convinced yet about Windows 8). It lets me get from A to B in an easy way. Most people are familiar with the Windows GUI; hence it’s easy to hire people with Windows skills lowering the need for special training in OS and applications. Also my experience is that especially the server versions are getting ever more stable in performance. The performance improvement I also see on the clients, but still I think there are improvement potentials. I might also add, that even though I’ve see the MAC user interface, and I like it, I have still not found any reason/need for moving away from Windows.
b. 3 benefits

Lots of software availability.
Aero Snap and Shake functions.
Improved flexibility in organizing icons in the taskbar.

c. 3 drawbacks

Their ever changing pricing structures are a great irritation issue, and it takes a lot of energy to be compliant, so much, that there are companies who specialize in offering their services in keeping the license compliance.
The OS is to vulnerable and gives to many problems, it should be better at handling every driver, software, process isolated ensuring that if e.g. one process is unstable then it should only be that affected, not other processes and/or the machine.
I don’t like to have user accounts everywhere hence I don’t like when I use my Surface for the first time, I have to have a Windows account.
Windows 8 performed to big a GUI jump, i.e. people are not ready for such drastic changes like from W7 to tiles in W8.
Security is an issue

3.3. OS improvement recommendations

I would like to see an OS which doesn’t have to be rebooted (of course hardware is also an issue here).
I would like Widows OS to be as persistent as Craig says his MAC OS X is (Thomas 2014), i.e. that I could run the machine for 6 years without having to re-build or recover from backup.
I’d wish it would be easier to choose which cloud storage solution to use.
I would like security to be even more an integrated part of the OS eventually not needing to have any add-on solution.
On the servers, and even the clients, I would like to see more user friendly analysing tools. There are lots of logs, but not all have the wide and deep knowledge to use them.
It should be better at handling all processes isolated.
The file manager could also be improved. (There are times when it seems that the file manager thinks the file is being used when I know it’s not being used.
The folder manager could also be improved. (There are times when I have looked inside a folder (A), then gone out to delete it, but the OS still thinks the folder A is being used while it’s not. Then I have to go inside another folder B, back out, and then I delete folder A.) (Brookshear, 2012)
It should be easier to do a “User switching”, i.e. the process where user A is logged on and user B needs to logon to the same machine. This process is too cumbersome.

4. Conclusion
I don’t like to be tied to just one vendor, no matter which. Having said that, I’m still all in for choosing just one vendor for business purposes. I could switch to another vendor, but then it should be all-inn, i.e. I believe that using just one vendor gives the most benefit, even synergy,
5. Appendix
References
Brookshear, J. Glenn, 2012. Computer Science An Overview. 11th edition. Boston Massachusetts: Pearson. Addison Wesley.
Garger, John (2011). [Online] http://www.brighthub.com/computing/windows-platform/articles/63618.aspx. (Accessed 2014-10-04)
Microsoft. A history of Windows. [Online] http://windows.microsoft.com/en-us/windows/history#T1=era0. [Accessed 2014-10-04]
&nbsp;
Best wishes
Bo W. Mogensen

	              
	              Hi Craig,
I used my Windows virtual machine on my Mac only because of the banks required IE. But seems I rarely needed them for a while.
Then I must have been to Mavericks since I've driven along Route 1 with my daughter. Still remembering that she bursted into tears when she saw the the sun set into the sea. And then smiled again after I told her that the sun was shining her younger brother on the other side of the Pacific ocean.
br, Terry
	              
	              Hi Bo,
My question was about the motivation of running Window virtual machine on Mac computer:-) And the reason is to see which features of Windows that a Mac user would miss.
br, Terry
	              
	              Hi Bo,
You have pointed out one sad fact of the IT related industry, often a technology is chosen because it's "easy to hire".
If you rely on Windows Server, perhaps you will need to abandon it in the near future because of the same reason (unless you are in Cuba http://www.google.com/trends/explore#q=windows%20server).
br, Terry
	              
	              Hi craig,
First of all: great post to an interesting topic.&nbsp;
What I see is this: many people (if not all) consider Apple as being innovative while Microsoft as not being innovative.


I don't think that this is true but in my opinion Apple is way better in communicating changes and advances of new products. And they have a great sense for taking PC downsides and make them their upsides.

Microsoft should adapt that. Instead many areas where Mac OS X shines feel abandoned: File Previews, File Visualisation (Coverflow), Gadgets, Style (the superbar looks like the dock in tiger) etc.


But the Windows Experience relies very much on third party applications  Too many software of low quality, slows down my pc even if I don't use the software, doesn't fit in with the interface and too many codec’s make problems and so on. Windows needs an app store as a quality control and an easy way to find, install and update software.


Microsoft can create the best Windows. It’s useless if it is installed on a crappy computer with crappy software.
Regards, Babatunde
	              
	              Week 5 Discussion Question – Functions of an Operating System

Babatunde KOLAWOLE – 4 October 2014

Currently, I am with Windows 7 operating system developed by Microsoft; a version of Windows NT. Development of Windows 7 occurred as early as 2006 under the codename "Blackcomb." Windows 7 was released to manufacturing on July 22, 2009 and became available on October 22, 2009 less than three years after the release of its predecessor, Windows Vista. (Wikipedia, 2009)

Windows 7 became one of the most broadly and deeply tested releases of software I saw. Starting with a pre-beta in October of 2008 with a few thousand developers using Windows 7 at the earliest stages, through the Beta, and then the Release Candidate in May when millions of people started running the product on multiple PCs successfully

Windows 7 is not just a product of Microsoft, but of the whole industry of partners of all kinds. Throughout the development of Windows 7, there were a lot of incredible engagements from so many people that contributed to making the Windows 7 engineering project one which I felt good about. The feedback and collaboration throughout the development of Windows 7 have been outstanding and valuable beyond measure.

Benefits: Using my Windows 7 professional, though through the means of Microsoft Virtual PC which I downloaded separately, I was able to share my start menu and share file types with an interesting feature named “Windows XP Mode. The XP Mode is an ingenious virtual machine that goes one step beyond running a copy of Windows XP alongside Windows 7 on your desktop. It holds the key advantage over software such as VirtualBox and Vmware because it includes a license for Windows XP.

Windows 7 pro also include features like Presentation Mode, which can reset your desktop wallpaper to a default image, specify a pre-set volume level and prevent your screensaver from appearing – a one-stop shop to set up your PC for use in the boardroom.

I am able to join a domain with my Windows 7pro, which is a necessity if your computer is centrally managed by the IT department using a domain.

Drawbacks and Omissions: The first omission is BitLocker. Bitlocker is a full-disk encryption technology firstly introduced with Windows Vista Ultimate and Enterprise, used to encrypt the whole hard disk in hardware, hence not allowing access to the drive unless it is plugged into the laptop, and provided you input the password which you setup.

Another omission is BitLocker-To-Go that offers encryption for external USB drives. Once encrypted, the drive can only be accessed by entering a lengthy password or swiping a smart card (if your company supports them). You can personalize your encrypted drive to work regularly without inputting password each time it’s being plugged. The drive can only be written to by Windows 7 systems but can be read-only on Windows XP and Vista systems

Less crucially, perhaps, another omission is a feature such as DirectAccess (control tools offered by AppLocker are only available in Windows 7 Ultimate) which acts like a virtual private network (VPN) allows users to access the company network remotely.

However, to improve Windows for better performance try the following steps below;



Delete programs you never use



Manufacturers pack new systems with programs you may not need which includes trial editions and limited edition versions of programs that software companies hope you will try, find useful, and then pay to upgrade to full versions or newer versions.

It’s a good idea to uninstall programs you don’t use. This should include both manufacturer-installed software and software you installed yourself, but don’t want anymore. For instructions, see [1]



Limit how many programs load at startup



Decide for yourself if you want a program to load at startup by using Windows Defender.





The Software Explorer screen in Windows Defender shows which programs automatically start when you launch Windows. For instructions how to use Windows Defender to check for programs that open automatically [2]



Try a troubleshooter



Windows includes two troubleshooting programs which could be used to automatically fix some common problems with your computer's performance.


Open the Performance troubleshooter by clicking Start button, then click Control Panel. In the search box, type troubleshooter, and then click Troubleshooting. Under System and Security, click Check for performance issues.
Open the System Maintenance troubleshooter by clicking Start button, and then click Control Panel. In the search box, type troubleshooter, and then click Troubleshooting. Under System and Security, click Run maintenance tasks.


&nbsp;


References

&nbsp;

Microsoft Windows 7 Professional review [online] Available from: http://www.pcpro.co.uk/reviews/software/352633/microsoft-windows-7-professional (Assessed 3 October 2014)

Dell Inc. (2012). The top three reasons to migrate to Windows 7 Professional [online] Available from: http://partnerdirect.dell.com/sites/channel/Documents/Top-3-Reasons-to-Migrate-to-Windows-7-Professional.pdf (Assessed 3 October 2014)


[1] Uninstall or change a program [online] Available from: http://windows.microsoft.com/en-us/windows-vista/uninstall-or-change-a-program (Accessed 3 October 2014)&nbsp; 
[2] Stop a program from running automatically when Windows starts. [Online] Available from: http://windows.microsoft.com/en-us/windows-vista/stop-a-program-from-running-automatically-when-windows-starts (Accessed 3 October 2014)
	              
	              Hi Chika,
Windows 8/8.1 was a new idea, like windows vista from Microsoft. Both OSs' did try to provide the best performance with new look and feel, but failed in that attempt.Windows 7 has more market share than any other OS [1]. The good news is, MS quickly realized their mistake and launching new OS Windows 10, which will provide seamless user experience among PC, tablet and mobile devices. Which will have Windows 7 performance and "Metro" look and feel along with more features to improve the overall user experience [2].
References:[1]&nbsp;http://en.wikipedia.org/wiki/Usage_share_of_operating_systems[2]&nbsp;http://www.pcworld.com/article/2689393/windows-10-revealed-microsofts-next-os-fuses-windows-7-and-8.html
Best regards,Kharavela
	              
	              Operating Systems: A Review of Windows 7
Introduction
Windows 7, the current operating system that I am currently working with is a software product from Microsoft Corporation released to manufacturing on July 22, 2009 less than three years after the release of its predecessor, Windows Vista.
Windows 7 comes from a family of operating systems from Microsoft that have evolved since its first operating system (MS-DOS) was shipped on an IBM PC in 1981. Other releases that preceded it include MS DOS, Windows 1.0, Windows 2.0–2.11, Windows 3.0–Windows NT, Windows 95, Windows 98, Windows 2000, Windows Me, Windows XP, Windows Vista (Microsoft, A history of Windows)
Purchase Decision
My purchase decision for Windows 7 was largely informed by the need to upgrade to an operating system that was more stable and had much better performance than its predecessor that I was using at the time - Windows Vista. Performance was a big issue that I experienced with Windows Vista as it was memory hungry and executed typical applications much more slowly than Windows XP with the same hardware configuration. Microsoft then made available Windows 7 to fix the problems that that were being experienced by Windows Vista and I naturally joined the bandwagon. Former Microsoft CEO admitted that Windows Vista wasn’t a great product and had many challenges originating from the technical approach http://www.tomshardware.com/news/microsoft-longhorn-windows-vista-steve-ballmer-mistake,25401.html
Key Benefits
Some of the key benefits other than being a relief from Windows Vista
1.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Intuitive Networking, driver device support and hardware detection
o&nbsp;&nbsp; Windows 7 was easy to deploy across a network environment making it easy to detect, pick and configure workstations on a network. Windows 7 also provided more driver support for hardware that was previously working under Windows XP
2.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Improved Security 
Windows 7 had enhanced security through use of some specific features such as the File and folder permissions, use of the Bit-locker to Go designed to protect data, as well as the AppLocker, a new application control feature introduced in Windows 7 that helps prevent the execution of unwanted and unknown applications within an organization's network while providing security, operational, and compliance benefits. (MSDN, 2013)
&nbsp;
3.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Improved speed and ease of use 



The GUI design is user-friendly and hides many of the complexities of hardware interactions. Working with multiple windows, search capability, sticky notes and other gadgets of windows 7 helped in getting around many common day to day tasks. The boot sequence in Windows 7 was much faster and the desktop more responsive which was a big improvement coming from Vista.



Drawbacks
1.&nbsp; &nbsp; &nbsp; &nbsp;1)&nbsp;Incompatibility with older software and hardware
Windows 7 did not provide support for some legacy applications, device drivers, old motherboards and other hardware that were available in Windows XP making migration to the new platform quite a challenge, despite widespread use of these applications in Windows XP platform. A check for Microsoft Software Compatibility list was required. http://pcsupport.about.com/od/driverssupport/tp/windows-7-drivers.htm
&nbsp;
2.&nbsp; &nbsp; &nbsp; &nbsp;2)&nbsp;Configuration Requirements &amp; system migration
Windows 7 requires a minimum of 1GB ram thus necessitating the need to upgrade previous machines that were running comfortably on Windows XP.&nbsp; In addition, migration from Windows XP was not so straightforward and often best required that you do a complete format of the hard disk and perform a clean install Wired (2009)
3)&nbsp;Mainstream support for Windows 7 is ending in Jan 2015 (http://windows.microsoft.com/en-us/windows/lifecycle). This puts end users at disadvantage and forces yet another upgrade at a point where relative stability has been achieved in use of the operating system.
Recommendations for improvement

Need for faster way to restart/reset the machine other than a cold boot. In the event of application crashes or the system ‘hangs’, we still have to use the 3 finger salute (CTRL+ALT+DEL) to restart the computer. A good way would be to have a single button that the use can use to do this. Bill Gates admitted that the use of the CTR+ALT+DEL was a mistake (Huffington Post , 26 Sep 2013)
Need for better security to be provided natively within the O/S to prevent unauthorized access to sensitive data (Brookshear 136). Flaws in operating system security that result in zero day exploits can be a big threat
Better support and ease of installation for SSD. Windows has evolved over many years with features that specifically target the behavior of conventional hard disks&nbsp; http://www.zdnet.com/blog/bott/windows-7-and-ssds-setup-secrets-and-tune-up-tweaks_p2/2910. Setting up Windows on an SSD requires a few extra steps that aren’t necessary with an installation on a conventional hard disk (ZDNET). In many installation cases, the SSD is not picked up automatically and requires you to go through a few hoops to set it up correctly. Future releases of Windows should provide this native support to make it easier for end users to ‘plug and play’

References

Microsoft,&nbsp; ‘A History of Windows’ Microsoft Website (Nov 2013), Available at http://windows.microsoft.com/en-us/windows/history#T1=era6
Parrish,K&nbsp; ‘Steve Ballmer Says Windows Vista Was His Biggest Mistake’, : ZDNet, &nbsp;DECEMBER 16, 2013, Available at http://www.tomshardware.com/news/microsoft-longhorn-windows-vista-steve-ballmer-mistake,25401.html, Accessed 5/10/2014
McFedries, Paul. Microsoft Windows 7 Unleashed (Kindle Locations 1136-1137, ‘Setting Security Permissions on files and Folders’ ).&nbsp; Pearson Education. Kindle Edition, Accessed 5/10/2014
Fisher, T.&nbsp; ‘Windows 7 Drivers: Where to Download the Latest Windows 7 Drivers for Popular Hardware’ , About Technology ( July 2014) Available at http://pcsupport.about.com/od/driverssupport/tp/windows-7-drivers.htm ,&nbsp; Accessed 5/10/2014
Microsoft, ‘Help With Windows 7 Compatibility Problems’, ‘Microsoft Website’ Available at http://windows.microsoft.com/en-us/windows7/help/compatibility#T1=softwarecompat , ,&nbsp; Accessed 5/10/2014
Williams, M. ‘22 common Windows 7 problems solved’ , Techradar (Dec 2009), Available at http://www.techradar.com/news/software/operating-systems/22-common-windows-7-problems-solved-655655/1#articleContent , Accessed 5/10/2014
BRIAN X. CHEN, ‘7 reasons to avoid windows 7’, Wired (2009) , Available at http://www.wired.com/2009/08/7-reasons-to-avoid-windows-7/ ,&nbsp; Accessed 5/10/2014
Microsoft Developer Network, ‘Windows 7 AppLocker Executive Overview’, MSDN, Dec 2013, Available at http://msdn.microsoft.com/en-us/library/dd548340(v=ws.10).aspx , Accessed 05/10/2014
Microsoft Developer Network, ‘BitLocker Drive Encryption’, MSDN, Dec 2013, Available http://windows.microsoft.com/en-us/windows7/products/features/bitlocker&nbsp; , Accessed 05/10/2014
The Huffington Post , ‘Bill Gates Admits That 'Control-Alt-Delete' Was A Mistake’, Huffington Post (Sep 2013),&nbsp; Available at http://www.huffingtonpost.com/2013/09/26/bill-gates-control-alt-delete_n_3995179.html , Accessed 05/10/2014
Brookshear, J. G. Computer Science: An Overview XML Vital Source ebook for Laureate Education, 11th Edition. Pearson Learning Solutions. &nbsp;
Bott, E. ‘Windows 7 and SSDs: Setup secrets and tune-up tweaks’, &nbsp;ZDNet, The Ed Bott Report |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; January 26, 2011, Available at http://www.zdnet.com/blog/bott/windows-7-and-ssds-setup-secrets-and-tune-up-tweaks_p2/2910, Accessed 5/10/2014

&nbsp;
&nbsp;
&nbsp;
&nbsp;

	              
	              Hey Terry
I case of me misunderstanding your “easy to hire”, than hopefully you can put a few more words on it. Do you mean that it’s just easy accessible, or perhaps “jumping over where the fence is lowest”? But until then I’ll give you this answer.
Yes, you are right about the “easy to hire”, but then again, everything is relative. I believe, correct me if I’m wrong, that the choice of OS is primarily dependent on you’re working “growing up” environment, i.e. if your used to a certain OS, then you’ll probably stick to it because you’re good at it.
I also believe that, when choosing an OS there’s not just the “easy to hire” argument, but I would also look at what’s the best technology for the given system. At work we have a library system running on Linux. But since we’re a small country with few (actually I think it’s high relative to the population) IT specialists we have had to outsource maintenance abroad. And in the bank, we choose OS depending on the type of data, e.g. transactional data would always end up on the IBM Mainframe instead of e.g. WS client/server solution.
I’m not sure I understand your second phrase. If your point is taken from the graph showing 0 search interest in Windows Server (WS), it’s hard for me to argue on that. But if you change your filter to WS08 and WS12, then it changes [1]. See also other statistics [2]+.
&nbsp;
[1] http://www.google.com/trends/explore#q=%2Fm%2F0gvv9b5%2C%20%2Fm%2F0266rj4%2C%20%2Fm%2F0278lsn&amp;cmpt=q
[2] http://w3techs.com/technologies/history_overview/operating_system/ms/y
[3] http://www.netmarketshare.com/operating-system-market-share.aspx?qprid=10&amp;qpcustomd=0

	              
	              Introduction
As I mentioned at week 1, my favorite OS is Windows XP, although its security and certain issues are criticized, its liable and stable (with respect to the other versions of windows) let me persist to use it over ten years. However, Windows XP has decommissioned during this year, I am forced to give it up. After end of support of XP, I chose windows 8 as the OS of my notebook.
Windows 8
Windows 8 is the PC OS produced by Microsoft as a member of the Windows NT family. It adopted significant alterations to the OS GUI (graphical user interface) and platform to ameliorate its user experience on tablets; this because Windows are confronting the competition of mobile OS recent years, such as iOS and Android. Particularly, these alterations comprise:

A Windows shell optimized for touchscreen interfaces
The Start screen that shows the dynamically updated content and programs on a grid of tiles 
A new platform for developing apps with a focal point on touchscreen key in
Online services integration that comprise the function to synchronize settings and apps between devices
Windows Market that provide an online store for purchasing and downloading apps and software.

Additionally, Windows 8 also put in cloud computing, near field communications, Advanced Format hard drives and support for USB 3.0. Extra security properties are applied, for example, the built-in antivirus software, support for UEFI Secure Boot on supported devices with UEFI firmware and integrated with SmartScreen Filter tools are introduced to protect the computer and avoid malware from infecting the booting process.
Security
OS security consists of a lot of distinct technologies and means that make sure security from attacks and threats. OS security enables distinct programs and applications to process necessary tasks and prevent unauthorized interfering.
OS security is able to be accomplished in a lot of methods, comprising insisting on the following:

Update regular OS patch
Update antivirus DB and engines
Scrutinize all access network traffic via a firewall
Create secure user with requested privileges

While Windows 8 provide three major new security functions:

The UEFI format called "Secure boot" -- adopts a public-key base installation to confirm the rightness of the OS and avoid risks from infecting the booting process.
Family Safety provides Parental manages that lets parents to control and monitor their children's doings on the computer.
SmartScreen filtering -- The extra of antivirus functions integrated to Windows Defender

Windows Market
Another one of new major features at windows 8 is the windows market; it is the new style of windows applications, known as "Windows Store apps". Those apps are developed to be optimized for touchscreen settings as well as more customized than traditional applications. Apps are able to execute either in a snapped to the side, or full-screen mode. They also can offer "toast notifications" and their dynamic content tiles are able to be animated on the Start screen. They also can be through the "contracts", which is a collection of hooks, to offer general functionality that is able to combine with other apps, comprising sharing and searching. Furthermore, they also can offer connection with other services, such as the communication app is able to link to a type of distinct services and social networks, for example, the Facebook and Google+. While the photographs app is able to combine photographs from services, for example, the Flickr and Facebook.
User Interface
Windows 8 applies evident alterations to the GUI of OS, much of that are took aim at bettering users experience on tablets and touchscreen devices (e.g. Smartphone). This new GUI is built on Microsoft's Metro (design language), and adopts a Start screen, which is like Windows Phone, as the main ways to launch apps. The Start screen shows a specialized tiles array for separately one by one programs and apps, certain of those are able to show regularly updated details via "live tiles". As a modus of multitasking, apps are able to be snapped to the side or full screen. 
A vertical toolbar located at right side is called "the charms" supplies the access to app-related functions and system settings, such as sharing, searching, settings, device management and a Start shortcut. The classical desktop for executing applications is visited through a tile on the Start screen. Via clicking the bottom-left corner of the screen enables one to switch between classical desktop and tiles apps.
Software compatibility
Software compatibility means the compatibility which a specific program or application has executing on a specific processor structures. It is also able to imply capability for the software to operate on a specific OS. Extremely uncommonly is a compiled program supported with diverse distinct processor structures. Generally, a software program is compiled for distinct processor structures and OS to enable it to be compatibility during distinct environments. However, interpreted software is able to execute on diverse distinct processor structures and OS normally when the interpreter is accessible for the OS or processor structures.
Windows 8 is compatibility with 64-bit and 32-bit structures; The 64-bit edition operates on CPUs supportable with x86 8th generations or newer, and is able to execute 64-bit and 32-bit programs. Theoretically, 64-bit systems is capable for supporting 2048 GB of memory, but 32-bit OS and programs are limited to support only 4 GB of memory. On the other hand, the 32-bit edition operates on CPUs supportable with x86 structure 3rd generation or newer, and is able to execute 32-bit and 16-bit applications. 
The only version of Windows 8 for system with ARM Processor Architecture is known as " Windows RT", it is only compatible with applications built-in during the the system, which is offered via the Windows Market or Windows update, to make sure that the system only executes apps which are optimized for the structure. This version is unable to support operating x64 or IA-32 apps.&nbsp; Windows Market apps are able to either be compatible with both ARM and x86 structure, or wrote to be compatible with a particular structure.
Pros and cons
Windows 8 has its pros and cons that it focuses on the touchscreen technology that is prominent for Tablet users yet is inconvenient to the desktop PC users. Certain built-in apps are insufficient functionality even others afford user entertaining experience.
Advantages 
Windows 8 is optimized for the touchscreen devices which adopt the ‘Metro’ User interface, in addition to this; there are some advantages as following:

Support low-power ARM structure that sustains secure boot and has advanced security features
Less of boot time, it need only 8 seconds to start up the OS
Hardware demand same as Windows 7 that unnecessary to upgrade PC
Windows Market provides certain of apps which are developed for Windows 8.
Support NFC (Near Field Communications) printing

Disadvantages 
However, there are also certain Disadvantages during Windows 8:

The overlapping of Aero and Metro User Interface makes confusion for the developers and users that switching between those interfaces is not user-friendly.
The Start screen become garbled after certain apps are installed, this because the home screen tiles are unable to turn into icons.
Hard to swap between the distinct screens, as there is not simple method to swap between apps
Metro multitasking makes the screen show two apps are lined
The UI is inconvenient with the desktop users
Not support flash content on Tablets

Conclusion
&nbsp;
As Technology Business Research Inc. (2013)’s conclusion, The major feature of windows 8 is " ... combine the strengths of PCs and tablets in a single device, to the benefit of both IT and users. " But precisely this feature makes the user interface confusion, especially the desktop users, it causes very inconvenient during operation. Another new design that I am not appreciate it that is the disappeared of "start menu", it allows me to increase the difficulty of finding out the required apps.
&nbsp;
References
&nbsp;
Aryeh Goretsky(2013), Windows 8: FUD for thought; Available on: (http://www.welivesecurity.com/media_files/white-papers/ESETNA_WP-Windows8-FUD.pdf) (Accessed on: 05-Oct-2014)
&nbsp;
Jerry Honeycutt (2012); Introducing Windows 8 - An Overview for IT professionals; Available on: (http://www.flane.com.au/WebResources/pdfs/ItCareer/Microsoft-ebook-Windows%208-Overview%20for%20IT%20Professionals.pdf) (Accessed on: 05-Oct-2014)
&nbsp;
Kevin C. Tofel (2012); Is the Windows 8 user experience as bad as experts say? Available on: (https://gigaom.com/2012/11/21/is-the-windows-8-user-experience-as-bad-as-experts-say/) (Accessed on: 05-Oct-2014)
&nbsp;
Mary Branscombe (2013); Windows 8 security: everything you need to know; Available on: (http://www.techradar.com/news/software/operating-systems/windows-8-security-explained-1107206) (Accessed on: 05-Oct-2014)
&nbsp;
Microsoft Corporation (2012); Windows 8 Release Preview - Product guide; Available on: (http://www.microsoft.com/about/mspreview/windows8/Windows8_RP_Product_guide.pdf) (Accessed on: 05-Oct-2014)
&nbsp;
Technology Business Research Inc. (2013); Windows 8 is Changing the Game; Available on: (http://h41112.www4.hp.com/pdf/xp-migration/tbr_hp_windows_8_is_changing_the_game.pdf) (Accessed on: 05-Oct-2014)
&nbsp;
Tom Spring (2012); Is all your gear Windows 8 compatible? Available on: (http://www.pcworld.com/article/2011807/is-all-your-gear-windows-8-compatible.html) (Accessed on: 05-Oct-2014)

	              
	              Yes Terry, the erroneus reference to&nbsp;developing and maintaining a blog site has been removed from the DQ instructions.
Regards,
Anthony
	              
	              Hi Bo
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; I am an Ex- Microsoft employee (worked in MCS for 4 years). I agree Microsoft didn't focus on innovation. They had the market share with their desktops. Windows 8 was a big product release for Microsoft, I recall the push internally to migrate our clients from XP-&gt;Win8 (which is a huge jump, due to application migration issue). Microsoft should not have changed Win8 GUI dramatically (no&nbsp;default START button and users home Desktop screen)&nbsp;&nbsp;and only focus touch screen look-and-feel (with the title design). We saw how they heard the market and brought out 8.1 with START button.Microsoft has one shot at getting Windows 10 (they skipped Win 9), if they get it right then Enterprise will move from XP-&gt; Win10 or Win 7-&gt; Win 10 after a year their release (to drive Service packs). From my contact inside Microsoft it looks like they get it and are really trying to develop a Windows release that will be embraced by the masses.
Robin

Any one interested in viewing Win 10 http://windows.microsoft.com/en-us/windows/preview
	              
	              Hello Class,A student in a previous class commented that –“There is acknowledgement, and even acceptance, of the fact that the envelope or locus of the Operating System is enlarging beyond just the physical computer, to encompass the web itself”.&nbsp;Do you agree with this notion of the move towards a “Web-OS”, or do you think that this is just fanciful thinking?Anthony
	              
	              The Operating System I use

 Since I was a kid, I started using computers with Microsoft operating systems. As I grew and started working in IT, my experience with Microsoft Windows allowed me to advance professionally. Thus, Microsoft Windows, is the Operating System (OS) I use most of the time (80% of my time, the other 20% between OS X and ESXi).

The current version of Microsoft Windows is NT 6.3 (and lately testing NT 6.4), normally recognized under the branding named “Windows 8.1 or Windows 2012 R2). It is the latest production version of Microsoft Windows. As the version number suggests it, it is based on the Windows NT family.

The Windows NT Family history is worth of mention here as it started as the first full 32-bit Operating System developed by Microsoft. It was intended to complement the Microsoft existing consumer focused OS (Windows 3.x through Windows 98/ME) (Microsoft. 2013, Micskei 2005). Eventually, Microsoft decided to invest into the NT family to produce a flexible OS that would run “everywhere” on both PC’s and servers in the corporate datacenters (Mardesich 1999). The first version of this vision was Windows 2000, followed by the long-lived, more than ten years, Windows XP (Microsoft. no date).

Microsoft Windows is a modern multi-programming, multi-tasking, multi-threaded and multicore OS. The main three reasons I purchased Windows OS (and continue to), besides historic attachment, are:


Because of professional reasons: at work 80% of corporate software runs on MS Windows servers.
Because it’s flexibility and large hardware compatibility. You can be very flexible in your hardware choice. This flexibility is an actual advantage when it comes down to prices as Windows OS running computers can be very cheap.
It’s user-friendly interface, which allows for the OS to be used by almost anyone, while giving the flexibility to professionals to dive deeper into complex implementations.


I can summarize three distinct benefits and drawbacks of the Microsoft Windows operating systems:





Microsoft Operating Systems




Benefits


Drawbacks




Vast hardware compatibility: it will run on almost any Intel based technology.


Vast hardware compatibility: more hardware combination increase the risk of issues or problems at the hardware level. OS can run into issues that can result in a system crash. Famously known as BSOD (Blue Screen of Death).




Ease of use: Very friendly user interface.


Closed Source: users and software depend on the vendor for both support, client/server proprietary extensions and prices (Microsoft holds the monopoly of his OS system prices).




Software Availability: large selection of application software and system software.


Security: malicious software (Malware) is particularly popular on Windows OS platforms. Primary causes are due to the user culture, security design of windows and its popularity as it makes the system a perfect target for malicious attacks.





All-in-all, Microsoft windows is a good OS, but I can think three ways to improve it:


Render the OS friendlier to be used on devices other than traditional desktop computers (i.e., tablet devices), better integration between these devices.
Improve security. Although, in part, a change in mentality of end users (stop using Administrative accounts for everything, for starters), it is also a change that Microsoft needs to enforce on the OS.
Enhance battery life for laptops and mobile devices. It only takes a look at Mac OS X power technology overview to understand the implications of an OS when it comes to managing CPU and hence power consumption (Apple. 2013).


&nbsp;

References

&nbsp;

Apple. (2013) Power Efficiency in OS X&lt;br /&gt;Technology Overview. Available at: https://www.apple.com/media/us/osx/2013/docs/OSX_Power_Efficiency_Technology_Overview.pdf (Accessed: 5 October 2014).

Mardesich, J. 1999, "What's Weighing Down Microsoft?", Fortune, 139(1), pp. 147-148.

Microsoft. (no date) Microsoft Support Lifecycle. Available at: http://support2.microsoft.com/lifecycle/?ln=en-gb&amp;c2=1173 (Accessed: 4 October 2014).

Microsoft. (2013) A history of Windows. Available at: http://windows.microsoft.com/en-us/windows/history#T1=era0 (Accessed: 4 October 2014).

Micskei, Z. (2005) The Windows Operating System. Available at: http://home.mit.bme.hu/~micskeiz/opre/files/eng/01-operating-systems-windows-introduction.pdf (Accessed: 4 October 2014).

&nbsp;

	              
	              Hi Craig,
You can move the harddisk icon off the desktop in Mac OS X. Open finder, the go to menu Finder -&gt; Preferences. Under the General tab, you can choose that in the "Show these items on the desktop": simply unckeck Hard Disks.
If you want to have the HDD icon in your dock bar, then simply drag the icon into the dock bar and make it stay (Right click, Options, Keep in Dock). Make sure that this is done before you do the above step, once the above step is done you will still see the HDD icon on your task bar.
Regards,
Augusto.
	              
	              Hi Anthony,
I'm a big believer of the "Web-OS" for the user end systems.&nbsp;From 1998, Microsoft was sued by the United States and was accused of bundling IE browser with its OS. 11 years later in 2009, Google announced their Chrome OS, which is basically just a browser.








Case 1: United States v. Microsoft Corp.
United States v. Microsoft Corporation&nbsp;is a United States law case, which was initiated in 1998 and settled in 2001,&nbsp;Geier&nbsp;(2001). Microsoft was accused of becoming a monopoly, and the main reason is to bundle its Internet Explorer, a web browser, with its operating system, the Microsoft Windows. “Microsoft’s Windows operating system is the sun in the solar system of the information economy,” as mentioned by&nbsp;Geier&nbsp;(2001). And when all the Windows user get an IE browser by default, it was considered not fair to the other browser makers. Microsoft was also questioned over whether it manipulated the Windows API to favor IE over other third party web browsers.


Case 2: Chrome OS
Google announced the Chrome OS for the first time in 2009. Chrome OS is an open source operating system designed by Google based on the Linux Kernal,&nbsp;Adee&nbsp;(2010). Chrome OS is often called a “web OS”, because it’s main functionality is just a browser. The Chrome OS depends heavily on the cloud computing and Web applications. With the Web application provided by Google and other vendors, you can pretty much do anything the Microsoft Of- fice application can do.













These two events in 11 years show the progress of people’s cognition of the operating system of the user end computer. Microsoft was accused of bundling OS with Internet browser, and now Internet browser becomes the core of the emerging OS.
Some people believe that Javascript is the new assembly language for the Web&nbsp;Hanselman &amp; Meijer&nbsp;(2011). A browser is the core feature of a client-side Web OS. We may see that in the future most user devices including laptop, netbook, handsets, wearables-devices, all run on OS that natively support HTML rendering and run JavaScript at the lower OS level.




The deeper reason for this prediction is the openness. I believe the only way to stay in the business is to create open systems so that you can exchange information with the other systems. HTML and the JavaScript running with HTML follow the "Rule of least power",&nbsp;Wikipedia. The reason they are so popular is not because they are "powerful", but on the contrary, it's because they are "the least powerful" way of representing data.
The even deeper reason for this believing is rooted in the Unix philosophy:
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"Store data in flat&nbsp;text files."&nbsp;Gancarz (2003)
PS. Dr. Ayoola, don't be surprised to see some of the same information in my individual assignment:-)
br, Terry











References






Adee, S. (2010), ‘Chrome the conqueror’, Spectrum, IEEE 47(1), 34–39. 
Geier, M. (2001), ‘United states v. microsoft corp.’, Berk. Tech. LJ 16, 297. 
Hanselman, S. &amp; Meijer, E. (2011), ‘Javascript is assembly language for the web: Semantic markup is dead! clean vs. machine-coded html’. 



Rule of least power. (2014, March 22). In&nbsp;Wikipedia, The Free Encyclopedia. Retrieved 14:06, October 5, 2014, from&nbsp;http://en.wikipedia.org/w/index.php?title=Rule_of_least_power&amp;oldid=600712913



Gancarz, M. (2003), Linux and the Unix philosophy, Digital Press. 




	              
	              Hi Colleagues,
What do you feel when somebody touches the screen of your laptop? (You can picture the scienario:-)
br, Terry
	              
	              Hi Bo,
Forget my second phrase, and I'm sorry for using the premature search information. As soon as I submit the reply I did some more trend search for other systems and I couldn't get a convincing result to back up my point.
I guess you didn't misunderstand my "easy to hire". I said that is because you said "hence it’s easy to hire people with Windows skills lowering the need for special training in OS and applications".
br, Terry
	              
	              Hi Ramin and Terry,
I would not say either that the most popular Brower is Internet Explorer (IE). However, I have to point out that most of the statistics that are out there are badly explained and, perhaps, badly performed.
First, and most importantly, the word "popularity" can be confusing. For example, Lost is a very popular TV show, yet, only 15.69 million average viewers per episode in the USA saw the first season. Compared to the USA population of around 293 million, the viewers are only around 5% of the population of the country (ABC Medianet. 2005, United States Census. 2011). How can be something popular when only 5% of the population “knows” about it?
For internet browsers, we should use the word “usage”. This wording will represent a tangible measure of which browser is being used. Unfortunately, this is not that easy to measure. In fact, the World Wide Web Consortium (the official standards organization for the World Wide Web) doesn’t even have such measurement, or at least I could not find any statistics made by this official entity.
So that leaves is with statistics gathered by other entities, most likely either commercial or non-commercial companies. The problem with these statistics is that often they display data without any explanation on how the data was gathered and how was analyzed. Take for example w3counter.com statistics (w3Counter.com. 2014), here are the problems I see:

First it’s based on a sample of 77’930 websites. But it doesn’t say which type of sites these are.

This information is an important factor. Consider, for example, that social sites are most likely being accessed today by apps rather than traditional browsers.
Is 77’930 websites a relevant statistical population? How many websites does internet have? (I do not know the number, but I will hazard the guess of about 1 billion).
Are these unique websites? For example www.windows.com redirects to microsoft.microsoft.com, are they being considered two different sites or one unique one?


Second, as you can see by the tables it seems (it is not being specified, so we are left to guess) that the browser statistics are unrelated to the Operating Systems (OS) or devices.

So how do we know how much of that Chrome % is mobile usage on Android systems? Because there’s no IE in Android that makes sense, but does that qualify for “Popular” or “most used”. Based on these statistics, a debatable statement could be “Chrome is the most used browser”, while a better, non-debatable claim will be “Chrome is the most used browser in the Android OS”. Unfortunately, we are left to wonder because that information is simply not included in that report.



But it can be worst, check out the w3schools.com statistics: it only says that is collected from log files since 2002. There’s not a mention of the sample population, nor any other indication that these numbers are relevant, gathered or analyzed in any scientific statistical way. At least they tell you “You cannot - as a web developer - rely ONLY on statistics. Statistics can be misleading.” (W3Schools.com. no date).
I can be worse than that. Take the CMO article “ADI Report: Google Controls The Browser Worldwide”&nbsp; (Adobe Digital Index. 2014), this article make some claims based on a research done by Adobe Digital Index (ADI), of note is that CMO is owned by Adobe. This article is the actual report, so one will think those are published numbers, or are they? In fact, nowhere have they specified their methods, nor there’s a link to the actual number tables. But since it is Adobe, one should be able to find number reports somewhere readily available, correct? I could not find any data related to “Adobe Digital Index analysis of Web browsers”. Anywhere. Adobe owns their data, so they can choose not to make it available to the public. But wouldn’t that help back up their claims? We are left wondering again about their data gathering methods and analysis.
Now, with all the claims around the web about Chrome being the most used browser, we can only assume that there is some truth about that. However, I do not seem to find some real scientific statistical data that prove this. Most of the claims are backed up by research done by private companies that give a shady information.
I wonder: why would anyone (other than the software vendors) make bad data about browser usage? Maybe people don’t care too much about this, and they are sloppy at producing the research data? Maybe it is just that IE is being mean to people, and so people are mean to IE.
Or perhaps we should stop believe everything that we can find on the internet, starting with stuff like this: http://theflatearthsociety.org/cms/ (this is real, sadly).
Best Regards,
Augusto
&nbsp;
References
ABC Medianet. (2005) Season Program Rankings. Available at: http://abcmedianet.com/web/dnr/dispDNR.aspx?id=062105_06 (Accessed: 5 October 2014).
Adobe Digital Index. (2014) ADI Report: Google Controls The Browser Worldwide. Available at: http://eu.cmo.com/articles/2014/6/2/adi_2014_browser_war.html (Accessed: 5 October 2014).
United States Census. (2011) Vintage 2004: National Tables. Available at: http://www.census.gov/popest/data/historical/2000s/vintage_2004/ (Accessed: 5 October 2014).
w3Counter.com. (2014) September 2014 Market Share. Available at: http://www.w3counter.com/globalstats.php?year=2014&amp;month=9 (Accessed: 5 October 2014).
W3Schools.com. (no date) Browser Statistics. Available at: ww.w3schools.com/browsers/browsers_stats.asp (Accessed: 5 October 2014).
&nbsp;

	              
	              Full disclosure: I use Chrome for all my personal browsing. I only use IE for corporate web applications that are compatible only with it. And I use Safari of all my mobile devices (iOS ones only).
	              
	              Introduction      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; My first memorable computer experiences began with DOS (Disk Operating System) and the BASIC programming language. I was fascinated with the way DOS used to process requests, but of course, in spite of its own limitations, it gave me an amazing feel interacting with the computer. Over the past decade or so, I have used many OS’s (Microsofts Windows 3.1-Windows 8.1, Unix, Linux, Mac and Chrome), but amongst these, Windows XP and Windows 7 have been the most majorly employed, and loved.     Windows XP was refreshing compared to its predecessors, very straightforward to use, and easily available in the market too. XP was a breeze - as per the user opinions in thetoptens website and also from my personal experience [1]. The Windows 7 is the most balanced OS from Microsoft till date [1]. It successfully balances GUIs and system performance even while you play hardware intensive games. The popularity of the Windows 7 OS can be seen in the statistics figure given below.     Figure 01- OS Market share [2].    My experience and OS summary I started my career as an ASP programmer, which is again Microsoft’s web application development platform. This led me to use: a. IIS to host the application   b. MS-SQL to store and manage application data   c. Visual Studio 6 to develop ASP.Net applications     All these software programmes are native windows and I firmly believe that native technology helps in interoperability between applications by reducing the overall maintenance cost. At home, I use many photo, video and sound editing applications which are all compatible with MS-OSs and are easily available too. At work, we have got windows 7 for client machines and windows server 2008 OS to host websites and MS-SQL server to host manage/store data. Benefits:  Easy to learn and use Numerous &amp; reliable application support Improved performance with booting of OS and opening an application [3]. The search has been improved by providing the indexing option for all the drives. Hard disks are automatically defragmented, hence improving the I/O operations[4]. Better OS restore options with automatic restore points being created.&nbsp;  Drawbacks:    More RAM is required. Many legacy applications suffered due to backward compatibility issues [4]. There are still (BSOD) issues due to lack of hardware (driver) compatibility [4].  Conclusion      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; I would definitely like to improve legacy application support and handle the BSOD* errors as they cause the maximum damage to both business and hardware. I wouldn't worry much on the RAM as high resolution graphics do call for good memory and, in any case, cost of RAM is reducing drastically in today’s market. Even if we use third party graphics applications, it would demand a good amount of RAM [5]. Windows 7 has been the most hassle-free experience, even to this day. &nbsp; *BSOD - Blue Screen Of Death &nbsp; References: [1] http://www.thetoptens.com/best-operating-systems/   [2] http://en.wikipedia.org/wiki/Usage_share_of_operating_systems   [3] http://www.allcovered.com/learning/it-support-services/benefits-of-windows-7/   [4] http://www.techulator.com/resources/3311-Disadvantages-windows.aspx   [5] http://helpx.adobe.com/fr/photoshop/system-requirements.html    Best regards,   Kharavela
	              
	              Hi Anthony,

I hope Web-OS will determine the future of OS's, as it requires moderate configurations [1] and with that specification itself it performs exceptionally well.
When we look at rich standalone applications, be it an image processing application or a graphical modeling application, their existing specifications [2] are very hardware intensive. I am slightly skeptical about the fact that web-applications, being stateless as they are, will require a lot of re-engineering to support extensive graphical modeling to be used on a Web-OS platform. Then again, the best part about the Web-OS would be that, any developer who is going to create apps on Web-OS platform, will no longer need to learn and code against platform-specific native APIs!
References:
[1] https://sites.google.com/site/chromeoswikisite/home/choosing-a-chromebook/technical-specs[2] http://knowledge.autodesk.com/support/autocad/troubleshooting/caas/sfdcarticles/ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;sfdcarticles/System-requirements-for-AutoCAD-2014.html
&nbsp;
Best regards,Kharavela
	              
	              
Week 5 DQ: Functions Of an Operating System
I use both MAC 10.9 and Microsoft Windows 7 operating systems. However, I use the Windows 7 Operating system (OS) the most.&nbsp;
Windows 7 is a Microsoft Windows OS that was released commercially in 2009. The OS is available in three&nbsp; retail versions i.e. Ultimate, Professional and Home Premium. I am using the&nbsp; Windows 7 Ultimate version. I did not have a say in the purchasing of the os, that decision was made by the organisation I work for. I generally find the os easy to use, it is easy to install software, easy to plug devices to it and it has a lot of support especially the FAQs.
My experience with the OS has been good, great, bad and terrible. For general everyday operations like emails using Microsoft outlook, Microsoft Word, Microsoft Excel etc. it is generally good. Advantages with this os include: the fact that it comes with prepackaged solutions&nbsp; that are easy to implement by just about anyone, Microsoft Office scores 100 % in compatibility with any document or file and&nbsp; Microsoft Windows also has a large range of available software.
The disadvantages include its weakness to viruses as most viruses are developed for Windows, it slows down incredibly overtime, Windows, particularly Windows 7 requires at least 1GB RAM and there are compatibility issues with Internet Explorer with JavaScript and User Interface rendering that you would not get on other browsers.
For Windows 7, to improve the OS experience. I found that going through the list of a lll installed programs and uninstalling programs I do not use frees up more memory and increases the performance of my os. Because of its susceptibility to viruses and malicious attacks, my organisation has anti-virus software that constantly scans, detects and removes any viruses. As someone who constantly has multiple programs and browser windows running at the same time, I realised this slows down the performance and&nbsp; so for better performance I make sure&nbsp; that I do not have multiple programs running simultaneously.&nbsp; I also recently found out that there is&nbsp; a way to&nbsp; run fewer programs at startup by installing and using the “AutotRuns for Windows” program to see all programs running and stopping the ones you do not want&nbsp; to start on windows startup.

References
[online] http://newsignature.com/articles/why-microsoft-deployment-toolkit-2012-will-make-your-windows-operating-system-deployments-easier - Accessed on 5 October 2014
[online]http://windows-operating-system-reviews.toptenreviews.com/windows-7-review.html-accessed on 4 October 2014
[online] http://technology.blurtit.com/461951/what-are-the-advantages-an-disadvantages-of-operating-systems -accessed on 5 October 2014
[online] http://www.mnsgroup.com/blog/mac-windows-and-linux-the-pros-and-cons-of-each-operating-system/ - accessed on 5 October 2014

	              
	              Currently I am using Windows 8, actually I would say I am forced to use Windows 8 as all our desktop PC(s) in office are upgraded from Windows 7 to Windows 8 and also when you go for purchase you will find devices available in market are coming with Windows 8. Honestly, if you remove the Windows 8 user interface (UI) from the discussion, the "Windows 8 hate" drops to near zero. Still, you can't ignore the Windows 8 UI.
This is the fastest, most secure, most battery-friendly version of Windows released till date.
We can have a slim, lightweight, cheap device with an OS, which can also run Office and turn into a notebook when you add a keyboard. Undoubtedly, Windows 8 is amazing on a touchscreen system. Even older touch notebooks that were awkward to use with touch under&nbsp;previous OS give you a great experience. The final desktop look makes the transition between Metro and desktop less obvious. You can still stay substantially in the desktop if you want to and enjoy a faster, more secure version of Windows with a better browser that has longer battery life.
Windows 8 is basically made for the users who are using the OS on tablets or touchscreen machines.&nbsp; Even after doing a refresh for your machine hardware the next worry is that the end-user probably wont be comfortable with the touch screen interface; imagine a old age user who had a tough time getting used to the previous UI and now you tell him that he has to use system that requires touch inputs. He wont be happy with this news. 
Following are some pros &amp; cons of Windows 8.
Advantages:
Installation time of Windows 8 takes only 10 minutes, which is pretty fast when compared with previous versions of Windows.
Windows 8 is having a faster boot time and is equally fast to shutdown and much faster to resume from hibernating state.
The new Metro theme and layout is unique with a whole lot of cool new features. It allows multitasking and shares the screen with two apps at the same time.
Finally now a Microsoft OS is been shipped with integrated antivirus known as Microsoft Security Essentials. This has improved the security of Windows OS by a huge margin.
Backup feature is now replaced by File history.
Windows Store is in my opinion a cool addition as I am a huge fan of appstore and used to miss this in Windows.
&nbsp;
Disadvantages:
After looking at Windows 8 the first thing I asked myself where is my desktop??? It took me some time to realize that these tiles are now my start menu and then I have to spend time to arrange it according to my requirement. 
My second disappointment was not having start menu. I believe every computer literate has grown up using this feature and suddenly removing it was not a good move on behalf of Microsoft.
Another sad thing is that the System tray is gone and determining where programs have gone when minimized is a tough job.
After having Windows 8 in your enterprise environment means that you need to give training to end-users on the how’s of Windows 8. And non-technical users will need a considerable more time.
After upgrading your computer to Windows 8 be ready that many of your peripheral devices like printers, scanners, cameras might not run as expected; they will need drivers upgrade.
Last but not the least is that most of the applications, which were designed to work with Windows 7 might not be working in Windows 8.
Recommendations:
Start menu and task bar should be available or at least an option should be provided to users to enable or disable these features
Search feature should include files, mails &amp; applications
The applications that a user own or have installed on other PC from Windows Store should automatically be installed or at least should ask user to install the apps.
References:
http://www.techradar.com/reviews/pc-mac/software/operating-systems/windows-8-1093002/review/12#articleContent [Online]. (Accessed on 5 October 2014)
http://balajibalaiyan.blogspot.com/2012/04/windows-8-advantages-and-disadvantages.html [Online]. (Accessed on 4 October 2014)
&nbsp;http://www.roseindia.net/windows8/advantages-and-disadvantages-of-windows-8.shtml [Online]. (Accessed on 4 October 2014)
&nbsp;
&nbsp;
&nbsp;
Bottom of Form
&nbsp;

	              
	              Week 5 DQ - Functions of an operating system
What is an operating system? Basically it is a software which is installed on the hard drive like any other software. It creates a link between the hardware and different software. It manages and executes programs in memory; he oversees the writing and reading of data, etc. 
Without it, you cannot use a computer, it can be considered as the pilot of a computer. 
The most popular operating system is undoubtedly Windows that is installed on nearly all machines sold. However, it is not the only one, there are many others, including Linux and Mac OS X.
Currently users have the choice between an operating system and another, based on the advantages and disadvantages of each. What are the benefits of Windows compared to Linux or Mac OS? And vice versa, it's an endless debate and with no winner....
For me, my most used OS is still window, i spend more time on window as it’s installed in my personal laptop, and in the office 98% of PC run window OS. The policy of our Company prohibits uninstall and install an operating system other than the one that was delivered with the computer while all PCs come with Windows, so somehow we are forced to use Windows.
I personally use Windows since I had access to a computer, and it's more out of habit that I cling to this system and I am currently among Windows users. 
I also work a lot on linux (redhat distribution) and Solarix (version 10), especially since all my servers, at least 90% run on Linux, But my most used OS is still Windows as my laptop with which I am writing this paper is on Windows.
I am not here to get into a debate and try to compare a window linux or Mac OS, but at least I'd like to still present some advantages and disadvantages of window.

The advantage of this system developed by Bill Gates is to be very easy to use for a beginner. Its handling is very fast and 
It has a beautiful interface that improves with the emergence of new releases. 
Most softwares are developed for Windows like video games. 
Windows run in most of the computing public park, so it’s easier to find a mate "who is very knowledgeable" to lend a hand. 
This is the environment for players to play latest 3D game that just came out. 
You can find software for Windows easily in stores. 
Etc.

However, this system has many drawbacks: 

It pays and quite expensive depending on version. 
When a fault is found, creating patch to fix this vulnerability often puts a lot of time. This is because it is not free (only Microsoft can view and modify the source code) so they are the only ones who can act. 
Most viruses circulating on the Internet are programmed to infect Windows. 
You have to restart it almost all the time. 
Etc

We would like Microsoft looks at various drawbacks of Windows known and offers real and effective solutions. 
In conclusion all operating systems have advantages and disadvantages, there is not one better than other on all points. Personally I'm still on Windows (and Android for mobile phones and tablets) and think to move very soon on Mac OS. 
I do have a question for other Microsoft Windows users, where is windows 9, we went from windows 8 to windows 10, it seems that we skipped a number?
Best wishes,
Tresor Lungu
Reference:
Wikipedia Inc. (2014) Usage share of operating systems [Online]: Wikipedia Inc. Available from: http://en.wikipedia.org/wiki/Usage_share_of_operating_systems (Accessed: 04 October 2014)
Lecture Notes, (2014). &nbsp;Week 5: Operating systems [Online]. Available from: https://elearning.uol.ohecampus.com/bbcswebdav/institution/UKL1/201480_SEPTEMBER/MS_CKIT/CKIT_501/readings/UKL1_CKIT_501_Week05_LectureNotes.pdf. (Accessed: 04 October 2014)
&nbsp;
&nbsp;

	              
	              Windows is the most democratic operating system
Having used Apple Microsoft, different Unix/Linux distributions and Microsoft Windows, both personally as professionally, I must conclude the last mentioned Operating System (OS) is the most democratic of them all. After all the other candidates are either not accessible enough, because they require an expertise knowledge level, or they are only available as closed ecosystem in combination with expensive hardware.
The Windows OS is the most popular OS since it was created in 1982. It is installed on most of the personal computers sold throughout the following two decades*. As result it can be found everywhere and if one does not own a PC, a Windows machine is available in one of the many Internet cafes that can be found worldwide. As a result from all OS-es most people know how to use Windows. Furthermore it is an open system in different ways; it can operate with different hardware components from different vendors under an equal regime** and the source code plus additional documentation is shared via Shared Source partner program.
The definition of democratic is a premise of this argument. It refers to a principle of equal chances and rights. According to Unger for an individual in a society this comes down to having&nbsp; equal access to resources we can build our future with, these are; political influence, economical capital and knowledge or technical expertise. In the computer context &nbsp;we call this a Digital Divide; an economic or social inequality of a group of people who has little access to, use of or knowledge of information and communication technologies.
Availability of knowledge and being able to participate in the system with economical and political means is crucial in our comparison. Looking at these aspects the Apple Macintosh OS does not do well. Even though the OS was build on Unix it is since then totally closed and works only with hardware developed and sold by Apple cooperation. It is expensive compared to other solutions blocking out the poorer layers of society to access their products. It has a rising group of users who claim that is so accessible, but from the perspective of the vast Windows use group the Macintosh OS is not intuitive as they simply are used to other systems.
Open Source Software (OSS) candidates as Unix and Linux are considered to be very democratic to many as all knowledge is open available. From a copyright perspective everybody can access it if one has access to an internet connection. OSS could even be the only solution to gap the digital divide with this open approach but at this moment this is not the case. OSS is very elitist, exclusively accessible by technical specialists and governed by highly hierarchical communities (Mark, Misra, Kozyreva, 2008). Even though in theory everyone could use it is only applicable by a limited small group of professionals.
The closest contender to Microsoft Windows in democratic terms would be the relatively new Google Android. It is almost totally OSS and agnostic to which hardware vendors using it. But it is the main player on the mobile device market with still a small installed bases in the computer world. Looking a converging trends I will probably have to reconsider my opinion in favour of Android.
* This is changing and new or other OS become more popular. Reasons are the rise of mobile devices using touch screen optimised GUIs and Microsoft changed to drastically with their pure tile Windows 8 approach alienating a lot of the users.
** Hardware vendors had to develop a device driver (DLL) and pay for compliance testing and branding rights (Windows logo usage on their products).
References
Mark, A, Misra, R, Kozyreva, A (2008), 'The Effect of License Type and Status Hierarchy on Developer Motivation in Open Source Communities' October 1, Proceedings of the Northeast Business &amp; Economic Association, October, pp. 76-81 [Online] Available from http://eds.a.ebscohost.com.ezproxy.liv.ac.uk/eds/pdfviewer/pdfviewer?sid=aeb6e282-f9f1-41d3-b4a2-89bd43b6db51@sessionmgr4002&amp;vid=0&amp;hid=4202 (Accessed on: 05 October 2014)
Unger, Roberto M. (2004). False Necessity. Anti-Necessitarian Social Theory in the Service of Radical Democacracy. London/New York: Verso
&nbsp;
&nbsp;

	              
	              Operating Systems      The operating systems I use most often and the reasons behind my choices.    &nbsp;&nbsp;&nbsp; My first thoughts to describe the decision making behind the choice of an operating system links back to my childhood and around consoles and games. The choice between buying a specific console may have lead from several angles.  1)&nbsp;&nbsp;&nbsp; The user currently having no console, but specifically wants to play a game that is only on one specific console platform.  2)&nbsp;&nbsp;&nbsp; The user already has a console already and requires a certain type of game and while the ideal game may not be available to them, there could be an alternative very similar to what they wanted on their platform.  3)&nbsp;&nbsp;&nbsp; Past experience, in particular the user may have used a specific device at a friend’s house or on display in store and likes the experience so far.  4)&nbsp;&nbsp;&nbsp; Brand loyalty or lack of. This is a strange one but I feel is important socially and psychologically because I feel many decisions are made due to advertising appearance, word of mouth or a user’s perception of a make or model.    I feel operating systems are similar in that extent, you either already have an existing system in place that you currently use for a past specific function or piece of software and you then require another specific function or software. Or you don’t have anything in place currently but a piece of software you feel you need to fill a function then dictates what system you need to then use it.    What OS I use and why.  &nbsp;&nbsp;&nbsp; It’s hard to individually pick which current OS I use the most because I use several and for different reasons all equally. At work I have a desktop PC and laptop both with Windows 7 operating systems on them. Socially at home I use the following:  •&nbsp;&nbsp;&nbsp; Apple iPhone 5s with iOS v 7.1.2.  •&nbsp;&nbsp;&nbsp; &nbsp;iPad mini with iOS v 7.1.2  •&nbsp;&nbsp;&nbsp; Apple MacBook Air with OS X Mavericks v 10.9.5  •&nbsp;&nbsp;&nbsp; PC with windows Vista Pro  •&nbsp;&nbsp;&nbsp; Laptop with Windows 7 pro  •&nbsp;&nbsp;&nbsp; Advent Vega Android OS v 2.1    Overall though I think I use Windows based operating systems the most between my work and home life and certainly in the past would use Windows more than others so I will use this for the basis of my discussion.    Windows 7  &nbsp;&nbsp;&nbsp; Windows 7 became available in late 2009 and to date has been very successful and generally praised by critics. In the first 6 months Windows 7 sold more than 100 million copies and increased to over 630 million licences by July 2012 according to Wikipedia (2014).&nbsp; This equates to a market share of 50.4% as of May 2014 making it the most widely used version of Windows to date.  Windows 7 was formally called ‘Blackcomb or Vienna’ and was developed as a successor to Windows NT and Windows Server 2003.&nbsp; It was designed to have a faster and more reliable operation performance over the previous Windows Vista and would try to take full advantage of the 64 bit architecture.  Microsoft asked users opinions on how they could make things better for them and the users mainly asked for more simplicity, such as switching between applications and programs, recording TV, sharing documents and images among others. Microsoft provided advances in touch/handwriting recognition, support for virtual hard disks, and improvements in multi-core processors. Improvements in the kernel, direct access and boot speeds were targeted in the advertising to show how quickly it could boot up from cold.    The following diagrams from Wikipedia (2014), show the operating systems architecture needs and memory limits:  Fig (1).   Fig (2)   What led me to purchase this OS?    &nbsp;&nbsp;&nbsp; When I bought my Windows 7 laptop I chose it because of past experience both with using it at work and using many of the past Windows OS platforms over the years. So I didn’t want to reinvent the wheel so to speak and re-learn a new OS. Because from the early days of using Windows 3.1 I’ve always thought of PC’s as being the obvious initial choice and you only ever went to an Apple Mac or other OS driven machine for a specific purpose such as CAD design or music production. Windows always felt like the front runner for games and general home use. This could simply be Microsoft and their marketing ploy working on me from a young age with the tactical inclusion of Internet Explorer etc and other products alongside the easy to use GUI interface. However on the other side there are many truths with what I mention. When I was growing up through my teenage years nobody else I knew of had anything else other than a Windows based PC.&nbsp; So my choice was very much a combination of points 2, 3 and 4 in what I mentioned in the first paragraph of this discussion. I chose a Windows 7 machine because:  1)&nbsp;&nbsp;&nbsp; They are far cheaper than other OS machines in general.  2)&nbsp;&nbsp;&nbsp; I had used Windows based machines for a number of years previous and the transferable knowledge is a plus when buying a new system.  3)&nbsp;&nbsp;&nbsp; I already had many pieces of software and games based on Windows which I could continue to use. If I had changed OS then the majority of this software would be useless or I’d have to buy them all again for the new platform.  4)&nbsp;&nbsp;&nbsp; My past experiences were in general very good with Windows so I didn’t see the reason to change.    My experiences using Windows 7    &nbsp;&nbsp;&nbsp; I’ll be totally honest and simply say that I haven’t had any real issues with Windows 7. That’s not to say there aren’t any issues such as:  1)&nbsp;&nbsp;&nbsp; Hardware requirements, such as highlighted earlier. It is far more system intensive than previous versions.  2)&nbsp;&nbsp;&nbsp; 32bit vs 64bit and which way to go. 64bit being more powerful if you need a system which uses more than 4 gig of RAM. 32bit being more accessible for compatible software.  3)&nbsp;&nbsp;&nbsp; Hard to find drivers for 64 bit and fewer applications natively designed to use it.  4)&nbsp;&nbsp;&nbsp; No Quick Launch Toolbar, although you can still use the pin service.   But in my experience the improvements from the previous version Vista and in general far outweigh the cons.    1)&nbsp;&nbsp;&nbsp; Aero Peek, I really like this where you can hover over and application and it shows you a peek of the running app.  2)&nbsp;&nbsp;&nbsp; A much improved Search function over the earlier versions. You can actually find things on your system now.  3)&nbsp;&nbsp;&nbsp; Security is both simpler and improved.  4)&nbsp;&nbsp;&nbsp; Networking improvements through HomeGroup.  5)&nbsp;&nbsp;&nbsp; Media Centre, I have setup movies and music through my pc now and play them through my TV.  6)&nbsp;&nbsp;&nbsp; Massive range of software and games.    TechNet (2014), describes the changes very well which I also agree with when they say “In designing Windows 7, the engineering team had a clear focus on what we call ‘the fundamentals'—performance, application compatibility, device compatibility, reliability, security and battery life.&nbsp; This effort was aided by telemetry data on how PCs are being used and issues that resulted in poor performance or disruption.&nbsp; The focus on fundamentals didn’t start with Windows 7; in fact it is the continuation of the work on Windows Vista that materialized in Service Pack 1.&nbsp; While the first release of Windows Vista faced challenges with hardware and application compatibility, improvements introduced in SP1 and a maturing of the ecosystem has helped alleviate these issues.   Most important to IT pros will be enhancements to manageability and security—how it impacts your day-to-day work.&nbsp; Like Windows Vista, Windows 7 is engineered to make managing a PC environment more automated, controllable and efficient.&nbsp; Both client operating systems bring tools and monitoring capabilities that are not available in a Windows XP environment.&nbsp;  Further, Windows 7 imaging builds on the fundamental improvements made in Windows Vista, adding enumeration and driver management features. Data migration is faster and more flexible with a new ‘Hardlink’ feature, along with Offline Migration support.“    The ways that I think would improve Widows 7  &nbsp;&nbsp;&nbsp; This is a difficult thing to pick from because of what I know about Windows 8. I would have listed touch screen empowerment and ease of use for tablet computers which is in essence what Windows 8 tried to do but in hindsight Windows 8 was or is the worst OS in the Microsoft cabinet.&nbsp; To the point where I have actually rolled back several laptops and desktop PC’s from Windows 8 back to Windows 7 simply because it is a terrible version so I’m conflicting my first improvement. I’d love the Windows 7 or new Windows 10 OS to be as fast as my Macbook Air OS X with SSD. My Mac is insanely quick and using, from opening the lid to doing a Google search I can literally be online within 10 seconds, whereas on Windows 7 it’s more like 2 minutes at best. The next thing I’d change would be the pre installed package software and loaded software. Windows has a knack for pre loading the setup with many pieces of test software or trial software that you have to spend a good part of your first or second day cleaning off all of the rubbish you don’t need. This is a pain to begin with but also simply not needed to be in place. If Microsoft spent more time producing a better faster and cleaner installation and reduce all the clutter the software brings then they could really produce a top OS in future. I’d also like to see a stronger security upgrade with a new version, my experience with Windows based PC’s although there is no real rule for this but in general there seems to be more issues with viruses on Windows based PC’s than any other OS. I’d like to see the OS more robust to handle attacks, whether this is validation checks on I/O processing I’m not sure but I’d like to see more direction in security and clarity.    Conclusion  I like the Mac OS X, but so far the best OS I’ve had is definitely Windows 7 to date.    References:    Brookshear, G. (2011) ‘Computer science: An overview’. 11th ed. Boston: Addison Wesley/Pearson.  •&nbsp;&nbsp;&nbsp; Chapter 3, ‘Operating Systems’  •&nbsp;&nbsp;&nbsp; Section 3.1, ‘The History of Operating Systems’  •&nbsp;&nbsp;&nbsp; Section 3.2, ‘Operating System Architecture’  •&nbsp;&nbsp;&nbsp; Section 3.3, ‘Coordinating the Machine’s Activities’  •&nbsp;&nbsp;&nbsp; Section 3.4, ‘Handling Competition Among Processes’  •&nbsp;&nbsp;&nbsp; Section 3.5, ‘Security’    Microsoft (2014) ‘Windows 7 Pro review’&nbsp; [http://www.pcpro.co.uk/reviews/software/352633/microsoft-windows-7-professional ] Assessed 5 Oct&nbsp; 2014.  TechNet (2014), ‘Windows 7’ [http://technet.microsoft.com/en-gb/library/dd266801.aspx] Accessed 5 Oct 2014.  Wikipedia (2014) ‘Windows 7’ [http://en.wikipedia.org/wiki/Windows_7] Accessed 2 Oct 2014.   
	              
	              Hello Terry,
After reading your discussion post I wonder why you do not use a Linux distribution like Ubuntu as OS. Probably you run it on a virtual machine already. But still, is it for your use not a better option?
Greetings from Bram
	              
	              Hi Terry/Augusto
&nbsp;&nbsp;&nbsp; Thanks for the statistics and your insights. I also agree with using the word USAGE. I would like to see an unbiased body doing an accurate survey on Chrome, Firefox, Safari and IE. I am using IE version 11 and happy. IE has come a long way and done a good job is securing the browser.
robin

	              
	              Terry
&nbsp;&nbsp;&nbsp;&nbsp; What do you mean by someone "touching your laptop screen" ? Can you explain ?
robin
	              
	              Hello Augusto,
Interesting points you make in your reply.&nbsp;
Popular is a confusing term as it always refers to a context. For a TV-series Lost is very popular and 5% of the population an immense figure. By the way measuring the use of TV-series (and other TV-media for that matter) is not clearly defined eiter as on line viewing statistics are not standardised yet. A 'view' in traditional broadcast would be someone from a dedicated panel selected on background, age and sexe tuning to a channel for 5 minutes. What is a view in on demand terms, starting a file? Is there an independent organiations gathering these data or does the content provider deliver these statistics?
I agree that browser use statistics are not well defined but I wonder how that can be done correctly? It concerns a lot of privacy sensitive data that should not be shared with anybody. For example you use IE for business portal applications. Surely you do not want that to be reported outside your business network. And again who is going to gather this information.
To my understanding the current reports are derived from server side statistics gathered from (popular) websites. &nbsp;The web-elements use play a role in this calculation as&nbsp;big players like Microsoft, Google and Apple try to block each others services (mis)using technology. Safari on iPad not allowing Google maps or Flash, Microsoft pre-installing IE etc.. The website measures only what browser is used to visit.
Greetings from Bram
	              
	              Hi Bram,
That's a good question. I actually do use Ubuntu on my desktop computer. But I have to stay with OS X on my MacBook. Ubuntu is not really a great GUI OS, and it's not quite stable as OS X. Like several colleagues already pointed out, I almost never reboot my MacBook. The GUI applications on OS X is much more friendly. You seldom see any "OK" or "Cancel" button on a dialog with OS X, and I like that simplicity philosophy.
So OS X provide enough similarity to a Linux system, and there are plus plus.
Say, if one day Apple removed the similarity to the Linux system, I would still immediately ditch it and switch to Ubuntu. But that won't happen because OS X is Unix.
br, Terry
	              
	              Hi Robin,
I meant this: when you were discussing with your colleague in front of your laptop, your colleagure used his figure to point at and touch the display of your laptop:-)
br, Terry
	              
	              Hi Augusto and Bram,
Yes, all the statistics are done under some assumption and evaluated only part of the fact, as they all claimed. That's why I cited them as a range (2 to 4 times), in the hope of softening the tone because of not having a sound source. I would like to learn the correct academical way of citing data like these.
br, Terry

	              
	                &nbsp; Hi Dr. Anthony, &nbsp; I. Summary of Windows Operating System. The history of the Windows Operating System (OS) began in 1981, when MS-DOS was developed by Microsoft for the IBM desktop computer. This OS could run using 8Kbytes of memory and the Intel 8086 microprocessor and was capable of running only one application at a time. In 1990 Windows 3.0 (16-bit) which had a Graphical User Interface (GUI), became the successor for MS_DOS. This was followed by Windows NT (New Technology) 3.1 in 1993, and “several versions of NT3.x followed with the support for additional hardware platforms”, that is Server platform. In 1995 a 32 Bit OS version, called Windows 95 was introduced for desktop computers, followed by NT 4.0 for servers, followed by Windows 2000, an upgrade to NT OS, in 2000. Windows 2000 had additional services that supported distributed processing and Active Directory and power management capabilities. After Windows 95, came the Windows 98, then Windows XP in 2001 which goal was to “replace the versions of Windows based on MS_DOS with an OS based on NT.” Windows Vista was later introduced in 2007 followed by Windows Server 2008. Windows 7 and Windows Server 2008 R2 were introduced in 2009, William Stallings (2012). These were later followed by Windows 8 and Windows Server 2012.  Today Windows based OS is popular among many owners of many desktop computers, laptops and servers.  At work, I use Windows 7 and at home I was using this as well, since I was using a laptop assigned to be by my organisation. Our home computer also had Windows 7, however we recently switched to Ubuntu (Linux OS), because of some issues we were having, mainly hardware associated.  II. Reasons behind my purchase or use of this OS. The Windows OS was the first OS that I was introduced to, additionally it was the OS used on the computers at work as well. Linux was introduced to me at university, however it seemed a bit complicated, thus trying to familiarize myself with it was more of a challenge then. To me the OS was not as user friendly compared to Windows. I was also introduced to NetWare (Novell) Software at my previous place of work, this once again because of lack of understand and user friendliness appeared a bit of a challenge as most of it was DOS based and required the learning of many DOS commands. Thus I think familiarity and user friendliness were the two primary reasons for my choosing a Windows OS. I also believe that Windows was more or less thrust upon me, since little of the other OS were popular at that time. Thus, upon purchasing and choosing a computer, Windows was my ultimate choice.  III. Experiences with Windows OS Prior to our switch in OS, my husband experienced challenges with his computers, laptop and desktop, hardware included. With the desktop some days it would booth and other days it did not. For this reason I purchased another computer to do my online studies. However I almost wish I did not do this, since these challenges are almost illuminated with our recent change in OS. A. Benefits of Windows OS 1. System Support: because of the popularity of Windows, it is easy to get system support for this OS as compared to others; the knowledge base for other OS is not as large as that of Windows. 2. Applications and drivers accessibility: it is easer to get compatible applications and drivers for Windows based OS as compared to other OS. 3. User friendliness: I have become very accustomed to Windows GUI and it is very user friendly, baring a lot of similar characteristics through out the years. It is also easy to maneuver. 4. Security features – these include system utilities such as windows defender (for Spyware protection), Windows updates, Internet security settings, Network firewall and User Account Control, P. Thurrott, (2009). 5. Maintenance features – its troubleshooting and problem reporting capabilities. An additional system utility feature that I liked about Windows is its System Recovery and other Windows repair tools take can make it easy to fix your personal computer. B. Drawbacks of Windows OS 1. Licensing – high licensing cost is a setback and a challenge to many in underdeveloped communities. Windows 8 costs over a US$100 while Linux is free.   2. System requirements - Windows 7 and 8 requires 1 gigahertz (GHz) processor, 1 gigabyte (GB) RAM&nbsp; or 2GB for 64 bit OS and 16 GB or 20 GB for 64 Bit OS available free space on a hard drive. While Ubuntu requires 32MB RAM and 500MB of hard disk space to perform a regular installation. In underdeveloped countries you will find persons with what we like to call 'hand me down' computers which may be old and as such having little memory and storage capabilities. However these individuals may still want the added features and functionalities of newer versions of Windows.  3. Stability – one of the features of windows is that it includes its graphical capabilities in its kernel while Linux does not, this makes Linux a more stable OS than Windows. At work, even though the system requirements for Windows is as stated above, I have installed Windows 7 on some of the computers with this minimum system requirement and these computer performance degraded. While with Linux from a personal perspective performance is fast and you often encounter little or no problems.      IV. Recommended ways to improve the Windows OS A. Lower licensing fees: I would first off make licensing cheaper for owners of personal computers, that is licensing for all Windows OS or at least offer persons from underdeveloped countries lower licensing fees. This may actually boost sales and reduce piracy in these areas as more individuals may be able to afford licensed copies on Windows. B. Lower System requirements:if the system requirements could be lowered to something that matches Linux, I believe it would be advantageous for Windows. C. Improved performance: once again based on experiences I believe that there is still room for improvement, where performance is concerned and sensitivity to hardware. In closing, I believe if more persons were familiar with the capabilities of Linux OS, especially in underdeveloped countries, they would be choosing Linux instead of Windows, Linux is also now more user friendly. Also if there was greater support in terms of applications compatability, Linux may have been preferred.  References: Windows 7 system requirements, [Online]. Available at: http://windows.microsoft.com/en-us/windows7/products/system-requirements(Accessed on October 5, 2014) Memory and Disk Space Requirements, [Online]. Available at: https://help.ubuntu.com/10.04/installation-guide/powerpc/memory-disk-requirements.html(Accessed on October 5, 2014) Stallings, W. (2012), Operating Systems: internals and design principles, 7thed, pages 80-81, New York: Prentice Hall. ISBN: 978-0132309981  Thurrott, P. &amp; Rivera, R. (2009), Windows 7 Secrets, pages 257-260, [Online]. Available at: http://books.google.gy/books?id=1EXt2U84WZ0C&amp;pg=PA254&amp;dq=benefits+of+windows+os&amp;hl=en&amp;sa=X&amp;ei=e_IxVJqaNcrjsASOkYHADA&amp;ved=0CD0Q6AEwBg#v=onepage&amp;q=benefits%20of%20windows%20os&amp;f=false(Accessed on October 5, 2014)    
	              
	              Hi Everyone,
I am using Ubuntu and sometimes when I copy and paste my work into the forum the fonts is messed up after it is posted. I don't know why. Resently I have been clicking on preview but when I clicked it today, the submission looks fine but after I submit, it is not as it was. Any help?
Tanisha

	              
	              The operating system I currently utilize is Microsoft Windows 7. Microsoft Windows has been the dominant operating system for many years and as such most corporations utilize Windows family of operating systems for their computing needs.
Windows 7 boast several advantages that makes it one of the easiest operating system on the market. It is very user friendly and easy to learn, faster start-up and shutdown times ,productivity features allow for efficient use, stability of the OS is rock solid and makes it resilient to application failures and other system issues. 
In spite the glowing reviews this OS received there are still a few disadvantages, which includes; the fact that there is no upgrade path from Windows XP and a clean install is required, removal of several applications that came preinstalled with older Windows OS's and incompatibility issues with several applications and hardware drivers.
&nbsp;
Windows is the primary operating system utilized in our local IT industry, its more affordable than competing OS's, its usability far outweighs the competition, most business applications/relational database systems and other production software are designed for windows OS environments. Having used the previous editions of Windows OS's, a decision to purchase Windows 7 was the next logical step in upgrading and keeping current. This decision also allowed me to take advantages of the new improvements and better familiarize myself with the product so I can advise clients.
&nbsp;
Windows 7 brought some significant benefits to desktop operating systems, three of these includes:

Address Space Layout Randomization - this feature "makes it more complicated for attackers to determine where core functions reside in memory". It works along with Data Execution Prevention which "prevents buffer overflow attacks from working on files or in storage areas that are specifically intended to hold data".(www.pcworld.com, 2009)These features makes it difficult for vulnerabilities to be exploited in some applications.
Live Taskbar - This feature allows you to preview open applications in thumbnail sized window by hovering your mouse application icon on the taskbar. You can click on a thumbnail preview to open into a full sized window or click the close button in the thumbnail preview to close an open file. 
Snapping Tool - By dragging an open windows to either side of the screen, the OS will resize the window and affix it to the left/right half of the screen. This feature allows you to do side by side comparison of documents.

&nbsp;
Windows 7 also has a few drawbacks that are worth mentioning and we will look at a few of these:

Unable to migrate from Windows XP - Most corporate organization used windows XP before moving to Windows 7 and had to do a clean install if they wanted to continue using previous hardware. This made upgrading a tedious process and windows 7 required greater memory resources than Windows XP.
Hidden Files Extensions - This isn't new with Windows 7, however as a continuation from previous OS's it is worth mentioning. File extension are left off when displaying a filename in windows explorer. Hiding the file extension can cause a security vulnerability by allowing executable malicious content to masquerade legitimate files, go undetected and be executed by an unsuspecting user.
Driver support - "is not provided for old systems and the old version of motherboard. This disables some of the features in Windows 7" (www.techlator.com, n.d.). This will force some users and corporations to maintain windows XP to ensure legacy applications can continue to operate.

&nbsp;
Windows 7 continues development through service packs, hot fixes and other updates from Microsoft to address operational issues with the OS. In the corporate environment Windows 7 provides a stable, reliable environment for running business applications and productivity tools. Additional improvements could be made in the following areas:

Security improvements to windows firewall to allow for other functionalities, like IPS and IDS.
Implementing multiple desktop environment.

&nbsp;
&nbsp;
&nbsp;
References
&nbsp;

Tony Bradley (2009) Pros and Cons of Windows 7 Security [online]. Available from http://www.pcworld.com/article/182917/pros_cons_windows_7_security.html (Accessed: October 5, 2014).
Technet Magazine (n.d.) 8 Common Issues in windows 7 Migrations [online]. Available from http://technet.microsoft.com/en-us/magazine/ee652552.aspx (Accessed: October 5, 2014).
Balasubramanian, S. (2010) Disadvantages of Windows 7 [online]. Available from http://www.techulator.com/resources/3311-Disadvantages-windows.aspx (Accessed: October 5, 2014).
Technet Magazine (n.d.) 8 Common Issues in windows 7 Migrations [online]. Available from http://technet.microsoft.com/en-us/magazine/ee652552.aspx (Accessed: October 5, 2014).
O'Neill, S. 2009, "Upgrading to Windows 7 could be heaven or hell", ComputerWorld Hong Kong, vol. 26, no. 8, pp. 42-43.


	              
	              Hi Bo,
All our graphical designersuse MACs because like you said they are better with graphics. Our IOS apps team also uses MACs because well they have to if &nbsp;as they are developing for IOS. Some Test Analysts like myself use MACs too for larger test coverage. I am an Automation Test Analyst and I write test automation scripts. There is a new tool I'm using for mobile apps test automation and that tool was developed mainly for MAC, it can also be run on WIndows and Linux but any other platform other than MAC has the barest features. This is how I got to move over to MAC, I'm using both MAC and a Windows 7 machine.
When I first started using MAC, I literally had to google nearly everything. How to right click, copy and paste hot keys, where to find the equivalent of Windows programs folder and something as easy as delete. It really was hectic. But once I got the hang of it I enjoyed somethings about it like how quickly it is to restart your machine and the performance is great.wha
Have you used a MAC before? If so what were/are your challenges?
kind regards,
Belinda
	              
	              Hi Belinda,
Are you using Calabash? I'm learning it.
br, Terry
	              
	              Hi Tanisha,
I have had similar issues with posting blank replies even though they looked fine when I typed them.

A good trick is I always cut my work into notepad or textpad first then cut from there and paste into here.

that seems to work and it takes all of the hidden formatiing and styles out of the text.

ta
Chris

	              
	              Hi Bram,
You make a good point about the TV show viewing statistics.&nbsp;
The problem with web statistics is that it's hard to find a way to done it correctly. Mainly because, as you mention, it depends on something "looking" at user usage on the actual website. I do not think it would be a privacy problem (but it could become one), it's mainly how do you make all web site developpers agree to put something on their code to send you usage data? But before that, people should start with some basic statistics, for example deciding which sites are the ones that you can monitor. In theory a good statistical population could even be a thousand sites, but you have to have a scientific method to determine why and which sites to choose. For example: I can imagine someone thinking that google.com is a good candidate, because everyeone assumes there is a good part of the internet users that use that search engine. However we should ask critically ourselves if that's a good site to pick for such statistics. Consider that being a google site, all google services and browsers will automatically go there, so it's a fair assumption to think that most of the users using google.com would actually use google products and hence use Chrome, wouldn't that give you a biased result for your statistics. Same would apply to bing.com, wouldn't most of the browsing users on that site use IE? I believe that before doing an statistical reaseach for browser usage, one should do a research about which sites to pick up for your research. This is a common practice when choosing the right statistical population.
But, hey, I guess we are going off topic here :).
Best Regards,
Augusto


	              
	              Hi Everyone,If you were asked to draw up a wish-list of functions, features and attributes that you would like to see included in the next generation of computer/network Operating Systems, what would you have on your list, and why?Anthony
	              
	              Hi Anthony
Yes, I definitely see this as an area of significant evolution, especially for consumer-level Operating Systems. 
If we take the traditional home user, a significant portion of their time is spent on the Internet. According to an article in the Guardian newspaper last year, they indicated that there was a report that claimed people in the UK spend 1 in 12 waking minutes on the internet. In the same article, Ofcom claimed that people in the UK spent on average 35 hours online each month. 
In my opinion, I actually think this is conservative however this does still suggest that a large proportion of activity that is done on a computer (or smart-phone, laptop, tablet) is actually done using the Internet. So, why not extend the OS to be more deeply integrated with the Web?
We have seen the introduction of the Google Chrome-book (which&nbsp;Terry has also alluded to), it being a computer laptop that is based around Internet-connection and interaction. 
Up until now, I didn’t necessarily realise that this could be referred as a Web-OS (or some distinction thereof), but as you start to think more about these topics, it draws your mind to other technologies that are exploiting this. 
Microsoft launched Office 365, a different licensing model for Office applications, which is a better option for many, both consumer and business. This is exploiting Web-based services. So again, why wouldn’t we see the OS evolve more towards the Web? 
We have also seen the integration of smart TV’s with the Web, the ability to access and download content. One other example that immediately springs to mind is the other electronic items that are already integrated with the Web. For example, home printers. I have an HP printer that has the ability to automatically connect to the Web and order new ink when needed. I don’t actually use the facility at the moment, but the option is there. 
Equally, my piece on smart metering a few weeks ago, in which this is integrating other technologies with the Web. 
I realise there is a distinction between simply accessing the Internet and Web based technologies, however I don’t see why we wouldn’t see the OS evolve even more, especially for the consumer end of the market.
In terms of the business end, I can see that we will exploit certain levels of Web integrated technology, and I am leaving my mind open, in terms of the extent of which we will see this for business users. I do however feel that for desktop type activity, this will naturally go hand-in-hand with my thinking for the consumer level integration. However, for the larger enterprise level OS’s, at this stage I think this may well just be to exploit features, rather than a clear evolutionary roadmap.
One last thing – In my other piece a couple of weeks ago I talked about energy consumption and the impact this has on our devices needing to be regularly charged. I suggested that, if the activity that consumes the most energy could be performed remotely then the devices could potentially be used for longer without needing to re-charge. What if the operating system was actually in the Cloud, a truly web-based OS, and that the device itself was simply a viewing device? Don’t get me wrong, I haven’t thought this through too much, so it could have a stack of holes in the theory, but could the OS be entirely in the web too?
In conclusion, we are seeing many things that are migrating to the Web, so why not the OS too?
Best Wishes, Craig
References
Chalabi, M. ed. (2013). Do we spend more time online or watching TV [Online]. Available from: http://www.theguardian.com/politics/reality-check/2013/oct/08/spend-more-time-online-or-watching-tv-internet (Accessed 5 October 2014)

	              
	              Thanks Augusto - I'll give this a go. Best Wishes, Craig
	              
	              Terry
&nbsp;&nbsp;&nbsp;&nbsp; Got you. I don't like people touching my screen. But that being said, I got use to Windows 8 touch screen laptops and really found the touch screen feature useful, especially when you want to enlarge a spreadsheet or powerpoint.&nbsp;&nbsp;
robin
	              
	              Hi Anthony,
Trying not to confusing this with webOS (http://en.wikipedia.org/wiki/WebOS) .
I take it you mean a more inclusive state of OS that could be termed 'web-OS' in that a large proportion if not all of the OS is web based.?
Or is it the re-terming of the older remote access explanation such as this: (http://www.engineersgarage.com/articles/web-operating-system-webos)

I'll be totally honest and say as a web designer and administrator for my job I have hardly ever come across this term other than what I mention above being the mobile OS or the remote access of machines version.

If I think of the term being either a partially based or fully based OS on the web so that a physical computer doesn't actually need to store or run the OS on the machine but instead is merely a bios boot to the web then storage if needed solution then I can't help but choke at the idea.
That's my worst nightmare.
What if you only want a stand alone machine?
What if you don't have Internet access ? how does it function?
What are the security implications not to mention the strain on global bandwidth across ISP's.?

However if the term is targeted at a Web designed OS like what others have mentioned so that it is still stand alone and installed completely on a PC or laptop and doesn't require Internet access but more about the OS itself being designed in Java and XHTML for example then this does have promise.&nbsp; With potentially easier integration of apps and more control over native reconfiguration it could be a powerful shift where you could get more open source OS and bespoke designed OS's based on the core 'webOS' model. It could also give businesses more control of what they want from a PC and OS to specifically target what they need rather than being given everything first and using what you want.


references
Preeti Jain , 'WebOS ( Web Operating System )' [http://www.engineersgarage.com/articles/web-operating-system-webos] accessed 6 oct 2014
Wikipedia (2014) 'webos' [http://en.wikipedia.org/wiki/WebOS] accessed 6 Oct 2014




	              
	              Hi Chris,
A fully web based OS isn't something unthinkable.
With the following open source project Jor1k, you can run a Linux virtual machine with in your browser. You can use the Linux command line, write code in C, browser the Internet, play games like Doom, and even watch movies!
https://github.com/s-macke/jor1k
You can try it here:&nbsp;http://simulationcorner.net/index.php?page=jor1k
I bet your computer will be very hot if you run the VM in you browser for 30 minutes:-)
br, Terry
	              
	              Hi Terry,
i'll take a look at that and have a bit of a play, see what I can break. :)

I may have not described my reply right , I understand that a web based OS is possible and not unthinkable like you said and the way you explained it is right.
What I mean is that if you buy a PC or laptop and it comes with no OS but instead has a boot link to the web where your OS is stored so in essence you have no OS on your machine at all. That situation was unthinkable.
The example you gave is fine but it is based on the fact that you already have an OS on your machine and if you like you can go online and run another OS on the web and like you mention do all the tasks and games and coding you like.

ta
Chris



	              
	              Hi Terry.

sorry forgot to add this , but you mentioned playing Doom.
Take a look at this : http://hol.abime.net/1799
Have a look at the artists and musicians... lol .....&nbsp;&nbsp;&nbsp; IT'S ME. !
In my earlier years I helped my school mate build Gloom 3 Zombie Edition.&nbsp; You'll notice 3 of the guys surname is
Murfin because they are the 3 brothers who i'm still friends with now.
I did some of the voiceovers and a whole load of code testing and playtesting while watching a load of classic George Romeo
Zombie movies and eating loads of buttered toast.
Not bad for an 18 year old first time coder to produce a game that got published on the Amiga eh.!

shame it's been downhill ever since lol

ta
Chris




	              
	              Hi Chika&nbsp;
I downgraded from Win 8 to 7 because I’m more comfortable with it. There are a few core irritants in Win 8.x that need fixed. Some really basic and irritatingly neglected like appearance controls between the Metro vs. Desktop that are completely disconnected and clumsy if not garish to look at or irritatingly distracting from the tasks at hand. 

I feel Microsoft needs 2 different OSs like Apple, one made FOR mobile and one made FOR the PC or what do you think?
	              
	              Hi Chris,
I just wanted to show what's possible:)
I agree with you that it's an OS's job to manage the lower level resource. But a more important job for an OS is to provide the consistent API to the software running on it.
Most of the software I know, including OS, is runing more or less on some sort of virtual machine. [ I certainly need to cite something here]. So whether the resource the OS is manging virtual or real doesn't really matter much. 15 years ago people were so keen to have the Java hardware. Nowdays most people have admitted that a Java Virtual Machine is good enough.
On the other hand, like you pointed out in your initial post, this is a different topic than the "WebOS", &nbsp;which is an OS that mostly runs Web application.
br, Terry
	              
	              
	              
	              Hi Babatunde,

I completely agree with you.

I've done many downgrades from windows 8 back to 7 simply because it's very hard to find machines that come with Win 7 on them now so you have to buy Win 8 machines and roll them back.
I hate Win 8 with a passion. That might seem extreme but like you I feel they completely missed the trick, They had a very good and market leading Windows 7 . All they had to do was make a few improvements and release windows 8 with a completely new version solely for tablets&nbsp; so you would have 2 versions to pick from.
They quickly realised the mistake and tried to resolve it with the 8.1 change but it's still no good. I just hope Windows 10 is as good as they are hyping it up to be and I hope it's got nothing to do with tablets and touch screen otherwise i'll be on windows 7 for a good more years yet to come.

ta
chris


	              
	              Hi Chris,
Wow! You rock! It must have been really fun and memorable, 17 years ago:-)
Sorry that I cannot enjoy the game. I got motion-sickness. I admire my friends who can read and even work in taxis and buses. But, still, this is cool. You may try it on Linux-in-a-browser:-)
br, Terry
	              
	              Hi Ramin,
You're talking about the same company that went from Xbox 360 to Xbox One.
We no longer live in a world where we are forced to use what Microsoft demands we use. Windows 10 is nothing more than Windows 8.1 with a reworked UI and a few features while keeping most of the LONG LIST of hated things about it. They believe giving the start button back is enough, it isn't.
I'd like to think that Microsoft finally learned their lesson with the W8 debacle, but it doesn't appear so. If all the company wants is to generate a continuous stream of revenue from selling OS software, I would have been happy pay for an improved version of XP Pro every couple of years. Same interface, but with improved reliability and security.
Regards, Babatunde
	              
	              Hi Chris.
I am one of those who would have loved to have remained on Windows XP, but there's this parameter Microsoft calls End of Life Cyle and in the case of &nbsp;Windows 7, &nbsp;Mainstream support for Windows 7 is ending in Jan 2015 (http://windows.microsoft.com/en-us/windows/lifecycle). This effectively means that automatic fixes, updates, or online technical assistance will no longer be provided and therefore&nbsp;forces yet another O/S upgrade even where relative stability has been achieved in use of the current operating system. Under the extended support which follows, the main support that will be given is only to do with security bugs but no service packs or additional features
The challenge as you are currently experiencing where shipping of new computers no longer comes with Windows 7, can sometimes be further compunded in the event that device driver vendors and other third party software vendors such as those for anti-virus also jump ship to the new O/S. The bigger challenge I often find is also when new machines that ship are incompatible with prior Operating Systems literary closing out your fall back Windows 7 option
References
1) Microsoft Website, 'Windows lifecycle fact sheet', April 2014, Available at http://windows.microsoft.com/en-us/windows/lifecycle, Accessed 06/10/2014
2) Jones, S. 'The end of Windows XP support: what it really means for businesses' , Enterprise | Real World Computing | PC Pro, (Nov 2013), Available at http://www.pcpro.co.uk/realworld/385723/the-end-of-windows-xp-support-what-it-really-means-for-businesses , Accessed 06/10/2014
3) Kelly, G. 'Microsoft To Abandon Windows 7 Mainstream Support. Pressure Builds On Windows 9', Oct 2014, Available at&nbsp;http://www.forbes.com/sites/gordonkelly/2014/07/10/microsoft-windows-7-mainstream-support/,&nbsp;&nbsp;Accessed 06/10/2014



	              
	              Hi Bo. One of the challenges that Ramin mentions above is the push to migrate clients from an O/S like Windows XP which preety well and for which many legacy applications still run very well. I recall one of the things that Microsoft did was create a situation where Windows 7 has 'known compatibility issues' with programs such Visual Basic 6. I do know that there are many programs that have been written in VB language and are providing the required Line of Business functionality that is required. However, with such a change to the level where Windows XP may not work on some new machines being shipped, programmers are preety much compelled to re-write or port their apps to new web based platforms or Microsoft .NET. The application migration issue can be really a challenge Joseph  
	              
	              Thanks Chris,
I just had to check to see if Ubuntu has notepad and it does. I will try your suggestion.
Tanisha
	              
	              Hi Craig,
Good post here but a common concern about Web-OS is that they require users to trust a third party to keep potentially sensitive data secure. Will the provider of web-OS be able to fend off hackers? 
I think we'll likely see a battle between hackers and security specialists in here or what do you think?
&nbsp;
Regards, Babatunde
	              
	              Hi Anthony,

This will be a significant evolution. 

Although, Web-OS can't replace your computer's native OS, in fact, they depend on traditional computer operating systems to work. But the most attractive feature of it is that you don’t need to worry about upgrading computer systems every few years. As long as your computer can run the browser or client software necessary to access the system, there's no need to upgrade.

Some people become frustrated when they have to purchase new computers in order to run current software. With Web-OS (distributed computing), it's the provider's responsibility to provide application functionality. If the provider isn't able to meet user demands, users might look elsewhere for services.

Best Regards, Babatunde


References

Jain. P. WebOS (Web Operating System) [online] Available from: http://www.engineersgarage.com/articles/web-operating-system-webos (Accessed 6 October 2014)

Jonathan S. How Web Operating Systems Work [online] Available from: http://computer.howstuffworks.com/web-operating-system.htm (Accessed 6 October 2014)
	              
	              Hi Anthony,

This probably would be an enormous list but the few I could list out for now would be the Merging features of laptop and tab.
Tab in the sense that the convenience to move around and the comfort to read on the go while, for a laptop, the variety of applications for robust use. For me, merging is super duper 
Also looking into Flexibility to adapt to radically different network environments, design that will automatically switch Windows modes depending on whether a device is being used as a tablet or as a laptop, where all content can be portable and users can toggle easily between phones, tablets, laptops and PCs.
Then my added list which may not be possible is for all tech companies to pool resources and work together. Because the extremes of isolation are robbing the consumers and their own bottom lines. If they work together we all benefit. If it becomes completely proprietary we are wasting our time bothering to entertain it.
I think Apple, Android, and MS need to work with each other more meaningfully with regard to the consumer communities.
&nbsp;
Best Regards, Babatunde
	              
	              Hi all
Contrary to me generally liking Windows, I totally agree that Microsoft has done really badly regarding W8.x. It looks like one of those cases where the developer, Microsoft, don’t care about the end users or any other user for that matter.
And then, adding the “End of mainstream support” date (Windows lifecycle fact sheet, 2014) for Windows 7 to January 13, 2015, is what I could be tempted to call a hostage situation. It will be my hope that Microsoft will change this date because I am very sure that many companies, and other consumers, will not upgrade to Windows 8.x yet because it’s too drastic.
On the other hand, I do understand that a vendor like Microsoft set a deadline. It’s just that the combination of W 8.x design and W 7 end date is terrible.
Windows lifecycle fact sheet (2014), [Online] http://windows.microsoft.com/en-us/windows/lifecycle (Acccessed 2014-10-06)

br. Bo
	              
	              Hi Belinda
No, except from my iPad, I'm not using MAC. Firstly I don't have the use for it. Secondly I think MAC is to proprietary, i.e. if I were to use MAC and later wanted to move to some other platform, it would be to difficult. That's way I stick to Windows because I don't feel as tied to just one hardware brand, and there are a lot of different Windows applications to switch between. Also regarding smartphones I stick to Android cause it allows me to switch between different producers.
br. Bo
	              
	              Hi Ramin
I completely agree on the waiting for Windows 10. I don't see our municipality to implement Windows 8.x.

br, Bo
	              
	              Hi Joseph
I agree with you that the programmers, not only will be compelled to, but actually forced to re-write the code. If I was deciding then I would go for .NET and web-services,&nbsp;of course&nbsp;depending on the situation. When I worked at a data center for the banks I worked on a project&nbsp;(we were 2 project managers and I was responsible for GUI and all techical issues),&nbsp;were we started the process of developing .Net solutions which in the longur run were supposed to replace mainframe systems, and using MS SQL instead of IBM DB/2, though we had to have integrations to the mainframe since the mainframe was the primariy system.
br, Bo

	              
	              Hi Anthony

I think the term “Web-OS” is very interesting, and I don’t think that it’s “just a fanciful thinking”. I think it’s just part of the IT evolution. But I am not sure just how this is going to happen. I think history has told us, that there will not be a single standard, and that there will always be competitors and “true believers” who will fight for their true standard.
And like I think Craig indicated, I expect the private consumers to be the first to use a Web-OS. And I conclude this from the increasing use of cloud solutions, e.g. Office360 and Internet of Things (Kelion 2014).
But like Chris and Babatunde rightly point out, there are some serious security issues to address.
References
Kelion, Leo (2014). BBC News Technology. [Online] http://www.bbc.com/news/technology-29410999. (Accessed 2014-10-06)   
br, Bo

	              
	              Hi Anthony,
Yes, I also think that many operating systems will evolve with the cloud computing. By combining the two technologies, we will probably in a near future, witnessed the birth of operating systems completely on the web and based only on browsers (which can become real OS), and Google is already on the way with its Chrome OS. Web OS contains just a browser with a Linux kernel for example. No local applications, no data on the machine. Everything must be on the internet and the online mode will be mandatory. 
Web-OS can be incredibly the operating system of the future.
Reference:
Wikipedia Inc. (2014) Chrome_OS [Online]: Wikipedia Inc. Available from: http://en.wikipedia.org/wiki/Chrome_OS (Accessed: 06 October 2014)

	              
	              Hi Ricardo,
I have been a user of Windows 7 for long time and there is no doubt that it is very stable OS from Microsoft with user friendly interfaces and improved performance. My company recently upgraded Windows 7 to Windows 8 as per the recommendation of security auditors as they claim there are many vulnerabilities in Windows 7, though almost all the employees where unhappy about this change but our company never compromise on security concerns.
It will be helpful if you can share your company policy that how they handle the situation where critical security related changes have conflict with employees comfort.
	              
	              Hi Anthony
I'm also with Babatunde, I think that we will see much more integration between certain devices.
I also agree that the list could be huge, so I will start with just one feature for now please, and that is based around cyber security. I'd like to see all OS's have the attributes of not being hackable and not being susceptable to any virus. Now, I realise this may well be an enormous challenge therefore, my next feature would be a self healing OS - an OS that has an immune system, and is able to intelligently recognise when something isn't quite right, and immediately and intuitively remedy and repair the issue without the need for user interaction.
Best Wishes, Craig
	              
	              Hi Bo
I completely agree, security is a vital part of this evolution.&nbsp;
Just trying to think beyond the current concerns and restrictions here... Do you think it could be possible, as this evolves, that the security features may become completely robust given they are to serve everyone (or at least a significant number) in the Cloud / or for Web OS?&nbsp;
I guess it could be argued that the technology vendors don't invest in this as heavily as they should, and perhaps this is because they treat it as an individual issue (narrow minded approach, albeit it is on a large scale).
What about if this is treated as a large scale issue (holistic approach, broad minded, still on a large scale) and took that level of responsibility...&nbsp;Could we find that security is actually better in the Cloud / or with Web OS, as apposed to 'having your own'?&nbsp;
Just a thought...
Best Wishes, Craig
	              
	              Hi Numan, Hi Ricardo
Just following your threads with interest
Numan, you have touched on something here that has a much broader impact on the business aswell.
One of the key challenges of any CIO or IT leadership is balancing the need for change with the impact on the business and as you have described, in relation to security, this must not be compromised. The challenge is on implementation.
I remember reading an article last year somewhere (I cant recall where, but if I come across it again I'll send over) whereby IT moved away from Outlook to Gmail (I know, it isn't a security issue, but the context is similar) and it had a devastating affect on the way the business operated, staff morale and productivity dropped massively and IT was seen as failing the business. I think the CIO was let go too. Of course, there may have been a very valid and compelling business case to proceed, but the business suffered as a result, and whilst IT was acting in the best interests, they were perceived as failures.&nbsp;
My point here is that this may not be just a policy issue, but also around how something is implemented to mitigate the impact across all areas, and not just the security risks in isolation.
Great that you highlighted this, thanks.
Best Wishes, Craig
	              
	              Hello all,
Here is my whish list:
A better support for different VPNs is a must. Nowadays one has a work VPN, VPN to different NASs and a VPN to access certain geo-located services. Even tough some VPN functionality is available they are a bit clunky and create conflicts when you switch between them a lot.
The integration within (home) networks with devices could be improved. Within one network one should be able to combine a selection of devices again and share resources more effectively.
More privacy control is another subject. It is pushed away here and there in some cornes of the OS and should be made more visible and user friendly so one could actually change certain settings or control its behavior over time.
An more friendly 'user terms' as it is not a 40 page document nobody will read or understand.
Version control of files and good file manager that allows to find, compare and manage doubles and backups on different locations. 
Better management tools to manipulate registries and temporal files for optimizing the OS even after it used for some time (taking into account users that install and de-install many programmes).
Switchability between touch screen and normal screen modes is the last one.
Greetings from Bram


	              
	              hi all,
I totally agree with you babatunde. Like you said I prefer working more with windows 7 and this is what i have been using for like 3 years now.
In my own opinion, Microsoft failed in the release of window 8. some of the issues i had with using this were on the user interface which were very clumsy. Also, the touch-screen part was a mess.&nbsp;I had an issue with fixing a window 8 system that had crashed and I went through alot resolving it because of some minor changes here an there that are not user friendly.
Regards
martins
	              
	              Jello Dr. Anthony,
I agree with both craig and Babatunde, I'll like to see a more robust application where companies like apple, microsoft, Android, Google will come together with their various expertise to develop something that would fit into any system and work well on any OS.&nbsp;
Regards
martins
	              
	              Hi Craig and Colleagues,
I believe as the WebOS getting mature, it will provide better security to protect user's data than traditional OS. This is because WebOS is using transparent technology like HTML and JavaScript. This might sound risky. But because of the data transparency, developers would not rely on their own formatting to protect data, but to rely on a secured and unified communication channel. This simplifies the overall architecture and clarifies the responsibilities, which will remove vulnerabilities and strengthen the security.
Another phenomenon I forgot to mention is AugularJS (Wikipedia). AngularJS is an open source web application framework originated by Google since 2009. It's simpler to develop a web application with AngularJS. Traditionally, a web service is implemented in the MVC model (Wikipedia, MVC). AngularJS provide client side MVC. The Model, which represent the data, is typically from the server. A large part of the Controller and View has been pushed to the client side browser. This is one of the trends that is happening in the web application world. AngularJS haven't got direct support from the browsers yet, so the JavaScript running at the background actually take a lot of computing resource. Web Applications that's developed with AugularJS in general consume a lot of energy. But I can see the situation is getting better when AugularJS gets mature, and it might eventually get direct support from the browsers. Say, at least from Google Chrome first.
br, Terry
References
AngularJS. (2014, October 3). In&nbsp;Wikipedia, The Free Encyclopedia. Retrieved 23:51, October 6, 2014, from&nbsp;http://en.wikipedia.org/w/index.php?title=AngularJS&amp;oldid=628079741
Model–view–controller. (2014, September 25). In&nbsp;Wikipedia, The Free Encyclopedia. Retrieved 23:58, October 6, 2014, from&nbsp;http://en.wikipedia.org/w/index.php?title=Model%E2%80%93view%E2%80%93controller&amp;oldid=627076332
	              
	              Hi Dr. Anthony,Below is my wish list of functions, &nbsp;features and attributes that I would like to see included in the next generation of computer/network &nbsp;Operating Systems (OS) and why.1. More Open Source or cheaper.OSLets face it most people like cheapness and freeness and I am no exception from that list. I believe the ever so popular Windows OS is over priced and in the future I would like to see this particular OS be sold cheaper. Currently the price is over US$100, free would be preferred but maybe that would be wishful thinking. A suggested price is between US$5-US$10. In the mean time I would stick to open source.
2. Reload the OS without the need for CD, DVD of Flash Drive.I would suggest a special store of the OS image; perhaps on an untouchable area on the hard drive or on MRAM if this is available cheaply. Which ever technology used, my computer must come with a OS image on it that can enable a fresh OS installation and not just a repair. Currently you can repair your current OS and do a recovery but if you need a fresh install of the OS you need CD, DVD of flash drives. I must also be able to add additional OS images to this storage area, if desired.3. Automatic OS updates.This is beyond system utility updates; if Windows introduces Windows 9, I must be able to automatically upgrade my system to Windows 9 in a similar manner to how Android OS updates automatically on my Tablet.&nbsp;&nbsp;&nbsp;
4. Application (App) Store and Package Manager.I noticed this feature on Ubuntu I was browsing for applications, similar to how I &nbsp;would browse for them on my android device, using a App Store. Additionally I must be able to purchase and install these Apps in a similar manner to installations on my android device. Windows 8.1 already has a App Store but I would like a package manager as in Ubuntu as well.
5. Automatic installation of more drives.Lets say I have a new hardware, a webcam perhaps that I plug into my OS, my OS must search on or offline and install the appropriate driver for this device automatically, in a timely manner. I should not have to go search independently for these drivers. Windows already has this feature but sometimes not all hardware is easily installed, and sometimes it takes too long to find or download the drivers. I think the key here is to be able to install drivers automatically without Internet access, thus there must be a good storage, on the computer, of all possible drivers.
6. Personalized OS.When I am about to purchase a machine the manufactures should have a line of questions to ask me with regards to what I will be using you computer for or my profession. From this, my OS must be personalized, as such the System utility should have all or most application software associated with my line of work or what I would be using the computer for. Example if I am a student, my laptop should come with OS Anti-virus software, Word Processing etc. If I am an architect it may come with these application and architect applications as well. Perhaps here more applications software can become utility software instead, thus Operating Systems personalized to suit different categories of users.7. Lower System requirements and greater stability.Greater stability like that on Linux on Windows OS. Additionally I would like to see lower system requirements to install Windows OS, Linux system requirements is good however Windows requires 1G RAM and 16-20GB hard drive.8. Automatic backups of all files.I recall one day I needed an important photo for the bank, but I was worried because I left my tablet at home and I had no other way of getting this photo. And a colleague of mine said, “Do you know that Google automatically backups your photos?” And he told me where to sign in online to get my photos, I was impressed. Automatic backups is already offered in Windows 8.1, but just as Google automatically backed up my photos to an online storage without me even knowing that this was being done or ever having to physically schedule a backup, I would like to see this type of functionality on all OS.
References:The familiar made better [Online]. Available at:http://windows.microsoft.com/en-us/windows-8/features#personalize=startscreen (Accessed on October 6, 2014)Windows 7 system requirements, [Online]. Available at:http://windows.microsoft.com/en-us/windows7/products/system-requirements (Accessed on October 5, 2014)Memory and Disk Space Requirements, [Online]. Available at:https://help.ubuntu.com/10.04/installation-guide/powerpc/memory-disk-requirements.html (Accessed on October 5, 2014)

	              
	              Move towards a Web-OS?
To begin, let's understand what Web-OS really is. Web-based OSs like Google’s Chrome OS, and notebooks built around the Chrome OS -- have collectively received a great deal of attention in the press. The question is do they make sense for all kinds of businesses?
For a lot of us that use the web frequently, we are very familiar with the Chrome Web browser. We can further understand the Chrome OS from here. The Chrome OS is essentially a bootable version of the Chrome Web browser, and it replaces a traditional operating system like Windows.
Unlike most operating systems, Chrome OS stores all of a user’s applications and data on the Web, as opposed to a local hard drive. Although much more limiting than a traditional operating system, a browser-based OS offers huge advantages that can make it a compelling option for small businesses.&nbsp; Let’s take a closer look at the pros and cons. 
The Advantages of a Web-based OS
Chromebooks start up quickly -- in about 10 -15 seconds -- which is significantly faster than any Windows PC. Plus, their impressive battery life lets you work an entire day on a single charge. More importantly, Chromebooks practically eliminate time-consuming IT tasks such as building system images, troubleshooting small business software or spending hours cleaning virus-infected systems.
Moreover, with all of a user’s data and applications relegated to the cloud, employee responsibilities such as maintaining the latest virus definitions, updating software or even performing daily backups are now automatically managed by Google. And, since a computer with a browser-based OS stores everything on the Web, data loss due to damage or to a lost or stolen computer is all but eliminated.
In fact, moving to new hardware involves nothing more than turning on the new machine and logging in. No need to transfer data, reinstall applications or even wait for IT assistance.
What we find most attractive about this concept, though, is how cost effective it could be. Google -- making a hard push to get people to buy into the benefits of a browser-based OS -- claims that companies can reduce their total cost of ownership by up to 70 percent over traditional PCs.
To that end, Google is offering small businesses and non-profit organizations the capability to lease Chromebooks in bulk for $28 a month per user. Educational institutions receive an even better deal at only $20 a month.
That price includes tech support, rapid hardware replacement, automatic background updates, a Web-based management console for IT professionals (for managing users, apps, and policies), and a hardware refresh every three years. For many businesses, potential savings of that magnitude are hard to ignore and definitely bear further investigation.
In spite of how good a deal this might be, not everyone will or should transition to this platform (graphics designers, accountants and architects come to mind). But if your employees only need to browse the Web, access email, and use typical office applications like a word processor, spreadsheet or presentation software, then a browser-based OS like Chrome OS might work for your business.
The Downside of Web-Based OS
While a browser-based OS offers plenty of benefits, it is also hampered by severe limitations. Most notably: everything is stored in the cloud. If you're working from the office or your home, that's generally not a concern. However if you travel, accessing a reliable and fast broadband connection can be tricky. Many areas have dead zones, limited coverage and inconsistent throughput rates.
Complicating matters further, many wireless ISPs impose a data cap on their mobile broadband service. A computer that requires constant online access to transmit data or stream music and video could hit those caps very quickly. It wouldn’t be as troubling if you could work offline, but the majority of apps currently available for Chrome OS won’t work without a broadband connection. This makes working while traveling difficult or, in some cases, impossible. 
Other issues include the lack of proper VPN support, limited file management and some weird browser compatibility issues that prevent some websites from loading or functioning correctly. And, while the Chrome App Store offers a wide variety of apps, it's still rather limited. As a result, finding what you need can sometimes prove difficult.

	              
	              Dear Dr. Ayoola,
Current operating systems have traditionally been built bottom-up: Start with the machine, then connect it somehow to the user. The goal is to package the processor, memory, disk and other peripherals (which are unmitigated nuisances to manipulate directly), so that you can manage them by remote control. Instead of moving bits around the disk, you drag file icons around the desktop.
The next-generation operating system should start with the user. Its role is to track your life event by event, moment by moment… thought by thought.
&nbsp;
A life is a sequence of events in time. The future of information management is narrative information management, in which all of your stored documents are arranged as a "documentary history" of your life.
&nbsp;
All the digital documents you create or receive, all the "community" documents you want or need to see, are laid out in one narrative stream with a past, present and future. The stream flows because time flows; the future (where your calendar notes, meeting reminders and plans are stored) flows into the present, then into the past. E-mail shows up at your "now line," and flows into the past. Everything you've got is on your stream.
&nbsp;
Every word in every document is indexed automatically. All metadata, such as a document's origin and type, is indexed automatically. 
&nbsp;
Your hardest task in today's information gale is to master the big picture. The mind's picture-processing capacities are amazing, and you need to use them to see as many electronic documents at a glance as you can. That's why people cram their desktops with icons. Suppose you were looking at a thousand people. If you lined them up side-by-side, you'd have to walk the length of the row or stand way back to see them all. But if you put them in a column and stood right in front, slightly to one side or above, then you'd see the whole parade in one glance. Foreshortening does that for you. Next-generation UIs will use foreshortening to show you a deep parade of digital documents instead of today's flat chaos. 
	              
	              Hello Babatunde,
I like you post, and I am in agreement with you.&nbsp;
	              
	              Chika,
Great post, thanks for this.
Last week, our assignment was based on Virtualisation and Client Server.
As part of my overall conclusion, although there are benefits associated with Client Server architectures, I suggested that we could actually see the eventual demise of these architectures, and that they are completely replaced with a combination of Cloud Computing services and web-browser Applications.&nbsp;
I sense that the evolution of Web OS, and in&nbsp;particular the Google Chrome OS could serve to hasten this evolution, especially for the Enterprise whereby we have many thousands of users, who are also geographically dispersed, and now moving more towards web-based applications.&nbsp;
Another point I'd like to touch on; if we take the mainstream user, is it just the MS Office suite that is keeping Microsofts foothold on the market?
According to Microsoft (2014), 1.1 Billion people use Office. That's 1 in 7 people, so it is obviously very popular. Being a Mac user myself, whilst I have used Pages, Numbers and Keynote, I still cannot drag myself away from the MS office suite. However, building on the Web OS discussion, imagine if Google develop a product set that is comparable in capability to the MS Office suite and is also interchangeable across other platforms. Could this be the end of the Microsoft Office suite as we know it?
Best Wishes, Craig.
	              
	              References
Microsoft. (2014). Microsoft by the Numbers [Online]. Available from:&nbsp;http://news.microsoft.com/bythenumbers/index.html (Accessed 7 October 2014)
	              
	              Hi Martins
The issue here is that they're all in the same competing markets. That is why we see collaborations, partnerships and allianmces with these and other Technology players in&nbsp;subtly different markets, such as Apple &amp; IBM, and Microsoft &amp; HP just to name a couple as examples.
However in my opinion, we should see more powers given to Global Operating Bodies who are able to enforce appropriate levels of collaboration when it comes to "general features" such as cyber security, just as car manufacturers for example, have to deliver against certain safety regulations.
Best Wishes, Craig
	              
	              Hi Martins
The issue here is that they're all in the same competing markets. That is why we see collaborations, partnerships and allianmces with these and other Technology players in&nbsp;subtly&nbsp;different markets, such as Apple &amp; IBM, and Microsoft &amp; HP just to name a couple as examples.
However in my opinion, we should see more powers given to Global Operating Bodies who are able to enforce appropriate levels of collaboration when it comes to "general features" such as cyber security, just as car manufacturers for example, must deliver against certain safety regulations.
Best Wishes, Craig
	              
	              Hi Everyone,Linux O.S. is a good example of a widely used commercial Open-Source software system. Why do many organizations prefer Linux to other proprietary operating systems, such as MS-Windows? Do you expect this trend to continue in the future, or is it just a temporary fad?Anthony
	              
	              Hi Anthony
As with all Open-Source software, it is free. There are no purchase or license costs associated with Linux. This is particularly attractive to all organisations in reducing their ITR costs. This of course isn’t the only benefit, as there are stacks of others. 
I’ll look to touch on a few of the other benefits.
Linux has been developed as Open-Source software and as such, benefits significantly from its interoperability with other platforms. So, there are no proprietary ownership restrictions. Another key benefit from Open-Source software is that it comes with a community support framework, with a wealth of help that is always on hand and not under the constraint of a support contrtact or subscription agreement. I also understand that Linux is also less prone to viruses and cyber security attacks, with most being targeted towards Windows OS.
The last key benefit I would like to touch on is that you are able to fully leverage your return on investments, as Open-Source software isn’t generally on any decommissioning roadmap, unlike other vendor driven technologies.&nbsp;
Open-Source is not just here to stay, it will become even stronger in the Enterprise.
Best Wishes, Craig
	              
	              Hi Craig,

Good post. A little added to the issue concerning viruses and cyber attacks.

Cyber attackers always spread viruses and worms by convincing computer users to do something they shouldn't, like open attachments that carry viruses and worms, and it's all too easy on Windows systems. Just send out an e-mail with a malicious attachment and a subject line like, "Check out these adorable puppies!” or “the porn equivalent” and some proportion of users is bound to click without thinking. The result? ….well, the result is always an open door for the attached malware, with potentially disastrous consequences.

But thanks to the fact that most Linux users don't have root access, however, it's much harder to accomplish any real damage on a Linux system by getting them to do something foolish. Before any real damage could occur, a Linux user would have to read the e-mail, save the attachment, give it executable permissions and then run the executable.

&nbsp;

Best Regards, Babatunde
	              
	              Hi Chika, Craig.
Contributing towards the topic of Web-OS, my view is that in terms of number of devices/applications that will run on Web-OS in the future will be much more than those found on the traditional platforms. This is mainly due to the evolving and pervasive nature of computing devices that are taking on newer forms and shapes with the likes of 'The internet of things'.
However there will still be a large role played in the Enterprise space where the more traditional mission critical business applications still need to sit on a local server. The issue of privacy and confidentiality of data still ensures that preference for private cloud spaces are still retained and that businesses running traditional office applications for instance would still want that ability to save files to the local disk.
So, I foresee a distinction between Web_OSs applied in more general, non-mission-critical usage and local OSs' applied to business enterprises where more control is still observed.
The whole issue of Confidentiality, Integrity and Availability (CIA) is still an evolving area for cloud based services and a recent post on the internet, talks about the scare that Netflix had as a result of Amazon needing to reboot their servers in order to address a vulnerability in their Xen Hypervisor (Computer World, Oct 3 2014). The level of comfort is still not quite there to justifiably say that Web-OS will completely take over. The use of Java programming language however is bound to greatly rise.
On the comment by Craid on MS-Office, I think Google are still a very long way in getting to the level and depth of an office productivity suite like MS Office. For light work, one may get away with Google Docs, but for the heavy lifting power users, nothing beats MS-Office - as yet.&nbsp;Office 365 and Google Apps are vastly different products. Office 365 is meant to be used with a locally installed version of Office (preferably Office 2010), whereas Google Apps lives 100 percent in the browser.&nbsp;Comparing the features of Office 365 with those of Google Apps is really an apples-and-oranges cliché as those for Google Apps are limited in comparison &nbsp;with Office which is feature-rich. Jon Gold (NetworkWorld:Productivity Showdown, 2014) in a review states that 'Office has remained the pre-eminent locally-run productivity suite out there'
So I wouldn't be so quick to write off the Microsoft Office Suite
References:
1) Jackson, J. 'How Netflix survived the Amazon EC2 reboot', Computerworld, IDG News Service, Oct 3, 2014, Available at&nbsp;http://www.computerworld.com/article/2691578/data-center-cloud/how-netflix-survived-the-amazon-ec2-reboot.html?source=CTWNLE_nlt_entsoft_2014-10-06#tk.rss_databaseadministration, Accessed 07/10/2014
2)&nbsp;Leonhard, W. 'Office 365 vs. Google Apps: The InfoWorld review',&nbsp;InfoWorld | Jun 28, 2011, Available at&nbsp;http://www.infoworld.com/article/2683268/saas/office-365-vs--google-apps--the-infoworld-review.html, Accessed 07/10/2014
3)&nbsp; Hardenburgh&nbsp;, I. 'Google Apps v. Office 365: Head-to-head comparison of features',&nbsp;TechRepublic| April 2, 2013, Available at&nbsp;http://www.techrepublic.com/blog/the-enterprise-cloud/google-apps-v-office-365-head-to-head-comparison-of-features/, Accessed 07/10/2014
4) Gold, J. 'Productivity showdown: Google Apps or Office 365?',&nbsp;NetworkWorld | Mar 19, 2014 , Available at&nbsp;http://www.networkworld.com/article/2175325/software/productivity-showdown--google-apps-or-office-365-.html, Accessed 07/10/2014


	              
	              Hi Anthony,


Organizations prefer Linux to other proprietary operating systems not just because it is stable, but because it is versatile: it can run on mainframe computers, PC’s, handhelds and electronic devices.

Many organizations have moved to open-source software (examples of Ubuntu, Red Hat Fedora, Mandriva) mostly to save money and also to improve operations.

The disadvantage I could think of is just the limited number of applications that can run on it, compared with the windows platform. But, like my collegue Craig T. said “Open-Source is not just here to stay, it will become even stronger in the Enterprise”.


Best Regards, Babatunde
	              
	              Hi Bo,
I agree with your thesis statement, in fact I have proven that using a single vendor especially for OS's in a business environment is a good practice. This extends far beyond just managing and administering the environment to include application usage, achieving productivity targets and delivering a uniform service to all the users.&nbsp;
Windows Servers and clients have provided very stable computing environment for businesses ever since NT4.0 and has continue to grow, improving on much of the basic functionalities to provide a robust, reliable, high performance system. Today Windows Servers can support most of the operating needs of major corporations and run some of the most business critical services. Despite the many challenges the OS has face, especially security wise, continuous development has allowed it mitigate these challenges and more often than not, is the first platform to be selected when building out a computing environment.
While Windows Server as a NOS can allow other client OS's to integrate into the environment and utilize the same server services i've also prefered to simplify things and use only windows clients.
Regards,


	              
	              Hi Craig.
I can relate to the issue of Outlook to Gmail migration as my organisation recently did the big switch from Outlook to Gmail. However, the reception in our local office wasn't that great. It largely also has to do with the fact that Old habits die hard. So for most users who are used to working with Outlook in a particular way and are at home with its functions, the move to Gmail was a drawback and one of the areas of most disatisfaction was the difficulty in accessing attachments while offline and the spaghetti manner in which gmail handles your email replies and forwards. This has essentially led to many people looking for alternative ways to "beat the system" and thanks to Google Sync, many have returned to their comfort zone in Outlook
Joseph
	              
	              Hi Bo, Craig,
The issue of security is to some extent a perception issue. For many, the fact that you have your computer and data right next to you within reach makes it secure. The thought of your data in the cloud somewhere, out of sight and out of reach can give one the perception that it is 'not secure'

Joseph
	              
	              Hi Ricardo,
I would imagine that your company must have done a risk assessment in which the benefit and user satisfaction of Windows 7 did not outweigh the security risks that the platform posed. One of the key aspects in such cases is the issue of change management and communication. When employees are not adequately sensitized as to the nature and impact of a security risks, their buy in as part of the security apparatus at the company is compromised.
The biggest security risk within a company is often the people themselves and not necessarily the lack of security controls. So even with a secure system, employees are prone to phishing and social engineering schemes thereby introducing virus and malware in a secure network. Companies may spend considerable resources hardening their networks and upgrading the technology but the human factor is one that is often most vulnerable. (WSJ, 2011)
Joseph

References
FOWLER, G. 'What's a Company's Biggest Security Risk? You', Wall Street Journal, Sept 2011, Available at&nbsp;http://online.wsj.com/news/articles/SB10001424053111904836104576556421692299218, Accessed 07/10/2014
	              
	              Hi Anthony.
My wishlist would be more in terms of security in such a way that operating systems are more able to natively prevent, detect and respond to attacks. With more research going into robotics, drones and the like, I would want my computer to be able to 'know' operations that I would consider to be allowable and not allowable. The user access control (UAC) that was introduced in Windows Vista was an initial good attempt at notifying me when some unauthorized activity is happening on my machine, however it was too intrusive and almost became a nag. A more intelligent version of this feature would be most welcome
Joseph
References
User Account Control, http://en.wikipedia.org/wiki/User_Account_Control&nbsp;
	              
	              Hi Dr. Ayoola,
In the next generation of Operating Systems (OS) I would like to see:

Security &amp; Privacy enforcement and settings: as consumer operating systems become more easy to use, Software companies should push to give tools to the end users to better understand and manage security and privacy, while enforcing a minimum standard for these. 
Real Cloud integration: with “all” the clouds out there and almost deadly competition among them, end users are left with integration issues between vendors and their data. There needs to be an effort on the OS vendor side to enforce current and future integration standards. 
Device versatility: An OS should be able to run in our most common computing devices, be it a desktop, laptop, mobile phone or tablet. End users should be able to seamless use and switch between those devices without the OS or applications to get into their way.

Those are the three things I would really love to see.
Best regards,
Augusto.

	              
	              Great response Joseph, thanks for this.
Different businesses have different criteria for what they class as mission critical. I have worked in a number of different businesses who have already migrated important mission crirtical services to the cloud, albeit I was not totally convinced they should be classified as mission critical, in comparison with say, an operating theatre or perhaps flying an aircraft. However, they were still deemed to have significant importance and the companies bought into the Cloud benefits versus potential risks associated (which of course are mitigated where possible). What I would go on to say is, even those services that may be deemed truly mission critical, are still probably delivered over a wide area network and as such, are not necessarilly positioned locally onsite. However, I agree to some extent with the argument that a sense of security is achieved by having systems locally positioned relative to the user.
You are correct to assert your reservations of writing off Microsoft Office suite just yet. I should have been more precise in my wording, and said "Could this be THE START OF the end of the Microsoft Office suite as we know it?&nbsp;
I am super impressed with the likes of&nbsp;Google in how quick they are to sensing trends, and in&nbsp;taking new products and services to market, and their ability to develop these at super fast speeds. So, I have no doubts that should Google want to compete, they simply will. I also feel that Microsoft have lost their way somewhat, and whether it was Ballmer who has somewhat been misguided over the last 5 years I am not so sure, but other players have emerged who have just been better at getting things done. Microsoft have a 20 year advantage over all other players when it comes to the Office Suite. In reality though, are they 20 years ahead?&nbsp;
I say all of this whilst still sitting in the category of not being dragged away from MS office... just yet :-)
Best Wishes, Craig
	              
	              Hi Craig and Terry
Yes i beleive that security features will become more robust, but not completly robust. Threats from hackers etc. will always find a way through despite being “treated as a large scale issue” (Craig) and/or “..simplifies the overall architecture...” (Terry). Depending on how things will evolve, i recon that the security vendors at some point will (be forced to) find a business case which suits them in regards to the new rules of game, but i don’t expect them to be front movers. 
Br. Bo
	              
	              Hi Dr Anthony,
I suppose the most obvious reasin would becost wise- the fact that Linux is free. On Windows for instance the Microsoft licenses are usually isnatlled on a single user basis while Linux allows for multiple users all at &nbsp;no cost.
Other reasons include but are not limited to: the immense support that comes with Linux through the community,it is more secure with hardly any viruses, its much easier to install applications on Linux without numerous steps of accepting agreement documents before and during installation and &nbsp;It's fairly easy to change options on Linux.
With that said,I expect this trend to continue. What Linux offers is not easy to deny. Organizations are always looking to count down on costs and the fact that it is free but also very effective. The enhanced security compared to Windows gives it staying power.

kind regards,
Belinda
Referrences:
http://ubuntu-artists.deviantart.com/journal/8-Advantages-of-using-Linux-over-Windows-291681914&nbsp;





	              
	              Hi Dr. Anthony,
Up until this questions was posted I have never heard of the term Web-OS. Just thinking about this concept in the context of a traditional operating system, it's function and the service is provides, would lead me to consider this to be just fanciful thinking. I thought why would you want to store your OS in the cloud then upon booting your computer the boot loader will look to some online storage facility to retrieve the OS.
Upon doing a little research I realise that the concept of Web-OS has long been in development and some service provides have been offering this type of service for some time now. A "Web-OS" mimics the appearance of a traditional operating system providing a user interface with applications and other services a desktop base operating system would provide, but this is done from an online platform, a cloud service. You will still however need a means of accessing this online Web-OS and therefore a device with an OS will be required.
 "We have internet on computer, so Web-OS is computer on the internet"(Jain, n.d.) 
So I do agree with the notion, in fact I have been using a form of Web-OS for many years and only thought of it as a web service, i.e. Google Docs. I have also realise that there are many Web-OS available online from those that provides a full blown desktop user interface, to others that provide some of the services, like document editor and file manager. Some of the Web-OS's online includes: iCloud, SkyDrive, EyeOS, GHOST, GlideOS and ZeroPC.
&nbsp;
Reference:
Jain, P. (n.d.) Web Operating Systems [online]. Available from http://www.engineersgarage.com/articles/web-operating-system-webos?page=1 (Accessed: October 7, 2014).
Strickland, J. (n.d.) How Web Operating Systems Work [online]. Available from http://computer.howstuffworks.com/web-operating-system.htm (Accessed: October 7, 2014).
Ashutosh, KS. (n.d.) 9 Cloud Operating Systems you can try out for free [online]. Available from http://www.hongkiat.com/blog/free-cloud-os/ (Accessed: October 7, 2014).

	              
	              Hi Dr Anthony and colleagues,
Please excuse my typos for the following:
"reasin" was meant to be "reason"
"becost" was meant to be " be cost"
"isnattled" was meant to be "installed"
Best regards,
Belinda
	              
	              Hi Anthony,
Around the world, everyone, institutions, companies, government agencies, and other organizations are opting more and more for open source operating systems like Linux instead of proprietary systems like Windows or Mac OS. 
Similarly, they also abandon proprietary and commercial applications for free softwares. 
And this is for several reasons, among others, 
Linux is registered under a free license; it can be obtained for free or for a small fee. 
Linux is free, in the sense that anyone can edit it, 
Linux does not have forced updates, as for example on windows 
Linux offers more security, namely a very low infection rate by virus, Trojans, and other malware 
Linux is highly resistant to system crashes and rarely requires a reboot. 
Linux offers great compatibility with other operating systems. 
There is a wide range of Linux distributions (several hundred), each with its own characteristics, but in general there are all compatible with each other 
No expensive equipment is required to move from one version of Linux to another. 
Here's a few of the numerous reasons why organizations choose Linux OS to another systems, and in my humble opinion it is not ready to change as long as things stay the same.

	              
	              Dear Craig,
Thank you so much for your valuable response. I totally agree with you that ignoring the other areas while implementing changes could affect the business. I personally think that in the implementation phase they must also consider to mentally prepare the end users/stokeholders that could be affected by the change other than just only considering and covering technical grounds. There must be trainings and seminars to educate about the importance of upcoming changes and how the situation can be handled after the changes are in place. Every related entity should be considered and should be on the same page to get positive results otherwise it is more likely to end up with a kind of failure.
Regards,,,
Numan
	              
	              Hi Dr. Ayoola,
To answer why many organizations use or prefer open source software to other proprietary systems we will need to answer first why people actually choose to work with this type of software.
As Bonaccorsi and Rossi (2003) describe in their article “Why Open Source software can succeed”, open source software is a type of intellectual gratification, similar to that of scientific discovery. It adopts elements of scientific research: sharing results improve the results of other programmers and gives recognition to participants. Thus, open source software has inevitability improved because of collaboration between programmers, making in the process a highly reliable and portable software solution. This collaboration adds another advantage: the assurance that software will continue to be developed, avoiding costs that can happen with commercial software in case of bankruptcies or vendor changes (Bonaccorsi, Rossi 2003).
With this information, it is easier to see why an organization chooses open source operating systems (OS). As Bonaccorsi and Rossi (2003) point again, commercial software is seen as not very reliable. This unreliability is a consequence of being a closed system, against the openness, scientific research alike feature, of the open source software.
I can take an example at my work: I asked the “Unix team” why all our critical company web servers run on Linux. Upon further discussion nobody could tell me the exact reason, but their first answer was “because they are better”. As simple, or biased, this answer may seem, it is perhaps the real one: open source OS’s are, in fact, highly reliable. If your company website means a lot to your business in terms of image, marketing and sales, you must certainly want to have it run in a highly reliable system. This reasoning hints that companies may want to use open source OS where it make sense to them, and also use commercial OS’s where is more appropriate. If your secretary or receptionist only do documents and emails, does she needs a highly reliable OS? Running open source and commercial software simultaneously is a key factor in the development of the open source software: it reinforces the programmer’s collaboration to address innovation and changes. And, hence, the assurance that the software will continue to be developed.
Coexistence of both commercial and open source OS’s is the norm today and will continue be so in the future. As companies have different needs in different scenarios, they will continue to choose several OS’s, not just one. This coexistence will give open source software the opportunity to thrive.
Best Regards,
Augusto.
&nbsp;
References
Bonaccorsi, A. &amp; Rossi, C. (2003) 'Why Open Source software can succeed', Research Policy 32(7), pp. 1243–1258 doi: 10.1016/S0048-7333(03)00051-9.
&nbsp;
	              
	              I agree with Joseph statements, I don't ever see Web-OS's replacing desktop computing, especially for mission critical applications that require hugh amounts of resources with demands that are just too much to process online. Take for example NLE Video Production, this class of workflow is very resource intensive and can scale desktop hardware requirements to server type components.
Then there is the always ever present concern of security, Joseph mentioned CIA above and while most cloud base providers will try to mitigate any associated issues with well wriiten SLA's that allow for flexibility in the event of an incident, the challenge still remains that services in the cloud are vulnerable and availability is dependent on broadband internet service.
The statistics Craig posted above, that MS Office is used by 1.1 Billion people is not surprising. MS Office have become the standard in document processing and while there are many other feature rich office type applications that provide comparable features, users aren't very interested in switching from something they already know how to use to one that would require some amount of getting use to or specific training sessions.
Regards,
	              
	              Hi Numan,
Auditors can be a pain and sometime will try to force policy change and system change to keep inline with what they have determined to be best practices. But as an ICT professional you must evaluate what is critical for your business operations and be able to relate that to your CEO/CIO and auditors alike. The approach I take to security is risk base, only the potential risk which is base on the threats the operation of the organization is exposed to is mitigated against. There is absolutely no way you should employ means to prevent a threat you're not exposed to.&nbsp;
My organizations has a simple change management structure which allows for period of introduction for such a change as the user interface of employees workstation. When changing from windows XP to Windows 7 all I had to do was role out a demo and tutorial on the new features and fun capabilities on the new environment and all users were ready for the upgrade within 3 days. There was no major change from XP to Windows 7 as you encountered with upgrading to Windows 8, but once there is any change to any user interface, even production applications, an introduction period must be observed. This allows for a smooth transition and for user issues to be addressed in context.
Other changes that will not affect their productive capabilities are implemented with an email notice. like web usage restrictions. If there was ever a situation that critical security changes would affect employees ability to work then a wholistic approach would be taken with all heads of department and the changes discussed with a work around to getting efficiency back to normal.

Regards,

	              
	              Hi Terry,

I hate it, it completely freaks me out... I keep screen wipes both at home and at work in case anyone does touch it...
Of course if it's a touch screen device such as my iPhone or iPad I don't mind so much so I guess it depends if it's supposed to be touched or not which dictates to me whether I like it or freak out.


	              
	              hah, thanks, yeah it was fun for sure, and I understand about the motion sickenss as I get the same.

It's funny I never had issues with it up to about 20 years old, I could play anything for hours but then overnight it changed and like you said I can't read on a bus or train or even a plane sometimes still moves too much, I might get through one chapter and then have to stop. So now I have to really pick my games, I can't play FPS games like COD which is a shame but it just means I get more time on platformers and strategy games lol.

Chris

	              
	                Here are 4 other reasons for companies opting to go with Open Source like Linux rather than Windows  Open source keeps costs down Open source improves quality Open source delivers business agility Open source mitigates business risk  . Linux had its shakeout in the market and survived. Open source is here to stay ! Robin
	              
	              And I agree with your remarks Ricardo. Web-OS will not replace the function of a full OS running on local hardware. It can have some appliances in domains where only a web desktop is needed to access virtual machines in the cloud that have applications or whole workflows installed on them. The thin client will not replace the full function of a PC but be one next to the others. 
Bram
http://en.wikipedia.org/wiki/Thin_client
http://en.wikipedia.org/wiki/Web_desktop
	              
	              Agreed Bo. It looks like I might have opened up the Microsoft Hate mail discussion!! I grew up with Windows 3.11 and still have faith with Microsoft next gen windows. Time will tell.
Robin
	              
	              Hi Anthony,

Oh I like that question.
I'd love to see an OS and network be more integrated with a cloud solution. Currently cloud solutions are what they are, as we all know but I would love some sort of inclusion to the OS where because you have the latest version Microsoft Windows or Mac OS installed you get a licence with an encrypted link between your new device and the cloud. It could be used for recovery disks, keeping restore points etc on top of tracking your installed software and license keys in case anything goes wrong with your machine. Once you recover it would already know what you had previously installed and rebuild it for you similar to synchronization between iTunes and iPhone purchases.
I'd love to see OS's be more interactive with each other and instead of competing they worked together. They could still be unique and have their own distinctions still but come closer so they are not so divisive.
I'd like to see security improved across the board, I believe it was mentioned by Craig in his post about this and I agree and thought the same on the lines that something could be produced so that it's AI could tell and self heal itself when attacked or when system files get corrupted. It could simply be snapshot encrypted algorithms in hidden partions where the OS controls and sets aside resources to continuously check / validate / recheck/ scan / update etc.

I'd like an OS to be stripped down more and use less resource. Crazy I know but sometimes less is more. Despite the increases and advances in technology sometimes it's best to step back making what you have better and not just add more features and pressure on the OS.
For example Windows 3.1 required just 1 meg RAM and 15 meg HD for installation on a 286 pc. Windows 7 needs 1 gig of RAM, 16 gig HD and 1GHz processor.
While this isn't massive per say, i'd still like to see simplification as a target.

&nbsp;ta
Chris

References
Wikipedia (2014) ' Windows 3.1' [http://en.wikipedia.org/wiki/Windows_3.1x] accessed 6 Oct 2014
Znet (2009) 'Windows 7 requirements'&nbsp; [http://www.zdnet.com/blog/microsoft/microsoft-releases-final-windows-7-system-requirements/2643] accessed 6 Oct 2014







	              
	              Hi Anthony, Many companies use Linux on their firewall systems as they provide highest level of security compared to any other server grade OS.   Unlike MS-Windows, Linux don't need to be updated on a regular basis. Firewall is built-into the Linux OS and it is available from Ver 2.2![1]   I think this trend will continue as these are still cost effective when compared to Solaris or MS-Windows OS's.     Figure-01. OS Cost [2] References: [1]&nbsp;http://books.google.co.uk/books?id=HeB1AQAAQBAJ&amp;pg=PA99&amp;lpg=PA99&amp;dq=linux+is+widely+used+as+firewall+system&amp;source=bl&amp;ots=RrHE_wvqtF&amp;sig=ybn_yqmWUEIM7lmEyUIH3gKagxE&amp;hl=en&amp;sa=X&amp;ei=oWc0VITxG8HgasjHgOAH&amp;ved=0CFsQ6AEwBw#v=onepage&amp;q=linux%20is%20widely%20used%20as%20firewall%20system&amp;f=false   [2]&nbsp;http://www-03.ibm.com/linux/whitepapers/robertFrancesGroupLinuxTCOAnalysis05.pdf Best Regards,   Kharavela
	              
	              Hi Bo,
The attackers and the defenders are the two side of one security coin, they always advance as the other advance. But the defenders are always a bit late than the attackers.
Hackers still cannot do anything about a computer that is disconnected, but as today, we cannot afford not connected. In fact, we need to be connected all the time and even use a WebOS.
An analogy could be putting money in the bank. In general, for a user it's safer than keeping the money by himself. Instead of stealing directly from them, attckers might cheat to get his identity. Only highly skilled attacker could successfully attack a bank.
Banks may have security consultancy, but it's not likely that they will purchase the security solution from others because it's their core competence. So, I agree with you that security vendors won't be the front movers. I actually think they might become consultant agents.

Br, Terry
	              
	              Hi Bram,
How would you define a "full OS running on local hardware"?
It seems in the future, we will be surrounded by enormous computing resources. All computers will be part of a bigger system. Comparing to all the resources a personal computer can utilize around it, its own computing power won't be that important any more.
br, Terry
Reference
http://en.wikipedia.org/wiki/Internet_of_Things
	              
	              Hi Belinda
oh thank goodness...
I wasnt sure whether "isnattled" was a new virus that was in circulation, or a new multiprocessing multicore multithreaded multitasking multiuser web-OS :-)
Best Wishes, Craig
	              
	              Hi Dr. Anthony,I was quite intrigued by the thought of a Web-OS. It sounds good to have such an Operating System (OS), however I was wondering about its integration or how the concept really would work. Thus, I did some searching on the topic and I was quite surprised that their was webOS already out there – Palm webOS, which is actually Linux based, Open Source, and can actually run on a plethora of devices or platforms.There is &nbsp;Windows OS for desktop and Windows OS for phones, the webOS &nbsp;uses a similar concept, however the applications are built using web-based technology such as HTML and JavaScript which makes the OS generally fast. This OS “integrates the power of a window-based operating system with the simplicity of a browser”, Mitch Allen (2009). The OS is designed to work on a plethora of platforms, giving users a fast and enjoyable user experience. Additionally, the OS updates automatically via the internet.With multitasking capabilities, the OS allows you to open and use more than one application at a time, for example you can read a document and listen to music at the same time. Data is stored online; because it is a web-based all your information is available online, thus you and access your documents by logging into webOS on different devices. This also safeguards your documents in cases where you may lose your device.I think I am going to look into this OS a bit more, and see if it can be installed on my desktop.References:Allen, M., (2009), Palm webOS, pages 1-2, [Online]. Available at:&nbsp;http://books.google.gy/books?id=sHT6PeMp1k8C&amp;pg=PR14&amp;dq=Web+OS&amp;hl=en&amp;sa=X&amp;ei=uHQ0VMClBfCLsQS1uoCICQ&amp;ved=0CBoQ6AEwAA#v=onepage&amp;q=Web%20OS&amp;f=false (Accessed on October 7, 2014)Ziegler, C., (2009), Palm Pre For Dummies, pages 11-13, [Online]. Available at:http://books.google.gy/books?id=MQlP0JNybaEC&amp;pg=PT24&amp;dq=Web-OS&amp;hl=en&amp;sa=X&amp;ei=HYQ0VNfIBeGCsQTXoIKACw&amp;ved=0CCIQ6AEwADgK#v=onepage&amp;q=Web-OS&amp;f=false (Accessed on October 7, 2014)
	              
	              Hello Class,As our usual way of rounding up the week's activity, please provide a short summary of the key 'Computer Structures' module lessons learnt in week 5, from your perspective.Anthony
	              
	              Hi Everyone,What special security issues would need to be considered if designing web-predicated network operating systems?Anthony
	              
	              Hi Dr Ayoola,
Being web operating systems, the key security feature that must be designed is protection of the network from unauthorized access. This feature must be designed on both the internet side of the OS and the local hardware. There must be a mechanism to detect that the device is the correct one and that anything that is exchanged between the device and the internet part of the OS is truly the data requested/transmitted.
User accounts and passwords will not be enough either, so a design based on very strong encryption, certificates and perhaps biometrics must be implemented.
Best Regards,
Augusto

	              
	              Hi Anthony
I just have a couple of additions to Augusto’s response, and a closing thought... 
The first of which is to ensure that the encryption follows the latest in cryptography protocols. Currently, these include the likes of TLS (Transport Layer Security) and SSL (Secure Sockets Layer) however the web-predicated network operating system should be able to evolve alongside any cryptography developments. 
The second is to apply the latest in dynamic forms of authentication such as public key cryptography however this may need to be a more advanced model. According to a comprehensive review of global strategic trends, as carried out by the Ministry of Defence Development, Concepts and Doctrine Centre – DCDC (2010), they suggest that the development of algorithms, such as Shor’s, will break cryptography keys with a one-way function and make public keys vulnerable to attack.
A closing thought: It is obvious and absolutely clear that we would naturally look to continue to develop security features to protect technology vulnerabilities however, how is a web based OS any different to our current OS architectures, relative to security? With the exception of completely non-connected technology devices, is it any more or any less vulnerable?
Best Wishes, Craig
Reference
Ministry of Defence DCDC. (2010). Global Strategic Trends – Out to 2040 [Online]. Available from: https://www.gov.uk/government/uploads/system/uploads/attachment_data/file/33717/GST4_v9_Feb10.pdf (Accessed 8 October 2014)
	              
	              Thanks Numan, yes I'm on the same page.
Best Wishes, Craig
	              
	              Hi Craig

I liked you final thought and share the view.

The only conclusion I gave was that any webOS was inherently more vulnerable simply from the fact that they are permanently on-line. In essence with the service any webOS would have to have 24/7 availability worldwide. That in operation simply makes it more vulnerable compared to a stand alone OS on a personal computer where it may only be active and switched on between 1-6 hours a day and of that only communicative to the on-line world for a 50% proportion of it's uptime.
If we take the line that both situations have the same level of security then like I mention then clearly any webOS is more vulnerable by default.




A closing thought: It is obvious and absolutely clear that we would naturally look to continue to develop security features to protect technology vulnerabilities however, how is a web based OS any different to our current OS architectures, relative to security? With the exception of completely non-connected technology devices, is it any more or any less vulnerable?



	              
	              Hi Anthony,
Another good week, and again it has been extremely intellectually challenging.
This week has been focused on functions, features and classifications of Operating Systems (OS).
I have really enjoyed learning a little bit about those OS that you don’t really come across, particularly real-time operation, and also the discussion question around web-based OS.
Also interesting to re-learn about how the OS actually works in practice. Something I have not genuinely thought about for 20 years or so. 
A number of my colleagues have commented on the intensity and time needed for the programme. I am also finding it quite challenging, in reading through all of the posts. I realise it was suggested that this may be difficult, but I just cannot resist, as I’m really keen to learn as much as possible, and to see different perspectives from my colleagues.
One thing I am a little concerned about, is figuring out how much content to include in the assignment. I feel that I could at least treble the amount of content and still only just touch the tip of the iceberg. Also, just needing to figure out how much time to spend on the assignments. Getting the balance just right is not that straightforward but it is becoming more familiar.
One last thing, which also may or may not be useful for my class colleagues,&nbsp; I am also re-playing the video(s) and re-reading the lecture notes at the end of each week, just to see if I find it easier in my understanding, having had a weeks worth of researching the subjects. This is particularly effective for me, but obviously adds to the time needed.
Another great week with some fantastic discussion posts, which continue to be insightful and educational. 
Best Wishes, Craig
References
Laureate Education. 2014. Computer Structures – Lecture Notes and Online Video – Week 5: Operating Systems&nbsp;[Online]. Available through the programme classroom (Accessed 2 October 2014]&nbsp;
Brookshear, J. G. (2012). Computer Science An Overview. 11th ed. Boston Massachusetts: Pearson. Addison Wesley
	              
	              Hi Chris
Thanks for your response.
Its an interesting question.
Let me ask... is it&nbsp;'more vulnerable', or is it a probability type question on being 'more likely'?
Best Wishes, Craig

	              
	              Hi Craig,

You right of course, it's all in the terminology. The vulnerability is technically equal however the probability is indeed 'more likely' over time with a webOS.
I wonder if we ever get to a stage where the roles are completely reversed as to what we knew of technology and an OS. We started with stand alone machines and then in turn being backed up onto the cloud etc. Then moving to webOS's that we may end up backing up to our local machines. lol

Chris

	              
	              Yes, lol ...that is certainly a possibility Chris.
...this programme hurts my head sometimes... :-)
Best Wishes, Craig
	              
	              Thanks Babatunde&nbsp;
Thanks for the additional context on 'how' a malicious attack would need to play out on Linux.
Best Wishes, Craig

	              
	              Thanks Joseph
Yes, quite a dilema. One which requires considerable thought and planning.
My understanding is that Google Sync only allows you to sync with a mobile device, so as long as the enterprise has that feature 'enabled' and it has gone through due-process, then I suppose the organisation is still benefiting in that productivity is not impacted, some may argue it is improved?, as well as the organisation continuing to benefit from its initial business case for gmail?
I guess this is an extension of BYOD.
Best Wishes, Craig.
	              
	              Hi Babatunde
I am not able to easily put my hands on one of our colleagues responses - I guess it is similar to their reference of trusting a bank to protect our safety deposit boxes? (Ha, not that I have one!). At some point, there needs to be trust and confidence that the service provider is doing everything possible to secure and protect.
Best Wishes, Craig
	              
	              Hi Dr. Ayoola,
Many organizations prefer Linux is because Linux is better. And I believe this is only the beginning of the Linux era.
Linux based systems are especially successful in two areas, the mobile device and the server.
Linux on Mobile Device
It turned out to be extremely hard to develop any new general-purposed OS kernal nowadays. Google choose to build the Android OS and Chrome OS based on Linux kernel. Then, Linux, as a large family, can get the similar amount of supports from every parties as the other big players can. And there are people like Linus Torvalds fighting for it:

Linux for the Servers
I believe most organizations choose to use Linux server not just because it's free, as free in beer, but free in speech. It's the freedom of choices. There are so many mature server side solutions on Linux server, it's a huge advantage.
Many companies using proprietary server operating systems still work in a mode where their software engineers don't even have the connection to the Internet, for example, in some of the Banks I know (actually all the banks I've worked with). This software eco-system has no way to compete with the highly dynamic eco-system of the Linux family.
br, Terry
Usage share of operating systems. (2014, October 7). In&nbsp;Wikipedia, The Free Encyclopedia. Retrieved 15:30, October 8, 2014, from&nbsp;http://en.wikipedia.org/w/index.php?title=Usage_share_of_operating_systems&amp;oldid=628623372
	              
	              Hi Chris,It’s funny your response. I never used Windows 8, however I noticed my husband installed it on his desktop, it looked good in terms of graphics of course and then he downgraded, to windows 7. I never asked him why. Additionally, I did hear a couple of persons complain about it at work, so it was quite interesting hearing other persons in the forum complaining as well. Looking at the website, the OS sounds good, but I guess it probably is not all what they claimed it to be.&nbsp;Maybe you folks should switch to Linux. I check out the Chrome OS which Terry spoke about, and it seems pretty nice. There is Chrome OS Linux or About Cr OS Linux maybe you can try that, it is free.References:About Chrome OS Linux, [Online]. Available at:https://sites.google.com/site/chromeoslinux/ (Accessed on October 7, 2014)Screenshots of Cr OS Linux, [Online]. Available at:http://getchrome.eu/screenshots (Accessed on October 7, 2014)

	              
	              Chris,
Sorry "Chrome OS Linux or About Cr OS Linux" should be &nbsp;"Chrome OS Linux or Cr OS Linux" in my previous reply.
Tanisha
	              
	              Hi everybody,
So I'm back from Bangkok. It was a business trip, by the way.
I got quite excited with the learning topics of week 5, because I'm very passionate about the operating systems I chose to use. To explore the reasons for my passion actually dominated my learning activities during this week.

Re-learnt the "Unix Philosophy"
Clarified some of the myths, biases and illusions I had with the operating systems for so many years
Studied the emerging WebOS
Got deeper knowledge about the Linux family
Studied the OS classification
Had fun on these topics both with my online classmates and my "off-line" friends.

I was approached by my 10 yo neighbour this Monday. He asked me if I can help with his homework, which is to design a smart watch and prepare a presentation with 10 pages of slides. It was the discussion with him helped me with my individual assignment before I pull all my hairs off. I really should have mentioned his name in my paper.
br, Terry
	              
	              Hi Bo,
Your statement about ensuring a good and sound business environment by using only one Operating System (OS) vendor, had me thinking a lot these past couple of days.
In all honestly, if I would have heard this 7 years ago I would have agreed 100%. Today, I am having trouble adhering to it. I suppose many companies have such an environment, but since the iPhone disrupted the whole PC and mobile market things have changed. At my work we used to be centric to Microsoft Windows for the desktops, but we are now allowing people to get Macs if they want. Since two years we are thinking about a Bring Your Own Device strategy (BYOD). With BYOD you can’t choose simply one OS vendor, rather your IT department and your software vendors should adapt to this business need.
It is absolutely true that having several OS’s running in a company make things complicated for the IT folks. But let’s not forget that it should be the business that dictates what they need and IT should adapt to this and provide the proper solutions, this also means that the company needs to acknowledge the resources needed by IT to provide these solutions.
Your statement may be true for some business, but definitely not for others. &nbsp;For example if I look at the gaming industry I can’t see how those companies that develop games for iOS, Android, PlayStation and PC’s can actually deploy only one OS. And that mix of OS will need to be supported by IT as developers relay on those to produce their work, which in turn produce revenue for the company.
In fact, rather than move from one OS to another is easier to have a mixed environment. I have yet to see anyone that has decided to replace Windows for Mac OS X, I’m sure there are some success stories out there, I just have not heard them. But I do know people in companies where the successfully implement another OS.
Best Regards,
Augusto.

	              
	              Hi Craig,
hahaha. My network was so bad last night. Some of the joys of being on this side of the world. I literally clicked submit 5 times.
kind regards,
Belinda
	              
	              Hi Tanisha,

Thanks for the response, I'll certainly look into those. I'll try to see if I can get hold of a cheap netbook and install Cr OS Linux. Always wanted to get a test box so this might give me a good enough excuse to buy one.

ta
Chris

	              
	              hi Chris,
I concur. A WebOS is more subsceptible to security issues, this is largely due to the fact that in the WebOS the platform itself is a browser which based on experience is harder to secure.
There is an interesting article of how private researchers were able to hack into the Palm's WebOS &nbsp;in 2010 using text messages to load potentially malicious Web pages or turn off the device's radio, one researcher even reported being able to remotely dial 911 from the handset as well as lift an enire contact list.
However, I'm also a firm believer in learning from failures; it's been nearly 5 years since this hackathon in 2010 and &nbsp;a lot can happen in 4-5 years.
References
http://www.cnbc.com/id/36601913#.
	              
	              Hi Bram,
I find your democratic argument about Microsoft Windows very interesting. I have some friends that work for Microsoft and they been telling me that they have been investing in Africa a lot for several years. In fact, Microsoft has a page dedicated to this (Microsoft. 2014). What’s interesting is that Microsoft actively produces windows versions in African languages and dialects. While Mac OS X is available in 33 languages and none are African ones (Apple Inc. no date).
I believe this is directly related to your democratic argument: an OS that is translated in all languages and that understand localization problems is in a position to give equal chances to use the provided technology compared to an user on the other side of the planet. Microsoft, by choosing to develop their software and make it available on a localized Africa is demonstrating that it really wants to enable those people with their technology. We can debate that interest is purely economic, but still, that is better than other competitors.
Best Regards,
Augusto.
&nbsp;
References
Apple Inc. (no date) OS X Specs. Available at: ttps://www.apple.com/osx/specs/ (Accessed: 8 October 2014).
Microsoft. (2014) 4 Afrika. Available at: http://www.microsoft.com/africa/4afrika/ (Accessed: 8 October 2014).
&nbsp;

	              
	              Hi Craig,
Very good question!
I agree that the vulnerability is technically equal. I suppose at least from the communication point of view. 
But with a traditional Operating System (OS) you can segregate it in special non routable networks (basically no network or internet access), or even completely off the network as an offline system (there is a use for those somewhere, I am sure). But with a WebOS that cannot happen, you must always be connected. 
I suppose that a WebOS is an easier target to certain attacks, like man in the middle ones, especially when there are network outages (one disadvantage of the Web OS): as the WebOS gets back online someone or something could be impersonating the other side of the OS. This can also happen with traditional OS’s but, removing internet from the equation, it is a little bit harder. For example if you had two servers in two datacenters, someone must gain physical access to one of the datacenters to be able to replace the server. In the WebOS world the access is purely logical and you could be anywhere in the world. 
Best regards,
Augusto

	              
	              Hi Craig
&nbsp;&nbsp;&nbsp; I echo your inquiry on the assignment inquiry. We have not seen others assignment work and really don't know what level of depth we should target for. The number of words give an idea, but would like some further insight from&nbsp;Anthony.
&nbsp;&nbsp;&nbsp; I did enjoy doing a deep dive into OS space. Especially Distributed and Grid computing
Robin
	              
	              Hi Dr Ayoola,
I thought that Operating Systems was going to be an easy topic. What was I thinking! Of course this topic is at pair with everything else and it’s quite challenge in this class. There is some much information and discussions that keeping up with them is very difficult.
What I learnt:

Never underestimate things (for some reasons I have keep forgetting that).
Reviewing how Operating System works and looking at how things have changed since I studied the theory years ago was a very valuable information (I cover this topic at school back when there was Windows NT 4.0).
Interrupts, Semaphores and Deadlocks were all good things to refresh (they were buried somewhere deep in a “sector” of my brain).
Researching about new OS’s and relative technologies was a good thing.
I discovered that there are way too many operating systems out there. Mind-boggling: I found this list of “academic, personal and small non-commercial” operating systems: http://wiki.osdev.org/Projects#A

As Craig mention, it is hard to figure out how much time to dedicate to the assignments and DQ. Everything is interesting I could easily spent entire days working on those.
Another fantastic week.
Best Regards,
Augusto

	              
	              Hi Craig, the tip to replay the video and re-read the lecture notes is good! Thanks, Augusto
	              
	              Hi Terry,
10 year old homework is designing a smart watch?? Man, I am feeling very old...
Best Regards,
Augusto.
	              
	              Hi Bram.
I do agree with Augusto on the 'magnanimity'&nbsp;of Microsoft who have for sure had a deliberate strategy for Africa. Being in Kenya, I recall participating in a Microsoft localisation program for the incorporation of Kiswahili language into Windows XP around the year 2004. It was quite an experience using the various tools and Local language kit to match and upload the translations from Kiswahili into English
On this one, Apple for sure has not ventured on a strategy for Africa and seems comfortable with the U.S. market.

Joseph
	              
	              Hi Anthony,
We will have lots security issues with WebOS, basically because of internet. All kind of vulnerability can be imagine, based on the experiences we have with web applications. We will probably encountered serious and critical security issues, like attacks on the system of validating the identity of a user, or Attacks on the user when using the OS (or other applications), or Attacks that use the application processes (change system password, system account creation ...) for hostile purposes., or violations of authentication and session management or failures on encryption algorithms etc.
Sure we can imagine all kind of security issues, but the most important is to be aware of these various problems and work on solutions, this should not be an obstacle but a motivation to make things evolve.
Regards,
Tresor

	              
	              Hi Craig.
There is a Google Sync for Microsoft Outlook as well and I must say it does a preety good job of keeping your outlook dashboard in sync with mail received on the Google Server.
You can check out this link:&nbsp;https://tools.google.com/dlpage/gappssync&nbsp;
Joseph
	              
	              Hi Augusto. In terms of security, the additional element that would be useful to make it even more secure is application of 2-factor authentication (something you know, and something you have) and lately even 3-factor authentication (adds an additional item of 'something that you are') . Google have added this level of security for their Gmail application for authentication purposes Joseph References 1) Radack,S. 'ELECTRONIC AUTHENTICATION: &nbsp;GUIDANCE FOR SELECTING SECURE TECHNIQUES', NIST, Available at&nbsp;http://www.itl.nist.gov/lab/bulletns/bltnaug04.htm, Accessed 08/10/2014 2) Google 2-Step verification, Available at&nbsp;https://www.google.com/landing/2step/ , Accessed 08/10/2014
	              
	              Hi Dr. Anthony,
Linux OS's offer several advantages over proprietary operating systems which makes it ideal for specific functions within a computing environment. They will always have a place and will continue the growing trend in desktop usage as the GUI simplifies.
A few of the reason to support my argument and why I use Linux OS in my environment are as follow:
Linux is cost effective: Being open source, there is no cost associated with acquiring the OS and depending on the distribution you use, there are many support communities available to help you with any challenges that may come up. And if you need professional support there are online entities that can offer a support contract at great prices.
The OS is light: The footprint of a linux OS is much smaller than Windows OS, it requires less hardware resources and the code is flexible enough to allow for customization.
Very Secure: Linux OS's are known for the security capabilities, which is partly due to the small footprint of the kernel, making it easier to configure, maintain and secure against threats. Infact several firewalls are built on linux kernel.
Reliability: Linux OS's are notoriously reliable and can operate for months without any upgrade, hotfixes or even reboots. I trusted linux for simple services that I need to stay on and be constantly available and it has never let me down.
Like with all operating systems linux also has its own weaknesses especially when compared to propreitary operating systems, I'll list a few of them:
Hardware Compatibility: Even though hardware manufacturers have made advancements in supporting linux OS in the driver designs, for newer hardware. There are still several hardware componenets on the market that area not compatible with linux OS's.
Ease Of Use: Being a command line base OS, most of the configuration for specific functionalities need to be done at the command line so a user will need to have knowledge of the commands in order to utilise Linux to its full potential. However desktop usage continues to improve on its user friendliness.
Open-Source software usage is increasing generally as companies and individuals seek means of controlling cost of technology investments. With that in mind I only see further acceptance and usage of Linux based operating systems in the future.

References:
www.computerhope.com (n.d.) Linux vs Windows [Online]. Available from&nbsp;http://www.computerhope.com/issues/ch000575.htm (Accessed: 8 October 2014)
Weiler, M. (2013) Why should I get Linux over Windows 8 [Online]. Available from&nbsp;http://www.linux.org/threads/why-should-i-get-linux-over-windows-8.4268/&nbsp;(Accessed: 8 October 2014)

Regards,
R.Biggs

	              
	              It was an other week full of information and learning about operating systems. It was really interesting to read about distributed operating systems. Got a chance to refresh OS concepts, types of OS, key functionalities and the critical factors to consider while going to take decision which OS is best suitable in our environment that can meet our requirements.
Regards,,,
Numan
	              
	              Hi Augusto,

What about tunneling. Would it not be enough to have a VPN with the WebOS? I know they are liable for phising kind of seyurity breaches but that responsability lies with the user.

Incidental one hears about some problems (http://appleinsider.com/articles/14/09/23/apple-to-address-problems-with-keyboards-vpn-more-in-ios-801---report) but this should be enough.

Greetings from Bram
	              
	              hmmm, kids are getting smart!...&nbsp;
	              
	              ha, lol. I like your "sector" reference.
...I've just booked myself in for a brain defrag! :-)
	              
	              Hi Bram, Hi Augusto, Hi Joseph,
Do you think it was Bill Gates that has instigated the focus on Africa? I know he and his Wife Melinda have spent significant time in Africa with their philanthropic foundation.
Best Wishes, Craig
	              
	              Hi Joseph,&nbsp;
Thanks for this.
Best Wishes, Craig
	              
	              Hi all,
&nbsp;
Have a nice day!
&nbsp;
In my opinion, "UNIX" is really just a trademarked name, applied by The Open Group, upon completion of a certification. Many different - not at all compatible - OSes are certified as a UNIX. OS X among them. Here is the current certification page for OS X 10.9 "Mavericks" as "UNIX 03" certified: http://www.opengroup.org/openbrand/register/brand3602.htm
&nbsp;
Apple has submitted OS X for certification (and received it,) every version since 10.5. However, versions prior to 10.5 (as with many 'UNIX-like' OSes such as many distributions of Linux,) could probably have passed certification had they applied for it.
&nbsp;
So it really depends on if you define "UNIX" as "the trademarked name by The Open Group, as applied to operating systems that have certification from The Open Group as a UNIX system" or if you define "unix" as "an operating system that functions like the original AT&amp;T Unix operating system, and meets the standards set forward in any version of the Single Unix Specification, even if it was never submitted to The Open Group for testing and certification," then every OS X back to the original one would likely qualify. (As would most Linux distributions, even though none have undergone The Open Group certification.)
&nbsp;
BR,
&nbsp;
KingTan Yu
&nbsp;
Reference
Apple Inc. Technology Brief(2011); OS X for UNIX Users; Online: https://ssl.apple.com/media/us/osx/2012/docs/OSX_for_UNIX_Users_TB_July2011.pdf; Accessed on: 09-Oct-2014

	              
	              Hi,
&nbsp;
For the browser, IE may not the most popular today, but it still has his advance, especially running at Windows platform, its speed is the champion
&nbsp;
Best browser for speed
&nbsp;
Browsers don't generally feel sluggish any more, but there are still crucial differences in the way they do things, especially web apps. The venerable Sunspider benchmark is a good indication of how well a browser performs under pressure, and the lower the score the faster the browser. At the risk of sounding like an Upworthy headline here: we tested all the major browsers in Sunspider. Their scores may surprise you.
&nbsp;
1.&nbsp;&nbsp;&nbsp;&nbsp; Safari 197.9ms
2.&nbsp;&nbsp;&nbsp;&nbsp; Opera 174.4ms
3.&nbsp;&nbsp;&nbsp;&nbsp; Chrome165.2ms
4.&nbsp;&nbsp;&nbsp;&nbsp; Firefox 157.9ms
5.&nbsp;&nbsp;&nbsp;&nbsp; Internet Explorer (desktop)94.7ms
6.&nbsp;&nbsp;&nbsp;&nbsp; Internet Explorer (modern)93.1ms
&nbsp;
As you can see, Internet Explorer (both the desktop and modern incarnations) isn't just ahead, but ahead by a significant amount.
&nbsp;
Best for add-ons
&nbsp;
Firefox has long been the king of this particular category, its combination of add-ons, Pin Tabs for web apps and Greasemonkey scripts making it the power user's friend.
Chrome isn't far behind, however, and its reach now extends to your desktop in the form of Chrome Desktop Apps and Google Now notifications, which began rolling out to Chrome users in late March.
Opera has changed its rendering engine and now uses the same technology as Chrome, which means Chromium extensions can work on the Opera browser. Opera's extension gallery is much smaller than Google's, but it's early days and big hitters such as Evernote, Pocket and AdBlock Plus are present. It's also a very nice looking browser; to our eyes it's much, much better looking than its rivals.
Internet Explorer isn't in last place here: that honour goes to Safari, whose extensions gallery is smaller than something very small indeed.
BR,
&nbsp;
KingTan Yu

	              
	              References
&nbsp;
Roland Waddilove (2014); Battle of the browsers 2014: which is the best web browser for Windows?l Online : http://www.pcadvisor.co.uk/test-centre/software/3493898/which-is-best-web-browser-for-windows-2014/ ; Accessed on: 09-Oct-2014
&nbsp;
TopTen REVIEWS (2014); Internet Browser Software Review; Online: http://internet-browser-review.toptenreviews.com/; Accessed on: 09-Oct-2014)

	              
	              
	              
	              Hi all,Have a Nice day!I don't think the OS and Web OS are same thing that Operating systems were built to communicate between the application and the hardware stuff of the computer. It deals with the internal operations running inside the computer. It includes scheduling of tasks, multi-tasking, utilization of resources etc. But on the other hand, WebOS are the operating systems that are composed of rich user interface which tries to mimic appearance of an operating system. Another myth related to web OS is its confusion with another word “webOS”. The webOS is actually an operating system developed under Linux for palm mobile devices.Consider the case of computers we have. The computer consists of many applications to work with such as one can use calculator to calculate, calendar to be scheduled, clock, games, and many other applications. Apart from these apps we also have diverse data like movies, memories, music, and files etc which we store in computer hard disk. We communicate with the computer through the user interface which is right now before your eyes (if you are viewing this page on a computer). If we want to share any data, internet is engaged. Sharing can be done through many websites available on the web. So this is how a local computer with a normal user works. Now consider yourself as a computer user right now working at office. Suddenly your boss calls you to show some random file to him (sometimes they like people feeling inconvenient). But you forgot to bring it. The file is in the hard disk of your home PC and it could not be teleport from there to your office computer in air. This is the situation when most brain thinks if there could be any method by which they could access their local content anywhere. This problem was resolved by the programmers by introducing the concept of “Web OS”.BR,KingTan YuReferenceJonathan Strickland (2014); How Web Operating Systems Work; online : http://computer.howstuffworks.com/web-operating-system.htm; Accessed on: 09-Oct-2014&nbsp;
	              
	              Hi all,Open-source software is free to use, distribute, and modify. It has lower costs, and in most cases this is only a fraction of the cost of their proprietary counterparts.Open-source software is more secured as the code is accessible to everyone. Anyone can fix bugs as they are found, and users do not have to wait for the next release. The fact that is continuously analyzed by a large community produces secure and stable code.Open source is not dependent on the company or author that originally created it. Even if the company fails, the code continues to exist and be developed by its users. Also, it uses open standards accessible to everyone; thus, it does not have the problem of incompatible formats that exist in proprietary software.Lastly, the companies using open-source software do not have to think about complex licensing models and do not need anti-piracy measures like product activation or serial number.ReferenceJesus M. Gonzalez-Barahona (2000); Advantages of open source software; online: http://eu.conecta.it/paper/Advantages_open_source_soft.html; accessed on: 09-Oct-2014BR,
	              
	              Hi Dr Ayoola,
Being web operating systems, the key security feature that must be designed is protection of email application and malicious code in emails. An email with malicious code could provide remote access files on the Web-OS device. If you know how potentially devastating this malicious code could be; all your emails and contacts could be snatched simply by opening a malicious email. It is highly necessary to look deep into the area of email protection.
Best Regards, Babatunde
	              
	              hi All,
Internet Explorer is good but of recent they have been having issue. i use google chrome and its been really good.
Any suggestions on something better i can try.

Regards
martins
	              
	              Hi Augusto,
Yes, I share your points on the benefit of having a diversified client side OSs.
I'm not quite sure about the pros/cons of having mutiple OS vendors on the sever side though.
br, Terry
	              
	              hello Craig,
I like your post and babatunde nice one for your addition of that word 'versatile', it sums it all. Linux open source system is versatile.
best regards
Martins&nbsp;
	              
	              Hi Augusto,
And 10 pages of keynotes plus the presentation! I suspect that the homework also involves talking to a neighbour.
br, Terry
	              
	              Hi KingTan,
Thanks for the reply:-)
Yes, "UNIX" is a trademark you get from The Open Group by getting the certification and paying the registration. But why are you calling the UNIX trademarked systems "not at all compatible"? They are compatible even on the C header file level.
None of the Linux distributions are registerered because it's very expensive. But they still follow the Single UNIX Specification. That's the advantage shared by all Unix-like system users.
br, Terry
Reference
http://pubs.opengroup.org/onlinepubs/9695969499/toc.pdf
	              
	              Hi Craig.
I believe Bill Gates and Microsoft have played a huge role on the focus on Africa not only through the philanthropic activities of his foundation, but from very early on in the 90s,
Microsoft has had a huge impact and focus going beyond the traditional selling of Windows and office solutions. One of the impactful Microsoft programs that has been running for a long time now is the&nbsp;
Partners In Learning program (http://www.pil-network.com/). This program has aimed to build capacity in learning institutions by empowering&nbsp;educators and school leaders connect,
collaborate, create, and share so that students can realise their greatest potential. The institutions are strengthened not just from a technical skills perspective and building of a culture of learning
and collaboration, but additionally putting in place the required infrastructure, systems and processes required for sustainability. This can also be seen in the light of Corporate Social Responsibility (CSR)
and has helped to build the mindshare for Microsoft in Kenya.

I don't know whether Apple for instance has a similar strategy but lately many of the big IT companies have set up shop here with the likes of Google, IBM, SAP having specific engagements with&nbsp;
learning institutions.

Joseph

	              
	              Hi Dr Anthony,I believe that cost is the driving factor behind the increasing use of Linux Operating Systems (OS) in enterprises, because it &nbsp;is becoming expensive to pay for the associated Client Access License (CALs) and server licenses of the popular and competitive Windows associated operating system.&nbsp;Before there was the issue of the knowledge gap where Linux was concerned, however more persons becoming familiar with the Linux OS. Organizations are also looking for persons that knowledgeable of Linux and they are investing in appropriated training for their Information Technology (IT) staff. This can be said for the company where I work that is currently looking to migrate to a Linux instead on Windows environment and or is looking to get both to work together before a complete migration to Linux.Linux stability, reliability, improved performance, lower system requirements and now ease of implementation and deployment and as compared with Windows, are other additives to this change factor.Basically you can get more for less with Linux and organisations are realizing this.References:Windows 7 system requirements, [Online]. Available at:&nbsp;http://windows.microsoft.com/en-us/windows7/products/system-requirements (Accessed on October 5, 2014)Memory and Disk Space Requirements, [Online]. Available at:https://help.ubuntu.com/10.04/installation-guide/powerpc/memory-disk-requirements.html (Accessed on October 5, 2014)


	              
	              Really? WHich country is that? That sounds interesting. They probably trying to make them more creative. Thats nice.
	              
	              Hi Anthony,This week was informative as it was over the past couple of weeks. Here is a highlight some of the areas which I covered as it relates to operating systems (OS).1. The History of Operating Systems - includes brief history on Windows and Linux OS2. Operating System Architecture3. Coordinating the Machine’s Activities4. The virtual computer5. Humble beginning of the OS6. Isolation and interrupts7. Multiprogramming8. GUIs9. Processes10. Deadlocks11. Security
Thanks,
Tanisha

	              
	              I think I will get one of those. LOL.
	              
	              Hi Colleagues,
I agree with craig suggested of re-reading the notes and playing over the videos. I try to go through the material atleast twice before trying to do any work and that the starting point, it really helps me get the theory down so I can reflect on my own lifes work and respond meaningfully.
This week was really underestimated but ver rewarding, its is quite thrilling when you realize how little you know and how vast a particular topic is. Operating systems gave me a hell of a ride this week and I know the challenge is gonna keep increase, so my friend I gearing up for it.
In all this week was very good and I enjoy the challenge and learnt a great deal about OS's. The discussion forum was very good and I enjoy reading and learning from all the post.
Thank you all for a good week.
Regards,


	              
	              Gavin is in an International school in Singapore.
	              
	              Hi Bram,
That could be a solution, but you still need a minium of security on a VPN tunnel (hackers can potentially still reach the tunnel). Also, they have the disadvantage that they need to be setup and may restrict your bandwidth. Just using any internet connection it is much easier.&nbsp;
Best Regards,
Augusto
	              
	              Hi Joseph and Craig,
Thanks for your insights! 
Craig, I believe Joseph is more qualified to answer your question than me, since he is actually there. I am glad that Gates foundation has an impact in Africa and that Joseph can attest to that. 
I'm sure that Microsoft focus in Africa has a part of philanthropy, but I am also sure there are huge economics in play.&nbsp;With a little over 1 billion population in Africa in 2010 (United Nations. 2012), the scale of economics is huge. Whatever the reason one thing is for sure: Microsoft is building a competitive advantage in Africa: it is democratizing and localizing technology through its products. In that regard I think that they really understand the importance of local dynamics. Other big software companies do not get this at all, they will need to understand this as the world grows and the western world shrinks.
Best Regards,
Augusto.
&nbsp;
References
United Nations. (2012) World Population Prospects: The 2012 Revision. Available at: http://esa.un.org/unpd/wpp/Excel-Data/population.htm (Accessed: 9 October 2014).
&nbsp;

	              
	              Hi Dr Anthony,

&nbsp;

This week turned out to an insightful good week. In this week, I was able to cover some areas related to operating systems of various types, its functions and purpose, also checking on the features and issues within the OS, such as isolation, multiprogramming, synchronization, deadlocks and security. One thing always leads to another in class discussions with colleagues who make it more educational and interesting. Thank you for an interesting week.&nbsp;

&nbsp;

Best Regards, Babatunde

&nbsp;

References

Laureate Education, (2014) Computer Structures – Lecture Notes and Online Video – Week 5: Operating Systems&nbsp;[Online]. Available through the programme classroom (Accessed 2 October 2014)

Laureate Education (2014) Understanding operating systems [Video, Online]. (Accessed 2 October 2014)

Brookshear, J.G. (2012) Computer science: An overview. 11th ed. Boston: Pearson Education / Addison-Wesley
	              
	              Hi Craig, Robin

Yeah I felt the same especially this week. I regularly write at least 50% more as far as word count and it's difficult to sometimes know what to leave in and take out to get decent marks. I guess why this is a Masters level degree. Like you though I could easily double what I currently write and still only scratch the surface at times.

ta
chris
	              
	              Hi

Key lessons learned for me this week is
1) The most important one is not to open a read only downloaded zip folder from dropbox and make hours of changes only to then close MS word and find out it hasnt saved any of it. Luckily this was on tuesday night so I still had wednesday to re-write the assignment out again. You'd think it would give you a warning messsage while your editing the doc but apparently not.... lesson learned the hard way.
2) I enjoyed delving deeper into operating systems. Like others have said I took most of the knowledge I did know for granted but it's not until I did this weeks topic that it's opened me up a bit more to try new things and look at alternatives and not just stay safe and stick to what I know.
3) webOS. I have to be honest and say I knew very little about it until this week. I knew of it but not about it and always assumed it was more targeted to mobiles.
4) OS classifications was fun to go through. Although I still feel there isn't an definitive rule for categorising them it was still good to go through what was know and talked about and what or how I would place them.

Thanks
Chris

	              
	              Hi Ricardo
I love your positivity and how you have referenced that you find it "thrilling" realising how little you know and how vast a particular topic is!
I find it completely heartbreaking and demoralising, realising I have somewhat stumbled through a computing career and doing so without knowing a damn thing. I wish I had done this programme 20 years ago, although to Belinda's point a few weeks ago, I'd prefer it to be without the need to visit the library and be able to take advantage of learning almost entirely through the information on the internet. It was such a pain back then but we didnt know anything different.
In one way, I wish I was growing up in this era - I keep saying to my son, kids have so much access to learning nowadays, don't miss out, 'carpe diem' as they say!
He is also 10... the pressure is on him now though, no messing about, just like Terry's neighbour, I want a smart watch designing!
Best Wishes, Craig
	              
	              Hi Terry, Hi Augusto
I agree, relative to having a diversified choice of OS on the client side, and having more of a consistent and standardised approach for the enterprise server side however with the exception, that there may be certain needs that dictate this choice, such as real time operations.&nbsp;
Cheers, Craig
	              
	              Brilliant!
Thanks Joseph
Fantastic update and well done to Microsoft for starting this off.&nbsp;
Best Wishes, Craig

	              
	              Thanks Augusto
I'm sure, as the market dictates, all Technology companies will be looking to exploit these opportunities. I agree, it seems Microsoft have stolen a march on the area, its just nice to think of it philanthropically though, even though deep down there are commercial and economic benefits.
Best Wishes, Craig
	              
	              Hi All,
Sorry if someone has already mentioned this - I didnt actually realise this until we embarked on this weeks assignment, worth noting the Operating Systems on the Fastest Supercomputers in the World are also Linux-based.
China's Tianhe-2 and Blue Gene just two examples
References
Owano, N. ed. (2013). Tianhe-2 supercomputer at 31 petaflops is title contender [Online]. Phys.org. Available from:&nbsp;http://phys.org/news/2013-06-tianhe-supercomputer-petaflops-title-contender.html (Accessed 6 October 2014)
IBM. (n.d.) Blue Gene [Online]. Available from:&nbsp;http://www-03.ibm.com/ibm/history/ibm100/us/en/icons/bluegene/breakthroughs/ (Accessed 6 October 2014)
	              
	              Sorry, China's Tianhe-2, and IBM's Blue Gene just two examples
References
Owano, N. ed. (2013).&nbsp;Tianhe-2 supercomputer at 31 petaflops is title contender&nbsp;[Online]. Phys.org. Available from:&nbsp;http://phys.org/news/2013-06-tianhe-supercomputer-petaflops-title-contender.html (Accessed 6 October 2014)
IBM. (n.d.).&nbsp;Blue Gene&nbsp;[Online]. Available from:&nbsp;http://www-03.ibm.com/ibm/history/ibm100/us/en/icons/bluegene/breakthroughs/ (Accessed 6 October 2014)

	              
	              Thanks Babatunde&nbsp;
Yes, I use OS X only because I wanting to be hip and get a Mac. OS X just happened to come with it. That said, I am really pleased and I have never really missed Windows. Perhaps the nature of my use has contributed to this, but I do like the MS Office products, which I have installed on my Mac (Office for Mac).
Best Wishes, Craig
	              
	              Thanks for the comments, Craig. &nbsp;Yes, I know what you mean about the amount of time needed to complete each week's assignments. &nbsp;The best advice is to stay focused and organized, although I know that this is easier said than done!
In connection with our expectations for the quality needed for DQ and HIA submissions, I will post further advice on this over the next day or so.
Regards,Anthony
	              
	              This week’s discussions dealt with issues relating to operating systems, and their basic functional requirements, with respect to resource management and provision of a secure user-friendly virtual machine. The posts additionally made reference to web-based and open-source operating system types, including looking at future developmental trends for operating systems.Good effort with the discussions, everybody!Anthony
	              
	              Hi
This week’s focus has been on Operating Systems. Like in the other week’s I got re-introduced to subjects that I have not thought about for a long time – you could say that I’m taken them for granted, i.e. not daily thing about what it really does, what’s inside the engine. For many people the choice of OS, I think, it often determined by the working environment one “grows up in”. In my case it has been Microsoft Windows which therefor has been in focus for me this week. I especially focused on the client side and read a lot about Windows 8.1, and everything pointed towards the conclusion that we should not upgrade yet but rather wait for Windows 10.
GUI is a part of the OS, and this subject has always interested me since I think that having a good GUI makes all the difference.
I think the discussion about WEB-OS was probably the most interesting because it rely makes one think about can be like, whether this is a good or bad idea, or is it all possible etc.
I normally try to answer all DQs and any question to me and more, but I must admit this week has been very hard on me cause I’ve been sick (there's a winther start flu going around) on top of hectic work, study and family life - so hopefully I learned that if I get sick again, I should probably not go to work, just stay in bed and get well.

Best regards
Bo W. Mogensen
	              
	              Hi Terry.
Reflecting on your comment that 'Hackers cannot do anything about a computer that is disconnected', there is this issue I can across about the Stutnex virus that allegedly was designed to infiltrate the Iranian Nuclear Facility at Natanz, (The Australian News)
Myth or fiction, it is said that the plant's computer control system is not connected to the internet precisely to prevent this kind of sabotage. So How the worm entered the Iranian nuclear facility at Natanz remains a mystery although it is attributed to having being transfered there through infected laptops of technicians.

Reference:
Hider, J. 'Computer virus used to sabotage Iran's nuclear plans 'built by US and Israel',&nbsp;Times Online Jan 2011,Available at&nbsp;
http://www.theaustralian.com.au/news/world/computer-virus-used-to-sabotage-irans-nuclear-plans-built-by-us-and-israel/story-e6frg6so-1225989304785?nk=9ddf4da1b768f7d5acdafdbd7df89952, Accessed 10/10/2014



	              
	              Hi Anthony.
This past week was quite interesting and felt like going back to basics. My first interaction and learning content was on operating systems with MS-DOS being the subject matter. The material covered and discussions held really went in-depth to enhance my understanding and appreciate the different perspectives of operating systems from the historical point of view to what's possible in the future. The topics on choice of operating system, classifications , understanding of web OS and other current trends helped in appreciating the importanct role played by Operating Systems.&nbsp;
Joseph
	              
	              Thanks for the comments and summary of areas covered, Terry. &nbsp;I believe that your passion for OS matters shone through your DQ posts :-).Regards,Anthony
	              
	              Thanks for the summary/comments, Augusto. &nbsp;Yes, OS-related topics are indeed interesting, but can be time-consuming to research - the key is good time-management, to ensure even and unstressed coverage of the material.Regards,Anthony
	              
	              Hi Joseph,
I agree with your perspective.
I think if we limit the topic to a very narrow "network security", then by definition it has no threat to a disconnected computer. But that narrowing down doesn't make any sense, as one of the TED talks said "Hackers don't limit themselves to any security model". And they can use an iPhone5 to detect the keystrokes just by puting it aside the keyboard!
br, Terry
	              
	              Thanks for the comments, Numan.
Regards,Anthony

	              
	              Thanks for the comments and summary of topics covered in week 5, Tanisha.
Regards,Anthony

	              
	              Thanks for the comments, Babatunde..
Regards,Anthony

	              
	              Hi guys
Thanks for the discussion. I was somewhat expecting this when I wrote it like a kind of "provocation", both to hear others opinion but also to make myself think.
There is no single true solution, and e.g. the BYOD will especially challenge the choice of OS on the client side. On the server side, it of course depends on the industry - like Augusto points out the gaming industry - but I am in still convinced that one should decide on primarily one server OS, and the only "bastards" should be for special needs.
br Bo
	              
	              Thanks for the comments and summary, Chris. &nbsp;Yes, it is good that you had the day's buffer to allow you to re-do the document you lost, for your HIA submission.
Regards,Anthony

	              
	              Thanks for the comments, Bo. &nbsp;I hope you are feeling better now - try to take things as easy as possible until you are fully recovered.
Regards,Anthony

	              
	              The first PC virus was detected in 1986. Earlier viruses during the 1980s and 1990s were easily detectable and less malicious than today’s viruses. Today organised criminals are responsible for most of the threats against networked systems. There are a variety of different ways criminals can attack a networked system. In this Discussion, you will analyse the security issues associated with networked systems. You will also suggest measures to overcome these security threats and prevent future attacks.
To prepare for this Discussion:

Review your weekly Learning Resources with a focus on networked systems and security threats.
Watch the TED Talk video, ‘All your devices can be hacked’.
Research additional security threats to computer networks as needed.
Reflect on how you could prevent security threats to a network.

To complete this Discussion: Post: Create an initial post in which you analyse the security issues associated with networked systems. Address the following:

Analyse the security vulnerabilities commonly associated with network systems.
Summarise each vulnerability and explain why it is a threat.
Recommend methods to prevent exploitations of these security vulnerabilities and prevent security attacks.
Fully state and justify any choices, assumptions or claims that you make using the suggested resources for this week and/or your own research.

Respond: Respond to your colleagues. Address the following:

Recommend additional methods for which your colleague can defend against his or her identified security vulnerabilities.
Be sure to support any claims you make.

For all Discussions (unless stated otherwise):

Create a single document with your initial post. Your document should be 350-500 words, though you will be marked based on the quality of your writing, not on the number of words.
By Sunday, post the text of your document to the Discussion Board for this Week, and upload the document using the Turnitin submission link for this Discussion.
By Wednesday, make 3–5 substantial follow-up responses to your colleagues. These can include responses to your colleagues’ initial posts, as well as responses to colleagues who responded to your own initial post. Your total Discussion Board participation must occur on at least 3 individual days during each week. Follow-up responses should be significant contributions to the Discussion. Do not submit your follow-up responses to Turnitin.
In general, online discussion is best when you:


Ask insightful questions.
Extend the discussion into new but relevant areas.
Model or promote critical reflection.
Support your arguments with citations and references from the assigned Learning Resources and other literature, using Harvard Liverpool Referencing Style.



Ensure that you spread your discussion posts across at least three separate days of each week. This will help maximise the value of your discussion with colleagues and serve to meet the learning objectives for each activity.
Click on the Reply button below to reveal the textbox for entering your message. Then click on the Submit button to post your message.
	              
	              Week 6 Discussion Question – Network Systems Security Risks
Craig Thomas – 10 October 2014
Cyber-Security should be at the very heart of our society, central to our way of life and at the top of our priorities, yet it is so often overlooked or in many cases, and even worse, completely mismanaged. 
The Digital revolution has created enormous opportunities to be more connected through technology than ever before. Unfortunately however, this era is also exposing our vulnerabilities and increasing the risk associated with unauthorised access to our data, including that of personal and sensitive information. 
This runs across all levels of society, from your traditional consumer and general users of technology, to the highest parts of governments and businesses across the world. But, do we really take it seriously, enough? In some cases yes, however in many more cases the answer is no. It is somewhat alarming that almost anything nowadays can be ‘hacked’ (Rubin, 2011).
Interestingly, there are many arguments which are not so unattractive and suggest that there is a need to identify skillsets typically seen in the world of ‘hacking’, to nurture those individuals in helping refocus their energies towards good legal practices, rather than towards acts of criminality. There are bounty schemes already in place whereby ‘hackers’ can disclose weaknesses in security practices to corporations, who reward for identification, and then act to remove that particular risk,
So, what are the rest of us doing to protect ourselves from these threats and vulnerabilities? 
As examples, we have governments who have Security Agencies responsible for the protection of their countries assets, with dedicated Cyber-Security Divisions focused on cyber threats and technology vulnerabilities. We also have many large corporations who have appointed key positions of Chief Information Security Officer (CISO) to their boards, or have other senior board executives such as the Chief Information Officers (CIO) assume the accountability and responsibility in protecting their company data and information from such threats and vulnerabilities. And, at the other end of the spectrum for individual consumer level users, we have passwords and passcodes to protect our computers and home WIFI routers.
As already outlined, the risks and vulnerabilities have been increased exponentially since moving into the Digital era, and are especially magnified in the connected world. That doesn’t mean that the non-connected and completely isolated computer networks are not vulnerable, as there are many ways around that level of risk mitigation, such as remote connectivity or even the penetration of other external mobile devices which are then subsequently and periodically reconnected to the isolated network. 
In the connected world, there are network reference models that support the communications process. These models have defined a number of network layers, of which enable and also interface with other layers within its host network, or with the same layer functions of another network. These are key areas of vulnerability and as such must be managed responsibly, as levels of threats can vary considerably, from common malware viruses through to organized crime and acts of terrorism. 
In summary, there are seven traditional layers as part of the OSI (Open Systems Interconnection) reference framework (Douligeris and Serpanos, 2007) which cover the hardware, operating system, middleware and services layers, plus three additional user layers, as depicted below:


Source: Wikipedia
Connection to these layers could be from fixed point or mobile, via traditional Ethernet network connectivity, WIFI Routers, 3G/4G Telecommunications Networks, Bluetooth, Infrared and Radio Wave Technologies just to name a few. As a result, a Networked System provides a clear starting point for all security vulnerabilities. 
I’d like to summarise a number of security vulnerabilities and provide brief explanations in cases as to why these are threats, and that they must be prevented, namely:

Eavesdropping attacks, which are unauthorised interception of communications and disclosure of information. As referenced by (Douligeris and Serpanos, 2007), this can be achieved at several different network layers, and including methods such as sniffing and wiretapping. 
Login attacks, which bypass the access control authentication mechanisms, and provide the user with more privileges than authorised (Douligeris and Serpanos, 2007). 
Spoofing, which is assuming an identity that the user has not right to use. The target system assumes that the user is the correct user, and continues to operate on that basis (Douligeris and Serpanos, 2007).
Intrusion attacks, is whereby access to the system is gained through the network, and suspected users target vulnerabilities within that network. This threat is open to mismanagement, for example whereby the latest patches and upgrades are not applied accordingly (Douligeris and Serpanos, 2007).
Hijacking attacks, which are attempts to gain unauthorised access however through a legitimate connection or identification including a user leaving a session open, which can be hijacked (Douligeris and Serpanos, 2007).
Denial of Service attacks, which is rendering a network or system unusable for legitimate hosts and users (Douligeris and Serpanos, 2007), and impacting subsequent operations. There are a number of different types of DOS attacks, which could be potentially devastating, for example public service type organisations, including energy, telecommunications and water services.
Application layer attacks, such as exploiting weaknesses in the webserver, or faulty controls in the filtering. These can include malicious attacks such as viruses, trojans etc (Douligeris and Serpanos, 2007).

There are a number basic security measures and recommendations on how to prevent exploitations of these security vulnerabilities under the network layer framework, and methods to prevent security attacks, namely: 

Authentication – this is needed to validate identities, ensuring the peer entity authentication and the data origin authentications.
Access Control – the access is controlled through access policies, including memberships, permissions, user access privileges, and are used to protect information from unauthorised access. It is recommended to use complex password structures and codes, as well as ensuring appropriate access settings on your devices and network connections.
Data Confidentiality – protection of data from disclosure to unauthorised subjects. There are a number of different security recommendations (X.800) including connection confidentiality, connectionless confidentiality, selective field confidentiality and traffic flow confidentiality (CCITT, 1991)
Data Integrity – ensuring the data is not altered by unauthorised subjects during transmission and has several forms of protection (Douligeris and Serpanos, 2007).
Nonrepudiation, ensuring transmission cannot be denied, can include proof of origin and proof of destination. 

There are many additional services that can be provided to aid with the security mechanisms, and as described by Douligeris and Serpanos (2007), these include Encipherment using cryptography, byzantine detection protocols, and public and private authentication keys.
Douligeris and Serpanos (2007) further suggest that at the core of network security is the protection of routing and relaying, with Firewalls used to protect access controls on the network, and the protection of message confidentiality and integrity relying on mechanisms as if it was closed network, such as Virtual Private Network (VPN) security mechanisms. Furthermore, they also reference security of data is also protected through Internet Protocol (IP), Secure Sockets Layer (SSL) and Transport Layer Security (TLS) security mechanisms, as well as state of the art Intrusion Detection Systems (IDS) and Intrusion prevention Systems (IPS). 
Digital Signatures can also be used, and are implemented using properly applied asymmetrical encipherment. Lastly, taking Cyber-Security to the highest levels also includes biometric identification, hand vein matching, finger printing, and voice recognition. This list is not exhaustive, but serves to provide summary examples of the vulnerability risks, and actions to prevent exploitation and attacks.
In summary conclusion, there are many ways in which to penetrate networked systems and as such, all necessary and appropriate actions and mechanisms need to be taken to prevent exploitation and prevent Cyber-Security attacks. Many of us go about our daily business completely unaware of the risks and vulnerabilities we face. There is a critical need to keep on top of all bug and security patches, and install in accordance with manufacturer guidelines. Mismanagement of this could have potentially devastating impacts. Equally, something as straightforward as basic password management is also something we should pay more attention to, and not just take for granted.
Best Wishes, Craig
References
Laureate Online Education. (2014). Computer Structures – Lecture Notes Week 6: Computer Networks [Online]. Available from: https://elearning.uol.ohecampus.com/bbcswebdav/institution/UKL1/201480_SEPTEMBER/MS_CKIT/CKIT_501/readings/UKL1_CKIT_501_Week06_LectureNotes.pdf (Accessed 9 October 2014)
Brookshear, J. G. (2012). Computer Science An Overview. 11th ed. Boston Massachusetts: Pearson. Addison Wesley
Douligeris, C., and Serpanos, D. N. (2007). Network Security – Current Status and Future Directions. Hoboken New Jersey: IEEE Press. John Wiley &amp; Sons Inc.
Rubin, A. (2011). All your devices can be hacked? [Online]. TED Video. Available from: http://www.ted.com/talks/avi_rubin_all_your_devices_can_be_hacked?language=en (Accessed 9 October 2014)
Elazari, K. (2014). Hackers: The Internets Immune System [Online]. TED Videos. Available from: http://www.ted.com/talks/keren_elazari_hackers_the_internet_s_immune_system (Accessed 9 October 2014) 
CCITT. (1991). Recommendation X.800 [Online]. Available from: http://www.itu.int/rec/T-REC-X.800-199103-I/e (Accessed 10 October 2014)
Wikipedia. (2014). OSI layers – Layer 8 [Online]. Available from: http://en.wikipedia.org/wiki/Layer_8 (Accessed 10 October 2014)
	              
	              


Network Vulnerabilities From A Hacker’s Perspective 
Terry Yin October 11, 2014 
1 Benefit Of Thinking Like A Hacker 
A hacker is a person who solve problem by breaking things, or more commonly, “A hacker is one who enjoys the intellectual challenge of creatively overcoming and circumventing limitations of programming systems and who tries to extend their capabilities.” Coleman (2010) 
So, a hacker knows how to break things. They know where to look for the unusual places in the system that can help them to achieve their goals. These unusual places are quite often security vulnerabilities of the system. On the other hand, a hacker doesn’t follow the rules and dos not limited themselves with any “security models”. This means, we may find network vulnerabilities more efficiently by thinking like a hacker. 
2 How Hackers Find Vulnerabilities 
2.1 The Quantity Makes The Difference 
The abstraction in Computer Science is a simplified specification about how something should work. The purpose of having an abstraction is to build other systems based on it without caring the details under it. There are some systems normally work perfectly according to the abstractions. But when the quantities that are handled by these systems exceed certain points, the abstractions will stop working. A hacker is aware and interested in the differences bring by the quantities. 
The Distributed Denial-of-Service attack (Wikipedia 2014b, DDoS) is one of the examples where quantity makes a difference. A server on the Internet might work fine when all the visits are from limited and intended users. But when there are too many requests, the server might not have enough resource to handle them in time. A DDoS attack is to use many computers on the network to send lots and lots of requests to the victim server so that it becomes unavailable to the intended users. 
Buffer Overflow Wikipedia (2014a) is another example. When the memory buffer that a computer program reserved for holding a certain input is not big enough, the input data might be written to the adjacent memory. Thus will trigger unexpected behavior from the program, or even the entire system. Hacks know how to use these overflowed data to manipulate the program and eventually take over the control. 
2.2 When Quantify Is Not Their Friend, Time Is 
Quantity is not always the friend of hackers, but hackers know how to deal with it. Sometimes, quantity is the challenge for a hacker. For example, to guess a password is very hard, because there are too many (not infinite, though) possibilities. A hacker might use a Brutal Force approach to crack it. For example, they may use a “dictionary” of common password people often use to try them one by one. 
You might think that’s not possible because typically a system will deny a user from trying to login after several failed attempts. But hackers can take the encrypted information “home”, and crack the password&nbsp;by these encrypted information at “home”. And they will come back only when they have a seemingly right password. So time actually becomes their friend.






2.3 When The Others See The Abstractions They See The Details 
Normally, engineers view the system as layers of abstractions. A computer network is a good example. But a hacker won’t think in layers. From their perspective, a computer system is just lots of detailed technologies that together makes things happen. 
A hacker might use the ARP spoofing (Brookshear 2011, p.176) to send fake data over the low level ARP message over the Local Area Network. Then a flawless service that depends on the high level TCP will be compromised by this attack. 
Say, a bored hacker stuck in an airport and couldn’t afdord the Internet fee, but he found the DNS service is available in the wireless network without signing in. The lack of resource might be a problem for a hacker, but the lack of abstraction isn’t! The hacker might build a TCP/IP layer over the DNS service and start surfing the free Internet. (Merlo et al. 2011, DNS Tunneling) 
Another example is SQL Injection Wikipedia (2014e). When most people see the fields in a Web form as “user name”, “email address”, “phone number”, a hacker might see them just as SQL code snippets. 
2.4 Side-effects 
Hackers like side-effects and know how to use them. 
The Heartbleed bug is revealed in April 2014 in the OpenSSL cryptography library Wikipedia (2014d). OpenSSL library is widely used in the services on the Internet. This software bug allows more data to be read then needed. So the client will receive some extra “random” data from the server. Because the data area in the server memory is used for keeping user private information, the extra data is not really “random” and might include information that the current user is not granted to access, including the other user’s information. This is a side-effect that will interest a hacker. 
A network equipment in a telecom service provider’s computer center might have a USB port. The USB port was designed for uploading and downloading files. But the USB driver in the network equipment could be from some open source software, which doesn’t only do file transfer but can also connect a USB sound card. The file transfer feature might be well tested, but the sound card feature is useless. A hacker can utilize the bugs in the USB sound card driver, and hack the entire network equipment by secretly plugging in a small USB device. Nobody would ever notice. 
The above examples showed the vulnerabilities brought by unwanted side-effects. So making a computer system shouldn’t just involve adding wanted features, but also removing unwanted feature. The process of reducing unwanted side-effect is called hardening Kopp (1997). 
3 Does A Hacker Hack Herself? 
The verb or action “hack” in computer security means “to break into computers and computer networks”; in computer science, it means “an inelegant but effective solution to a computing problem” Wikipedia (2014c). So when we are talking about creating your own solution, we should take the second meaning. 
So, no, a hacker won’t hack herself. Because a hacker solves problem rather than creating more problems and they always seek to become better (Raymond 2003, How To Become A Hacker). A hacker knows the importance of an elegant solution. She has read too much ugly code that’s vulnerable.






4 Conclusion 
To create a less vulnerable computer network system, we should not only think like a hacker but also work like a hacker. Hackers do not limit themselves by any models, and network vulnerabilities can be anywhere. Hackers have sharp eyes to see through abstractions, and they don’t let side-effects go easily. Hackers don’t hack their work. 
References 
Brookshear, J. G. (2011), Computer science: an overview, Paul Muljadi. 
Coleman, G. (2010), ‘The anthropology of hackers’, The Atlantic p. 2. 
Kopp, C. (1997), ‘Hardening your computing assets’, Asia/Pacific Open Systems Review. Computer Maga- zine Group, NSW under the title of “Information Warfare—Part 2. 
Merlo, A., Papaleo, G., Veneziano, S. &amp; Aiello, M. (2011), A comparative performance evaluation of dns tunneling tools, in ‘Computational Intelligence in Security for Information Systems’, Springer, pp. 84–91. 
Raymond, E. S. (2003), ‘How to become a hacker’, Database and Network Journal 33(2), 8–9. Wikipedia (2014a), ‘Buffer overflow — wikipedia, the free encyclopedia’. [Online; accessed 11-October-2014]. 
URL: http: // en. wikipedia. org/ w/ index. php? title= Buffer_ overflow&amp;oldid= 626899160 
Wikipedia (2014b), ‘Denial-of-service attack — wikipedia, the free encyclopedia’. [Online; accessed 11-October-2014]. URL: http: // en. wikipedia. org/ w/ index. php? title= Denial-of-service_ attack&amp;oldid= 628762198 
Wikipedia (2014c), ‘Hack — wikipedia, the free encyclopedia’. [Online; accessed 11-October-2014]. URL: http: // en. wikipedia. org/ w/ index. php? title= Hack&amp;oldid= 617604388 
Wikipedia (2014d), ‘Heartbleed — wikipedia, the free encyclopedia’. [Online; accessed 11-October-2014]. URL: http: // en. wikipedia. org/ w/ index. php? title= Heartbleed&amp;oldid= 629015819 
Wikipedia (2014e), ‘Sql injection — wikipedia, the free encyclopedia’. [Online; accessed 11-October-2014]. URL: http: // en. wikipedia. org/ w/ index. php? title= SQL_ injection&amp;oldid= 627843772 




&nbsp;




	              
	              Hi Craig,
First, about the Layer 8 of OSI model.
When I saw your picture of the OSI model, I thought to myself "a government layer in the OSI model? this must be a joke." Because I've heard too many "layer 8" jokes during so many years (they typically involve one of the US presidents). Then I googled the picture and I found "oh my god, they are serious!" They are serious! But the question is, where is the religious layer?

I think you asked a very important question "what are the rest of us doing to protect ourselves from these threats and vulnerabilities?"
I feel this is more of a philosophical question rather than technical question. I think "the rest of us" shouldn't mean "the rest of us who are not hackers", but "the rest of us who don't have that level of skills". I have been thinking about it for several days but don't have a clue yet. The only idea I'm having now is still "we can only try to be technically rich to avoid being the new poor of the new world."
br, Terry
	              
	              Security issues associated with network systems.&nbsp;&nbsp;&nbsp; Computer security is arguably the most talked about field in technology to date and rightly so. While other areas have certain trends and moments in the spotlight, computer security and specifically security threats have been an important mainstay in the industry since the first recorded virus was known to be detected called ‘Brain’ in 1986. While this is the first MS-DOS, IBM PC virus it’s certainly not the first actual classification of a virus. Spencer, S (2014) explains an overview of the virus timeline where John Von Neumann’s article on the "Theory of self-reproducing automata" way back in 1949, discussed the theory of automated self-replicating worms and viruses.
While this was not an actual functioning virus it shows the forward thinking and rational plausibility of what could be done before computers as we know them were even in operation. In reality the first know virus was called the ‘Creeper’ virus created by Bob Thomas in 1971 while at BBN Technologies. It was a self-replicating experimental program, “Creeper gained access via the ARPANET and copied itself to the remote system where the message, "I'm the creeper, catch me if you can!" was displayed. The Reaper program was later created to delete Creeper”, Wikipedia, ‘Timeline of computer viruses and worms’ (2014).

The most commonly associated security vulnerabilities.
&nbsp;&nbsp;&nbsp; There have been so many different categorisations and types of attacks over the years but the most common and the most talked about security vulnerabilities can be listed as: CISCO (2014)•&nbsp;&nbsp;&nbsp; Viruses, Trojan horses and worms.•&nbsp;&nbsp;&nbsp; Spyware and adware.•&nbsp;&nbsp;&nbsp; Zero-day attacks or zero-hour attacks.•&nbsp;&nbsp;&nbsp; Hacker attacks.•&nbsp;&nbsp;&nbsp; Denial of service attacks.•&nbsp;&nbsp;&nbsp; Data interception and theft.•&nbsp;&nbsp;&nbsp; Identity theft.
All of these can be the result of or exploitation of ‘Backdoors’, ‘Direct access attacks’, ‘Eavesdropping’, ‘Exploits’ and even ‘Indirect attacks’.
Violino, B (Dec 2013) shared the top 8 security threats of 2013 as a feature of security awareness after talking to some security and IT experts at the end of that year. More sophisticated DDoS attacks were discussed first by John South, CSO at Heartland Payment Systems. He explained that in the previous year, most DDoS attacks were in the order of 3 to 4 Gbps however this year they increased to 100 Gbps. This was the rise of the DDoS attacks being used as a cyber-weapon for a specific purpose other than just trying to take a site down. "Many institutions have had to rearchitect their network security strategies under the bandwidths that the newer threats pose."
Linked with these DDoS attacks were botnets, spreading through networked systems with a much higher level of sophisticated malware to attempt phishing scams on a much greater scale than before. Although phishing scams have been around for a while there have been an increased number of successful account takeovers in 2013 than before. It is believed to be the inclusion of Android Trojans to help distribute botnet code which has helped to increase this number.
Companies can try their level best to reduce their susceptibility to known attacks but insider threats are still very real. Whether this is through inappropriate or just lack of staff awareness and training or the growing reliance upon security software on systems or just the plain old ‘it will never happen to me’ mentality, but it only takes 1 member of staff to accidentally click on an email link or hook up their personal laptop or tablet to the works network to introduce a virus and cause chaos.
Many others including insecure applications due to the proliferation of e-commerce, zero-day vulnerabilities in popular software or data supply chain threats and former employees gaining unauthorised access are all high concerns.&nbsp;&nbsp;&nbsp;
Collins, H (2010) from Govtech.com listed the top 10 internal network vulnerabilities as:
1.&nbsp;&nbsp;&nbsp; USB drives2.&nbsp;&nbsp;&nbsp; Laptops and netbooks3.&nbsp;&nbsp;&nbsp; Wireless access points4.&nbsp;&nbsp;&nbsp; Miscellaneous USB devices (digital cameras, MP3 players, etc.)5.&nbsp;&nbsp;&nbsp; Employees borrowing others’ machines or devices6.&nbsp;&nbsp;&nbsp; The Trojan Human (attackers who visit sites disguised as employee personnel or contractors)7.&nbsp;&nbsp;&nbsp; Optical media (CDs, DVDs, etc.)8.&nbsp;&nbsp;&nbsp; Lack of employee alertness 9.&nbsp;&nbsp;&nbsp; Smartphone’s10.&nbsp;&nbsp;&nbsp; E-mail

Why are these vulnerabilities such a threat?
So why are these vulnerabilities such a threat I hear you ask. Well firstly because we use computers in such critical areas these days. No longer is it just home users and nutty professors typing away on an old IBM PC, they are simply everywhere. We use computers for aviation and guidance, banking, tax, voting, driving, communication literally everything we do has some form of computer behind it at some point. This is why it is dangerous and such a threat. Wall Street and the stock market could crash plunging the world into crisis as briefly highlighted by the silly hacked tweet that President Obama had died. A simple innocuously hacked twitter account can send shares plummeting.
Threats can come from many different angles right from global effects to regional and simple personal ones. The cloning of your bank card details and the removal of cash from your personal accounts. Rigging voting patterns and subsequently major elections to potentially overthrow or change governments. At the core of this it is simply down to money and power. There are huge potential sums of money that can be made through this illegal activity. Then there is simply the power and God like feeling they get knowing they have exploited something in some way to give them an advantage.
Imagine a power station being hacked and taken down, what if that power station is nuclear and the safety protocols are hindered. It’s not just cash it’s potentially life threatening which his why it is so serious.

Methods how these threats are prevented.
Rubins, P (March 2013) helps to show us the methods in how we can prevent these threats when he says “At the very least, limit networking hardware purchases to trusted vendors. Additional measures include carrying out network listening to detect hardware acting maliciously, and carrying out random tests on devices to look for indications that they contain extra components or malicious firmware. At the highest level, some companies may choose to assume that all hardware is compromised and continuously monitor it for unexpected behaviour.” And when talking about mobile technology and their vulnerabilities he says “The most practical way an organization can protect itself from malware on users' mobile devices is to implement some form of mobile device management (MDM). This can impose security policies and restrict application downloads to a corporate app store which contains approved applications only.”
This second quote specifically rings true to me because at the NHS where I work we have such processes in place. No external devices are allowed on site whatsoever. No USB sticks can work on our systems as the USB ports have been locked down to only the already registered and pre approved ones. We have software lists from trusted vendors only and all machines are not allowed to have admin rights on them and or allowed to download any online material. We have a web monitoring tool which through the network proxy blocks specific websites from being viewed and many more restrictions. These restrictions are there for a very good reason though. I work for the NHS as previously mentioned and would you want your personal health information leaked in any way? Or the simple fact that we oversee billions of pounds worth of payment transactions per year on behalf of the taxpaying public that we can’t afford to make a mistake with.
There are many personal ways you can keep protecting yourself from these threats such as making sure you have an up to date and working virus scanner on your PC. Making sure you password protect all accounts and even documents. Encrypting emails and clearing your browser cache after every use or simply using the ‘in private’ browser function that most browsers have these days.
Mainly you have to balance the prevention vs. cure situation. Many think because they have a fairly secure system then the cure isn’t so important or in reverse people and businesses will not look to use more secure prevention because the ability to get on the spot cures if something does happen is available.Governments will play a major part however as they have the power to introduce policy and regulation. They have the ability to set rules and then prosecute those that ignore them. 
Conclusion
In summary this issue I don’t think will ever be actually stopped or solved. Hackers will always come up with new and improved ways to do what they do simply because they have their own reasons for doing it. Software and hardware is forever changing and with that it naturally introduces new areas of vulnerabilities such as when mobile phones entered our world. All we can do both as consumers and as businesses is to minimise any potential threats through education, always prepare for the worst and put in place policies and procedures to counter them. We can make positives steps however as described by Glenny, M (July 2011) where we can recognise the steps that past and current hackers lives have and try to hire them or steer them into more productive avenues available in such things as being the security provider rather that perpetrator. Giving incentives via a legal way to promote the exposure of bugs and loopholes without exploiting them.
References:
Brookshear, G. (2011) ‘Computer science: An overview’. 11th ed. Boston: Addison Wesley/Pearson.CISCO, (2014) ‘What is network security’ [http://www.cisco.com/cisco/web/solutions/small_business/resource_center/articles/secure_my_business/what_is_network_security/index.html?referring_site=smartnavRD] accessed 9 Oct 2014.
Collins, H (Sept 2010) ‘Top 10 network security threats’ [http://www.govtech.com/security/Top-10-Network-Security-Threats.html] accessed 9 Oct 2014.
Glenny, M (July 2011) ‘Hire the hackers’ [http://www.ted.com/playlists/10/who_are_the_hackers] accessed 9 Oct 2014 Rubens, P (March 2013) ‘6 emerging security threats, and how to fight them’ [http://www.esecurityplanet.com/network-security/6-emerging-security-threats-and-how-to-fight-them.html] accessed 9 Oct 2014.
Spencer, S (2014) ‘Timeline of computer Viruses’ [http://www.mapcon.com/timeline-of-computer-viruses] accessed 10 Oct 2014.
Violino, B (Dec 2013) ‘The top 8 security threats of 2013’ [http://www.csoonline.com/article/2134216/security-awareness/the-top-8-security-threats-of-2013.html] accessed 9 Oct 2014.
Wikipedia, ‘Computer Security’ (2014) ‘Computer Security’ [http://en.wikipedia.org/wiki/Computer_security] accessed 9 Oct 2014.
Wikipedia, ‘Timeline of computer viruses and worms’ (2014) ‘Timeline of computer viruses and worms’ [http://en.wikipedia.org/wiki/Timeline_of_computer_viruses_and_worms] accessed 10 Oct 2014
	              
	              Hi Terry
Its an interesting point. I was going back and forth on whether to use the fuller descriptions on OSI layering framework - I wanted to cover the "User" layer, hence including it, even though I would also question the reference to 'government' in the terminology.
On your second point, I agree that Computer Science is full of philosophical questions - I also dont have answers, but I do have 'right' and 'wrong' as a starting point.
Watching the TED videos this week has opened up a very different perspective for me, in that whilst it can be really disruptive personally, it isn't necessarily the 'hackers' who are in the business of messing around at a traditional consumer level that deeply worry me, it is the 'hackers' who are at the organised criminal end of the scale who can potentially impact on our lives and safety. That said, and as also referenced in the TED videos, I can see that 'hackers' at the low end of the spectrum could simply 'hack' our cars for example, and cause an accident. Equally, they could rise the ladder of criminality, and be drawn into a complete new underworld of organised crime. So, I like the idea of identifying this type of talent, and nurturing for better and not for worse.&nbsp;
Whatever the perspective, we all need to do everything possible to protect ourselves - and our technology vendors also need to step up and bneyond the plate, without question.
Thanks for your response Terry
Best Wishes, Craig
	              
	              Security vulnerabilities in network systems can be summed up as the “soft spots” that are present in almost every network. These soft spots or weaknesses are present in the network and individual devices that make up the network.
Networks typically have all or one of the following three primary vulnerabilities or weaknesses:

Technology weaknesses:

Technologies for computer and networks have security weaknesses in their core, these include network equipment weaknesses, TCP/IP protocol weaknesses and operating system weaknesses. Following tables describes these three weaknesses.




Network equipment weaknesses


Network equipment such as firewalls, routers and switches have security weaknesses which includes firewall holes, routing protocols, lack of authentication and password protection.




Operating system weaknesses


According to website http://www.cert.org, UNIX, Linux, Windows and OS/2 operating systems all have security issues.




TCP/IP protocol weaknesses


ICMP, FTP and HTTP are inherently insecure. SNMP, SMTP and SYN floods are related to the inherently insecure structure upon which TCP was designed.




&nbsp;

Configuration weaknesses 

Misconfiguration of devices can itself cause big security problems. Administrators of computing and network devices need to learn what the configuration weaknesses are and correctly configure their devices. Following table lists some common configuration weaknesses.




Unsecured users accounts


User account information might be transmitted insecurely across the network which can expose it to the snoopers.




Accounts with easily guessed passwords


This common problem is the result of poorly selected and easily guessed passwords.




Misconfigured Internet Services


A common problem is to turn on Java Script in web browsers, enabling attacks by way of hostile JavaScript when accessing untrusted sites. IIS, Apache, FTP and terminal services also pose problems.




Default settings within products


Many products have default setting that enable security holes.




Misconfigured network equipment


Misconfiguration of equipment can cause significant security problems e.g. misconfigured access list, routing protocols or SNMP community strings can open up large security holes. Lack of encryption and remote access control can also cause issues, as can the practice of leaving ports open on a switch.




&nbsp;
3.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Security policy weaknesses:
Unforeseen security threats can be created by weakness of security policies. If users don’t follow security policies, the network can pose security issues. Follow table list some common security policy weaknesses.
&nbsp;




Lack of written security policy


An unwritten policy cannot be consistently applied of enforces.




Politics


Political battles and turf wars can make it difficult to implement a consistent security policy.




Lack of continuity


Poorly chosen, easily cracked or default passwords can allow unauthorized access to the network.




Logical access controls not applied


Inadequate monitoring and auditing allow attacks and unauthorized use to continue, wasting company resources. It can make hard to enforce security policies.




Changes do not follow policy.


Unauthorized changes to the network topology or installation of unapproved applications create security holes.




Nonexistence of disaster recovery plan.


The lack of disaster recovery plan allows chaos, panic and is confusion to occur when someone attacks the enterprise.




&nbsp;
&nbsp;
Recommendations to prevent exploitations of security vulnerabilities:

Security Patches: Many security incidents happen where hackers take advantage of unpatched systems and attack the network. We should follow recommendations and best practices by updating security patches for OS and other software running on it (Kevin, 2013).
Change weak or default passwords: Databases, web applications and other management systems are normally configured with weak or default passwords which can cause security issue. Admins should use password management tools to regularly test for weak passwords and change accordingly. Also it is recommended to implement intruder lockout policy after a certain number of failed login attempts.
Maintaining written security policy: Unwritten security policy cannot be consistently implemented. Organization should maintain written security policies, review it regularly and keep it relevant. 
Policy for Mobile devices: Necessary data encryption and security policy should be there if the employees are connecting their personal devices to the company network.
USB Flash Drives: Control the computers that can read USB drives, this prevent unauthorized access by encrypting the data and securing the network.

In order to minimize your risks the best approach is to perform in-depth vulnerability scans and analysis to ensure a comfortable level of security. It is always helpful to find weaknesses in the network first so you can do something about them.
&nbsp;
References:
Kevin, Beaver (JULY 31, 2013). Top 5 Common Network Security Vulnerabilities that Are Often Overlooked [Online]. Available from http://www.acunetix.com/blog/articles/the-top-5-network-security-vulnerabilitie (Accessed: 11 October 2014).
Vulnerabilities, Threats, and Attacks [Online]. Available from http://ptgmedia.pearsoncmg.com/ images/1587131625/samplechapter/1587131625content.pdf (Accessed: 11 October 2014) 
TED Video [Online]. Available from: http://www.ted.com/talks/avi_rubin_all_your_devices_can_be_hacked?language=en (Accessed: 8 October 2014)
	              
	              Hi Numan,
Ah, configuration weakness! This is a whole category I overlooked when I did my study. Thanks!
br, Terry
	              
	              Hi Chris,
Thanks for the great post.
I believe an open environment and a closed environment are both security strategy choices. In the NHS as you mentioned, probably a closed environment is the right choice, as NHS is the single instance of its kind and is too sensitive. In some other businesses where a more aggressive security strategy might be chosen, it's perhaps helpful to keep the system open. "Companies should not overlook the importance of trust and loyalty," as the cited article said.
What really scared me are the Zero-Day Attacks. We don't know how many of them are at the hands of the gevernments, the militaries and the people with bad intent. They are zero-day attacks because we don't know them exist. So the closed/open environment doesn't really matter to them.
As you mentioned "the government will play a major part as they have the power." But the question is "do they have the power?" In the end of the day, does it depend on "who's the richest to buy most of the zero-day attacks"?
br, Terry
The business value of balancing openness with security to manage risk,&nbsp;Warwick Ashford,&nbsp;http://www.computerweekly.com/feature/The-business-value-of-balancing-openness-with-security-to-manage-risk
Zero-day attack. (2014, August 30). In&nbsp;Wikipedia, The Free Encyclopedia. Retrieved 00:40, October 12, 2014, from&nbsp;http://en.wikipedia.org/w/index.php?title=Zero-day_attack&amp;oldid=623504460
	              
	              Hi Everyone,
In quite a few notable instances, hackers have been subsequently employed by the very companies whose systems they hacked, to act as security consultants.
What are the pros and cons of hiring such individuals for this role?

Anthony

	              
	              
Hi Anthony,
&nbsp;
I’d start by defining who or what a hacker is: 

What’s a Hacker?

A hacker, as we all know by now, is someone who utilizes his understanding of computers and software coding to bypass security layers. One who is An Out-of-the-box thinker. One who bends computers to his/her will.

&nbsp;What’s with the hats?

§&nbsp; Black

§&nbsp; White

§&nbsp; Gray
 


White-hat-hackers these are ethical hackers who use their hacking skills for the purpose of improving security. Legal organizations and Governments hire them in order to discover or solve vulnerabilities and exploits.
Grey-hat-hackers these are a mixed bunch and it is hard to understand what their motives are. They prefer to remain anonymous. They use their hacking skills for legal or illegal purposes, but never for personal gain. In most cases, they exist to share information and to accomplish something specific that is known only to them.
Black-hat-hackers these are stereotypical illegal crackers who utilize their hacking skills solely for personal gain, either in monetary or non-monetary terms. Any criminal activities related to hacked networks can be attributed to black-hat hackers. They make networks unusable for others and attempt to destroy or steal data for selfish motives.


Pros

In most companies, we all know that the IT staffs have acceptable level of security knowledge, but they must focus most of their attention on the day to day responsibilities of keeping the network up and running. But hiring a reformed hacker as security consultant is a plus because he focuses almost solely on security and consequently has a level of security knowledge that goes far beyond that of most other IT professionals, having adequate preventative measures in place to prevent security breaches.

Cons

In all companies, by far the major problem in hiring a hacker is “trust issues”. In the company I work for, the main premise of security is deciding who they trust and then declining access to every other person.

Now in the case of hiring a former hacker as a security consultant in your company, this simply means that you basically trust the sanctity of your network to a former criminal. 

Let’s talk about this from Home point of view, if you think about it, that’s a lot like letting someone who was convicted of burglary stay in your home when you aren’t there. If you are concerned with your network’s security, it sounds crazy to trust it to a criminal.

Conclusion

Hiring former hackers to act as security consultants to evaluate your security is worthwhile. At the same time though, if you are going to hire a former hacker (or any security consultant) there are steps to be taken in other to prevent your company’s security from being exploited or victimized by a security consultant.


Don’t completely outsource your security needs. Doing this will cost your company a huge amount of cash and is unlikely to make your network any more secure than if you just had your security evaluated by a consultant a few times a year.
Anything not worth giving to a security consultant should be kept secret. For example, an Administrative password. Since you are paying the consultant to look for holes in your network. If major security holes exist, the hacker might be able to get administrative access on their own, but you shouldn’t just hand it to them.
It is advisable to use a variety of consulting firms, and let them know that you will not be using them exclusively. The reason being that different consultants have different skill sets and there is possibility that they may not catch same security problems. This doesn’t mean that the consultant who missed the problem is incompetent. It just means that the two consultants have different skill sets.


Best Regards, Babatunde 


References

Rahul .T (2012) How to tell the difference between White-Hat, Grey-Hat and Black-Hat hackers [online] Available from: http://blogs.quickheal.com/wp/how-to-tell-the-difference-between-white-hat-grey-hat-and-black-hat-hackers/ (Accessed 12 October 2014) 

Brien .P (2005) Hiring Hackers as Security Consultants [online] Available from: http://www.windowsecurity.com/articles-tutorials/misc_network_security/Hackers-Security-Consultants.html (Accessed 12 October 2014)

Daintry .D (2000) (Computerworld) Security: Hiring Hackers [online] Available from: http://www.computerworld.com.au/article/97949/security_hiring_hackers/ (Accessed 12 October 2014)&nbsp; 

	              
	              
Week 6 DQ - Network systems security risks

A network vulnerability is a weakness or “soft spot” on a network or individual devices that make up the network while a threat is is a person or event that exploits that weakness.There are typically three classifications of these vulnerabilities which are; technology, security policy and configuration weaknesses.

Technology vulnerabilities and associated threats include:

TCP/IP weaknesses such as HTTP,FTP and ICMP which are inherently insecure along with any other TCP based protocols like SMTP. Because&nbsp; the TCP/IP suite is badly designed it is easy to spoof a packet i.e. fake origination of packet such that the&nbsp; end-user believes it is from a trusted host. The threat here is that IP spoofing might then be used to gain access into a system as a trusted host or to cause a Denial Of Service attack whereby an attacker continually sends bogus requests, failure messages or any other commands to a network or targeted Access Point which may result in the network crushing or&nbsp; legitimate users not having access to their network.
Operating system vulnerabilities- these are at OS level, on OSs such as Windows , UNIX, Macintosh, Linux, each&nbsp; of these their own unique security issues which need&nbsp; to be addressed. For example failure to update an OS on a web server e.g on Windows is a weakness which can result in loss of data, prolonged site downtime, time required to rebuild server after it has been compromised.&nbsp;
Network equipment vulnerabilities- these weaknesses are at network equipment/device level, on devices such as routers, switchers etc. The weaknesses exposed by these devices include lack of authentication, firewall holes, routing protocols and basic password protection. There is also the risk of unauthorised physical access and or vandalism particularly for off site devices. Attackers can also use communication equipment to launch future attacks


Configuration vulnerabilities and associated threats include:

unsecured user accounts-the weakness is exploited by the insecure&nbsp; transmission of sensitive user account information such as usernames and passwords across the network&nbsp; that is then exposed to snoopers.
System passwords that are easy to guess which may result in elevation of privilege whereby a user with limited privilege can assume the identity of a user with certain privileges e.g an admin and gain access to a system.
Security holes that are enabled by unsecured product default settings&nbsp;
internet services that are not properly configured such that they are prone to attacks from hostile JavaScript when accessing untrusted sites.&nbsp;
Network equipment that is not configured properly also causes problems, for instance remote-access control, lack of encryption, frequently open ports on a switch which could then allow unauthorised equipment to be mounted on the network.



Security vulnerabilities&nbsp; and associated threats include:

The lack of a written security policy, if it is not written it cannot be enforced or applied.
Passwords that can be easily cracked or the use of default passwords can allow unauthorised users onto the network, for instance access to a company’s database
Lack of a disaster recovery plan results in confusion and chaos in the advent of an attack
When software and hardware changes do not follow policy this can allow the download and installation of harmful software which could degrade a system’s performance
Lack of adequate monitoring and auditing&nbsp; which result in unauthorised users to continue to have access to the system e.g. former employees thus waisting company resources or masquerading as other users, or accidental data disclosure. Another potential threat culminating from lack of adequate auditing is&nbsp; repudiation where users both legitimate and non-legitimate may deny ever performing a specific action or transaction. 


The above mentioned vulnerabilities and threats can be reduced by:

Use of stronger authentication for both users and devices
Encryption of sensitive user data e.g passwords and credit card information
Using&nbsp; Secure Socket layer (SSL) to protect authentication cookies
Using data hashing and signing
Using stronger authorisation to secure Wireless Access points by eliminating rogue access points, authenticating all devices plugged onto the network by using 802.1x to authenticate and ensuring that all authorised access points are properly configured.
securing communication links by using protocols that are tamper resistant and provide message integrity and confidentiality
creating secure audit trails and digital signatures to ensure non-repudiation
Using techniques to throttle bandwidth and resources to prevent denial of service attacks
Using a firewall, install antivirus&nbsp; and anti spy software and keep them up to date
Ensuring that identifier broadcasting is turned off to devices&nbsp;
Changing the default settings e.g. of your router as hackers tend to know the default IDs



References
PCWorld [Online] http://www.pcworld.com/article/2010278/10-common-mobile-security-problems-to-attack.html accessed 10 October 2014
[online]http://www.itl.nist.gov/lab/bulletns/bltnmar03.htm accessed 12 October 2014
[online]http://www.acunetix.com/blog/articles/the-top-5-network-security-vulnerabilities/accessed 12 October 2014
International Journal OfMultimedia [online]http://www.sersc.org/journals/IJMUE/vol3_no3_2008/8.pdfaccessed 12 October 2014
http://www.governmentsecurity.org/forum/topic/1753-tcpip-vulnerabilities-and-weaknesses/ accessed 12 October 2014


	              
	              Hi Terry,
Great post. It got me thinking, since I am a tester by profession I'd like to think I'm a welcome hacker:-)
It also reminded me of a famous qoute by the late Nelson Mandela "Know your enemy and learn about his favorite sport." -Nelson Mandela. &nbsp; Seemingly unrelated to this but I cant help but think it is, because I think in order to protect ourselves and systems we have to&nbsp;understand and speak the opponent’s language and to formulate strategies and tactics to win.
Great post Terry, quite insightful.
Best regards,
Belinda
	              
	              Security and Networking
&nbsp;
To understand the security vulnerabilities in networking and identify methods we need to model communication operations. The standard model for communication systems is the Open Systems Interconnection (OSI) by the International Organization for Standardization (ISO) under the standard ISO/IEC 7498-1:1994. According to this standard, there are seven layers of networking (ISO. no date). Douligeris and Dimitrios (2007) identified seven fundamental network attacks or threats based on the OSI model. Figure 1 shows the OSI model and relative attacks as classified by Douligeris and Dimitrios (Douligeris, Dimitrios 2007).


Figure 1 Type of network attacks
One thing that we can notice right away is that all attacks can involve any of the seven OSI layers, Application-Level and Logon attacks are in different colors because they mainly focus on the Application Layer.
As per Douligeris and Dimitrios (2007), we can summarize the attacks and explain why they are threats to security:
The Application-Level attacks capitalize on the application layer vulnerabilities. Most of these attacks are intrusion types; most commonly known as malicious software or Malware such as (but not limited to) viruses, Worms, Trojan Horses, etc.
Why a threat? Malware is a refined set of software used for criminal activity, and sometimes associated with terrorist organizations. It impacts the economy by generating prevention and remediation costs for individuals and organizations (van Eeten, Bauer 2008).
The Logon Abuse attacks focus on legitimate users bypassing authentication and access control mechanisms thus allowing them to access higher privileges in a system.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Why a threat? Users obtaining higher privileges can leverage such access, even if accidentally, to compromise entire systems or networks, even an entire organization system.
The Resource-based attacks leverage resources outside of the system being targeted, to overwhelm the target. The most common attack in this category is the denial of service (DoS) attack.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Why a threat? These attacks can bring down resources on networks making services unavailable. As our society heavily rely on networked services, these attacks can easily compromise critical services.
The Intrusion attacks aim at obtaining, though vulnerabilities, unauthorized access to a networked system.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Why a threat? Intruder’s leverage compromised systems for other activities, such the propagation of malware. Intruders on a system are no different than intruders in a physical home: they can access information, data and money and use that information illegally.&nbsp;
The Eavesdropping (or sniffing) attacks catch without approval network communications, disclose it or use it for other purposes. While this can happen at any OSI layer, perhaps the most known form of this is Spyware.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Why a threat? Eavesdroppers can listen to sensitive communication, from banking transactions to police communications. Information obtained this way can be used for illicit purposes.
The Spoofing (or sniffing) attacks aim to impersonate the identity of a person, program or message to trick the other side. Perhaps the most known attack of this type is email phishing.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Why a threat? These attacks can trick people into giving out money, or transmit illegal information. Identity theft can result from such attacks.
The Hijacking attacks, as hinted by their name are attempts to seize control over of a system by using legitimate connections.&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Why a threat? These type of attacks allow hijackers to access resources in the network and impersonate or reroute networking for illicit purposes.
Prevention Methods
 Several software solutions exist to combat network attacks such as (but not limited to): Antivirus, anti-Spyware, Firewalls, anti-Spam. Another way to protect systems is via encryption, which purpose is to encrypt communication in a way that only the intended recipient can decrypt the message. Still another way is government and laws that enforce legislation against security breaches (Bookshear 2011).
However, I believe there are two other ways to combat network attacks:

Changing culture and educating people

People play an important role in overall computing security. If someone falls for any network attack, at least at the application layer, is partially their fault too. If someone opens an email with a funny attachment that contains malware, we could question why a person could not identify this. It can only be only because they are careless, or they are not informed on how to recognize this threat. This is valid for both individuals and programmers as well as organizations such as Internet Service Providers (ISP), Cloud providers or software vendors.


Leveraging the same people that execute these attacks: hackers.

As described Elazari (Elazari 2014) in her TED Talk, not all hackers are “bad”. By leveraging hacking skills companies and institutions, can fix their software or hardware and improve security overall. Our perception of a hacker will need to change to accept this behavior and encourage it in a certain context, and with limits.



Overall security issues in networking are crucial for our day to day work even if we do not realize it. What transpires from my search about network security is that there is plenty of research done to identify threads and tied them with economic issues or criminal behavior.&nbsp; However, there is little serious research about the physiological effects of network attacks victims, but there is certainly some consequences to individuals and families. As the recent Snapchat hacking reveals (Cook 2014), there are underage nude pictures that become publicly available. One can only wonder to what degree there can be damage for those kids and their families. With social networking having its roots in deep physiological and social principles, I wonder if any psychological damage done by security threats can have any impact on our globalized, social networked world.
&nbsp;
References
Bookshear, J. G. (2011) '4.5 Security' in Computer Science: An Overview, 11th edn, Addison-Wesley, pp. 173-181.
Cook, J. 2014, Hackers Access At Least 100,000 Snapchat Photos And Prepare To Leak Them, Including Underage Nude Pictures, Business Insider.
Douligeris, C. &amp; Dimitrios, N.S. (eds) 2007, Network security: current status and future directions, 1st edn, Wiley-IEEE Press.
K. Elazari. (2014) Hackers: the Internet's immune system. [Video conference]. Available at: http://www.ted.com/talks/avi_rubin_all_your_devices_can_be_hacked. (Accessed: 12 October 2014).
ISO. (no date) Freely Available Standards: ISO/IEC 7498-1:1994. Available at: http://standards.iso.org/ittf/PubliclyAvailableStandards/index.html (Accessed: 11 October 2014).
van Eeten, M.J. &amp; Bauer, M.J. (2008) 'Economics of Malware: Security Decisions, Incentives and Externalities', OECD Science, Technology and Industry Working Papers 2008(01), pp. 2014 doi: 10.1787/18151965.
&nbsp;Weimann, G. (2004) Cyberterrorism.&nbsp;How Real Is the Threat?, UNITED STATES INSTITUTE OF PEACE. Available at: http://www.usip.org/sites/default/files/sr119.pdf (Accessed: 11 October 2014). 

	              
	              Hi Terry and Craig,
What the rest of us (that do not have the technical skills or at not hackers) should be doing is mainly one thing: learn. There is way too many people that are handled technology without any instruction or guidance, just because it is very easy to use. But that leaves a huge gap on the technology usage. That is like giving a 4 year old kid an automatic weapon and believe he fully understands what he can do, or cannot do with it. Just take a look at a smart phone and how many settings are there, we, as technically savvy, can understand it. But what about someone else, what about a kid or an 80 year old lady? Yet, those people can easily use a smart phone.
For the technically savvy, like us we can do more. At a minimum try to make the life of a hacker harder, start by encrypting our hard disks (Bitlocker for Windows, FireValut for Mac OS X), use cloud providers that allow you to encrypt your data, or use 3rd party encryption software before moving stuff into the cloud. Get a password management tool with two factor authentication (like Password Safe with yubico) and start using crazy, long, and impossible to guess passwords for all your accounts. And enable two factor authentication for all the accounts that you have, if possible. If not possible perhaps is good to question ourselves: is there a better alternative?
Because we know the technical side of IT, we should be able to identify safe sites versus unsafe ones, same for emails and attachments. I personally use my machines to browse only safe sites. If, for some reason, I must go to an unsafe or unknown site, I use a sand box (virtual machine) that I rebuild once a month.
But perhaps, the best we can do is actually teach people what we know. I do not think that the above is terribly complicated, but we can teach family and friends how to do the same thing, once they learn we would have at least taken steps to make the life of a hacker harder, and perhaps stop certain threats altogether. The world is always saved one man at a time, so perhaps security can be improved one man at a time.
Best Regards,
Augusto.

	              
	              Hi Belinda,
That's exactly what's in my mind while I was writing my post. A tester is a hacker by nature:-)
I'm a typical developer, and I like to focus on creating a solution to the problem. When I go to work, it's just from A. home to B. office. But when my tester friends go to work, they know the shop outside was just renovated and switched to a new business. It sometimes requires quite different kind of thinking style to be a tester, and also to be a hacker, I believe.
Thanks for the Mandela quote. Very insightful, and yes, seemingly unrelated. I guess that's probably one of the thinking skills shared by a tester and a hacker --- to relate things, which is only a myth to many developers, and they may never manage to understand this romantic thinking style. Thus will lead to the underestimation of the value brought by a tester/hacker, and will then lead to the vulnerabilities of the system.
This is just like the Yin and Yang in the development of a computer system.
br, Terry YIN
	              
	              Week 6 Discussion Question - Network systems security risks


Babatunde KOLAWOLE 

&nbsp;

Once a computer is connected to a network, it becomes subject to unauthorized access and vandalism.


In computer security, vulnerability is a weakness which allows an attacker to reduce a system's information assurance [a]. Vulnerabilities may be software flaws or configuration mistakes.

Network security definition in a layman’s language is securing your web site and related applications. Talking about securing the networks, because of the sensitive data they usually give access to, which are targeted part of an organization.

I will summarize a number of security vulnerabilities that are often overlooked in a network environment and provide brief explanations to why they are threats and then recommended methods to prevent exploitations of these risks and attacks associated with networked systems.

&nbsp;

Missing patches
When patches are missing on a server, it permits an unauthenticated command prompt or other backdoor path/holes into the web environment. Absolute care needs to be taken when applying patches to the servers.


Methods to prevent exploitations: It is best to use the latest security patches in updating your operating system and all software running on it. This should be a network security best practice to be carried out at any given time. Because criminal hackers take advantage and exploit un-patched systems.

&nbsp;

Weak or default passwords Many web applications, content management systems, and even database servers today are still configured with weak or default passwords. Although, weak or default password issue shouldn’t be part of network security vulnerability but in the case where some companies are not fully aware; hackers generally use file inclusion/Cross-site Scripting or SQL injection [b][c]. Therefore, measures should be taken because if these passwords are weak, the file systems and database could be accessed directly.


Methods to prevent exploitations: it is a best practice to implement intruder lockout after a defined number of failed login attempt. Also change and test for weak passwords regularly and consider using a password management tool. For advice on choosing good passwords, check [d]

&nbsp;

Misconfigured firewall rulebases Companies shouldn’t assume everything is ok in the firewall all because it’s been working fine. It would be a very dangerous assumption because if one digs into a firewall rulebase that has never been analyzed, surely it will inevitably turn up serious configuration weaknesses that will allow unauthorized access into the web environment.

Methods to prevent exploitations: it is highly recommended to start with your company’s security policy; regularly review while keeping it relevant your company’s firewall rulebase since firewall rulebase is one of its technical implementation of security policy.

&nbsp;

Mobile devices Phones, tablets, and unencrypted laptops are devices that present risks to web security. Let’s look into the area of VPN connections, cached passwords in web browsers, emails containing sensitive login information that that may have been sent by you or anyone else responsible for managing your web environment must have been stored one way or the other on mobile devices. This alone gives physical access to hackers if they by chance have access to your device.


Methods to prevent exploitations: data encryption should be a mandatory part of your security policy. Also, transfuse clear data management rules for all employees because so many employees connect their private devices into the corporate network therefore, Port security should be implemented to limit connectivity to hardware interfaces.

USB Flash Drives
The dangers of these innocent-looking portable devices have been known for long enough. But still, all that Edward Snowden [e] reportedly needed to walk away from the National Security Agency building with a cache of national secrets was a USB flash drive. Unauthorized network access through physical access to network equipment includes the lack of physical access control to the equipment. It also includes the lack of security configurations functions that limit functionality even if physical access is obtained.

Methods to prevent exploitations: security measures should be taken on who can use USB devices and in what place they are using it. Restrictions means should be on computers that read Flash devices to help prevent unauthorized accesses (encrypting the data as soon as it hits the device) would be a major restriction.

In conclusion, Accessing web environments from either inside or outside company’s computer network, these vulnerabilities are often overlooked and are likely to put the environment at risk. Performing in-depth web vulnerability scans is very good; also it is best to properly review every other thing that touches the company’s web environment.

In most companies, it is known that the IT staffs have acceptable level of security knowledge, but they must focus most of their attention on the day to day responsibilities of keeping the network up and running. Therefore it will be ok for a company to hire a reformed hacker (another smart approach to minimize company’s risks) as security consultant because a reformed hacker focuses almost solely on security and consequently has a level of security knowledge that goes far beyond that of most other IT professionals, having adequate preventative measures in place to prevent security breaches. [f]

Hiring former hackers to act as security consultants to evaluate company’s security is worthwhile and at the same time though.&nbsp; If need be to hire a former hacker (or any security consultant) there are steps to be taken in other to prevent company’s security from being exploited or victimized by a security consultant.

Below are some of them;


Don’t completely outsource your security needs. Doing this will cost your company a huge amount of cash and is unlikely to make your network any more secure than if you just had your security evaluated by a consultant a few times a year.
Anything not worth giving to a security consultant should be kept secret. For example, an Administrative password. Since you are paying the consultant to look for holes in your network. If major security holes exist, the hacker might be able to get administrative access on their own, but you shouldn’t just hand it to them.
It is advisable to use a variety of consulting firms, and let them know that you will not be using them exclusively. The reason being that different consultants have different skill sets and there is possibility that they may not catch same security problems. This doesn’t mean that the consultant who missed the problem is incompetent. It just means that the two consultants have different skill sets.


&nbsp;

References

Laureate Online Education. (2014). Computer Structures, Lecture Notes Week 6: Computer Networks [Online]. Available from: https://elearning.uol.ohecampus.com/bbcswebdav/institution/ukl1/201480_september/ms_ckit/ckit_501/readings/ukl1_ckit_501_week06_lecturenotes.pdf (Accessed 10 October 2014)


Bookshear, J. G. (2012) Computer Science: an Overview, ‘Security' 11th ed. Boston Massachusetts: Addison Wesley, pp. 173-181.

Kevin B. (2013) Common Network Security Vulnerabilities that Are Often Overlooked [online] http://www.acunetix.com/blog/articles/the-top-5-network-security-vulnerabilities/ (Accessed 10 October 2014)


[a] Wikipedia, the free encyclopedia Vulnerability (computing) [online] Available from:&nbsp; http://en.wikipedia.org/wiki/Vulnerability_%28computing%29 (Accessed 10 October 2014)


[b] Acunectic, SQL Injection: What is it? [online] http://www.acunetix.com/websitesecurity/sql-injection/ (Accessed 10 October 2014) 


[c] Acunectic, Cross-site Scripting (XSS) Attack What is Cross-site Scripting? [online] http://www.acunetix.com/websitesecurity/cross-site-scripting/ (Accessed 10 October 2014)


[d] Microsoft Safety &amp; Security Center Create strong passwords [online] Accessed from: https://www.microsoft.com/security/pc-security/password-checker.aspx (Accessed 10 October 2014)


[e] Wikipedia, the free encyclopedia Edward Snowden [online] Available from: http://en.wikipedia.org/wiki/Edward_Snowden (Accessed 10 October 2014)


[f] Brien P. (2005) Hiring Hackers as Security Consultants [online] Available from: http://www.windowsecurity.com/articles-tutorials/misc_network_security/Hackers-Security-Consultants.html (Accessed 10 October 2014)
	              
	              Hi Terry,
You are welcome dear! I would also like to thank you for your contribution and sharing knowledge with us. I really enjoy reading your posts and it always helped me to understand the given discussion topics.
Best wishes,,,Numan
	              
	              Hackers are security researcher having the ability to think Out-of-the-box and find the security holes in the system. When they find any security weakness in the system they never let it go easily, they either exploit it or take positive measures to close the security holes in the system (Keren, 2014).

Companies often think to hire hackers when they need experienced security professionals with Innovation, forward thinking. People who know how to defend network systems but there are many pros and cons of hiring such individuals for this role.

Pros:

•&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Bright, creative people

•&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Has a level of security knowledge that goes far beyond that of most other IT professionals

•&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Many not motivated by money

•&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Highly motivated to learn

•&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Very productive

•&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; In touch with latest trends

•&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Impress your customers

Cons:

•&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; May be hard to focus them on company’s goals

•&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; The youngsters require supervision

•&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; That damn superior attitude

•&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Tend to see things in black &amp; white

•&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; May pursue dead-end directions too long

•&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Politics

•&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Not easy to fire Hackers

•&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Trustworthiness

Conclusion:

If you are going to hire a former hacker then you need to take some steps to prevent yourself from getting ripped off and to prevent your company’s security from being exploited.

&nbsp;

References:

Keren, Elazari. (2014). Hackers: the Internet's immune system [Online]. TED Video. Available from: http://www.ted.com/playlists/10/who_are_the_hackers (Accessed 12 October 2014).

Brien Posey. (2005). Hiring Hackers As Security Consultants [Online]. http://www.windowsecurity.com/articles-tutorials/misc_network_security/Hackers-Security-Consultants.html (Accessed 12 October 2014).

Scott, Blake. (2014). The&nbsp;Pros and Cons of Hiring Hackers. [Online]. www.blackhat.com/presentations/bh-usa-00/Scott.../Scott-Blake.ppt (Accessed 12 October 2014).
	              
	               Week 6 Discussion Question – Network Systems Security Risks    In the recent years as the cyber attackers have got wiser and smarter &nbsp;we have seen an increased cyber attacks.  There are different way computers are attached, most common way is through the internet via use of the malicious software (also known as malware). Examples of these are viruses, worms, Trojan horses, and spywares. (Brookshear 174)    Computer Virus       A software virus is a malware program.&nbsp; When the program is executed it replicates itself to other computers.&nbsp; The successful delivery of one computer (“host”) to another is referred to as ‘infected”.&nbsp; The virus usually preforms harmful and un warranted activities on the computers, with such results as corrupting data, deleting data, making operations in the computer difficult and displaying annoying messages.&nbsp;    Computer Worm       Computer worm is standalone program that moves through the computer networks. It will reside on the infected computer and attempt to forward copies of itself to other computers e.g. using email to sending attachment to contact on the host computer. A worm typical mission is to cause some harm and vandalize a computer and or network e.g. corrupt data, hard disks etc.    Trojan horse       A Trojan horse is a program that can enter computers disguised as a safe program e.g. games and useful tools. Some Trojan horse start their malicious activities and other stay dormant and get triggered by a specific event or selected date. &nbsp;    Spyware       Spyware is another example of&nbsp; malicious software. Its primary objective is to gather information (and activities) about the computer and send to another entity without permission of the user. Some companies use    Phishing       Phishing is the attempt to obtain information by requesting it.&nbsp; Cyber attackers fish a ‘line’ hoping for some to “take the bait.” &nbsp;Phishing attacks take place by email,, the attacker posing as a reputable &nbsp;institution&nbsp; and requests a victim legitimate information, the victim provides the information assuming they are providing requested information to &nbsp;reputable &nbsp;institution.    Denial of service (DoS)       Denial of service (DoS) is a term when an attack to attempt to make the computers or Network unavailable. Examples are overwhelming a computer with messages or disrupts a corporations business and halt its day to day business.  Receiving unwanted messages via email is called Spam. Spam is a vehicle for phishing attacks.  &nbsp;    Programming Bug       Programming bugs can leave software vulnerable, which could open up doors for cyber attacker easy access to be exploited  &nbsp;  &nbsp;    Common network security factors    Three common factors emerges when dealing with network security, these are vulnerability, threat, and attack.  &nbsp;    Vulnerabilities       Cyber attackers know many networks or computers have some degree of vulnerability or weakness that they can exploit. Cyber attackers work hard, sometimes around the clock, searching for unsecured networks or computers to exploit. Some examples of devices are switches, routers, bridges, laptops desktop PC and&nbsp; servers. They use sophisticated tools and programs to conducts these threats. The primary vulnerabilities or weaknesses are: (Orbit-Computer)  System Configuration: these include TCP/IP protocol, Operating system and network equipment weaknesses.&nbsp; Configuration weaknesses: these include bad configuration of security software, firewall devices or network. &nbsp;Unsecured user accounts information or passwords are also examples of configuration weakness. Security policy weaknesses: All corporations have a security policy that oversee how their network and information should be utilized. If users do not follow these policies, the corporations will be at risk. This highlights the security policy by:    To Inform users on the corporate policies and standards and its importance to protecting the corporations technology and information assets   Identify the process to follow in an event the polices are not followed   Identify process to procure, baseline, configure and audit systems per policy    &nbsp;    &nbsp;&nbsp;Steps to prevent Cyber Security threats    Eliminating&nbsp; cyber security &nbsp;breaches may be an impossible task -- but measures need to be prevent attackers access, here are some examples of presentation measures corporations are taking; (Techrepublic)    Change default passwords    All computer devices come with default passwords, ensure all these passwords are changes from default settings    Don’t reuse passwords    Don’t allow any one password be used more than once    Disable/Delete &nbsp;user accounts when an employee leaves    Terminate all employee credentials as soon as they leave the corporations    Examine Security logs    Establish a baseline, Capture your normal operating state. This will enable a tool to identify a change in state.  Review and monitor security logs. A lot can be identified by alerts in the logs    Conduct regular network scans &nbsp;    Using the baseline data to identify anomalies.    Monitor the outgoing network Traffic    Using the &nbsp;baseline data , one can detect outbound &nbsp;to identify anomalies such as traffic deviations.    Patch and update on a regular basis    Keep the OS and application software up to date with the latest software updates. This is a good way to prevent open perimeter vulnerabilities.    Implement a security Plan    This is a vital plan for all corporations any size. This will ensure everyone is working off of the same playbook. Also when an organization is in a panic mode, the plan will provide procedures and steps to be followed. The security needs to be developed custom to the corporation.    Socialize the awareness for information security internally    Provide appropriate training. This will enable users well educated and informed in Internet security.    Obtain management buy-in &nbsp;    The hardest endeavor but the most important. It will be crucial &nbsp;to have the management on board to provide the required funding for required software, hardware and security policies implementations.  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  References  Orbit-Computer URL http://www.orbit-computer-solutions.com/Common-Network-Security-Threats.php  Brookshear, J. G. Computer Science: An Overview XML Vital Source ebook for Laureate Education, 11th Edition. Pearson Learning Solutions. VitalBook file.  &nbsp; Techrepublic URL http://www.techrepublic.com/blog/10-things/-10-ways-to-avoid-it-security-breaches/ 
	              
	              hello Terry and Craig,
A good point you raise there about the OSI model but what hit me the most was after watching the TED video. I started asking myself
a lot of questions as - are we really safe? what she pointed out in the video of what hackers do to personal PC, websites, the car thing etc.
Has got me thinking. we really need to safe guard ourselves because whether good or bad, hackers can harm us. Imagine someone taking hold of your car while driving, he or she could do anything with you.
Regards
Martins&nbsp;
	              
	              &nbsp;
Introduction
&nbsp;
Accompanied with the viruses was found during the 1980s, the technologies of information security and palliative care were importance when the principles of circumference security, such as via screening routers and firewalls, and malware protection, mainly in the shape of the early technologies of antivirus, turned into can be used. Today, since network security vulnerabilities formed during their capability and sophistication, other factors devised to guard organizational or business systems and networks turned into able to be applied to respond such issues comprising phishing protection, file screening, email checks, black-lists and white-lists for URLs and IP, and so on. 
Five common network security vulnerabilities and its prevention
The security of computer network is as critical as protecting the web server and the concerning applications. Due to the sensitive information is often transmission across the networks, which are one of the majorly faced the public interfaces of an enterprise. I will look into the five common network security vulnerabilities here and provide some suggestions to prevent those threats.
1.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Default or weak passwords policy - Originally, passwords should not be a section of the network security vulnerabilities, but in fact, a lot of database servers, content management systems and even web applications are still set up the configuration with default or weak policy passwords. It is the easiest way to invade an organization networks through cracking the passwords, the less complexity of password policy means the less networks security. 
prevention: To avoid this threat is to configure the proper Password Policy, such as The password does not contain the account name of the user, at least eight characters long and contains particular characters required (Microsoft (2014)). What is more, utilizing the password management tool to compel users to change the password regularly and achieve intruder account lockout after three times of failed login attempts. 
2.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Misconfigured firewall rule-bases - It is an illusion that the network is safety because the firewall is working fine. This assumptions is the biggest dangers that looking into the rule-base of firewall which has never been dissected will certainly discover severe configuration vulnerabilities that enable for non-permitted invade into the network circumstance. It maybe invades directly sometimes while it invades indirectly via other network sections such as the network Wi-Fi – parts. 
Prevention: Set up the security policy that reviews the present state of affairs and predicted business demands. In the end, the rule-bases of firewall is the technological performance of the policy of network security. Re-examine it constantly and remain it corresponding. Open Web Application Security Project (OWASP) offers certain benefit guidelines on founding operational security guides.
&nbsp;
3.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Lacking patches - The lacking patches on a server usually attract the hackers or the rogue insider that enables a backdoor path or an authentication Commands exposing during internet circumstance. It is undoubtedly that the organization must attentive to apply patches to servers, but aborting to update the patches will provide the attackers convenience. 
Prevention: Abide by the network security best behaviors by applying the OS and any applications or other software programs processing on it with the most updated security patches. Too much issues arise due to criminal attackers make use and utilize unpatched devices.
4.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; USB Drives (flash memory) - The threats of these "innocent" portable equipments were one of the serious vulnerabilities to make the sensitive data leaking that the causes of a lot of information leakage case we have been known are through the USB drivers as well as the common approaches to make the system get infected virus via inside the firewall. 
Prevention: Restrict personal users adopt these type of storage devices to access the security networks, in fact, most of enterprise, such as IBM, explicitly prohibit using USB drives during the company networks, even disable the USB functions at all workstations and servers.
5.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Mobile devices - Unencrypted laptops, tablets and Mobile phones form certain of the biggest security vulnerabilities to company networks. Imagine that all the VPN links, the cache of passwords in the browsers, as well as emails including private access data that saved on the mobile devices. The using of unsecured WI-FI through mobile devices is the major risks. 
Prevention: Setup obvious and clear information management guidelines for all staffs and enable compulsory encryption of data being the section of the company security policy. This is turning into even more critical with staffs linking whose personal devices to the company networks.
&nbsp;Conclusion
Whether access the network since outside or inside, these common security vulnerabilities are probably exposing the company networks at the threats. The wise way to lower the threats is to go through deeply network manual analysis and vulnerability scans and make sure the network settings has been correctly re-examined. 
References
Derek Manky (2010); Top 10 vulnerabilities inside the network; available on: (http://www.networkworld.com/article/2193965/tech-primers/top-10-vulnerabilities-inside-the-network.html) (Accessed on: 12-Oct-2014)
&nbsp;
Oludele Awodele, Ernest Enyinnaya Onuiri, and Samuel O. Okol(2012); Vulnerabilities in Network Infrastructures and Prevention/Containment Measures; Available on: (http://proceedings.informingscience.org/InSITE2012/InSITE12p053-067Awodele0012.pdf) (Accessed on: 12-Oct-2014)
Matt Curtin (1997); Introduction to Network Security; Available on: (http://www.interhack.net/pubs/network-security/) (Accessed on: 12-Oct-2014)
Microsoft (2014); Password Policy; Available on: (http://msdn.microsoft.com/en-us/library/ms161959.aspx) (Accessed on: 12-Oct-2014)
&nbsp;
Trent Nelson (2011); Common Cybersecurity Vulnerabilities in Industrial Control Systems; Available on :( https://ics-cert.us-cert.gov/sites/default/files/documents/DHS_Common_Cybersecurity_Vulnerabilities_ICS_2010.pdf) (Accessed on: 12-Oct-2014)
&nbsp;
U.S. Department of Energy Office of Electricity Delivery and Energy Reliability (2008); Common Cyber Security Vulnerabilities Observed in Control System Assessments by the INL NSTB Program; Available on: (http://www.inl.gov/scada/publications/d/inl_nstb_common_vulnerabilities.pdf) (Accessed on: 12-Oct-2014)
	              
	              
DQ1, Week 6, Network systems security risks

1. Introduction
This assignment will go through some of the most common threats to network security, and I will try to explain the issues in a down-to-earth way.
As Information and communication technology (ICT) evolves, the world gets ever more interconnected. Every day we get new technologies, possibilities and complexity. And “naturally” we also get ever more security risks either just because of the complexity or because there are people who are in the business of exploiting these security risks for different purposes, be it curiosity or evil in forms of damage, destruction, or financial benefit. Hence we have a long battle ahead of us trying to bring good to people at fighting those trying to misuse it.
2. Thesis
No matter how much we fight those trying to exploit the vulnerabilities, we will always be a step behind.
3. Security vulnerabilities commonly associated with network systems
3.1 Analysis of the common network security vulnerabilities




Vulnerability


Summery




Hackers


Hackers in this case are the bad guys who try to circumvent IT systems to exploit possible security holes.
Quotes:

“there are two kind of companies in the world, those that know they have been hacked, and those who don’t” (Glenny, 2011)
“Hacking continued to be the primary cause of data breaches in 2013” and “Hacking accounted for 34 percent of data breaches in 2013” (Symantec 2014 p13).
“It’s estimated that any newly computer connected to the internet will experience a hacking attempt within 20 minutes” (Brookshear. p. 175)
“Sony said thay did 150 million dollars damage” (BBC 2014).





Malware


Malware include viruses, trojan horses, worms, adware etc.
Some statistics according to Cisco (p. 38)

Trojan&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 64%
Adware&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 20%
Worm&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 8%
Virus&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4%
Downloader&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4%

400 locations were hit by malware caused by poor passwords (Roberts 2014).




Viruses and worms


This is software that gets on your machine and activates by user interaction and can cause a lot of damage, e.g. deleting data.




Worms


Worms are autonomous software which contrary to viruses can transfer and copy themselves, often making the machines work slower or even “overload an entire system” (Brookshear).




Trojan horses


Trojans get on the machine after a user action, e.g. clicking on an email attachment or running a game, and they can start working immediately or wait till something triggers them.




Spam


Spamming will flood the network and in worst case make it crash.
“75% of all emails are unwanted” (Microsoft p. 84)




Passwords


Passwords get to be vulnerabilities because of many reasons, e.g. poor passwords, lack of complexity, none or to long intervals between changes, and actually sharing the password with family and friends.
52% of the mobile users are store sensitive data online and are sharing their passwords with family (21%) and friends (18%). (Symantec 2014)




Phishing


This is also widely used and is typically done by sending emails to recipients disguised as someone you trust (bank etc.) asking you to send back personal information’s (username, passwords, bank accounts, etc.) for future financial theft or identity theft.




Spyware / Sniffing


This is a more anonymously kind of phishing where something is eavesdropping on traffic between computers on any network with the purpose of intercept personal information.




DoS and DDoS


Denial of Service attacks tries to flood the network with messages. DoS attacks use one machine ore one connection. DDoS uses a distributed version where multiple machines and connections are used.




* See the next paragraph for more explanations of the preventions.
3.2 Recommended preventions
3.2.1 Intangible preventions
3.2.1.1 Awareness
Creating awareness, I think, is probably one of the most important tasks, and at the same time much under estimated and/or forgotten. We should continuously inform and educate employees and people in general about what they should be aware of and what they should to mitigate the threats. And one of the key words here is the “weakest link philosophy”.
3.2.1.2 Audit
I guess everybody knows the situation “staring you blind”, i.e. you don’t see the threats right in front of you. Therefore it is always a good idea to have someone internal and especially external, to perform a regular audit.
3.2.1.3 Hire the hacker
I believe that not all hackers are “born” to be bad. Most of them are very intellectual and they are just curious, and the situations just lead them the wrong way. So I believe many of them want to work in legitimate industries (Glenny 2011).
3.2.2 Tangible preventions
3.2.2.1 Firewall
Firewalls are a must to implement, be it on business or consumer networks or machines. Firewalls can do many things, e.g.:

Block incoming traffic, hereunder blocking certain file types, blocking traffic where the sender tries to act as coming from an internal address.
Block outgoing traffic, hereunder stopping DoS attacks.
Spam filters can block unwanted incoming emails
Etc.

Adage: “An ounce of prevention is worth a pound of cure.” (Brookshear. p .175).
But firewalls don’t maintain themselves, they need updates and regular monitoring and penetration tests. The last one is a process where you hire someone outside who have good guy hackers who think like hackers, hence are more likely to find possible holes in the system.
3.2.2.2 Proxy server
Proxy servers try to separate the end client and the server, i.e. it is an intermediary between the two parts preventing the client to have direct connection with the server thus preventing the server to learn about the client. &nbsp;
3.2.2.3 Encryption
Encryption could seem as the perfect way to secure data. But even this is not secure enough. “Encryption should be a component of a security plan, but not the entirety of it.” (Olavsrud, 2014, p8).
3.3.2.4 Passwords
Password policies must be enforced, and they must force the users to change passwords regularly and with a certain complexity. This policy depends on the type of data. An example could be:

Interval:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Every 8 weeks
Complexity:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 8-10 chars, a mix of big, small, numbers, and special chars

But there is a dilemma. In practice the users are very frustrated about these password policies and systems, they are having difficulties understanding the need and don’t know how to choose a password and even harder remembering them. Thus they tend to write them down somewhere and in worst case on a note next to them or perhaps on their phones. So you might say that a stronger policy will actually result in worse security.
A combination of biometrics could help, but even here they bad guys have found their way.
Also 2-step verification could be helpful. Like the one on Gmail where, if you log on a new device you’re asked for a code send to either your phone or mail.
3.3.2.5 Updates incl. patching
All the reports that I have read on security threats mention the lack of updates being a main reason for threats, hence everybody must be urged to continuously update their system.
The reports that I have been through show that e.g. Microsoft XP and Java (91% of web exploits, Cisco p. 3) as two of the big sinners. The question is perhaps: Is it the systems or the users that are the sinners? I would say it’s the users since the systems don’t update themselves, thus it’s “fair” to say that the users only have them self to blame – or?
 
&nbsp;
4. Conclusion
It is and will be an eternal struggle to keep pace with the bad guys. Security threats have gone commercial; you can now by products that let you exploit weaknesses. And every time there is a cure, they just move on to the next threat. The annual report 2013 from the firewall producer Sophos states: “In 2014 we need to start watching not just the evolution of existing attacks, but new types emerging that we haven’t previously dealt with”. And one of the upcoming big threats the Internet of Things (IoT) with Near field communication (NFC) and GPS solutions. This is further supported by the TED video “All your devices can be hacked” (Rubin 2011).
As I mentioned in the introduction, the bad guys will always be one step ahead, that is, they will always find new ways. E.g. it is expected that the Botnets, that usually use Client/server (C/S) solutions, will change to Peer-to-peer (P2P) (Channelpartnersonline, p. 8)
Questions: Are we always running behind because of “bad” programming? Are the programmers (i.e. vendors) releasing their software to early either not tested fully or just released with “known” holes? Could it be possible to make systems better in the first place?
5. References
BBC. “How Hackers Changed The World: We Are Legion - BBC Documentary”. [Online] Available from: http://www.youtube.com/watch?v=ncuwOYBZhX0. (Accessed 12-10-2014)
Brookshear, J. Glenn, (2012). Computer Science An Overview. 11th edition. Boston Massachusetts: Pearson. Addison Wesley.
Channelpartnersonline “Top 10 Security Threat Predictions for 2014”. [Online] Available from: http://www.channelpartnersonline.com/Galleries/2013/11/Top-10-Security-Threats-for-2014.aspx?pg=8#gallery. (Accessed 12-10-2014)
Cisco. 2014. “Cisco 2014 Annual Security Report”. [Online] Available from: http://www.cisco.com/web/offer/gist_ty2_asset/Cisco_2014_ASR.pdf. (Accessed 12-10-2014)
Lecture notes.
Fortinet. “Fortinet’s FortiGuard Labs Reveals Top 10 Threat Predictions for 2014”. [Online] Available from: http://www.fortinet.com/press_releases/2013/fortiguard-labs-reveals-top-10-threat-predictions-2014.html. (Accessed 12-10-2014)
Glenny, Misha. (2011). TED. “Hire the hackers. [Online] Available from: http://www.ted.com/playlists/10/who_are_the_hackers%202m10. (Accessed 11-10-2014) 
Incapsula. “Denial of Service Attacks”. [Online] Available from: http://www.incapsula.com/ddos/ddos-attacks/denial-of-service.html. (Accessed 12-10-2014)
Microsoftt. 2014. “Microsoft Security Intelligence Report”. [Online] Available from: http://www.microsoft.com/security/sir/default.aspx. (Accessed 12-10-2014) 
Olavsrud, Thor. (2014). CIO. “10 Top Information Security Threats for the Next Two Years” [Online] Available from: http://www.cio.com/article/2368648/security0/149359-10-Top-Information-Security-Threats-for-the-Next-Two-Years.html. &nbsp;(Accessed 12-10-2014)
PCW. PC World, “The top 5 security threats to watch for in 2014 “. [Online] Available from: http://www.pcworld.com/article/2092226/the-top-5-security-threats-to-watch-for-in-2014.html. (Accessed 12-10-2014)
Roberts, Paul. (2014). The security ledger. “Report: Hacked Password Behind Compromise of 75m JPMorgan Accounts”. [Online] Available from: https://securityledger.com/2014/10/hacked_password_behind_compromise_of_75m_jpmorgan_accounts/#.VDlVd_l5OJ0 (Accessed 11-10-2014)
Roberts, Paul. (2014) The security ledger. “Dairy Queen Confirms 400 Locations Hit By Backoff Malware”. [Online] Available from: https://securityledger.com/2014/10/dairy-queen-confirms-400-locations-hit-by-backoff-malware/#.VDqwKKPUwc9­­­. (Accessed 11-10-2014)
Rubin, Avi. (2011) TED, “All your devices can be hacked”. [Online] Available from: http://www.ted.com/playlists/10/who_are_the_hackers%202m10. (Accessed 11-10-2014)
Schwartz, Matthew J. (2014) Hackers Grab 800,000 Banking Credentials”. [Online] Available from: http://www.databreachtoday.com/hackers-get-800000-banking-credentials-a-7416. (Accessed 12-10-2014)
Sophos. (2014). [Online] Available from: http://www.sophos.com/en-us/threat-center/medialibrary/PDFs/other/sophos-security-threat-report-2014.pdf. (Accessed 12-10-2014)
Symantect 2014 [Online] Available from: http://www.symantec.com/content/en/us/enterprise/other_resources/b-istr_main_report_v19_21291018.en-us.pdf. (Accessed 12-10-2014)
&nbsp;
Best wishes
Bo W. Mogensen

	              
	              Babatunade and Numan highlighted good points and pros and cons.
From my experience having managed an ethical hackers group, nothing beats having someone in your team with real life hacking experience who knows the ticks of the trade from hours of challenging hacking ! One has to weigh-in the risk of trusting an Ex-&nbsp;hacker to&nbsp;having full access to your network (see below for precautions). Will they ever&nbsp;what to challenge them selves again and digging into your networks ?&nbsp;or are they really reformed ! Some Ex-hackers though they&nbsp;may have &nbsp;turned around, are still part of the hacking social community. And could&nbsp; share some of your network vulnerabilities to their (hacking) friends ?!...
Some suggested steps to take, if companies are thinking of hiring Ex-hackers: (techrepublic)

Do a thorough background check. Don't assume that what the hacker tells you is true. Believe it or not, some people will claim to be criminals when they really aren't, if they think it will get them a high paying job that makes them look "cool" to their friends.
Have the hacker sign an employment contract (or independent contractor agreement) that very explicitly sets boundaries and prohibits any access not specifically authorized, prohibits any use or sharing with others of information gathered in penetration testing or other parts of the job, and specifies the penalties for violation.
Consider having the hacker covered by an employee dishonesty/fidelity bond, or if the hacker is a contractor, require that he provide proof of insurance that will reimburse you if he steals from you, defrauds you or otherwise deliberately causes a loss to your business.
Don't give the hacker access to any more than he needs to do the job for which you've hired him. Never give him administrative passwords. If he can obtain those credentials on his own, you know you have a security problem, but you should not provide him with them.
If the hacker leaves or when his contract work is over, change passwords (even if you think he didn't have them) and make sure strong intrusion detection/prevention controls are in place.
Monitor network access while and after the hacker works for you and be on the lookout for any suspicious activity. Remember that the hacker may use some other user's account, not necessarily one that you've given him for his own use.

Robin

References
TECHREPUBLIC URL http://www.techrepublic.com/blog/it-security/hiring-hackers-the-good-the-bad-and-the-ugly/
	              
	              hi Belinda,
Nice post! In summary to your post and what was on my mind-we have to be Hackers to be safe. which ever way you look at it this is the only
way we can protect ourselves. Hackers think deeply, look in areas that we dont think is possible, see things we normally would ignore and many more. This is the way we should go to be safe.
regards
martins&nbsp;
	              
	              Hello Anthony,
The best approach in preventing an attack on any system is a thorough end-to-end testing. Irrespective of whether we follow Agile SDLC Testing or Scrum PDLC Testing, we may still end up delivering a vulnerable application. To determine the extent of the security of our application, it is a good idea to hire hackers, but of course, with an understanding of their background. 
The firm that hires a hacker must first establish a certainty against getting victimized by the hacker. Few things to keep in mind when doing so:1. Determine the extent of the protection your firm really needs.2. Ensure minimum dependencies on a single firm or a single consultant.3. Not to completely outsource your security needs and share only the relevant requirements.
Pros (Brien Posey, 2005):


Hackers are well aware of the multiple techniques they can use to break/attack a system; depending on the type of network; using their real world hacking experience.


Staying up-to-date with the latest security exploits and countermeasures is a full time vocation; which is why the knowledge of a regular IT system security personnel might not be sufficient.


&nbsp;
Cons (Brien Posey, 2005):


Question of Trustability - whether or not to include hackers in a team as it might hamper the trust of the customers who are associated with the firm.


Consultant hackers may demand large fees upon understanding that you have huge security issues in the firm.


Consultant hackers may compel the firm to perform a repeated analysis/multiple security checks in a year.


References:Brien Posey - http://www.windowsecurity.com/articles-tutorials/misc_network_security/Hackers-Security-Consultants.htmlhttp://www.geek.com/news/using-ethical-hackers-for-security-consulting-547519/
Best Regards,Kharavela

	              
	              Hi
Regarding the question about what we should do, I think to key words are usability and awareness. Someone said approx. “the masses are stupid” (yes, I know, it’s a harsh sentence), and in this case it just means that everybody is not an expert, hence those who are experts must find a way for the masses to be secure. And this must be done by continuously improving:


Usability in such a way that normal users know what to do “in plain English”.
Awareness about threats and what not to do, e.g. do not open mails from unknown senders, do not reply a phone call or sms from unknown senders etd.


And then, all the experts in the companies, public sector, ISP’s, Cloud providers, etc. must continuously improve the platforms that we all rely on.

Other than that I don’t know what to do.

Br Bo
	              
	              Week 6 DQ - Network systems security risks

I.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Introduction 
A computer network can be defined as a system that connects physically or via radio waves, desktops or set of servers, and this in order to exchange information and data securely.  The computer network allows among others to:

Share data and documents, 
Share applications and other resources such as printers, ... 
Secure stored data 
Develop communications tools, 
Access the Internet. 
Etc.

The computer network has many benefits like encourage teamwork, secures data and optimizes resources use. But these systems are also vulnerable.
System vulnerabilities often arise from negligence or inexperience of users. There may be other causes related to the context. Vulnerability usually allows the attacker to fool the system, such as bypassing checks access control or by executing commands on the system.
By default, a computer network is not secure; it is completely permissive and must put in place a number of mechanisms and strategies to make it almost waterproof.
II.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Security Vulnerability
Vulnerabilities: they are security failures in one or more systems. Any system seen as a whole, have vulnerabilities that can be exploited or not.
Generally hackers take advantage of vulnerabilities in a system to damage system, by disrupt system’s usual operation, and violate data integrity as well as confidentiality of data it contains. (Wikipedia, 2014)
Attacks may at first be classified into two main categories:
Passive attacks: here a hacker can listen a network without changing data or network operation. They are usually undetectable but prevention is possible.
Active attacks: involves modifying data or messages, to break into network equipment or interfere with proper operation of the network. Note that an active attack can be executed without the ability to listen. In addition, there is generally not possible to prevent these attacks, although detectable (allowing an appropriate response).
There are several types of security vulnerabilities in a network system, and we're presenting a few of them with their prevention methods:

Network Scanning

As soon as the network topology is familiar to a hacker, the hacker can now scan the network, that is to say, determined using a tool which IP addresses are actives or not on the network, open ports, and the OS used.
You cannot prevent the IP addresses and ports, to be scanned, but can be prevented the machine to respond by using a good firewall to close all ports and monitor all inputs and outputs of your computer. 
May also have to close critical ports and services, and Disable dangerous and unnecessary services, etc.

Intrusion

If a hacker succeeds to get a mapping of computers on a network using scanning, thus he can now plan an intrusion,
To get into a network, hackers will need valid and normal accounts, so he can access these accounts on computers already identified before. 
Hence the term "intrusion" because hackers use normal accounts and is therefore almost undetectable.

Threats of identity theft

Threats in this category correspond to operations to obtain or use unlawfully authentication of another person's data, For example, their user name or password. This category includes threats of attacks "man-in-the-middle" and communication between a trusted host and untrusted hosts.
Attacks “man-in-the-middle”
"Man in the middle", this type of attacks are often used by almost all hackers. The principle is simple; you need to place a third machine between two machines which are communicating within a network. Thus the computer in the middle can intercepted communication between these 2 computers and these computers doesn't doubt that their communications are listening or intercepted
One way to prevent this kind of attack is to allow the sending of e-mail through one or more known servers (your own servers).

Extension of privileges

If Hacker now has some accounts within the network, if the selected accounts don’t have sufficient privileges, he can grant more privileges to these accounts. He can for example get root access to the computer or the network.
He is also able to install a sniffer in the network, to retrieve information such as IDs / passwords allowing access to accounts that have extended to other computers on the network thus he can control the network.

Session Hijacking

Hackers may use session hijacking to capture a session after the normal user has been authenticated and authorized. Session hijacking allows a hacker to use the privileges of a normal user to access a database or modify, or to install software to enable future intrusions that do not require the credentials of the normal user.
The use of IPSec for encryption or authentication protects endpoints of session hijacking.

Compromise

With the above steps, it’s possible to a hacker to get a complete map of computers in the network, a list of vulnerability and flaws, in additional if he have root access to a computer; he can now exploit the relationships between computers in the network to destroy the entire system.

Backdoor

When a hacker is able to infiltrate a network and compromise a computer, it may be possible that he want to come back again in the future, so generally he can install software which will constantly create a security flaw that he can use to access again the network and compromise a computer.
&nbsp;
&nbsp;III.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Conclusion
In conclusion, all computer networks is a source of threats and susceptible to attack. Vulnerabilities in network systems are mainly due to the fact that the data come from unknown sources are generally dangerous. 
The threat increases with the increasing importance of economic intelligence, the establishment of for-profit organizations and character more and more central to computing. 
It is worth remembering that all networks are potential targets or contain interesting information, or they can then be used to carry out attacks to other sites or networks. So a good configuration of network is very important to avoid most of threats. The best method to prevent exploitations of security vulnerabilities and prevent security attacks is a correct configuration of network and a constant monitoring and checks.
Network security is well taken very seriously and should be an integral part of organizations and businesses.
IV.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Reference:
Laureate Online Education. (2014).&nbsp;Computer Structures – Lecture Notes&nbsp;Week 6: Computer Networks&nbsp;[Online]. Available from: https://elearning.uol.ohecampus.com/bbcswebdav/institution/UKL1/201480_SEPTEMBER/MS_CKIT/CKIT_501/readings/UKL1_CKIT_501_Week06_LectureNotes.pdf (Accessed 12 October 2014)
Brookshear, J. G. (2012).&nbsp;Computer Science, an Overview.&nbsp;11th&nbsp;ed. Boston Massachusetts: Pearson. Addison Wesley
Wikipedia Inc. (2014) Network security [Online]: Wikipedia Inc. Available from http://en.wikipedia.org/wiki/Network_security (Accessed: 12 October 2014)
Wikipedia Inc. (2014) Network security [Online]: Wikipedia Inc. Available from http://en.wikipedia.org/wiki/Threat_(computer) (Accessed: 12 October 2014)
&nbsp;
&nbsp;

	              
	              Introduction&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;The Internet started out as a small-scale university research project and now it has turned out to be a globally used commercial product - utilized by millions of computers, ranging from mobiles and watches to servers and supercomputers. The Internet has opened up newer and quicker ways of communicating among people. The communication channels that we use have greatly evolved during the past two decades - the evolution from Leased-Line to Broadband to VOIP to Fibre Optics to 3G to 4G has been remarkable and inspiring.
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Today, we are exceedingly dependent on the Internet, GSM/CDMA/3G/4G and WiFi etc., and its varied services. It is very essential that these services be effective and highly responsive, seeing as they are being used by millions across the globe.
Common Network Security Vulnerabilities &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; When we construct any network and monitoring its configuration, we need to emphasize on the following 5 key points (KEVIN BEAVER, Acunetix.com, The Top 5 Network Security Vulnerabilities) :  
&nbsp;&nbsp;&nbsp; 1.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Weak or default passwords
We all know that password management/setting up strong passwords is extremely important. However, many web applications, content management systems, and even database servers are still configured with weak or default passwords. Hackers can just as easily access the database, instead of bothering with the attacks.  Solution: Change and test for weak passwords regularly and consider using a password management tool. Implement intruder lockout after a defined number of failed login attempts.&nbsp;
&nbsp;&nbsp;&nbsp; 2.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Missing patches
A missing patch (security update) on a server that allows any unauthenticated command prompt or a backdoor entry is an easy threat to your system. Applying periodic patches is as important as having firewall protection and antivirus systems in place. Criminal hackers usually take full advantage of and exploit unpatched systems.
 Solution: Follow network security best practices by updating your operating system and any other software running on it with the latest security patches.
&nbsp;&nbsp;&nbsp; 3.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Misconfigured firewall rulebases
One of the biggest, most dangerous, assumptions is that everything is well in the firewall because it’s been working fine. Digging into a firewall rule base that has never been analyzed will inevitably turn up serious configuration weaknesses that allow for unauthorized access into the web environment. It is highly important that all network segments, including Wi-Fi, be scrutinized regularly.
Solution: Review the organization’s security policy regularly and keep it relevant. OWASP provides some good guidance on building operational security guides.
&nbsp;&nbsp;&nbsp; 4.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Mobile devices
Phones, tablets, and unencrypted laptops pose some of the greatest risks to web security. All the VPN connections, cached passwords in web browsers, and emails containing sensitive login information, that you and likely everyone else might have stored on the mobile devices, are a piece of cake for hackers and intruders. The use of unsecured Wi-Fi via mobile devices is the proverbial icing on the cake.  Solution: Instill clear data management rules for all employees of the firm and make mandatory data encryption part of your security policy. This is becoming exceedingly important with employees connecting their personal devices to the corporate network.
&nbsp;&nbsp;&nbsp; 5.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; USB Flash Drives
Today, most of us are familiar with the dangers of these cute little portable devices. USB Flash drives can just as easily store cached information as your mobile devices. USB drives are also one of the most common ways a network can get infected from inside a firewall.
 Solution: Have clear security policies regarding personal storage devices including who can use them and in what places. Restrict the computers that can read USB flash drives and help prevent unauthorized access by encrypting the data as soon as it hits the device.
Some of the other practices that would help prevent exploitation of the vulnerabilities are (Michael Cooney, 2012):
a. perform risk analysisb. restricting unsafe applicationsc. training employees to ensure that all devices are configured and operated within the security policy rules.&nbsp;
Conclusion
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; When it comes to network security, the foremost dimension that must be protected is Access. Unauthorized access - be it Remote access or VPN access or by firewall filtering deficiencies, or even physical access to network equipment - is the be-all and end-all of security issues. Custom software design, firewall configurations, patches, robust antivirus solutions/authentication controls and a meticulous implementation of security policies - are all dire necessities to help combat network security threats (Published in INL NTSB Program, Common Cyber Security Vulnerabilities, 2008).
References
 Top 5 Common Network Security Vulnerabilities by Kevin - 
http://www.acunetix.com/blog/articles/the-top-5-network-security-vulnerabilities/
 Common Cyber Security Vulnerabilities - http://www.inl.gov/scada/publications/d/inl_nstb_common_vulnerabilities.pdf
10 common mobile security problems to attack - http://www.pcworld.com/article/2010278/10-common-mobile-security-problems-to-attack.html&nbsp;
Best Regards,Kharavela
	              
	              Hi Chris and Terry Hi Just a few statistics for your DQ: Microsoft state that they are experiencing fewer zero-day malware these last 3 years.    Symantec write in their 2013 annual report  There has been a 61% increase of zero-day threats in 2013 than 2012. (p. 6). The figure below is on page 34 “Even though the top five zero-day vulnerabilities were patched on average within four days, Symantec detected a total of 174,651 attacks within 30 days of these top five becoming known”. (p. 6) “97 percent of attacks using exploits for vulnerabilities initially identified as zero-days were Java-based.” (p. 23)    References: Microsoft: http://www.microsoft.com/security/sir/default.aspx Symantec: http://www.symantec.com/content/en/us/enterprise/other_resources/b-istr_main_report_v19_21291018.en-us.pdf Br Bo 
	              
	              Hi Chris,
I wanted to build on your closing remarks about hiring hackers.
My perspective has definitely changed in recent weeks and I am now leaning towards the opinion of nurturing this type of skillset. My continued worry however is providing the appropriate level of oversight.
It appears that hiring ‘Hackers’ is exactly what many organisations are doing, including governments of countries. I now genuinely believe this is a positive step and to the points also made in the posts by Belinda and Terry, there is a need to think like 'Hackers'.
A survey (Qubic, n.d.) of IT professionals found that 70% believed that businesses should hire ex ‘Hackers’ to safeguard and improve online security with it already being common practice with many companies in the US, recruiting at ‘Hacker’ conventions.
Last year, it was reported by the Guardian (2013) that hundreds of computer experts will be recruited as reservists, and trained with the help of the GCHQ’s joint cyber unit at Cheltenham – with the possibility that this could even include convicted ‘Hackers’. So, in line with your reference, it seems that the UK government has a change of mindset towards ‘Hackers’ and is open to changing policy and hiring and steering them into more positive avenues. It was noted that the armed forces don’t necessarily exclude people who have criminal convictions but would look to examined on its merits. 
In the same report, it was highlighted that the UK Defence secretary also went on to declare it would have the capability to mount cyber-attacks against other countries as means of deterrent from striking against the UK. I don’t intend to take this down a political path, I am just hoping we have appropriate skills in the Intelligence and Security committees to provide the appropriate levels of oversight and governance – similar in principle to nuclear submarines in that two or more senior people have the keys or codes to launch.
The same report highlighted that the UK cyberdefence unit blocked around 400,000 advanced malicious cyberthreats to the governments secure intranet alone. British Ministers have not publicly named the culprits partly because it is so difficult to prove, but privately, officials apparently point the finger at Russia and China.
Interestingly, I found another report (Cherney, 2014) that said that the US are also open to hiring ‘Hackers’ but aren’t necessarily prepared to change the rules, relative to boundaries of certain behaviour. The FBI are struggling to hire ‘Hackers’ as in addition to better deals being offered elsewhere (I hope this being the private business sector, rather than the criminal underworld) that their drug testing policies are keeping many of them off the payroll, referring to the fact that they would have to let government ‘Hackers’ get stoned! Perhaps..., to literally be in “Cyberspace” J
Best Wishes, Craig
References
Qubic. (n.d.). Ex Hackers may become the IT Guardians of the future [Online]. Available from: http://www.qubicgroup.com/pages/press/news-feed/ex-hackers-may-become-the-it-guardians-of-the-future.php (Accessed 12 October 2014)
Siddique, H. ed. (2013). Ex-Hackers could be recruited to UK cyberdefence force [Online]. The Guardian. Available from: http://www.theguardian.com/technology/2013/oct/22/uk-cyber-defence-force-ex-hackers-gchq (Accessed 12 October 2014)
Cherney, M. (2014). The FBI Says It Cant Find Hackers To Hire Because They All Smoke Pot [Online]. Available from: http://motherboard.vice.com/en_uk/read/the-fbi-cant-find-hackers-that-dont-smoke-pot (Accessed 12 October 2014)
	              
	              Hi Anthony and colleagues
Having read the answers above I don’t feel I can add much more, they are really well written. And I especially found it interesting to read Ramin’s comments from his own experiences.
I have often thought about this situation and other similar life situations where you have to choose whether or not you should give someone a 2nd or even 3rd chance. There is no easy answer to this. I would have to consider the precise situation and person (e.g. white, gray or red hat). And I think that Ramin is correct in demanding all kinds of contracts etc. Also don’t give information to the person since it is his/her job to hack.
Br. Bo

	              
	              Hi Bo
Yes, my closing points exactly, with particular emphasis on tech vendors stepping up.&nbsp;
Best Wishes, Craig
	              
	              Hi Martins
Yes, agreed.
And to Augustos points, there are things we can and should be doing.
Best Wishes, Craig
	              
	              War driving with broadcast channels
Hybrid Broadcast Broadband Television (HbbTV) is a standard for merging linear and on demand services on a connected television. The standard is becoming very popular especially in Europe. For example 90 percent of televisions sold in Germany are HbbTV compliant. This rising profile attracts attention from different security experts and in 2014 the first hack was published.
The hackers backward engineered the HbbTV-system, looked at the communication lines and build exploits using a spoofed version of the broadcast signal to enter the connected television via the front door (Like the TED examples). By broadcasting locally, by instance from a car in front of a house, a manipulated but stronger television signal makes the targeted device antenna switch to the local broadcast. By adding extra code to the video-layer malicious software could installed and from that moment everything is possible. The Wifi could be opened and used to send data and retrieve data, or activate and access the camera on that connected device. The end-user would only see a glitch that could easily be misunderstood as defect due to weather conditions.
HbbTV uses a Digital Video Broadcast (DVB-signal) as authentication and carrier of the application that run automatically on the background. At the moment the viewer hits the red button on his remote control the data is retrieved from the broadcast server and used as a overlay using HTML on the television screen. By war driving and broadcasting a spoofed signal the hackers can auto-load a different programme in the background by combining it with a tricked video stream. The connected device is from that moment controlled remotely. 
A solution would be to use the HTTPs-protocol instead of HTTP for authentication and retrieval of the application. This is not totally secure as everybody can register for an HTTPs certificate and could for that matter still spoof the television. Perhaps it would be more easy to track such a person as he or she does not delete all tracks on the hacked device. 
In order to improve the security level further the standardisation HbbTV.org could run their own certificate authority involving all allowed applications to be registered in order to get access on connected televisions. But this would involve great amounts of work and would limit the self organising flexible application environment of the system. 
Explanatory list:
Social hacking is tricking somebody to give their personal account details in person (face to face tricking) or phishing (sending 'fake' emails trying to provoke someone to give their data) or using spyware and that (malicious software installed on device that can be used to track the personal data in question). 
Local installed Malicious software (Malware): Software installed without consent of the device owner. Examples are local running programmes like Viruses, &nbsp;Worms, Trojan Horses and specific Spyware. 
Networked attacks: Denial of service (DOS) attack using mostly malware installations on different computers on the internet with the goal to create a burst of messages to one system with the intention to overload it. 
Spam is automated mail to 'phish' for reactions of the user, be it for luring or buy a product, or as actual phishing action or installing viruses or Trojan horses that can be used to take over the system. Spam filters are Firewalls that block unwanted mail.
A solution for many security breaches is the Firewall that can be installed at a Gateway to block IP-sources for inbound traffic or outbound traffic to specific locations. Or recognises 'external to internal to external' traffic for instance used spoofing actions. Individual Firewall's installed on devices track ports and activity on the ports and relates this to allowed activity to identify potential attacks.
Proxy server can be used for security as traffic intermediate by 'anonymousing' internal network settings and clients, but also filter incoming messages from external accessed servers. &nbsp;In combination with auditing software system administrators can detect sudden changes in network activity and act on that information. 
Antivirus and anti-spam software are solutions for both virus attacks and spam and consist of regularly updated lists of malicious software or email sources. 
Encryption protocols for 'secure internet access' like FTPs, HTTPs, SSL (Secure Socket Layer) that rely on private and public encryption keys. Certificate authorities publish owners of public keys authentication grade of the keys . Another authentication tool is creating digital signatures derived from a private key. 
References: 
- Brookshear, G. (2012), Computer Science: An Overview, Perasoon Education Inc. publishing as Addison-Wesley, Boston, United States, Version 11, pp 173-181.
- Oren, Y, Keromytis, A (2014) From the Aether to the Ethernet: Attacking the Internet using Broadcast Digital Television [Online]. 23rd USENIX Security Symposium (USENIX Security 14), August. Available from https://www.usenix.org/conference/usenixsecurity14/technical-sessions/presentation/oren (Accessed on 08-10-2014).
- TED (n.d.) Avi Rubin: All your devices can be hacked [Online]. Available from: http://www.ted.com/playlists/10/who_are_the_hackers.html, (Accessed: 12 October 2014).

	              
	              Network Systems Security Risks
Introduction:-
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Security is a main component of any company’s agenda. Hence, huge sums of moneys are spent on securing the company’s documents whether on the server or on their local area network, Likewise, personal computers need to be protected security-wise.
Some of the security vulnerabilities associated with networks include:-

The security products used by networks have insufficient information to self-configure for reliable detections of policy violations. This means that security products do not have the mechanism to ascertain the network information for the network in which they are meant to be used for. This leads to issues such as predicting the effect of a particular packet upon a destination device.
No mechanism to detect the type of network topology-some security products used by networks have ability to detect the type of network topology and thus cannot predict if a certain packet will reach its intended destination. This type of loophole of network information can compromise the security products ability to detect such attacks like denial of service attacks, evasion attack etc.
Scarcity of memory resources and processors-conventional security products due to lack of memory or shortage of memory and processors may begin to drop packets and eventually short down certain tasks in an unpredictable fashion. As the size of network grows, such failure become more likely, as the greater number of connections onto the network requires a greater number of lookups performed by the security products.
Increase in traffic in a network=this can pose a threat for vulnerability because an increase in traffic flow will drain a security product resources that protect the network. As a result, conventional ID systems cannot operate effectively at high network bandwidth utilization.
Increase in number of devices or services in a network=since networks are very unstable in nature, the addition or deletion of devices or services can make the original network information given by the user inaccurate.
Unsecure user accounts=users of computer systems in a network should secure their accounts by not giving out details to other users or persons. This can pose a serious threat as this could be used against them if someone else knows how to get into your user account.
Computer systems are not designed to function in an environment where the traffic exceeds their memory or processor capacity hence when this happens they start dropping packets and degrading performance which can leave the network prone to an attack.
Details in the systems is what the hackers are after- A hacker might use the ARP spoofing to send false data to the local network which compromises the transmission control protocol (TCP) services.
Employees and human Errors – humans make mistakes which may be due to inexperience or lack of training. These errors can pose a threat to any network.

However, there are ways or methods of preventing exploitation of these security vulnerabilities and prevent security attacks of our networks. These ways are listed below:-

Raise consumer and industry awareness of the importance of network security=this can go a long way in creating a secure environment in your networks when people or the entire organization are informed of the possible threats of not protecting your network or system, they become more security conscious and hence more careful in whatever they do with their systems or on the network.
Educate users about best practices=as mentioned above, let the user know what type of security protocol or product is best for them is very vital.
Aggressively enforcing the law against criminals that use the network for crime or theft – when criminals are caught they should be punished or prosecuted for committing those offences. This will reduce the rate of crime.
Provide a scan engine that will be coupled to the network – This can direct a request unto a network and access a response to the request to discover the network information. This way the network is on check as information gotten from the network will be useful to checkmate any intruder or faulty loopholes.
Provide the network with a protocol engine coupled to the network – This protocol engine performs a plurality of protocols analysis on the network data traffic to identify attacks on the network.
Attach a signature engine to the network – This compares the network data traffic to a plurality of attack signatures to identify incoming attacks upon the network. A vital tool that most network should use to safeguard their networks
Attach a Priority engine on the network – This is coupled to the analysis engine, the protocol engine, and the signature engine. It prioritizes the plurality of protocols analysis and the plurality of attack signatures due to the network information. It prioritizes a plurality of systems services base on the network information. It detects policy violations and patterns of misuse due to the network information. This above information help the network administrator detect faults in the network.
Maintenance of the network map or layout- This allows different types of misuse patterns to be detected. 
When resources are deleted on the network, it is recommended that a reliable prioritized shutdown of analysis task be carried down.
The use of strong passwords to secure systems – the systems on the network should have strong passwords. Passwords that will be difficult to figure out.
Install security Patches – The use of security best practice should be employed. Hackers use loopholes in systems and networks to attack hence our security patches for software and operating systems should be updated regularly.

In summary, the best way to secure your network and minimize vulnerability is to keep checking out loopholes in the network and fixing it on a daily basis as vulnerability to an attack can happen anytime.
REFERENCES:
Brookshear, J.G. (2012). Computer science: An overview. 11th ed. Boston Massachusetts: Pearson. Addison Wesley
Gleichauf, R.E. et al. (2001)”methods and system for adaptive network security using network vulnerability assessment” U.S Patent No.6,301,668. Available from: http://scholar.google.com/scholar?q=security+issues+associated+with+networked+system. (Accessed 11 October 2014)
Security-issues-Cisco systems. Available from: http://www.cisco.com/web/about/gov/issues/security/html. (Accessed 11 October 2014)
Network security: common network security problems. Available from: http://www.brighthub.com/computing/smb-security/articles/54723.aspx. (Accessed 11 October 2014)
Douligeris, C. &amp; Serpanos, D.N.(2007). Network Security - Current Status and Future Directions. Hoboken new jersey: IEEE Press. John wiley &amp; sons Inc.
&nbsp;
&nbsp;
&nbsp;

	              
	              Hello Anthony,
The pro's are that the company internalises&nbsp;very specific knowledge about certain security issues that the clearly did not have before. It shows their clientèle they are taken matters seriously and are working on a solution. An extra advantage for the company is that they let the attacker change camps.&nbsp;
A disadvantage, from a wider community perspective, is that the hacker is not showing new security leaks anymore but is spending her or his time to prevent security breaches for one specific product.&nbsp;
All comments in this week 6 discussion agree with taking hackers (or at least the hacking mentality) seriously. Every company who gives access to services over the Internet or delivers content that needs to be installed on devices should take security seriously. Perhaps as suggested by Mikko Hypponen in the referenced TED Talks we should even think of an international 'police' force for finding malicious hackers that are out to&nbsp;steal.&nbsp;
But I did not see any comments about too much security. There is a point when security goes to far and defeats its purpose. A good example is prescribing too complicated passwords with the result people are writing them down on post-its on their computer screen. Usability and security are often not an happy married couple.&nbsp; For instance DRM is the best protection solution for content delivery to different devices. It is mandated by Hollywood production houses for their content as it limits what one can do with it. At the same time one can ask if this is in the best interest of end users. What does DRM do for the 'Mash up' culture on Internet? In this light hiring hackers that previously helped content to be available for the Mash-up culture now prevents them for gaining access.
greetings from Bram

	              
	              Hi Anthony
Some great responses from my class colleagues and as with Bo, it is difficult to add.
One thing that struck me when I was considering my response, and it also references the trust issues already outlined by my colleagues, is the role that the hackers actually plays, and the environment that they're exposed to.

I think the starting point is that perhaps the hackers work in a consultancy capacity only, working side by side with other highly skilled technical permanent staff and maybe the hackers don't even touch the system, just point and show the way with their colleagues? Or, as Robin refers, they must be completely monitored by other highly skilled technical staff for all activity. A good way for skills transfer too.
My last point would be that I wouldn't allow any of them to work remotely, or to bring or take any devices in or out of the work environment.
Best Wishes, Craig
	              
	              Hi Martins,
But isn't human life ALWAYS surounded by all kinds of threats?
br, Terry
	              
	              Hi Bo,
Thanks for the detailed references and data.
I don't think Microsoft's 0day attacks are fewer. From their figures it seems they have been "quite stable", except 2010 and 2011 are pretty bad.
In the report,&nbsp;“97 percent of attacks using exploits for vulnerabilities initially identified as zero-days were Java-based.” really shocked me. I know Java is vulnerable but never knew it's so flawed. This gives me another reason for keeping a distance with it.
br, Terry
	              
	              Network Systems Security Risks
A vulnerability is defined in the ISO 27002 standard as “A weakness of an asset or group of assets that can be exploited by one or more threats” (International Organization for Standardization, 2005)
Another good definition comes from Microsoft Technet, a Mirosoft online resource that defines it as “ &nbsp;a weakness in a product that could allow an attacker to compromise the integrity, availability, or confidentiality of that product” http://technet.microsoft.com/en-us/library/cc751383.aspx 
This definition touches on 3 core security aspects of information security– Confidentiality, Integrity, Availability (CIA). A system is therefore vulnerable when there are flaws that could lead to the exploit of any of these three aspects. In network security, a vulnerability is a weakness that is inherent in every network and device including routers, switches, desktops, servers, and even security devices themselves.
The relationship between a vulnerability and a threat. is that a threat (by computer security definition) is anything that has the potential to exploit a vulnerability and cause an undesirable impact on a computer system.
Common vulnerabilities associated with network systems include
&nbsp;1.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Misconfigured hardware or software
A misconfigured hardware or software often presents a flaw that can be exploited leaving room for an attacker to infiltrate or penetrate the system. The misconfiguration can result in a security weakness within the network and an example is a network firewall that fails to prevent unauthorized access. Maintaining a default configuration for a service set ID (SSID), could mean that an attacker could exploit the default SSIDs and access a base station that is still using the default ID. SSID (Service Set ID) is a configurable identification mechanism that enables a client to communicate with the correct base station.( http://www.spamlaws.com/misconfiguration-attacks.html )
2.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Poor network design
A poorly designed network makes it easy for an attacker to infiltrate an organizations network. If no intrusion prevention and detection system is provided to an organizations’ internal network, this presents a vulnerability that can be exploited. In other cases, lack of proper sub netting within a network leaves the network prone to attack and visible to unauthorized users. A subnetwork, or subnet, is a logically visible subdivision of an IP network. The practice of dividing a network into two or more networks is called subnetting. A subnet mask allows IP networks to be subdivided for security and performance purpose
3.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Inherent technology weaknesses
In most cases, every computer network and device has an inherent weakness or vulnerability. An experienced hacker knows that every network or device has a certain degree of vulnerability or weakness, and they take advantages of each security weakness or loophole to exploit the network. These include TCP/IP protocol (HTTP, FTP, SMTP, SNMP), operating system, and network equipment weaknesses (Routers, Firewalls, Switches etc) . - ( http://orbit-computer-solutions.com/Common-Network-Security-Threats.php#sthash.9tbNrZKA.dpuf )
4.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; End-user carelessness
End users are often pose a big risk to network security due to basic mistakes. Use of weak passwords is often one of the areas that is commonly exploited by attackers. A hacker can conduct a brute-force attack and compromise a system in the event that weak passwords are used.
5.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Intentional end-user acts (that is, disgruntled employees)
Disgruntled employees could deliberately plant viruses or compromise the security of a system in order to revenge or cause problems to the system. A knowledgeable person could overload the system, perform a denial of service attack or leak to the public, confidential information about an organization.
(http://ptgmedia.pearsoncmg.com/images/1587131625/samplechapter/1587131625content.pdf)

There are several methods to prevent exploitations of these security vulnerabilities and prevent security attacks.
These include

User of a bastion host:

A bastion host is a computer that is deliberately exposed on a public network and is used to defend against attacks to an internal network. The system is on the public side of the demilitarized zone (DMZ), unprotected by a firewall or filtering router. Firewalls and routers can be considered bastion hosts (SANS, Intrusion Detection FAQ: What is a bastion host?).

Anti-Virus software

The use of anti-virus software is one of the commonly used approaches in reducing security vulnerabilities. These detect and prevent malicious attack and damage to a computer. Examples include Kaspersky, McAfee, AVG, Symantec
Network security measures are also needed to protect data during their transmission. Other techniques include (Stallings, W, 4th Edition)

Authentication
Digital signatures
Encipherment
Authentication Exchange
Access Control
Traffic Padding
Routing Control
Data Confidentiality
Data Integrity
Nonrepudiation
Availability Service

References:

Laureate Education, (2014) Computer Networks: Week 6 Lecture Notes [Video, Online], (accessed: 12/10/2014)
Brookshear, G. (2011) Computer science: An overview. 11th Ed. Boston: Addison Wesley/Pearson.
SANS, ’Implementing Vulnerability Management Process’, http://www.sans.org/reading-room/whitepapers/threats/implementing-vulnerability-management-process-34180, Accessed 13/10/2014
Nadeem Ahmad (771102-5598), ‘Analysis of Network Security Threats and&nbsp; Vulnerabilities by Development &amp; Implementation of a Security Network Monitoring Solution’ , Master Thesis Electrical Engineering Thesis No: MEE10:76 Sep 2010, Blekinge Institute of Technology http://www.bth.se/fou/cuppsats.nsf/all/d2a1ccf9a9fdb82ec125779a0059f032/$file/Thesis%20Final%20Report.pdf
http://ptgmedia.pearsoncmg.com/images/1587131625/samplechapter/1587131625content.pdf
Stallings, W. ‘Network Security Essentials, Applications and Standards’ , Fourth Edition, Prentice Hall, Available at http://ph.sbmu.ac.ir.servercms1.com/uploads/3._Network-security-essentials-4th-edition-william-stallings.pdf , Accessed 12/10/2014
Understanding Security Using OSI model, SANS, Available at http://www.sans.org/reading-room/whitepapers/protocols/understanding-security-osi-model-377
Orbit Computer Solutions, ‘Common Network Security Threats’, Available at http://orbit-computer-solutions.com/Common-Network-Security-Threats.php, Accessed 13/10/2014


	              
	              
Network systems security risks
I. Introduction
Two or more computers connecting together forms what is called a network. There are different kinds or forms of networks; Local Area Network (LAN), Metropolitan Area Network (MAN) and Wide Area Network (WAN). The Internet is the most common WAN today and as more and more organisations and individuals are using the Internet, it has become the playground for security attacks. As such they are many scams and attacks such as shopping scams, credit card scams, email scams among others

II. Security vulnerabilities commonly associated with network systems
While there are many security vulnerabilities associated with the internet the majority of the treats occurs over the Internet. These vulnerabilities includes; Denial of service attacks, Identity theft, Zero-day attacks or zero-hour attacks, Hacker attacks, Data interception and theft, Spyware and adware, Viruses, worms, and Trojan horses [1]. Viruses, trojans, spyware, worms and are usually called malware. Malware refers to Malicious Software, and as the name implies, it is unwanted software that makes its way into our computer systems, with the aim of destroying or disrupting the computer systems.
III. Summarise each vulnerability and explain why it is a threat.
A. Denial of service attacks
 This is usually an attack on the network with an aim of crashing it, slowing it down or stopping services. Attackers can get software that can flood the network traffic making it appear busy and making services unavailable. In the cases of a network crash, this can possibly lead to damage of network resources. This is a threat because if services are disrupted, it can lead to a loss in revenue and additional resources needed to fix the problem.

 B. Identity theft
 Identity theft is where an individual pretends to be someone else in order to have access to another person's resources, which is often money related. A popular method used by identity thieves is phishing; soliciting of personal information from another usually for purposes of scamming them. Typically the scammer sends emails to an individual asking for certain his or her personal information, for example claims that individual has won some money and as such they are requesting banking information from that individual. Emails are also sent with claims to be union representatives, bank officials among others, requesting personal information.

 C. Zero-day attacks or zero-hour attacks
 This is security vulnerability in software applications, which the designers or developers are not aware of or did not have an opportunity to address.
 
 D. Hacker attacks
 Barnaby Jack in 2010 found a way to make ATM throw money at him [2]. Hackers are persons that can gain unauthorized access into computer systems, networks and other devices. In his Ted video, 'all your devices can be hacked', Avi Rubin the talks about devices that are implanted into humans. However in order to control these devices they need wireless connectivity, thus these devices like other devices having some form of wireless communication can be hacked into. He also made reference of a car which his colleagues hacked into. They read the software on the car and used reverse engineering to find out what the software did and then hacked into the software and stole the car, [3]. There are numerous other forms of hacking that is taking place today.

 E. Data interception and theft
 This deals with the intercepting of data as it travels between computers systems. Hackers or attackers can use packet sniffing tools that can intercept data, as it travels across the Internet, thus getting passwords, credit card information, social security numbers etc.

 F. Spyware
 As the may have implied, spyware or sniffing software are software, usually in other software, that collects private information about a user or their computer without their knowledge. This informations is usually relayed back to the parties who may have requested it. Spyware software can also to “collect such as recording the symbol sequences 
 typed at the computer’s keyboard in search of passwords or credit card numbers”, (175)

 G. Adware
 This refers to Advertising Software, that can be installed on users computers. This type of software applications is usually not requested by the user. Because these applications are unwanted, they are often annoying and distracting to the users, thus hampering productivity.

 H. Viruses
 “A virus is a bit of code that is inserted (and inserts copies of itself) into another piece of software, so that it is executed as part of the execution of that piece of software”[4]. As indicated a virus is executed while the software is executed and it can do this in one or more software. When a virus infects your computer, it can do several things, which in turn degrades the overall performance of your computer. These activities includes: installation of backdoors; which allows remote access to systems “without normal authentication”, they can destroy or corrupt data and or applications, they also cause computer failures. [4] This can lead to a lot of downtime and additional time spent restoring or fixing computers, which can be added forms of expense.
 I. Worms
 Worms perform in a similar manner to viruses, making copies of themselves and degrading the performance of computer applications. However, unlike viruses, worms are actually programs and not bits of codes. They can also spread themselves across computer networks, further infecting other computer and causing further damages. [4]
 
 K. Trojan horses
 This form of malware, that are installed on the users computers by them. Users are often mislead by this type of software as it may come in different forms resembling actual software, e.g. utiltiy programs and games. Further degrading or hampering a computer's performance.
IV. Recommended methods to prevent exploitations of these security vulnerabilities and prevent security attacks.
 A. Anti-virus Anti-virus software are software specifically designed to detect and remove computer viruses. They are many forms for anti-virus software out there some designed to detect all forms of malware and others to detect specific forms of malware. However, new forms of viruses and malware are introduced everyday, thus it is important that anti-virus software are always updated, so that they can detect new forms of attacks. [5]
 C. Firewall There are many different kinds of firewalls, however firewalls can be installed on a network gateway to filter packets. In the instance of a Denial of Service Packet, a firewall can work well here in that it would recognise that the packet is a malicious packet or a strange packet and as such it will be blocked. Firewalls can also serve as spam filters filtering out or blocking unwanted emails, the firewall can go through a learning process of learning the safe emails and unsafe ones as well. They can also be installed on users computers to protect them from malware. [5]
 D. Proxy server These act as an intermediary server between the client and the main server. Protecting the main server from capturing private information about the client computer. Thus if someone is to hack into the server they cannot access information about the client machines.
 H. Encryption Because data travelling across the internet can be intercepted and hackers or attackers can get this information through various interception methods. Stronger forms of security has been introduced to saveguard our information. A few examples of these are SSH, HTTPS, SSL and public key encryption. Public key encryption encrypts data as it travels from one computer to the next however both a public and a private key is needed to encrypt the data, thus offering better security.
 Other forms of security includes, using legal approaches, “Intrusion prevention systems (IPS), to identify fast- spreading threats, such as zero-day or zero-hour attacks” and “Virtual Private Networks VPNs), to provide secure remote access” [1]. 
Conclusion 
Once your information is on the Internet or can become accessible on the internet, there will be vulnerabilities. But vulnerabilities while it may be most rampant on the Internet, it is not limited to the Internet alone. As it exists on all other internets or networks, and devises as well, which was illustrated here. The key to have secure passwords, change your passwords regularly, and try to enforce proper security practices so as to secure your network and personal information from attacks. It is also important to keep update with new technology and security threats that may arise from time to time. It is also important that all security software are keep up to date.
References:
CISCO (n.d.) What is network security, [1] [Online]. Available from: 
http://www.cisco.com/cisco/web/solutions/small_business/resource_center/articles/s
ecure_my_business/what_is_network_security/index.html, (Accessed: October 12, 
2014). 

TED (n.d.) Keren Elazari: Hackers: the Internet's immune system, [2] [Online]. Available from: 
http://www.ted.com/playlists/10/who_are_the_hackers (Accessed: Octiober 10, 
2014). 

TED (n.d.) Avi Rubin: All your devices can be hacked, [3] [Online]. Available from: 
http://www.ted.com/playlists/10/who_are_the_hackers.html, (Accessed: 26 April 
2014).

Peterson, L. L. &amp; Davie, B. S. (2007) Computer Networks: a systems approach [4] pages 630. Morgan Kaufmann Publishers
Douligeris, C. and Serpaos, D.N. (2007) Network security: Current status and future 
directions. John Wiley and Sons, Inc
Brookshear, J.G. (2011)&nbsp;Computer science: An overview.&nbsp;11th ed. [5] Pages 176-177, Pages 173-180. Boston: Pearson Education / Addison-Wesley. pages 114)

	              
	              Networked systems are faced with many security issues. Any network node, whether it be computers or devices that uses software or any program to communicate are vulnerable to the potential threats this technology faces.
One of the main threat faced is that of hacking, an individual how seeks to gain unauthorized access to a private network is a hacker. They are usually technical savvy individual who seek to gain some benefit or simply point out security vulnerabilities in corporate networks. The many type of vulnerabilities computers and network communicating devices are exposed to are consistently increasing but falls into distinct categories, such as spyware, malware, trojan horses, viruses, worms, hijacking, spoofing, botnets, zombies, social engineering, etc. All these threats are critical and can cause devastating effects on businesses and private networks. However there are systems, programs appliances, security management systems and best practices that can be incorporated to ensure that these threats are managed effectively.
Below I will summarise some of the vulnerabilities network systems face and explain why they are a threat.
Un-patched Systems:&nbsp; These software use in network communication have not addressed issues within the underling code. This type of vulnerability allows a would be attacker to exploit a defect within the software of a computer or network device. Software developers continuously provide updates to their published programmes, fixing any known issues or defects that may have been reported to them. Network systems not periodically updated are exposed through these holes in the codes.
Weak Passwords: This is one of the oldest vulnerabilities and still exist today. Weak passwords describe anything that can be easily guessed and doesn't follow any specific guidelines and use few characters. This presents a huge threat to the security of a system. If access is granted to an unauthorised person, several challenges become possible, like reconfiguring systems, injecting malwares and other forms of attacks against the system.
Default Configured Firewall: "A firewall primary prevention technique is to filter traffic passing through a point in the network" (Bookshear, 2012). But when implemented with its default configuration or not properly configured can expose a private network to many forms of attacks, such as denial of service (DoS), malware, spyware, etc. Take for example a DoS attack, which is design to overwhelm a communication interface with irrelevant messages, making it difficult for the interface to operate hence halting its function, which can cripple a business's commercial activity.
BYOD Access: Bring your own device (BYOD) is becoming prevalent in many organizations and as such access to corporate resources and information is being required. This present a threat of infected systems accessing the corporate network, information being stored on these devices and one of the not so easily recognized threat of BYOD is the local caching of password&nbsp; in the browsers of these device. This is especially critical, when&nbsp; the unsuspecting user accesses a public Wi-Fi network and a would be attacker gains access to that device and retrieves the cached information from the browser (Beaver, 2013).
Removable Storage: These devices are compact and can carry large amounts of data, but when use without control can infect&nbsp; corporate networks. Allow unauthorized persons to remove data from&nbsp; systems, possibly leaking trade secrets to the competition resulting in loss of revenue.
&nbsp;
The exploitation of these vulnerabilities summarised above can be prevented and security attacks mitigated against. Below are some of the recommendations that can be employed to prevent the threats associated:
Patch Systems: &nbsp;Ensure that all systems are up to date with all the vendor updates, fixes, patches and service packs, this will ensure that all known defects have been addressed. Continuously monitor these vendors support websites for new software releases and updates, review release notes&nbsp; and check for any associates vulnerabilities.
Strong Passwords: Use complex password configurations to allow for best possible security this method can other. Where possible use two factor authentication to protect access to critical systems. Employ a password management tool that can facilitate the security and complexity of the passwords &nbsp;used. Implement a lockout mechanism that will lock the interface a define amount of failed attempts.
Custom Configure Firewall:&nbsp; Implement a security policy that will inform the configuration of the firewall rules to ensure that the set objectives for security are met. Continuous evaluation of the firewalls operation, make changes as necessary to keep up to date with the applications and data traversing the network.
BYOD Devices: "Instill clear data management rules for all employees and make mandatory data encryption part of your security policy" (Beaver, 2013). This will ensure a level of security and control of the data accessed and other resources utilised. This can even extend to disabling the device in lost, stolen or otherwise compromised.
Restrict Removable Storage: Implement&nbsp; policy that will restrict the usage of these devices in the corporate environment and when the use is necessary ensure that critical data leaving the organization is encrypted and can only be accessed by those authorized to do so.
&nbsp;
References:
TED (n.d.) Avi Rubin: All your devices can be hacked [Online]. Available from: http://www.ted.com/playlists/10/who_are_the_hackers.html, (Accessed: 10 October 2014).
Brookshear, J. G. Computer Science: An Overview XML Vital Source ebook for Laureate Education, 11th Edition. Pearson Learning Solutions. VitalBook file.
Beaver, K. (2013) Top 5 Common Network Security Vulnerabilities that are Often Overlooked [Online]. Available from http://www.acunetix.com/blog/articles/the-top-5-network-security-vulnerabilities/ (Accessed: 12 October 2014).
Mark Ciampa (2007) Security Awareness: Applying Practical Security in Your World. 2nd ed. Boston: Course Technology.

	              
	              What is Network Security?
We start off by understanding what network security is all about. What is network security? How does it protect our network? How does network security work? What are the business benefits of network security?
What Is Network Security?
Network security refers to activities/solutions designed to protect your network. Specifically, these activities protect the usability, reliability, integrity, and safety of your network and data. Effective network security targets a variety of threats and stops them from entering or spreading on your network.
How Does It Protect You?
After answering the question about network security, the next question to ask is, what are the threats to my network?
Many network security threats today are spread over the Internet. The most common include:

Viruses, worms, and Trojan horses
Spyware and adware
Zero-day attacks, also called zero-hour attacks
Hacker attacks
Denial of service attacks
Data interception and theft
Identity theft

How Does Network Security Work?
To understand what network security really is, it helps to understand that no single solution protects you from a variety of threats. You need multiple layers of security. If one fails, others still stand.
Network security is accomplished through hardware and software. The software must be constantly updated and managed to protect you from emerging threats.
A network security system usually consists of many components. Ideally, all components work together, which minimizes maintenance and improves security.
Network security components often include:

Anti-virus and anti-spyware
Firewall, to block unauthorized access to your network
Intrusion prevention systems (IPS), to identify fast-spreading threats, such as zero-day or zero-hour attacks
Virtual Private Networks (VPNs), to provide secure remote access

What are the Business Benefits of Network Security?
With network security in place, your network infrastructure will experience many business benefits. Your company is protected against business disruption, which helps keep employees productive. Network security can also help companies meet mandatory regulatory compliance. Because network security helps protect customers' data, it reduces the risk of legal action from data theft.
Ultimately, network security helps protect a business's reputation, which is one of its most important assets.
References
http://www.cisco.com/cisco/web/solutions/small_business/resource_center/articles/secure_my_business/what_is_network_security/index.html?referring_site=smartnavRD

	              
	              Hi Ramin,
I really appreciate your post and found our own experience very interesting, so thank you for sharing it with us.
but I have one question, it's seems like there is a lot of precautions to take into account when hiring a hacker as consultant or employee, do you not think that this will increase the volume of work and monitoring? and it seems also like you will be constantly looking behind you, and expend more time worrying&nbsp;about your own employee(or consultant) instead of worrying&nbsp;about external attacks (hackers).
Best Regards,
Tresor
	              
	              Hiring Hackers as Security Consultants – The Good, The Bad, The Ugly. 
The subject of whether it is ethical to use former hackers to evaluate a network’s security is a topic that is often hotly debated. It’s a paradox to say no one can test the security of computer systems better than former attackers/hackers themselves considering that some security experts also feel this notion is nonsense.
Although the practice has been going on for quite some time, the subject of whether or not you should hire reformed hackers as security consultants has been receiving a lot of attention lately. This seems to be a very sensitive issue, and there are strong opinions on both sides. 
First, let us look at the positive aspects of hiring former hackers as security consultants. The most obvious advantage to hiring former hackers is that they have real world hacking experience. Clearly, some things cannot just be learn from a book. Books do a good job of explaining basic hacking techniques. However, it is safe to say from a hacker’s first-hand experience that every hack is different because every network is different. 
It is rare for a hacker to be able to use a single technique to gain full access to a network. Often, hackers have to combine multiple techniques or apply techniques in a different way than normal to compensate for various network defense challenges. Only someone with plenty of real world hacking experience can efficiently go from using one technique to another as required by the present situation.
Another positive aspect to hiring reformed hackers as security consultants is that staying up with the latest security exploits and countermeasures is a full time job. In most companies, the IT staff has an acceptable level of security knowledge, but they must focus most of their attention on the day to day responsibilities of keeping the network up and running. A good security consultant focuses almost solely on security and consequently has a level of security knowledge that goes far beyond that of most other IT professionals.
The Negative Aspects of Hiring Hackers
As we have discussed some of the positive aspects to hiring former hackers as security consultants, Let us look at the negatives. By far the biggest negative is the question of trust. The main premise of security is deciding who you trust and then locking out everyone else. When you hire a former hacker as a security consultant, you are basically trusting the sanctity of your network to a former criminal. If you think about it, that’s a lot like letting someone who was convicted of burglary stay in your home when you aren’t there. If you are concerned with your network’s security, it sounds crazy to trust it to a criminal.
As you think about how much you trust a former hacker, you must also consider the impact that a decision to hire the person will have on your customers and shareholders. What would your customers think if they knew that you were using a former criminal to test the security of a database that contains their credit card number?
One other negative aspect to using ex-hackers as security consultants has to do with the way that many security consultants operate in general.
A security consultant’s job is not to secure your network, but rather to make your company completely dependent on them. Security consultants will typically offer you a free evaluation of your network’s security. Once the evaluation is complete, they will show you a report documenting thousands of potential vulnerabilities. They try to make it seem as though it is urgent for you to secure your network. However, they make it clear that your IT staff shouldn’t be trusted to patch the vulnerabilities since they weren’t even aware that the vulnerabilities existed. As a part of the sales pitch, the consultant will discuss some of the more high profile hacks that have been in the media lately. They will compare those hacks to your network. The consultant will probably even tell you how the company that got hacked is teetering on the edge of bankruptcy because they have lost customers and because the hack did so much internal damage.
Then there's the question of whether the hacker really is&nbsp;completely&nbsp;reformed. Maybe he's sworn off cracking passwords and writing viruses, but will he be tempted to dip into your company's confidential files and take a look around, just because he can? Can you trust him not to illegally download copy protected music and movies or install spyware on computers on your network in his spare time? If he gets bored, might he decide to peruse the personnel files just for fun, or whip up a "harmless" little practical joke script to turn everyone's desktop wallpaper into a graphic of the blue screen of death?
What if the hacker has not reformed at all, but has merely learned to play the game in a more sophisticated way? Social engineering is the art of manipulating people, rather than or in addition to code, to gain entry into a network or system. It is found it interesting when supposedly reformed hackers, who themselves go around preaching the dangers of social engineering, are then hired by companies in spite of the fact that they're basically telling you that what they're doing now could easily be another big social engineering ploy. Posing as a reformed hacker/consultant is a great way to gain access to networks - much better than pretending to be a phone company employee or someone from "headquarters" that you're not. 
Conclusion
Even though it makes sense that hiring former hackers to evaluate your security is worthwhile, it’s important to take some steps to avoid getting ripped off and to prevent a company’s security from being exploited. Here are a few things that you can do to keep from being victimized by a security consultant.

Don’t completely outsource your security needs. Completely outsourcing security will cost your company a fortune and is unlikely to make your network any more secure than if you just had your security evaluated by a consultant a few times a year.
Don’t give a security consultant anything that you don’t have to. For example, never give a security consultant the Administrative password. Remember that you are paying the consultant to look for holes in your network. If major security holes exist, the hacker might be able to get administrative access on their own, but you shouldn’t just hand it to them.
Use a variety of consulting firms, and let the consultants know that you will not be using them exclusively. Different consultants have different skill sets and it is likely that one consultant will catch a security problem that another missed. This doesn’t mean that the consultant who missed the problem is incompetent. It just means that the two consultants have different skill sets. Another reason for using multiple consulting firms is that it prevents you from being put in a position in which your company is completely dependent on a specific firm.
Finally, decide how much protection your network really needs. No computer system is ever completely secure, and your company can spend an astronomical amount of money pursuing total security. To avoid spending too much money on security consultants, set realistic goals of what you want the consultant to do for you.

References
http://www.sfgate.com/business/article/Pros-cons-of-hiring-ex-criminal-hackers-2621914.php
http://www.techrepublic.com/blog/it-security/hiring-hackers-the-good-the-bad-and-the-ugly/
	              
	              Dear Babatunde,
Lovely post indeed. We seem to have referenced some articles that are similar, and I couldn't agree with you more. However, there's also the issue of not being able to verify the 100% sanctity of this hacker that is being considered for a consultancy role. There's a scenario where they might not have been completely reformed, or not even reformed at all.&nbsp;

I personally consider this highly paradoxical. I say this because it would also be fair to look at this from the angle of vertical markets i.e. what would be the perception of the a vendor's customer base i.e. a bank that has it's fraud mitigation and maybe its anti-money laundering solutions being managed by an ex-hacker. This information may not settle well with end customers and there may well be threats of doing their businesses elsewhere.
Thanks for such an informative post.
Kind regards,
Chika Acha
&nbsp;
	              
	              Hi Tresor,
Yes, you raise a valid point, which I would like to make some comments on.
We have seen IT advancements result in the removal for certain jobs, so any job creation is a positive thing for me, especially from an economic perspective.&nbsp;
The other thing I would look to highlight is, any self respecting cyber security team will need to consider risks and exposures internally, as well externally. So, I would expect this to be an extension of that function.
Interestingly, I was doing some research last evening, and whilst not related to 'Hacking', it is related to security of data and information, in which the NSA whistle-blower Edward Snowden, who incidentally had enrolled on a University of Liverpool Masters in Computer Security in 2011, is an example of a threat within.&nbsp;
I refer to this irrespective of our own views on whether he has done right or wrong, ultimately he has breach security and exposed practices which are forcing us all to reconsider policy and oversight.
This includes internal employees whether they're a 'Hacker' or not.&nbsp;
Best Wishes,&nbsp;Craig
References
Waddington, M. ed. (2013).&nbsp;Prism whistleblower Edward Snowden trained with University of Liverpool in cyber security&nbsp;[Online]. Liverpool Echo. Available from:&nbsp;http://www.liverpoolecho.co.uk/news/liverpool-news/prism-whistleblower-edward-snowden-trained-4313851 (Accessed 12 October 2014)
Durbin, S. ed. (2014).&nbsp;Security Think Tank: ISF's top security threats for 2014&nbsp;[Online]. Computer Weekly. Available from:&nbsp;http://www.computerweekly.com/opinion/Security-Think-Tank-ISFs-top-security-threats-for-2014 (Accessed 12 October 2014)
Nusca, A. ed. (2014). Three types of cybersecurity threats (and two employees) to worry about most [Online]. Fortune. Available from: http://fortune.com/2014/07/16/cybersecurity-threats-ciso/ (Accessed 12 October 2014)
	              
	              Hi Belinda
Love that quote. What a great man
There is a similar quote from a well known book that is also used for “Business Strategy” and has the same meaning:
“If you know the enemy and know yourself, you need not fear the result of a hundred battles. If you know yourself but not the enemy, for every victory gained you will also suffer a defeat. If you know neither the enemy nor yourself, you will succumb in every battle”.&nbsp;Sun Tzu, The Art of War
Best Wishes, Craig
 
References
Tze, S., Trapp, J. (Translator) (2014). The Art of War. Bilingual ed. Chartwell Books.
	              
	              Hi Augusto,
Thanks for the post.&nbsp;
The Snapchat example is a clear warning to all of us - think twice before you upload anything to the internet, as once you upload it, it is there forever. As Keren Elazari says on the Ted Video, "the internet doesn't like it when you try to remove things from it!"
Best Wishes, Craig
Reference
Elazari, K. (2014). Hackers: The Internets Immune System [Online]. TED Videos. Available from: http://www.ted.com/talks/keren_elazari_hackers_the_internet_s_immune_system (Accessed 9 October 2014)
	              
	              Hi Chika
Great point.
When it comes to trust and recruitment of ex "Hackers", that is exactky it!... &nbsp;"the Hacker Paradox"&nbsp;
Best Wishes, Craig

	              
	              Hi Terry,

Thanks for the reply to my post.
Your right, in the companies shouldn't overlook trust and loyalty. One of the things we have found where I work to promote that fact is simply because many things are locked down form a procedural aspect for example to get a piece of software installed on your local machine it can sometimes take weeks even though we have the software and licenses available it's physically booking a trusted support worker to come to your machine and install the software.

In the past when we were trusted to do installations ourselves this was not an issue because we had a LAN drive which contained all the software we used and we could simply request a license and then install it ourselves within a day. But now because we don't have local admin rights etc and the rules/policy around who can and cant install software on the system draws this out.

I also agree when you say do they really have the power and who can pay the most for zero-day attacks. The essence of the threat is like that. I think however what I meant to write, I may not have explained it was that you and I can't make national policy or prosecute any hackers personally.
We are the consumer like many others, whereas governments do have the power to introduce specific security policies that business has to comply to. Also they are the ones who will have the power to prosecute people who do hack systems and bring them to account. Which in turn shows the hackers out there that there are people trying to catch them as a deterrent.

ta
Chris

	              
	              Hi Bo,

thanks for the info,

Yeah that's interesting to see that Microsofts statistics show a decrease in recent years but Symantec shows and increase. I'm not sure how to read into this as a trend globally but it may link into a previous weeks topic when I forgot who but students here talked about actually how they get the statistics and what sites they monitor and in what countries. It could also be a trend with the hackers themselves by moving their targets to other areas or systems with these types of attacks.

Not sure but interesting all the same.

Thanks
Chris


	              
	              Hi Craig,
I'd go with your point of not allowing any hacker to either work remotely nor bring or take any device in or out of the working environment.
Thanks.
	              
	              Hi Tresor
Most white hat hackers look into computer security, they look to drum up business. I think nearly every independent hacker considers the same simple plan: Scan lots of businesses, find weaknesses, and offer fix-it or find-it services. Every website and computer network on the Internet has existing, publicly accessible weaknesses and flaws. I feel they're just highlighting them to the owners. If not for the issue of “TRUST” between employer and the hacker (employee/consultant) which could be sorted out with part of what Raman has posted, I think the rest is a proactive service for the good of the community.
Best Regards, Babatunde
	              
	              Hi Robin
Thanks for your post.
I am completely fascinated by this subject matter – both researching the subject, and beginning to see the level of creativity and thinking employed by “Hackers”
One thing I came across this morning, was the example described by Department of Homeland Security, whereby a manufacturing firm vital to US economy suffered a prolonged attack – this company had a wider attack surface, having undergone corporate acquisitions. To your point, “Hackers” are wise and smart to all potential vulnerabilities and this is a great example of crucially needing to place your Cyber Security plan central to any acquisition and integration strategy, irrespective of your existing confidence levels.
Best Wishes, Craig
Reference
Sternstein, A. ed. (2014). Hackers hacked critical manufacturing firm for months [Online]. NextGov - CyberSecurity. Available from: http://m.nextgov.com/cybersecurity/2014/10/dhs-attackers-hacked-critical-manufacturing-firm-months/96317/ (Accessed 13 October 2014)

	              
	              Hi Craig.
Good response on the network systems security risks topic. I always have it that in the area of computer security, there are so many definitions that one probably needs a 'drill sergeant' or a 'fork lift' to help in crystallizing the different terminologies that are in play. For instance getting the distinctions between the terms threat, attack, vulnerability, risk,risk exposure, risk tolerance... (and all the other 'risk' variants in risk management)&nbsp;can be a walk in a minefield.
In your post you have given a good account on various types of attacks that can occur in a network setting. Working backwards from an attack, is the task of establishing what possible vulnerabilities existed that were exploited by the attacks that you have just mentioned. This in my opinion is what is often the tricky bit because performing a root cause analysis to identify where a weakness exists in a system is sometimes like searching for a needle in a haystack. In many instances, the extent of the attack (as has been witnessed in the many electronic identity theft cases), the source of the attack have to be established in order to getter a better view of the vulnerabilities that exists before a patch can be applied.
This is often compounded by the fact that in any network security system, there are many existing layers (as also displayed in your OSI model)with different levels of protection inplace. This makes finding out where the weaknesses lie in a system quite a challenge.
Wikepaedia for instance defines a&nbsp;zero-day&nbsp;attack or threat as an attack that exploits a previously unknown vulnerability in a computer application, one that developers have not had time to address and patch.&nbsp;http://en.wikipedia.org/wiki/Zero-day_attack&nbsp;
&nbsp;
Joseph
References:
Rufi , A. ‘Network Security 1 and 2 Companion Guide', Chapter 1: Vulnerabilities, Threats, and Attacks’, Oct 5, 2006 by Cisco Press. Available at http://www.ciscopress.com/store/network-security-1-and-2-companion-guide-cisco-networking-9781587131622 , download,&nbsp;http://ptgmedia.pearsoncmg.com/images/1587131625/samplechapter/1587131625content.pdf,&nbsp; Accessed 12/10/2014





	              
	              Hi Craig,
Wow, Edward Snowden enrolled in UoL 3 years ago! How many degree is that from us?! According to the reference I have, it's the online pogram he enrolled.
I'm not sure how good a hacker he might be, but what he did has nothing to do with his hacker title.
br, Terry
Reference:
"U.S. Fears Edward Snowden May Defect to China: Sources". ABC News. June 13, 2013. p. 3.

	              
	              Hi Anthony,
There is a quote associated with Lyndon B. Johnson who said something politely translated to the effect that it is better to have a trouble maker in your tent causing trouble for others outside, as opposed to having him on the outside causing trouble for you.(Source/Notes: On FBI Director J. Edgar Hoover, as quoted in The New York Times (31 October 1971) http://izquotes.com/quote/241192)
I submit that this is the likely scenario with companies hiring the very people who hacked them. The pro side of it is that in the event that the company acquires the services of a reformed hacker, then they are likely to benefit from the ingenious Jerkyl and Hyde (dual personality) of the individual that they have just hired. However, in the event that this new hire becomes disgruntled, they could easily hold the company at ransom because of their added knowledge of the inner workings of the company with the added benefit of inside knowledge. 
Apple co-founder Steve Wozniak takes a good view of hackers and is quoted giving an advisory to hackers to “have the highest ethics and to be open and truthful about things and not hide something” (Livingstone, J. ’Founders at Work’, 2007).&nbsp; It is noteworthy that in 1990, he joined Mitchell Kapor in establishing the Electronic Frontier Foundation, an organization that provides legal aid for computer hackers facing criminal prosecution http://www.biography.com/people/steve-wozniak-9537334#critique-on-jobs . Previously Steve Wozniak and Steve Jobs, the Apple Inc. co-founders were early "phone phreakers" who used flaws in the telephone network to make free calls http://www.bloomberg.com/slideshow/2012-04-18/famous-hackers-then-and-now.html#slide10 . 
So with subtle minds like those of the duo above, it is fair to say that there is a positive side to hacking, and one that could lead to other great inventions for a company.
&nbsp;References 

Livingston, J. ’Founders at Work: Stories of Startups: Early Days' Jan 2007, &nbsp;p 55 Kindle edition)
Steve Wozniak Biography, Available at http://www.biography.com/people/steve-wozniak-9537334#critique-on-jobs, Accessed 13/10/2014
Electronic Frontier Foundation, Available at https://www.eff.org/about, Accessed 13/10/2014
IZ quotes, ‘Famous Quotes - Lyndon B. Johnson Quote’ http://izquotes.com/quote/241192
Robertson, J. ‘Famous Hackers: Then and Now’ Bloomberg Apr 19, 2012, Available at http://www.bloomberg.com/slideshow/2012-04-18/famous-hackers-then-and-now.html#slide10 


	              
	              Hi Terry.
The understanding I have is that Edward Snowden in 2010, while working at Dell as a National Security Agency contractor,&nbsp;had received a certificate as an ethical hacker (http://mashable.com/2013/07/05/snowden-ethical-hacker/) . So it's possible that using this knowledge, he was able to gain access to confidential files that would not ordinarily be under his purview. The nature of the files were classified so it's most unlikely that those were obtained during day to day routine duty operations.&nbsp;
Joseph
References
Bicchierai, L. ‘Edward Snowden Trained as an 'Ethical Hacker’, Mashable, July 2013, Available at http://mashable.com/2013/07/05/snowden-ethical-hacker/ , Accessed 13/10/2014

	              
	              Hi Craig
&nbsp;&nbsp;&nbsp;&nbsp; Good call out. We always tell our customers to do a thorough&nbsp;vulnerability assessment and Penetration testing in the company they are acquiring and to also preform ongoing&nbsp;vulnerability assessment&nbsp;Penetration testing. This will ensure a secure perimeter&nbsp;for the corporation on an ongoing basis.
robin
	              
	              Good post Babatunde,
In addition to what you have suggested in your conclusion, a background check on anyone you hire would be invaluable. This is just to get a sense of what makes this person tick. Trust is a big issue, but I'd imagine that it is also extremely difficult under such circumstances to fully know whether a person is trustworthy or not and only time will tell. For a serious hacker, it may be imaterial whether you grant or restrict certain access to him, just the fact that they are now working from the inside means that theywould now have additional access to the inner workings of your systems.
The company is probably hiring them because they are very good at what they do, which incidentally includes using their tools, techniques and know-how to by-pass your security system and gain access to their hearts content. They would by then have good understanding of all the routine protocols and in time would have identified existing vulnerabilities to be exploited.
Joseph
	              
	              hi Terry,
Yes it is, human life is always surrounded with threats but we can do our bit in minimizing it. If we check our networks daily for threats and put in good security mearsures to secure our networks we could reduce the threats on our systems. Take for example you have a house and you put in a brick fence around it to safeguard you from armed robbers. Next you put in an electronic wireing system to the&nbsp;brick fence that electricutes anything around it. You also&nbsp;put in place bullet proof doors in the house with secured locking systems. As far as you check all this measures on a daily basis to see if they are all functioning right, the possibility of you being attacked would be low. But when you relax and say i have it all and maybe&nbsp;your lighting system fails on a particular day then thats a loophole for an attack.&nbsp;
Regards
Martins
	              
	              Hello Joseph,
I like the choice of word in your post- "what makes this person thick". when hiring a hacker for our companies its a welcome idea but as you mentionned having a background check is necessary as we would still not trust he or she 100%. Reasons would be from not sure if he is transfering your companies information outside or leaking your strenght to your competitors etc.
Study whoever you employ to secure your networks.
Regards
martins
	              
	              Hello Everyone,
Can you recall and recount any story of a high-profile computer systems security breach, in either the public or private sectors, over the last decade or two? &nbsp;What was the impact of that security breach on stakeholders and the public, and how could the breach have been prevented?
Anthony
	              
	              Hi Chika and Craig

Good Point!

The challenge here for banks and other financial institutions including the ones that act as third parties to test the security of systems is how to find the best hackers. Many recruiters say there is a dearth of talent. While hiring privately educated graduates might be a good way of tapping into financial roles, it is unlikely to recover the full range of coders many of whom are still teenagers and some have been involved in fraud or other criminal activity.

Some of the most talented hackers will not be interested in working for a financial institution and it is possible they would not have the interpersonal skills or clean records to do so even if they wanted to.
Best Regards, Babatunde


Reference

Paul H. (2014) Bring in the geeks: Banks must adapt their hiring in cyber battle [online] Available from: http://www.efinancialnews.com/story/2014-08-11/banks-must-adapt-their-hiring-cyber-security-battle?ea9c8a2de0ee111045601ab04d673622 (Accessed 13 October 2014) 
	              
	              Hi Dr. Ayoola,
There are way too many (sadly). But if I have to pick one security breach, then I will go for the Sony PlayStation on in 2011.
The impact of this particular breach was massive, perhaps the very first massive one in IT (not the last, unfortunately). First was the number of people affected: 77 million user accounts. Second was the amount of data gathered: names, addresses, emails, login information, credit card information and more. Third was the delayed response and information from Sony (one week after the attack) (Noer 2011). Forth the amount of money that the security breach cost: &nbsp;Sony acknowledge spending $171 million alone for “fixing” and “cleaning” up, $400,000 from a fine in UK and non-published amounts for Sony account credits (McMillan 2013, Sangani 2011).
Sadly, as many security breaches around, Sony could have avoided it. Riley and Vance (2011), based on information from McDanel (2011), speculate that the attack originated thanks to one of the Sony server logs publicly available containing insight into Sony places (Rikey, Ashlee 2011). Sony never publicly disclose the root cause of the breach, but it is easy to guess by the fine inflicted by the UK and the evidence from McDanel that Sony had responsibility over it. The fact that hackers were able to obtain login and credit card information also questions Sony encryption habits (very weak, if at all).
I think that many companies today are not different than Sony was in 2011, and yet they can prevent such breaches if proper encryption is used and confidential data is protected. If I have to guess, the problem is that companies are still operating Infrastructure Operations with the same budgets (and often, less) than 10 year ago, however the security stakes have increased exponentially and to address them there is a need for more resources.
Best Regards,
Augusto
&nbsp;
References
McMillan, G. (2013) Sony fined almost $400,000 for 2011 PlayStation security breach. Available at: http://www.digitaltrends.com/gaming/sony-fined-almost-400000-for-2011-playstation-security-breach/ (Accessed: 13 October 2014).
Noer, M. (2011) Sony Response to PlayStation Security Breach Abysmal. Available at: http://www.forbes.com/sites/michaelnoer/2011/05/04/sony-response-to-playstation-security-breach-abysmal/ (Accessed: 13 October 2014).
Rikey, M. &amp; Ashlee, V. (2011) Sony: The Company That Kicked the Hornet's Nest. Available at: http://www.businessweek.com/stories/2011-05-11/sony-the-company-that-kicked-the-hornets-nest (Accessed: 13 October 2014).
Sangani, K. (2011) 'Sony security laid bare', Engineering &amp; Technology (17509637) 6(8), pp. 13 October 2014-74-77 doi: 10.1049/et.2011.0810.
&nbsp;
	              
	              Agreed.&nbsp;
Hackers will not stop me from saving money and buy a Tesla. Even if it is for carshing it due to hacking. When I was a kid in Colombia there was a period where there were bombs every day and that didn't stop any of us to go out and do things.&nbsp;
We shoul not be scared of hackers, but we must also protect from them. And be careful.
Best Regards,
Augusto.

	              
	              
Very true!
I would generalize it as&nbsp;"lack of configuration or misconfiguration" of any kind, whereas is hardware, software or programming related.&nbsp;  
When you look at the recent Snapchat&nbsp;hack “The Snappening”, it is clear that since the Snapchat servers were not hacked but the information was hacked via 3rd party sites or Apps, there must be some configuration or API that is “bad”. Who knows, even unencrypted communications? (Cook 2014)
Misconfiguration is a bad habit though. How many of you change your home internet router default password? Perhaps you are still using admin/admin? How many non-technical people will have those default passwords?
Best Regards,
Augusto
References
Cook, J. 2014, Hackers Access At Least 100,000 Snapchat Photos And Prepare To Leak Them, Including Underage Nude Pictures, Business Insider.
&nbsp;

	              
	              
Hello Anthony,&nbsp;
&nbsp;
In the recent years,&nbsp;security breaches are turning out to&nbsp;be&nbsp;the master&nbsp;strategy of&nbsp;cyber warfare.&nbsp;According to me,&nbsp;the worst&nbsp;and&nbsp;the&nbsp;most massive security breach was the&nbsp;Stuxnet&nbsp;virus designed&nbsp;to stunt Iranian&nbsp;Nuclear Development&nbsp;at&nbsp;the&nbsp;Bushehr nuclear plant!&nbsp;(Bruce&nbsp;Schneier, 2010).&nbsp;
&nbsp;
The&nbsp;Stuxnet&nbsp;virus was discovered in June 2010 as an aggressive computer virus that was created to stop development of Iran's Nuclear Power program by breaching its information systems and destroy key codes.&nbsp;After its release, 5 countries reported viral infections in addition to Iran. The virus was designed to&nbsp;target&nbsp;Siemens industrial software&nbsp;and conveniently&nbsp;spread through Microsoft Windows operating systems.&nbsp;It was discovered that the virus was developed&nbsp;to try and slow Iran's&nbsp;venture into&nbsp;nuclear development&nbsp;(Lewis University article, 2014).&nbsp;
&nbsp;
Stuxnet is an unmistakable&nbsp;example of the next&nbsp;big&nbsp;threat to national security: cyber warfare. The digitalization of basically every aspect of national security elements&nbsp;-&nbsp;including military personnel files, launch code sequences and prison structures&nbsp;etc.&nbsp;-&nbsp;largely extends&nbsp;the scope&nbsp;of&nbsp;security threats.&nbsp;
&nbsp;
As described by&nbsp;Bruce&nbsp;Schneier, 2010, "Stuxnet&nbsp;is an Internet worm that infects Windows computers. It primarily spreads via USB sticks, which allows it to get into computers and networks not normally connected to the Internet. Once inside a network, it uses a variety of mechanisms to propagate to other machines within that network and gain privilege once it has infected those machines.&nbsp;What&nbsp;Stuxnet&nbsp;looks for is a particular model of Programmable Logic Controller (PLC) made by Siemens.&nbsp;These are small embedded industrial control systems that run all sorts of automated processes.&nbsp;These PLCs are often controlled by computers, and&nbsp;Stuxnet&nbsp;looks for&nbsp;the&nbsp;Siemens SIMATIC&nbsp;WinCC/Step 7 controller software&nbsp;in particular. It infected 50,000 windows systems and&nbsp;even the&nbsp;Indian INSAT-4B&nbsp;satellite".  
One can imagine&nbsp;the&nbsp;sheer power of this breach and&nbsp;the&nbsp;fatalistic&nbsp;effects on&nbsp;a nation, and its&nbsp;billions of people,&nbsp;if&nbsp;the viral infections&nbsp;wouldn’t have been controlled. Prevention is&nbsp;very&nbsp;tricky in such scenarios as these breaches&nbsp;will&nbsp;be&nbsp;impacting&nbsp;all the&nbsp;core systems&nbsp;of an organization/government;&nbsp;as&nbsp;in the case of&nbsp;the&nbsp;Stuxnet&nbsp;virus.&nbsp;
&nbsp;
References:  
Lewis&nbsp;University -&nbsp;http://online.lewisu.edu/resource/engineering-technology/top-3-high-profile-information-security-breaches.asp&nbsp;
Bruce, 2010 -&nbsp;http://www.forbes.com/2010/10/06/iran-nuclear-computer-technology-security-stuxnet-worm.html&nbsp;
&nbsp;
Best regards,&nbsp; Kharavela 

	              
	              Hi Terry and Chris
Microsoft 0day's have been very stable, though falling if you read the numers in percentage :-). Also these are 0day that they have found in Microsoft environments while Symantec, I presume, have a much wider customer group, hence their numbers are higher.
Br Bo

	              
	              Hi Craig and Babatunde Yes, that's really good advice.  Br. Bo
	              
	              Hi Chika
This is a little digress, and just a little remark to your first bullet in your conclusion regarding outsourcing of security. I don't know how it is in other countries, but in faroese/danish laws you cannot outsource security, i.e. you cannot outsource the responsibility, you have to take responsibility, and you should actually do this by going to the vendor and inspect. But then again, this can easily be a problem because; who am I to go to e.g. Amazon, knock on their door, and say "Hi, I'm here to inspect your security". I pracsis we just have to do our risk analysis and "leave" it at that.
Br. Bo
	              
	              Hi Craig,
Thanks for the response.

I know what you mean about hiring hackers. Like you said I was the same. I never quite understood the reason why people would hire hackers, and I don't mean from the skill set point of view. Clearly hackers have skills and most probably many of them are more skillful than the prevention teams countering them. More simply from the moral and ethical side, in that can you ever TRUELY trust a former hacker.?
Its a tough question and I guess it was harder in the beginning to solve that quandary as it was a new dangerous prospect to hire such people but like you said nowadays it is far more accepted and even promoted to hire the people who originally cause the problems.
My question is though, while this outcome for the majority may be good; other posts talk about the different versions or levels of hacker, white hat black hat, whatever the classification or moral compass the hacker has I do worry that because of the trend of hiring hackers now I feel some hackers may see this a a way into the business so to speak. EG "Hey if I can hack the FBI then maybe they will give me a job" type situation which in itself is dangerous for more than one reason. So indirectly the business has fed its own vicious circle if you will.

Despite that though I still think its right to hire certain ones however and I do like the approach they take, after all don't we all have a little devil inside us at times that wants to break the rules once or twice to see if we can get away with it.

ta
Chris



	              
	              Hi Anthony
It’s like Augusto say’s “There are too many (sadly)”.
But what is a high-profile security breach? I don’t think it has to involve money, though in the end it almost ends up with money anyhow.
Account information
When I researched for security risks, I came across two cases:

Report: Hacked Password Behind Compromise of 75m JPMorgan Accounts (Roberts 2014)
Hackers Grab 800,000 Banking Credentials

As far as I can see, the hackers did not get any money. But they got a lot of information’s which potentially can be used to steal money. Even though the articles don’t mention it, I am sure, that both the stakeholders and public loose trust in the system and get nervous. But what can you do as a stakeholder or customer, move your accounts? “Everything can be hacked”.
$45 million
This case I think it’s fair to call a “distributed attach” since it was coordinated through 20 countries. In a few hours, though spread over a few months, thieves took $45 million in ATM scheme. “It was, prosecutors said, one of the largest heists in New York City history, rivaling the 1978 Lufthansa robbery, which inspired a scene in the movie “Goodfellas.”. The thieves got hold of information’s for a few debit/credit cards and they sent to these information’s to their accomplices in the different countries. And at a planned time they all went to ATM to withdraw money:

5 cards, 20 countries, 4.500 ATM transactions, $5 million
12 cars, 36.000 ATM transactions, $40 million

I have unfortunately not found any more information on this case explaining the different effects. But the article writes “It was unclear to whom the hacked accounts belonged, and who might ultimately be responsible for the losses”.
It is extremely hard to prevent these happenings. But in these cases it’s seems sure that the succeeded their hacking because of bad security. The 5 card information’s were fetched from an Indian credit-card company, and according to the article, experts have said, that the security is less secure in such companies compared to banks. The second time, they hacked another credit-card company, but this time it was in the United States. So if the banks have better security, then the credit-card companies should adopt the same security standards. After all, they mostly produce card to be connected to bank accounts.
Conclusion
My conclusion is that everything can be hacked and it’s just a matter of time. But we have to fight to continuously improve the security. And I would really expect credit-card companies to have just as high standards, or even higher than the banks. I could add, that I have worked in a bank where I e.g. handled security and also cards, and I have visited credit-card companies in Denmark, and my impression was that the security was on top.&nbsp;
References
Roberts, Paul. (2014). The security ledger. “Report: Hacked Password Behind Compromise of 75m JPMorgan Accounts”. [Online] Available from: https://securityledger.com/2014/10/hacked_password_behind_compromise_of_75m_jpmorgan_accounts/#.VDlVd_l5OJ0 (Accessed 13-10-2014)
Santora, Marc. (2013). The New York Times. “In Hours, Thieves Took $45 Million in A.T.M. Scheme”. [Online] Available from: http://www.nytimes.com/2013/05/10/nyregion/eight-charged-in-45-million-global-cyber-bank-thefts.html?pagewanted=all&amp;_r=0 (Accessed 13-10-2014)
Schwartz, Matthew J. (2014) Hackers Grab 800,000 Banking Credentials”. [Online] Available from: http://www.databreachtoday.com/hackers-get-800000-banking-credentials-a-7416. (Accessed 13-10-2014)
Secure wishes J
Bo

	              
	              Hi Anthony,
A report by Taylor Armerding (2012) outlines many of the top security breaches of the 21st century.
I remember hearing about the TJ Maxx credit card thefts a few years back, but couldn’t recall the detail. 
Having researched this, it appears that credit and debit card details were stolen back in 2006, by accessing retailer payment systems and installing sniffer programs to capture data on those networks that were unsecured. This was done through a process called “wardriving”, which is searching for wireless networks using software that is freely available, and includes NetStumbler for Windows, or KisMac for Macs. 
This had a massive impact on the public. According to a report by Taylor Armerding (2012), 96 million credit cards were exposed. The credit card information was sold on and used to create new cards to withdraw tens of thousands of dollars at a time from cash machines. 
As highlighted in an article by Robert Macmillan (2008), during 2007 an employee went on to speak publicly about information security concerns he had uncovered and that bad practices were still in place at TJ Maxx. The employee, Nick Benson, went on to say that “it’s hard to sleep at night knowing the same network stores my employee information”. The employee was subsequently fired for violating corporate policy by disclosing proprietary information inappropriately, and not reporting this through the correct channels.&nbsp;The same article suggested that Benson was a frequent poster to computer security discussion groups such as Full Disclosure, and goes by the hacker name, “Cryptic Mauler”
This was a good example of highlighting the vulnerabilities and needing adopt the basic principles of computer security by ensuring any wireless network access points are completely secured through password protections as a minimum. 
There are also a number of other types of security methods that can be employed, as outlined in a very useful 5-step guide by Check Point Technologies (n.d.), namely:

Signal coverage. The strength of your wireless network should be restricted, so that it cannot be detected outside the boundaries of your building.
SSID broadcasting. The Service Set Identifier (SSID) code is attached to packets on a wireless network and used to identify each packet as part of that network. When the SSID broadcasting is enabled on a wireless network, all wireless clients within range can identify it. Conversely, when SSID broadcasting is disabled, the wireless network is not visible to casual users, unless the code is entered in advance into the client's network setting. If you have remote wireless LANs, ensure that the SSID is changed from the default setting and is secured to prevent unauthorised wireless users from connecting. 
WPA/WEP encryption. Encrypted communication will protect confidential information from being disclosed. If the traffic over your wireless network is encrypted, an attacker must decrypt the password before retrieving information transmitted over the network. There are two encryption schemes available, which are Wi-Fi Protected Access (WPA) and Wired Equivalent Privacy (WEP). In practice, only one of them can be used at a time. Regularly changing the encryption key may also help to protect your network.&nbsp;Please note, whenever possible, WPA should be used because it is possible for hackers to decrypt WEP, by using special software. In any event, try not to use WEP for encryption, as it is poor, unsecure, and weak. Use WPA or WPA2, also known as 802.11i, ensuring that users always operate with it switched on. The default is with it switched off.
Key management. Even if encryption is used, if the key to this encryption is not changed often, a hacker might crack it and decrypt the communication. Therefore, the key must be changed regularly.
MAC addresses. Another security option is to implement media access control (MAC) filtering. A MAC address is essentially a serial number unique to each manufactured network adapter. It is a physical address, so that if you restrict access to devices whose addresses you have authorised, you can eliminate many unauthorised wireless access issues. If a wireless access point only accepts connections from known MAC addresses, a potential attacker will need to learn the addresses of legitimate computers in order to access the wireless network.&nbsp;

As a footnote, the TJ Maxx breach was one of the largest identity fraud cases ever (McCullagh, 2010) and was masterminded by Albert Gonzalez. Gonzalez was also behind the Heartland Data Systems breach in 2008, where 134 million credit cards were exposed (Armerding, 2012). Gonzalez is serving 20 years in prison (McCullagh, 2010).
Best Wishes, Craig
References
Armerding, T. ed. (2012). The 15 worst data security breaches of the 21st century [Online]. CSO. Available from: http://www.csoonline.com/article/2130877/data-protection/the-15-worst-data-security-breaches-of-the-21st-century.html (Accessed 14 October 2014)
McCullagh, D. ed. (2010). TJ Maxx Hacker sentenced to 20 years in prison [Online]. c|net. Available from: http://www.cnet.com/news/t-j-maxx-hacker-sentenced-to-20-years-in-prison/ (Accessed 14 October 2014)
McMillan, R. ed. (2008). TJX Staffer Sacked after talking about security problems [Online]. CSO. Available from: http://www.csoonline.com/article/2122737/identity-theft-prevention/tjx-staffer-sacked-after-talking-about-security-problems.html (Accessed 14 October 2014)
Checkpoint Software Technologies Ltd. (n.d.). More than Passwords: Five Rules to ward off Wireless Pests [Online]. Available from: http://www.checkpoint.com/securitycafe/readingroom/perimeter/ward_off_wireless_pests.html (Accessed 14 October 2014)

	              
	              Thanks Chris
Yes, I agree - as Chika has highlighted, the "Hacker Paradox".
I have posted subsequently, suggesting that there needs to be the right level of oversight beyond the traditional employee monitoring - I guess like having functions similar to HMIC (UK: Her Majesty's Inspectorate of Constabulary) or IA (USA:&nbsp;Internal Affairs).
Best Wishes, Craig
	              
	               

In 2014 &nbsp;40 Million Target credit cards got compromised! The CISO (Chief Information Security Officer) of target was blamed for not being Security savvy enough for having have let &nbsp;a third party gained access to Target systems. Customer names, credit and debit card numbers, as well as card expiration dates and card verification values - three-digit security codes - were exposed during the breach, the went through the targets POS (point of sale) &nbsp;infrastructure terminals. 
Analysts predict Smart Card technology (chip in the credit card) &nbsp;could be a good deterrents for major retailers. Note many retailer in Europe have already adopted the Smart Card, US is lagging in the adopting of this technology 
Robin

Reference
http://ems-solutionsinc.com/blog/real-cause-targets-security-breach-lack-ciso/ 
http://www.bankinfosecurity.com/target-breach-what-happened-a-6312/op-1 
http://www.fool.com/investing/general/2014/01/21/targets-security-breach-could-have-been-avoided.aspx 
 
	              
	              Tresor
&nbsp;&nbsp;&nbsp; Your point is valid. But the value you get from having a ex-hacker in your team who has tuned around, is worth the risk. The expereince they bring to the table...Remember the monitoring recomemndation I highlighted in my previous&nbsp;post can all be automated and alerts will be sent to respective IT Risk analysts/Admin who can take appropiate actions.
robin
	              
	              Gents
Though tempting and makes logical sense. You can't discriminate or target an employee with no remote access or no laptop to leave the work environment.&nbsp; Once the ex-hacker is hired in the firm, he has the same privileges as other team members. This is not feasible from HR prospective and would led to discriminations law suites.

robin

	              
	              Hi Dr Ayoola and Colleagues, Interesting enough, when I tried to search for the computer systems security breach that I'm very impressive, they seemed to be hiding from me. Some are just like they had never happened, and it's very hard to find any case study from scholar.google.com or the university online library. Is it some big evil companies manipulating the Internet? I'm especially impressive with the "Epic Windows 98 Login" breach, which actually got worst in Windows 2000 and Window 2000 Server Edition. I could still use it on some servers over the Internet at least in year 2004. The breach is because, in the login window of Window 98, there's a button for help. The Windows help application has many security issues as you can browse and even delete the files.   (image from:&nbsp;http://9gag.com/gag/43525/epic-windows-98-logon-step-by-step-gif) Scary, right? But that's not the really scary part of the story yet. Microsoft did fix the problem on the surface level but didn't solve the root of the problem. The root of the problem is Windows didn't have a correct user privilege model that a user before login had a "system" level right, and if he got access to any application then he can do anything to the system. The breach eventually became something big. In a Chinese (or other none English, I guess) version of Windows 2000 and its server edition, you don't have that help button any more, but you can choose a different IME (Input Method Editor). The IME toolbar will still have a help button. And, geniusly, from Windows 2000, Windows is already very integrated with the Internet Explore. The Microsoft Internet Explore 6 is such an insecure browser that you can run C:\win\system32\command.exe from it. Now you have access to the Windows command line, and your identity is "system". The "system" user is not the administrator, but, I think it's higher. Now you can change the admin password, formatting the hard disk... you name it. The really horrible part of the story is many servers on the Internet was running Windows 2000 server at that time. Things in China got even worse, because most of the Windows 2000 copies are pirate. So this backdoor had been laying there for many years. I cannot find any statistics on the lost caused by this breach. I'm not sure if it's purposefully covered but someone. This breach tells us, hardening (Wikipedia)&nbsp;is very important part of the software development cycle. The unnecessary features should be removed, especially in security-sensitive parts. And having a right user model is the root to the problem, which took Microsoft so many years to learn. br, Terry Reference Hardening (computing). (2014, May 17). In&nbsp;Wikipedia, The Free Encyclopedia. Retrieved 02:31, October 14, 2014, from&nbsp;http://en.wikipedia.org/w/index.php?title=Hardening_(computing)&amp;oldid=609016724
	              
	              Hi Dr. Anthony,On February 8, 2000, Amazon, CNN, Ebay and Buy.com were subjected to Denial of Service (DOS) attacks, while Yahoo was a victim of this same form of attack the day before. A DOS attack can basically attempt to render a computer system or service unavailable. This can be done by simply crashing the network or flooding it with traffic making it appear busy, slowing it down, resulting in it being inaccessible. &nbsp;ImpactThe Yahoo attack reportedly came from “50 different points on the web”, &nbsp;and the website was flooded with one gigabyte of traffic per second, [1]. This rendered the website inaccessible for over three hours. While Buy.com, who were inaccessible for about the same time, was flooded with over 800 MB of traffic per second. Ebay on the other hand was inaccessible for approximately 2 hours. However due to this and other related issues, this downtime resulted in millions of revenue lost. Amazon, was also attacked in a similar manner, however the website was only down for about an hour. While CNN suffered the same faith, resulting in over an hour of inaccessibility of the website. These attacks and similar attacks to well known websites in that year, &nbsp;may have resulted in millions of loses as a result of equipment damage, lost credibility, and downtime, to the companies affected.Possible preventionInstalling of filters to filter out network traffic and block unwanted or inappropriate IP addresses is a good start to preventing this form of attack. It is also important to implement proper network designs or systems; having several servers or computers systems at various locations, thus making it harder for an attacker to bring the entire system down and reducing the impact of the attack. Other forms of prevention includes: running of few services for better manageability and the likelihood of less services being attacked, update patches regularly, intrusion detection systems, using scanning software and bandwidth limitations. &nbsp;It is also important to keep abreast with technological trends so that you can be aware of new forms of attacks and possible ways of keeping them at bay.References:Greisler, D. S. &amp; Stupak, R. J., Handbook of Technology Management in Public Administration, pages 581[1] and pages 582 [Online]. Available at:http://books.google.gy/books?id=IwVYYuT3sqgC&amp;pg=PA581&amp;dq=attacks+on+Yahoo,+eBay,+Buy.com,+CNN+and+Amazon&amp;hl=en&amp;sa=X&amp;ei=6nc8VJaoJfHGsQT8gYK4Aw&amp;redir_esc=y#v=onepage&amp;q=attacks%20on%20Yahoo%2C%20eBay%2C%20Buy.com%2C%20CNN%20and%20Amazon&amp;f=false &nbsp;(Accessed on October 13, 2014)Russel, R. &amp; Bidwell, T., Hack Proofing Your E-commerce Web Site: The Only Way to Stop a Hacker is to Think Like One, Pages 63-66, &nbsp;[Online]. Available at:&nbsp;http://books.google.gy/books?id=h2yhIjn0LoMC&amp;dq=attacks+on+Yahoo,+eBay,+Buy.com,+CNN+and+Amazon&amp;source=gbs_navlinks_s (Accessed on October 13, 2014)Cole E. (2001), Hackers beware: defending your network form wiley hacker, Pages 269-273, &nbsp;Publisher: New Riders Publishing. ISBN: 0-7357-1009-0
	              
	              Hi Dr. Anthony,
My colleagues have already thoroughly discussed the pros and cons of hiring hackers, so I will not add to that part of the discussion. However I will like to add a significant item to the discussion and that is taking into account the characteristics of hackers.
Inspire of all the measures an organization can employ to ensure that a hired hacker only do that which he is contracted to do and remain true to the terms of his contract, we must remember that most hackers are socialize a little differently from what is considered normal and may not conform to a work environment and the tenets of a structured job.
In the Misha Glenny TEDx talks "Hire the Hacker!"(TED, n.d.), he makes mention of the UN lead research initiative the "Hackers Profiling Project" that is researching the characteristics, abilities and socialization of hackers, the findings of this type of research will guide organizations in the way they can engage the hacking community in formal enterprise activities. He further went on to profile a few well known hackers, all of which have similar characteristics, watch the video "Hire the Hacker!", referenced below.
&nbsp;
References:
TED (n.d.) Misha Glenny: Hire the Hackers! [Online]. Available from: http://www.ted.com/playlists/10/who_are_the_hackers.html, (Accessed: 13 October 2014).

Regards,

	              
	              Hello Babatunde,
If you take that approach, then you might as well prepare for a disgruntled employee (an ex-hacker at that :)). With such restrictions, you would be creating the very same platform that every hacker relishes. I'd say, go the route of doing adequate background checks in ensuring the sanctity of the resource in question, and ensure their job descriptions in no way spill beyond what is required.&nbsp;

Thanks
	              
	              Hello Bo,
I guess it pretty much differs from mine, and probably some other countries within Europe and America. I guess it also depends on what type of security that is being referred to here. But regardless, we must understand that security like any other aspect of an enterprise infrastructure hardly forms the core part &nbsp;of the business in other words, a bank's core business is banking, and not security so it would not be such a bad idea to consider outsourcing aspects of your business that do not make up the core....but are still major business drivers. What would normally happen is, companies would invite 3rd party vendors to conduct security assessments i.e. penetration tests etc. just to test the vulnerability of their networks, after which recommendations are made, and systems integrators &amp; Originial Equipement Manufacturers (OEMs) are &nbsp;invited to bid for the upgrades et al.
Once again....this exercise cannot be conducted by the bank itself because it's really not core to their business hence the need to outsource the security assessment, or even outsource the security services as "Managed Security Services".
My 2 cents.
Brgds,
CA&nbsp;

	              
	              Hi Babatunde,
A very lovely post indeed. I would just like to talk about some experiences I have had my past life where my customers have had to make major investments in upgrading their security infrastructure. With the era of BYOD, one important aspect of network security we cannot afford to overlook is network access or admission control. Before a mobile device is granted access into the network via VPN or what have you, it's imperative to confirm the device meets the company's compliance policies i.e. patches, ant-virus software etc. before being granted access. It means that your security system must be robust enough to do device profiling, quarantining, compliance updating etc.&nbsp;
	              
	              Hi Craig,
I totally agree with you on this one.&nbsp;
	              
	              5 Million Gmail Usernames, Passwords Stolen
Nearly five million usernames and passwords associated with Google Gmail accounts were hacked and leaked on a Russian Bitcoin security forum.
According to a Tweet from Peter Kruse, a Danish cybersecurity expert, the&nbsp;data&nbsp;likely originated from a number of data breaches, not just one. Most of the passwords were more than three years old, he added.
Even though the information appears to be outdated, security experts recommend that people regularly update their passwords in the event of such breaches. They also suggest that Gmail users take advantage of the two-factor authentication system, which offers an added layer of security.
Google: ‘No Evidence of Compromise’
Gmail and other Google services have been the target of numerous hacking attacks in recent weeks and months. We reached out to Google to learn more about this latest incident.
“The security of our users' information is a top priority for us,” a Google spokesperson said. “We have no evidence that our systems have been compromised, but whenever we become aware that accounts may have been, we take steps to help those users&nbsp;secure their accounts.”
Other recent high-profile hacking incidents include a malware attack on Salesforce.com users, a security breach on JPMorgan Chase’s computer systems, the publication of nude photos stolen from a number of celebrities' iCloud accounts, and payments-related security breaches at Home Depot, Goodwill and Target.
In the wake of Tuesday's leak, many news sites recommended that Gmail users check IsLeaked.com to see whether their personal information was among the data that had been stolen. By midday Wednesday, the IsLeaked.com Web site was unavailable, presumably overloaded with visitors attempting to check their account information.
Security Cat-and-Mouse
Like other tech companies, Google must play a non-stop game of cat-and-mouse to stay ahead of spam, hacks and other security breaches. After rolling out support for non-Latin characters in Gmail last month, for example, Google announced it would begin rejecting emails with combinations of letters determined to be suspicious under the Unicode Consortium's "Highly Restricted" specifications.
According to Google insiders, there are no indications that Google's internal systems were broken into or otherwise illicitly accessed in this latest data leak. Instead, such breaches could be the result of someone stealing usernames and passwords from malware-infected computers. That would explain why the list published Tuesday appears to have been pulled together from a number of older lists that had been assembled over time.
Reference
Shirley Siluk / Sci-Tech Today (2014) 5 Million Gmail Usernames, Passwords Stolen. Available at: http://www.sci-tech today.com/news/5_Million_Gmail_Passwords_Stolen/story.xhtml?story_id=01000180973M

	              
	              Hi Ramin
I think a best approach is to simply change passwords (even if you think he didn't have them) and make sure strong intrusion detection/prevention controls are in place once the hacker leaves or when his contract work is over
Regards, Babatunde
	              
	              Hi colleagues,
We all making great points here but one thing I would want to point out is that hackers have friends. These friends are likely also hackers. Probably the one you hire may talk to a friend or two, but he also knows these people. He probably doesn't trust them any more than you do. If he values his relationship with his employer at least as much as his friendship, you are well and thoroughly protected, and the shared expertise of his friends will be at your disposal.
What good is that? Okay, you have hired "Babatunde" who is God's gift when it comes to network intrusions. How are you on the desktop or in physical security? These areas may not be Babatunde's areas of expertise, but he might know "Tresor, Craig, and Ramin”, who are quite good in these areas. Be ready to throw contracts at these friends of his if you want to be truly secure, but above all listen and act on as many recommendations as you can. By doing so, and letting them see the fruits of their labor, you will earn their respect and the respect of the community. This is a powerful and valuable asset. Hopefully you caught the tone of this missive. If it seems that hackers value relationships more than enforced duty or money, you got the right idea. They have a completely different ethical system than the average cog, and this is something that cannot be changed.
nb: hope no worries about the names mentioned as it's just for an example 
Best Regards, Babatunde
	              
	              Hi Robin
You raise a really interesting point.&nbsp;
I guess initially, I probably wouldn't look hire the "Hacker" as a direct employee to the company. I'd hire the "Hacker" on a consultative basis, with very strict contractual obligations.
Also, I'm not so sure I actually subscribe the the view that you can't monitor screens etc. Of course, under normal working conditions I do accept this is the case and this requires employee consent. However, Cyber Security is, and should be very different.
I guess my thinking is based on, and as an example, those employees who perhaps work in bank counting and cash functions who all have cameras all over them. In principle, I am not sure I see this as any different to monitoring screens. Each employee has to sign up to this working practice, which may be different to other employees in other parts of the same business, but who do not handle cash.
Taking this same principle, I would suggest that anyone in the Cyber Security function should be expected to sign up to specific agreements, which may differ to other employees in other parts of the business, and who are not in Cyber Security.
All of this is my opinion, and&nbsp;I am presuming there would be occurences when this is accepted and not deemed as discrimination.
My perspective is that if you want to work in Cyber Security, you should accept and sign up to a whole new set of rights and operating practices. I'd also expect that employees and&nbsp;consultants are compensated for this through role-specific development, opportunities, reward and remuneration.&nbsp;
You have highlighted a really interesting component to the management of such areas. I'm going to research this a little more as my thinking is entirely theoretical, and not based on specific facts.
Thanks for raising such a great point.
Best Wishes, Craig
	              
	              Hi Craig,
A good post from Robin but I think it all point fingers at the HR departments and hiring departments. Some are not capable of hiring good security people. They don't even know where to begin. Out of desperation they hire "hacker criminals" because they don't know where else to look. They may have superior security people already within their staffs in a different role, but it doesn't show up on their CV. It’s not really about cyber-crooks, it’s about the ineffectuality and incompetence inherent in the standard HR department of many companies.
&nbsp;
Best Regards, Babatunde
	              
	              Yes, you are right - it does not necessarily have to involve money initially .... but as you said, it often does eventually e.g. loss of confidence in a banks e-banking system will invariably affect numbers of customers using those services or patronizing the bank.
Anthony
	              
	              Hello All,
There are often rumours of sovereign states (and sometimes companies) using hackers to break into or incapacitate competitor government/corporate systems, or to sometimes steal confidential data (industrial espionage).
Has anyone come across recent evidence or clearly document examples of this happening?
Anthony&nbsp;
	              
	              Dear Dr. Ayoola,
All I can say is I've never seen any evidence of this happening. So all these accuses must be pointless insulting to those states and their supreme leaders.
br, Terry&nbsp;
	              
	              Hi Terry,
Absolutely! It requires a whole new way of thinking:-)
kind regards,
Belinda&nbsp;
	              
	              hi Martins,
"We have to be hackers to be safe". I like that and it is so true. That should be a mantra when thinking of security testing.
kind regards,
Belinda
	              
	              Hi Craig,
Wow, that is deep. I should get my hands on that book.
Thank you.
Best regards,
Belinda
	              
	              Hi Belinda,
...ancient text, originated from China, 2500 years ago. Translated in the western world in 18th century.
I came across it during management training many years ago, read from the perspective of strategy rather than warfare.
Best Wishes, Craig
	              
	              Hi Anthony,&nbsp;
...not so much "Hackers", however there is the issue that the US government suggested that Huawei telecoms equipment can be accessed by the Chinese government, and that the former CIA chief has seen hard evidence of spying?&nbsp;
These concerns have moved across the western world. Obviously, I have not seen any evidence myself, just reports in the news. This has done the rounds and has a lot of reports, both positive and negative press.
Last year, amid security risk warnings, UK Ministers defended the deals with the Chinese telecoms giant. However, GCHQ had acknowledged that the risk of unauthorised access cannot be entirely eliminated, and a Parliamentary Committee attacked the British governments failure to investigate the use of Huawei equipment in the UK national telecommunications network.
What, and Who to believe?
 
Best Wishes, Craig
References
&nbsp;
Wardell, J., Tait, P. ed. (2013).&nbsp;Former CIA boss says aware of evidence Huawei spying for China&nbsp;[Online]. Available from: http://www.reuters.com/article/2013/07/19/us-huawei-security-idUSBRE96I06I20130719 (Accessed 14 October 2014)
&nbsp;
Blitz, J., and Thomas, D. (2013). UK security committee ‘shocked’ over Huawei contract with BT [Online]. Available from: 
http://www.ft.com/cms/s/0/24bbea6e-ce87-11e2-ae25-00144feab7de.html#axzz3G45bLWGc (Accessed 14 October 2013)
&nbsp;
BBC News politics. (2013).&nbsp;UK Ministers defend Chinese deals after security risk warning&nbsp;[Online]. Available from:&nbsp;http://www.bbc.co.uk/news/uk-politics-22795226 (Accessed 14 October 2014)
&nbsp;
Corera, G. (2013). Should the UK be worried about Huawei? [Online]. BBC News UK. Available from: http://www.bbc.co.uk/news/uk-22803510 (Accessed 14 October 2014)
&nbsp;
Mehta, S. N. (2013). BT CEO: Huawei is a “good partner” [Online]. Available from: http://fortune.com/2013/10/25/bt-ceo-huawei-is-a-good-partner/ (Accessed 14 October 2014)
	              
	              Hello Babatunde,
Nice post, but in my own opinion I think the best thing to do is to learn what the hacker knows because changing passwords will not help&nbsp;that company. The hacker is smart, so smartness should be learnt and&nbsp;ALL possible loopholes blocked.
Regards
Martins&nbsp;
	              
	              Hi Anthony,


Iranian Nuclear Development Security Breach



While reading on issues of high-profile computer systems security breach, in either the public or private sectors, I came across a virus named STUXNET

This virus was first detected in June 2010 by a security firm based in Belarus, but may have been circulating since 2009. Unlike most viruses, the worm targets systems that are traditionally not connected to the internet for security reasons.

This virus (infected with malware) infects Windows machines via USB keys. It is commonly used to move files around. Once it has infected a machine on a firm's internal network, it seeks out a specific configuration of industrial control software.

The virus was used to stop development of Iran's Nuclear Power program by breaching its information systems and destroy key codes; after its release, 5 countries reported viral infections in addition to Iran, including Indonesia, India, Pakistan and Azerbaijan. The virus was designed to spread through Microsoft Windows operating systems and specifically targets Siemens industrial software. 

It was discovered according to the article that the virus was developed by American and Israeli intelligence officials to try and slow Iran's nuclear development. The program, which had started development in the earlier part of the 21st century, has received the support of both President George W. Bush and President Barack Obama as a strategy to hinder the nuclear arming of the Iranian government. 

Stuxnet is a clear example of the next great threat to national security: cyber warfare. The digitalization of basically every aspect of national security elements, including military personnel files, launch code sequences and prison structures, have also created a new battleground for international disputes. Going forward, it is likely there will be an increasing presence of individuals with information security degrees in military organizations for just this reason.

NB: Stuxnet's complexity suggests it could only have been written by a "nation state", some researchers have claimed.

Best Regards, Babatunde


References

Stuxnet [online] Available from: http://en.wikipedia.org/wiki/Stuxnet (Accessed 14 October 2014)

Jonathan F. (2010) Stuxnet worm 'targeted high-value Iranian assets' [online] Available from: http://www.bbc.com/news/technology-11388018 (Accessed 14 October 2014)


Top 3 High Profile Information Security Breaches of the 21st Century [online] Available from: http://online.lewisu.edu/resource/engineering-technology/top-3-high-profile-information-security-breaches.asp (Accessed 14 October 2014)

An Overview of the Industrial Worm Called StuxNet [online] Available from: http://embeddedsw.net/doc/Stuxnet_white_paper.html (Accessed 14 October 2014)
	              
	              Hi Anthony,
Sony PlayStation Security breach
An unknown hacker in April 2011 infiltrated more than 77 million Sony PlayStation Network user accounts. Over 12 million credit card numbers were accessed including data which were names, home addresses, emails, birth dates and passwords.
Sony had to shut down online network while trying to figure out the extent and method of the breach, Initially Sony indicated that the networks would be brought online again within a few days, however they were down for a month. The company has since released software to try and correct the problem.
This incident represented the impact a security breach can have on individuals, both in terms of the breach itself and how the breach is handled and controlled. Sony received a lot of criticism from customers as well as analysts for waiting over a week to go public with their knowledge of the breach, leaving many feeling frustrated they were unable to act to tighten their personal security immediately. 
One thing is certain: as society continues to digitize all aspects of life, threats will grow, both in number and severity. But hopefully by examining the kinds of breaches that have been perpetrated, individuals in the information security field can create a safer technological tomorrow.
&nbsp;
References
Top 3 High Profile Information Security Breaches of the 21st Century [online] Available from: http://online.lewisu.edu/resource/engineering-technology/top-3-high-profile-information-security-breaches.asp (Accessed 14 October 2014) &nbsp;
Mark M. (2011) CNN Sony: Hacker stole PlayStation users' personal info [online] Available from: http://edition.cnn.com/2011/TECH/gaming.gadgets/04/26/playstation.network.hack/index.html (Accessed 14 October 2014) 
	              
	              Hi Anthony,
 
 
In 1994, Russian Vladimir Levin became the first cyber hacker to break into the database of a financial institution to divert funds. The victim, Citibank, has seen more than $ 10 million from well-trimmed accounts transferred to other accounts in the United States, Finland, the Netherlands, Germany and Israel. But when they tried to withdraw money stolen three accomplices of Levin were arrested by police in San Francisco, Rotterdam and Tel Aviv, and denounced Levin.
At the time, there was no extradition treaty between Russia and the United States, and it was at the London airport that Levin was finally arrested in 1995, and then returned to American authorities. He was sentenced to three years in prison and had to return nearly 250,000 dollars. Citibank has managed to recover almost all monies stolen, but 400,000 dollars has disappeared.
CitiBank reportedly lost 20 top clients, who thought that he bank's systems were not secure enough.
Reference:
cab (2014) [Online]: cab Inc. Available from: http://www.cab.org.in/Lists/Knowledge%20Bank/Attachments/64/InternetFraud-VL.pdf(Accessed: 14 October 2014)
 



	              
	              There are number of incidents related to cyber security. But one of famous and massive cybercrime attack was on oil and gas sector in August 2012. Saudi Aramco, a major manufacturer, producer, refiner and marketer of natural gas and crude oil was attacked by a virus latter named Shamoon. This virus affected 30,000 computers and it took couple of weeks to recover from damage. Although Shamoon did not affected major oil operations of Aramco butbusiness processes were significantly affected. Data from computer hard drives indiscriminately deleted by Shamoon.

Iran’s nuclear facility at Natanz was also attacked by a worm called Stuxnet. It affected the uranium enrichment process and there are reports that say the involvement of other states in this incident (David Sanger, 2012).

There are hacking groups called Anonymous and OpPetrol that are specifically targeting oil and gas sector. They attacked in 2013 and again announced that on June 20, 2014 that they are going to attack the same targets (Internation Business Times, 2013).

Cyber-attacks had widely affected the economy and confidentiality of various organizations. It is one of the most growing and serious threats to global economy. So there is a need to develop and upgrade cyber defending strategies with passage of time. Organizations and individuals need to keep themselves updated with the latest trends and policies related to security. Future security trends need to be more focused on dealing with cyber threats and maintaining data security.

&nbsp;
References:

&nbsp;Christopher Bronk &amp; Eneken Tikk-Ringas (2013) The Cyber Attack on Saudi Aramco, Survival: Global Politics and Strategy, 55:2, 81-96, DOI: 10.1080/00396338.2013.784468
David Sanger, Confront and Conceal: Obama’s Secret Wars and Surprising Use of American Power(New York: Crown Publishers, 2012).

Internation Business Times. (2013) Anonymous Announces #OpPetrol Attack On Local Oil And Gas Companies [Online] Available from http://www.ibtimes.com/anonymous-announces-oppetrol-attack-local-oil-gas-companies-video-1267817 (Accessed 14 Oct 2014).
	              
	              Hi Anthony,

It's a difficult situation in many ways and for many reasons. I'm sure over the years these companies who have hired the hackers have done their due diligence or at least I hope they have. One thing i'd like to know is to find out just how successful these hired employees have been. I'm sure they would sweep under the carpet any mistakes with them and only put out press releases for the handful that get converted into good law abiding citizens so it's hard to quantify just how successful it has been without knowing the full story.
Potential pros

New raw skills to both teach other staff and to harness new ways of thinking
Outside the box viewpoint
Highly motivated
Willingness to learn
In touch with latest trends
Not necessarily motivated by money

Potential Cons

Moral issues
Trust
Tend to see things with tunnel vision
More speficically focused and not got a wide understanding
Politics
Adaptability to the working practice
Not a team player


Conclusions
Overall I favour the rewards in hiring them over the potential risks. You can't make an ommlette without breaking a few eggs. My worry is the inherent viscious circle or hacker paradox which in turn feeds the isssue and the solution. If companies are hiring hackers then there will be more hackers trying to hack companies just to get a job. This can be both good and bad. Hence the 'white' 'grey' and 'black' hat types of hacker.

Thanks
Chris


	              
	              Hi Craig,

I remember reading about that as well. I still don't know if it is real or not but I was intreagued when I heard about it.
Personally my first instinct was "NAAAH" that can't be real, but you just never know I'm sure governments have influenced tech companies like this that we probably have never been aware of yet.

Scary stuff

Chris

	              
	              Hello all, The high-profile system security breach I would like to address is the one of the Game King 5.0 poker machine in 2009 by John Kane. He found out that when he had a winning 'hand' he could switch a game on the machine and come back to raise the stakes using the Raising Up function while having the winning combination. The glitch was of course not noticed and found only by 'luck'. The gambler went naturally to far and played out his luck, was caught but during his trial he could not be made fully responsible for using the options that were made available by the programme. Especially because 'beating the system' is an intrinsic part of the game. This does not count for other systems and therefore the CFAA law could not be used in this instance. Hacking a system for personal gain is not always wrong! Greetings from Bram Poulsen, K (2014). Finding a Video Poker Bug Made These Guys Rich—Then Vegas Made Them Pay [Online], Wired. Available from: http://www.wired.com/2014/10/cheating-video-poker/ (Accessed on 13 October 2014) &nbsp; 
	              
	              Hi Anthony,

When I read this topic I didn't know of any that immediately came to mind.

After some research I found this great article: "Case Studies: Operations Aurora, The Elderwood project, Flame and Red October"&nbsp; It's a large article but an interesting read.
The one that caught my eye the most was the "red October" operation as they say that it started in 2007 and they believe it is still in operation now. I also liked it because it reminded me of a type of James Bond plot with diplomatic espionage.
One paragraph that did catch my eye was this "Small business is the most vulnerable to cyber espionage. It represents an attractive target, due the lack of security mechanisms and processes as well as – in many cases – the direct relationship between enterprises and governments. In recent years, the number of attacks against government contractors has increased. A cyber attack against a subcontractor is easy to realize, as the line of defense penetrated is often fragile, allowing the attackers to acquire sensitive information from targets of interest."

You'd initially assume, or should I say that I initially assumed that they would be direct assaults to the locations they wanted to hack but from this article it was interesting to see that they look for back door routes through sub contractors and small/medium businesses.


References
Paganini, P (2013) 'Case Studies: Operations Aurora, The Elderwood project, Flame and Red October' [http://resources.infosecinstitute.com/cyber-espionage-the-greatest-transfer-of-wealth-in-history/] accessed 14 Oct 2014
	              
	              Hello all,
I have a fresh one here:
iSIGHT discovers zero-day vulnerability CVE-2014-4114 used in Russian cyber-espionage campaign. The security leak allowed hackers to use Microsoft Powerpoint documents to install malicious software on different devices. The publication of this leak yesterday on 14 October 2014 is much discussed and is now well documented. Insight claims that the Russian government used this leak for espionage with the targets like NAVO, Ukraine government, different Western European countries and academic organisations in the US. The last 5 weeks the company worked on a solution an Microsoft will distribute updates for its software starting today.
Zero-day vulnerabilities are security weaknesses that are not breached yet or not documented as being a security risk yet. The example I posted in the above discussion about the Game King 5.0 poker machine is one of them as the problem did not show up during the audit of the code. The machines were just out there and the problem became a problem when one found this weakness and started 'stealing' money.
Sandworm is the Russian company allegedly behind exploiting the zero-day Microsoft PowerPoint leak. &nbsp;It started hacking in 2009 and was followed since 2013. This did not lead to insight in what was stolen by the hackers. As the leak was not public yet it was difficult for companies to protect themselves and therefore the company assumes that all targets to some extend became victim.
What is did show is that full security is not possible as potential leaks are always available.
Insight (2014) Sandworm [Online] Insight partners. Available from: http://www.isightpartners.com/2014/10/cve-2014-4114/ (Accessed on 14 October 2014)

	              
	              Hello Chika,
What I find interesting about this cat and mouse game is the fact that leaks first have to be exposed and that this is clearly not that easy. Even for superbig companieslike Google who will have a reputation to hold up for their cloud based services.&nbsp;
I also think everything can be hacked and only when this takes place an antidote to that specific breach or class of breaches can be found. An active company will try to stay on top of the game, others only react when they are confronted with a break in. Like security of a house it only needs a person who is smart enough and has the mindset plus tools to actually find the weak spot.
Greetings from Bram

	              
	              I have a real life example. Whilst working for Microsoft, I was called to deploy a team in Adobe (San Jose) to deal with their cyber attack. Microsoft came in and built a RED FORREST (a secure Active Directory island) and allowed secured admin's to &nbsp;administrator the network and manage other forrest in the&nbsp;network. Only IPSEC communications was allowed to and fro the red Forrest. &nbsp;We then re-built the compromised AD environment.
During our engagement&nbsp;we found a&nbsp;trail of Chinese hackers leaving messages on the network !!!
robin
	              
	              Hi Anthony,
There are only unconfirmed reports about such incidents and I haven’t seen any acknowledgement of those. In one of the TED videos clip recommended for this week learning resources, the speaker was talking how Iran’s nuclear facility at Natanz was attacked by a worm called Stuxnet which affected the uranium enrichment process and due to the dynamic nature of this worm it has the capability to attach other targets as cyber weapon of mass destruction. He did great research and studied the code but when a question was asked about the involvement of other states behind this attack, he was only able to give his opinion but evidence (Ralph, 2011).
Regards,,,
Numan
References:
Ralph, Langner (2011).Cracking Stuxnet, a 21st-century cyber weapon. TED Video [Online]. Available from: http://www.ted.com/talks/avi_rubin_all_your_devices_can_be_hacked?language=en (Accessed: 14 October 2014)
Christopher Bronk &amp; Eneken Tikk-Ringas (2013) The Cyber Attack on Saudi Aramco, Survival: Global Politics and Strategy, 55:2, 81-96, DOI: 10.1080/00396338.2013.784468
	              
	              Hi Dr. Anthony,
There are several computer systems breaches that have been reported within the last decade, some much more high profile than others. Most of these breaches tend to follow a common pattern of stealing credit/debit card information, personal information, trade secrets, employee/ customer records, etc. as reported by Taylor Armerding&nbsp; in his CSO online article "The 15 worst data security breaches of the 21st Century" (Armerding, 2012).
The breach I consider to be high profile and would like to recall is the hack of the Associated Press (AP) Twitter account on April 23, 2013, where an advisory was posted&nbsp; as follow:
&nbsp;"Two explosions in the White House and Barack Obama is injured."(Jackson, 2013)
The AP Twitter account has over 1.9 million followers and a report from such an authority sent panic throughout the USA. 
The guardian reported that the Wall Street collided social media due to the false tweet resulting in the Dow Jones industrial average falling by 143 points.(Moore &amp; Roberts, 2013). Sky News also reported that stock markets plunged just as the report came out, causing the S&amp;P 500 to drop 12 points or 0.8% (news.sky.com, 2013). 
Reports are that the market recovered in a few minutes, but the results of such massive drops as outline in the previous paragraph must have impacted the financial performance of the many stakeholders.&nbsp; This also goes to show how sensitive the markets are to social media and the type of impact possible.
The breach could have been prevented by employing an alternative secured site or access portal for corporate Twitter accounts of this nature. Multi-factor authentication on top of highly complex password and an account lockout mechanism to restrict the number of login attempts. Another approach could be IP restrictions since this a corporate account you should only be able to access from list of public IP addresses.
Inspite of all the measures that can be employed to secure computer systems&nbsp; and networks , a dedicated and determined hacker may still be able to find a vulnerability that is unknown.&nbsp; And that is why constant monitoring, evaluating and trend analysis is important in the fight against security breaches.
&nbsp;
References:
Armerding, T. (2012) The 15 worst data security breaches of the 21st Century [Online]. Available from http://www.csoonline.com/article/2130877/data-protection/the-15-worst-data-security-breaches-of-the-21st-century.html (Accessed: 14 October 2014).
TED (n.d.) Hackers: the Internet's immune system [Online]. Available from: http://www.ted.com/playlists/10/who_are_the_hackers.html, (Accessed: 14 October 2014).
Jackson, D. (2013) AP Twitter feed hacked; no attack at White House [Online]. Available from http://www.usatoday.com/story/theoval/2013/04/23/obama-carney-associated-press-hack-white-house/2106757/ (Accessed: 14 October 2014).
Moore, H. &amp; Roberts, D. (2013) AP Twitter hack causes panic on Wall Street and sends Dow Plunging [Online]. Available from http://www.theguardian.com/business/2013/apr/23/ap-tweet-hack-wall-street-freefall (Accessed: 14 October 2014).
Sky News (2013) Dow Jones dives after fake Obama attack tweet [Online]. Available from http://www.usatoday.com/story/theoval/2013/04/23/obama-carney-associated-press-hack-white-house/2106757/ (Accessed: 14 October 2014).

	              
	              Hello Dr. Anthony,The term 'Corporate Espionage' is usually referred to stealing confidential information in order to get a company's trade secrets, thus gaining &nbsp;a competitive advantage of a competitor. This can be done through all mannerisms or forms of hacking. Here hackers usually want to gain access to a company information temporarily and perhaps download of copy it onto some other storage device to be accessed later.The Denial of Service (DOS) on Yahoo, Amazon, Ebay in February 2000, where users were unable to gain access to such sites, hackers may have possibly taken these systems down and gain access to confidential information. There many examples of this type of security breach, another being the fact that in 2012, Microsoft ex-employee was charged for leaking Windows 8 source code to a blogger.In 2010 there was an Cyber attacked which they called 'Aurora' on Google server. The attackers were reportedly based in China. The perpetrators apparently chatted with a Google employee, sent them a website link and when the employee clicked on the link, a code was downloaded onto his or her computer, allowing the perpetrators to gain access their user-name and password, thus using this information to access Google servers. The penetration was linked to two schools in China, one which “has ties to the Chinese military.” [1]There are corporations that hire hackers for such hacking and it is believed that the persons responsible may have been hired accordingly to carry out this task.References:Slane, D. (2010), Report to Congress of the U.S.China Economic and Security Review Commission, Pages 239 [1], pages 237-239 [Online]. Available at:&nbsp;http://books.google.gy/books?id=qU4uE-A1pUAC&amp;pg=PA253&amp;dq=aurora+hack&amp;hl=en&amp;sa=X&amp;ei=itI9VKPgFc_esATVwIDoCQ&amp;ved=0CDEQ6AEwBA#v=onepage&amp;q=aurora&amp;f=false (Accessed on October 13, 2014)Cole, E. (2001), Hackers beware: defending your network form wiley hacker, Pages 44-45, Publisher: New Riders Publishing. ISBN: 0-7357-1009-0&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
	              
	              Hello Class,As our usual way of rounding up the week's activity, please provide a short summary of the key 'Computer Structures' lessons learnt in week 6, from your perspective.Anthony
	              
	              Hi Creq,&nbsp;
in my company we have started working with Huawei equipment recently, two years ago and I have to listen to this story and many people here are talking about it.&nbsp;
the truth is that nobody have evidence of this, and we can not at this stage know whether it is true or not.but people are really worry about it.&nbsp;
so wait and see...
kind regards,
Tresor
	              
	              Hi Everyone,
Do VPN (Virtual Private Network) providers, and the increasing use of VPNs, enhance or diminish the overall security of computer systems on corporate networks?
Regards,
Anthony
	              
	              Hi Dr Ayoola,
I think there's pros and cons by the widely use of VPN for the network security. VPN is an efficient way of creating a safe virtual network. But on the other hand it also create trouble for tracking the&nbsp;attackers or forensic investigators.
Many people from China use VPN service outside China to bypass the Great Firewall (wikipedia, Golden Shield Project). From the above mentioned government's perspective, this usage is implicitly/explicitly againest the regulation.
Hackers do use VPN to hide their identity (Izadinia, 2006).&nbsp;
br, Terry

References:
Izadinia, V. D., Kourie, D. G., &amp; Eloff, J. H. (2006). Uncovering identities: A study into VPN tunnel fingerprinting.&nbsp;Computers &amp; Security,&nbsp;25(2), 97-105.
Golden Shield Project. (2014, October 14). In&nbsp;Wikipedia, The Free Encyclopedia. Retrieved 11:50, October 15, 2014, from&nbsp;http://en.wikipedia.org/w/index.php?title=Golden_Shield_Project&amp;oldid=629522507
	              
	              Hello all,
As in every security approach there are two sides of the story. The increasing use of corporate VPNs is good as it is one of the most secure ways to communicate over interent. I am in bit worried about the overhead this tunneling creates. This is a lower percentage when more data is transferred but the transfer overhead increases tremendously when little data is moved. The increase of use of VPNs probably means it is used more for general purpose activities involving only little data transfers which increase the overall overhead.&nbsp;
A negative impact is that with the increase of use more devices have easy use VPN access to the corporate network. It is like this very secure door that is unbreachable but with a tremendous amount of keys that circulate relatively uncontrolled (one only knows after misuse which key was involved).&nbsp;
Greetings from Bram
	              
	              Hi Dr Ayoola,
This week has been quite hard for me. Partly from my work, as I started with a new client which is a local bank from last week. This week, I learnt:

Revisited the basic network knowledge
Network security issues, especially from a hackers perspective
There are people working on "Layer 8" of OSI model, seriously!
Explored the bus network topology and star network topology
Understood CSMA/CD better
Recall some unhappy memory:(

I have to say that oh my goodness this week's study is over. Sticking to the network security topics made me nervous. I cannot wait to move on to the next week's topics.
br, Terry
	              
	              Hi Dr Anthony,

This week for me was a combination of forward thinking and back to basics.
We got in depth with security, particularly hackers and it was inetresting to hear what my colleagues had to say.
I had to go back to the different topologies i learnt about a few years ago.
All in all an interesting week.
Thank you.

Best regards,
belinda&nbsp;
	              
	              Hi Anthony,
VPN’s are a critical component to enable businesses to transfer sensitive information to different locations.
According to Douligeris and Serpanos (2007, pg. 52), a VPN is a network that provides a secure link between two private networks. This is virtual, in that it is done through a process called tunnelling, through a public network.
VPN’s offer benefits in relation to 1) cost, having a virtual private network rather than having a dedicated point-to-point network, 2) they can be scalable, in that it is straightforward to extend the geographic reach and to change the access points, and 3) they're also flexible, in relation to the bandwidth use. It is extremely fast to provision new capacity on a VPN as and when required, whereas dedicated leased lines are much longer in timescales for delivery.
According to Douligeris and Serpanos (2007, pg. 55), an effective protocol designed to establish tunnels should meet certain security criteria, including but not limited to:

Confidentiality - to prevent eavesdropping
Integrity - to ensure data sent is identical to data received
Authentication - in that the use is by a legitimate client
Certification - establish identify of tunnel entities before keys are exchanged
Access control – limit access to legitimate users
Key management – efficient mechanism by which the keys are negotiated and exchanged

Dougileris and Serpanos (2007, pg. 61) further conclude that VPN technology will continue to see accelerated growth in commercial, research and military environments.
So, in conclusion surely they're a good thing and also provide better flexibility than traditional dedicated point-to-point leased line networks?.
 
Best Wishes, Craig
References
Douligeris, C., and Serpanos, D. N. (2007). Network Security – Current Status and Future Directions. pp. 52-61. Hoboken New Jersey: IEEE Press. John Wiley &amp; Sons Inc.
	              
	              
Hi Anthony,
The details of the NSA’s spying program, MUSCULAR, disclosed by Edward Snowden, is possibly an example of 'state sponsored computer hacking'. ITBusiness Edge, an online news and technology trends organisation, reported this as one of the top 10 security breaches of 2013 and reportedly one with greatest impact of any breach in 2013. According to J.J. Thompson, managing director and CEO of&nbsp;Rook Security, the MUSCULAR program involved intercepting data from Yahoo and Google private clouds where the data is unencrypted. The data collected included email, pictures, video, text documents, spreadsheets, and an array of other similar file types.
This has resulted in many of these tech companies putting in place measures to&nbsp;encrypt their internal network traffic and lobby through public relations this breach by NSA.
References
ITBusinessEdge, 'The 10 Worst Data Breaches of 2013',&nbsp;Available at&nbsp;http://www.itbusinessedge.com/slideshows/the-10-worst-data-breaches-of-2013-11.html , accessed 15/10/2014
	              
	              Hi Anthony.
One of the high profile security breaches in 2013 involved the security breach experienced by Adobe in 2013&nbsp;that exposed user account information&nbsp;including customer names, encrypted credit or debit card numbers, expiration dates, and other information relating to customer orders (Adobe News, Oct 2013)
Adobe's initial estimate was that information on nearly 3 million user accounts was compromised during the intrusion. However Naked Securitys’ Paul Ducklin alleged that a database of Adobe user data had turned up online at a website frequented by cyber criminals with over 150 million "breached records" found in the database dump. Adobe however revised their figure to 38 million compromised accounts. (The Verge, Nov 2013)

The breach required a password reset by Adobe on affected customers. Other customers were also advised to change their passswords as a precaution. Banks processing customer payments for Adobe were also notified, so that they could work with the payment card companies and card-issuing banks to help protect customers’ accounts
 The challenge with this kind of breach is that the data can be used again and again in a widening circle of breaches and fraud on other sites as it consists of Personally Identifiable Information (PII) that is persistent over time.&nbsp;
The breach could have been prevented through some of the following ways
1) Use of 2-factor authentication to validate credit card transactions -&nbsp;Two-factor authentication stops easy access with stolen credentials by requiring a second level of authentication after the user enters their username and password. An attacker with this stolen data still wouldn’t have the crucial second factor to use these passwords in a meaningful way.
 
2) Use of best practice for password storage
References


Welch, C. ‘Over 150 million breached records from Adobe hack have surfaced online’, The Verge, Nov 2013, Available at http://www.theverge.com/2013/11/7/5078560/over-150-million-breached-records-from-adobe-hack-surface-online , Accessed 15/10/2014
Arkin, B. ‘IMPORTANT CUSTOMER SECURITY ANNOUNCEMEN’, Adobe News, Oct 2013, Available at http://blogs.adobe.com/conversations/2013/10/important-customer-security-announcement.html , Accessed 15/10/2014
Duo Security Inc, '3 Breaches, 1 Solution: 2 Factor Authentication', Available at&nbsp;https://www.duosecurity.com/articles/three-breaches/adobe-ap-twitter-club-nintendo, Accessed 15/10/2014


 
	              
	              There are many VPN service providers in the market offering services. There are pros and cons for getting services from these VPN service provides. Some key factors to consider, are speed, availability of bandwidth, VPN server locations, security etc. VPN solutions performs all the encryption and decryption which frees up company resources for other tasks. Benefit of VPN service installed at the router is that all your devices won't have to be specially configured to use the VPN. They won't even know that they are running through a VPN. The one main downside of using a VPN is the delay associated with the encryption / decryption process. Websites might not be as lightning fast to load up as they were before you added the VPN service. It's up to you whether the delay is acceptable or not. Similarly, it might be difficult to get VPN solutions from different providers to work with each other due to the different standards and protocols that may be in use. The biggest concern is the security and confidentiality of your data which is available to the VPN service provider. It’s very risky to trust these service provider and handover your data to them.  Regards,,, Numan &nbsp; References: &nbsp; Andy O'Donnell (2013). Why You Need a Personal VPN Service [Online] Available from: http://netsecurity.about.com/od/perimetersecurity/a/Why-You-Need-A-Personal-Vpn-Service.htm (Accessed: 15 October 2014).
	              
	              Hi Anthony,

I must say it was another great week, full of constructive discussion on very important and hot topics of security weaknesses in technologies for computer and networks, network topologies, famous cyber-attacks and the report on the involvement of states in cyber-attacks. I really enjoyed the perspectives about hackers that how they are helping to identify the security lope holes and how their skills can be properly utilized for the betterment of technology.
Best Wishes,
Numan

	              
	              Hello Anthony,
In 2003, the saphire worm also known as the SQL Slammer worm infected vulnerabilities in the microsoft structured query language server-(SQL) and microsoft query server engine. It crashed the internet within 15minutes of release making it the fastest spreading worm of all time.
Reference:
David, K.(2013) The real story of stuxnet[Online]. Availabe from:www.spectrum.ieee.org/telecom/security/the-real-story-of-stuxnet. (Accessed on 15 october 2014)
	              
	              Hi Dr Ayoola,
I think we need to differentiate Virtual Private Networks (VPN) in two different categories: first are the corporate/institution VPNs, second there are “consumer” VPNs.
Consumer VPNs are services such as HMA, where VPN is offered as a commercial service with claims of better security and privacy. These services are just like any other Internet/Cloud services they are secure as far as they do not get hacked; their security will have the same exact pros and cons like any other network connectivity. In fact, this type of VPN is just another “public” (but somewhat more private) logical network layer a top of your Internet (public) connection, nothing else. What if hackers are inside that VPN?
Corporate or institutional VPNs are different because their primary goal is to allow secure access to company resources. Without VPNs the only way to share internal resources would be to open it to the Internet, not a good approach. VPNs provide better security than Internet as the access is at a minimum encrypted with an identification/authentication mechanism (normally a Login). In this context, then yes a VPN does enhance the overall security on corporate/institutional networks.
Best Regards,
Augusto Schoonewolff
&nbsp;
References
&nbsp;
Scott, C., Wolfe, P. and Erwin, M. (1999) 'Chapter 1. Why Build a Virtual Private Network?' in&nbsp;Virtual Private Networks, Second edn, O'Reilly, pp. 6-7.

	              
	              Hi Anthony,
Yes, a good week, focused on Network Systems, covering security risks and network topologies.
It seemed as though there was quite a bit more reading this week than in previous weeks, perhaps because of the research around network and cyber security, vulnerabilities and threats. 
I have been fascinated by the subject matter, and particularly enjoyed watching all the TED videos. It has been really useful learning about the different perspectives and opinions on hackers and it has opened my eyes somewhat.
Another good insightful week, thanks.
Best Wishes, Craig
References
Laureate Education. 2014. Computer Structures – Lecture Notes and Online Video – Week 6: Computer Networks [Online]. Available through the programme classroom (Accessed 9 October 2014]&nbsp;
Brookshear, J. G. (2012). Computer Science An Overview. 11th ed. Boston Massachusetts: Pearson. Addison Wesley
Ted Videos playlist. (2014). Who are the Hackers? [Online]. TED Videos. Available from: http://www.ted.com/playlists/10/who_are_the_hackers (Accessed 9 October 2014)

	              
	              Hi Dr. Anthony, My classmates have discussed some of the suspected cases of state sponsored attacks, however non of which I have come across in some of the online news reports have been claimed by any government or state. The type of threats uncovered and the targets, greatly suggests that they are state lead and design to achieve a political, industrial, commercial and other types of advantages over a specific state or government. I would like to point out the DarkSeoul group of hackers that have claimed responsibility for attacks on South Korea and the US military. McAfee Labs have reported uncovering sophisticated military spying networks targeting South Korea, the use of malware to attempt infitration of specific South Korean targets and numerous attacks on banks and broadcasters. These attacks have been deemed domestic espionage and have been crippling (Stevenson, 2013).             "Numerous security companies and law enforcement agencies have suggested the DarkSoul attacks were state sponsored.&nbsp;McAfee said its research could neither prove nor disprove this theory, but added that the hackers behaviour was consistent with that of attackers operating under the Anonymous hacktivist collective's banner" (Stevenson, 2013). There are several other campaigns that I have come across that can be deemed state sponsored, primarily due to the targets and the objectives uncovered. V3 News reports list the top 10 worst state-sponsored hacks, which has identified reported breaches within this category (www.v3.co.uk, 2014).  References: Stevenson, A. (2013) DarkSeoul hacks on South Korea uncovered spying on nation's networks as far back as 2009 [Online]. Available from http://www.v3.co.uk/v3-uk/news/2280133/darkseoul-hacks-on-south-korea-uncovered-spying-on-nations-networks-as-far-back-as-2009&nbsp;(Accessed: 15 October 2014).  V3.co.uk (2014) Top 10 worst state-sponsored hack campaigns: From PRISM to Stuxnet and Mask [Online]. Available from http://www.v3.co.uk/v3-uk/news/2329347/top-10-worst-state-sponsored-hack-campaigns-from-prism-to-stuxnet-and-mask/page/3&nbsp;(Accessed: 15 October 2014).  Regards,                 
	              
	              Hi Dr Ayoola,
This was a really interesting week, for me it was good to delve deep into the Networking area as I normally do not work in that area.
My key learnings this week are:

Better understanding of CSMA/CD &amp; CA.
Better understanding of network security overall. The basics are important!
A good discussion about Hackers and better understanding of them overall. The TED videos were fantastic.

Overall another good week, with challenges at work (way too busy) but I think I’m finding my methodology and organization between work, home and studies.
Best Regards,
Augusto.

	              
	              Hi Anthony
Like my fellow students I have not seen any case with clear evidence (perhaps it’s in the countries fellow interest not to inform of such?).
The only cases I can find are:

The Alabama governor says that he has proof of hacking by another state. But he says he’s not allowed to comment further. (Tuscaloosa news, 2014)
JPMorgan was hacked, and allegedly Russia is behind it. There is no proof of Russia being behind it, but there is apparently proof that they were Russian hackers. And according to a James Lewis (director of the Strategic Technologies Program at the Center for Strategic and International Studies), these hackers have connections to the Russian government, and the are encouraged to act as so-called “patriotic hackers”. (Guynn and Johnson, 2014)
Estonia is hacked, and according to a known Russian hacker called Sp0raw, an attach like that “could not have been done without the blessing of Russian authorities”. &nbsp;(Wikipedia 2014)

My conclusion is that it happens every day and a lot (all?) the big countries do it. We just don’t hear so much about it nor do we see the evidence.
References
Guynn, Jessica and Johnson, Kevin. (2014) USA Today. “Is Russia tied to JPMorgan hacking?”[Online] Available from: http://www.usatoday.com/story/tech/2014/08/28/russia-jpmorgan-hacking-attack/14735649/ (Accessed 15-10-2014)
Tuscaloosa News. (2014). “Alabama governor says country behind hacking of state computer network”. [Online] Available from: http://www.tuscaloosanews.com/article/20140822/news/140829844. (Accessed 15-10-2014)
Wikipedia. (2014). “2007 cyberattacks on Estonia” [Online] Available from: http://en.wikipedia.org/wiki/2007_cyberattacks_on_Estonia. (Accessed 15-10-2014)

	              
	              Hi Anthony,

&nbsp;It's been an interesting week. I've looked back at networking as I havent really touched upon topologies for a good few years.
But the best and most interesting part for me has been the discussions around hackers for sure.
I've liked watching the ted.com channel videos and discussing this outside of the normal DQ's.

Overall a good and valuable week.
thanks
Chris


	              
	              Hi Dr. Anthony,
VPN used on corporate networks enhances the overall security of the companies computer system. The VPN will allow for the security policy and measures implemented with the organization to be extended to the connected user/site over a encrypted facility through the Internet which is felxible and highly secure offering the similar protection to the end user as if they were siting in the corporate office/headquarters. However these benefits derived from a VPN can be significantly altered when routed through a VPN provider. The terms of the agreement with the provider must be clear about privacy related matters and the activities associated with the service, including how information traversing the end user through the VPN providers software or management console to the destination will be handled. At the providers interface several things can happen to your information stream, including; sniffing snoopinng, eavesdropping, activity logging and many others.
The increase use of VPN through a VPN service provider will diminish the overall security of computer systems on corporate network and this is primarily because a third party is introduced to the process of establishing the VPN connection to the corporate network, allowing for a possible layer of attack.

Regards,

	              
	              Hi Anthony,

I believe that inherently that it diminishes the overall security simply because it by default introduces more opportunity and chance for more issues to be exploited. However I feel VPN's are still essential in some situations and even vital to help with communication especially over the Internet and to help companies transfer important information more securely.
But more importantly they add flexibility so can in theory increase overall security.

Thanks
Chris



	              
	              Hi Terry
I stubled over the video that might have some interest for you. I'm guessing you already know about it :-)
https://www.youtube.com/watch?v=-O6RCWO8weU
Br Bo
	              
	              Hi Anthony

VPNs give a higher level of security than other methods (Thrivenetworks). They give an encrypted tunnel through the general Internet instead of investing in e.g. dedicated leased lines. But of course the VPN also brings along configuration, and this can be complex, and complexity can be hard to handle and cause wrongful configurations in e.g. firewalls hence the possibility of diminishing the security. Another concern can be if you try to connect two or more VPN solutions, this can cause incompatibility and problems (Thrivenetworks). And of course there is the concern that organized crime also uses these tools.

As security continues to be a critical issue in Internet communications, VPN technology will continue to see an accelerated growth in commercial, research, and military environments.

(Sampoli 2007)

My conclusion is, after pros and cons, that VPN is a good and necessary solution, and that VPN’s are enhancing the security on corporate networks.

References

 
Sampalli, Srinivas.&nbsp;(2007). Chapter 4. ”Security in Virtual Private Networks”. [Online] Available from:&nbsp;http://ieeexplore.ieee.org.ezproxy.liv.ac.uk/xpl/ebooks/bookPdfWithBanner.jsp?fileName=5237809.pdf&amp;bkn=5237765&amp;pdfType=chapter. (Accessed 15-10-2014)
 Thrivenetworks. “The Pros and Cons of Using a Virtual Private Network”. [Online] Available from: http://www.thrivenetworks.com/blog/2011/07/28/the-pros-and-cons-of-using-a-virtual-private-network/. (Accessed 15-10-2014)


Best wishes
Bo
	              
	              Hi everybody
This week, I have:

Re-learned basic things of networks, protocol’s etc., things I’ve kind of falling in the habit of taken for granted.
Thought back to the days (80s and 90s) when I used to work with coaxial networks and the problems we had of finding the errors.
Thought back to the days (the 90s) I worked with Token Ring which I thought was a super network compared to other networks at that time. But also these networks could tease, especially when there was a defect Token Ring card in a PC.
Focused on security issues, e.g. a great focus and discussion on hackers and super TED videos (I think I saw all of the hacker videos + more)

Best wishes, and good night (or perhaps I’ll take a sneak peak at week 7) J
Bo
	              
	              Hello Anthony,
It was a good recap week with respect to all network topologies and their usage. The discussions focussed on latest trends in the field of computer security and how exposed and vulnerable we are in today's digital era. It was also very interesting to read through others' DQs and understand their thoughts and preferences in computer security.
Best regards,Kharavela
	              
	              Dear Dr. Anthony and All,
&nbsp;
VPNs need four key elements from the IPSec Protocol security protocols, key exchange mechanisms, and algorithms required for encryption and secure key exchange, and SA definitions and maintenance. IPSec however accomplishes this at layer 3.Because of the native security in IPSec VPNs, this technology has become the dominating protocol used in today’s enterprise, service providers, and government networks (James Henry Carmouche, 2007).
&nbsp;
IPSec was developed while the next generation of IP protocols, IPv6, was being developed. This meant that IPSec would be natively supported in IPv6. But when adoption of IPv6 was slow, it was decided to make IPSec backward compatible with IPv4 to provide security for the IPv6 packets. Through IPSec VPNs can accomplish data confidentiality, data integrity, sender non-repudiation and message authentication. IPSec’s native cryptography, hashes, and headers protocol produce a very secure and stable VPN application (James Henry Carmouche, 2007).
&nbsp;
Utilizing a VPN results in a significant increase in network load and time delay. This is, however, a small price to pay for the security and privacy offered by a virtual private network. VPN is the most effective and versatile form of secure communication across long distances. More bandwidth is required to handle the additional network load. A VPN may require computer hardware upgrade or even additional hardware. If network resources are not developed and expanded to meet the new VPN needs; companies may experience slower response times in e-mail, file delivery, and database inquires. 
&nbsp;
Model research showed that using a VPN to conduct database transactions adds an additional 446% delay to the query. Significant delay is also added to e-mail and FTP transactions. Leased lines and frame relay networks were the early expensive solution for private networks. Their higher expenses and greater hardware requirements lead to the spread of VPN technology. 
&nbsp;
Development of PPTP and L2FP protocols led to the integration of VPN technology. The need for increased security led to the integration of IPSec technology into the existing VPN framework. This also changed the focus of VPN technology from layer 2 to layer 3. Today users can remotely access resources through a secure, cheap and convenient virtual private network.
&nbsp;
References
&nbsp;
Catherine Paquet(2013); Network Security Concepts and Policies; Available on: (http://www.ciscopress.com/articles/article.asp?p=1998559) (Accessed on: 15-Oct-2014)
&nbsp;
James Henry Carmouche (2007); IPsec virtual private network fundamentals; Available on: (https://openlibrary.org/books/OL23139089M/IPsec_virtual_private_network_fundamentals) (Accessed: 15-Oct-2014)
&nbsp;
BR,
&nbsp;
KingTan Yu

	              
	              Hi Bo,
Thanks for the information. As I'm not in China that often as I used to be, now I only use free VPN service of GoAgent by Google when I visit China. GoAgent isn't technically a service. It's just the VPN/proxy service source code you can install on free Google App Engine, the Google cloud. Then you build your own VPN/proxy server.
When I stayed in China most of the time, I used commercial VPN (but a different one, namely StrongVPN). I needed more reliable connection as I need Google on daily, oh sorry, every-minute-bases.
Technically it shouldn't be very hard for China to completely block all these VPN usages (e.g. by machine learning). It's widely believed that China can actually decrypt the VPN encryption, in real time. So everything is still under the supervision. And that's why people are pardoned for using it. One evidence is that China blocked the PPTP and L2TP connections and if you increase your encryption from 128bit to 256 bit, you will also be denyed.
I will be travelling in Shanghai, China next week. It's time to test if my GoAgent still works:-)
br, Terry
References:
http://yro-beta.slashdot.org/story/12/12/20/1456251/vpn-providers-say-china-blocks-encryption-using-machine-learning-algorithms
http://community.spiceworks.com/topic/555064-china-blocking-vpn-traffic-if-256bit-encryption-used
	              
	              Dear Anthony and Everyone,
&nbsp;
Just as my initial discuss, one of the network security vulnerabilities that we always overlook is the USB Flash Drives. The best of the case of this security risk is the instance of Edward Snowden that Edward Snowden reportedly needed to walk away from the National Security Agency building with a cache of national secrets was a USB flash drive.
&nbsp;
In 2009 Snowden began work as a contractor for Dell, which manages computer systems for multiple government agencies. Assigned to an NSA facility at Yokota Air Base near Tokyo, Snowden instructed top officials and military officers on how to defend their networks from Chinese hackers. During his four years with Dell, he rose from supervising NSA computer system upgrades to working as what his résumé termed a "cyberstrategist" and an "expert in cyber counterintelligence" at several U.S. locations. In 2011 he returned to Maryland, where he spent a year as lead technologist on Dell's CIA account. In that capacity, he was consulted by the chiefs of the CIA's technical branches, including the agency's chief information officer and its chief technology officer. U.S. officials and other sources familiar with the investigation said Snowden began downloading documents describing the government's electronic spying programs while working for Dell in April 2012. Investigators estimated that of the 50,000 to 200,000 documents Snowden gave to Greenwald and Poitras, most were copied by Snowden while working at Dell.
&nbsp;
BR,
&nbsp;
KingTan Yu
&nbsp;
Reference:
&nbsp;
Bamford, James (2014); Edward Snowden: The untold story of the most wanted man in the world; Available on: (http://www.wired.com/2014/08/edward-snowden/) (Accessed on: 15-Oct-2014)

	              
	              
  
VPN's do provide a good secure tunnel for corporations and their employees. The benefits&nbsp;include additional security enablement, lower cost compared to other technology solutions, flexibility for expansion. Some challenges include complex infrastructure, supportability, interoperability and added security concerns. The increased VPN use increases the overall computer security profiles
 
robin

 
Reference
 http://www.thrivenetworks.com/blog/2011/07/28/the-pros-and-cons-of-using-a-virtual-private-network/ 
 
	              
	              Hi all,
&nbsp;
Over the Labor Day weekend, hackers leaked nude images of a number of celebrities including "Hunger Games" star Jennifer Lawrence. The images appear to have been acquired from Apple’s iCloud. So, iCloud is obviously insecure and everyone should stop using it—right?
&nbsp;
Let's just cool our jets. Yes, iCloud appears to have played a role in at least some of the hacked nude celebrity images, but details are still too sketchy to start connecting dots that indict the entire Apple cloud storage service.
&nbsp;
Apple has issued a statement confirming that certain celebrity iCloud accounts were compromised but notes, "None of the cases we have investigated has resulted from any breach in any of Apple's systems including iCloud or Find my iPhone. We are continuing to work with law enforcement to help identify the criminals involved."
&nbsp;
Boris Gorin, head of security engineering at FireLayers, thinks we shouldn’t be throwing stones at iCloud. “The images leaked have been gradually appearing on several boards on the net prior to the post at 4chan—making it reasonable to believe they were not part of a single hack, but of several compromises that occurred over time.”
&nbsp;
Gorin shared a theory the celebrities may have been hacked while connected to an open public Wi-Fi network at the Emmy Awards. If they accessed their personal iCloud accounts, attackers connected to that network would have been able to intercept and capture the username and password credentials. That's not a security flaw with iCloud and having a strong or complex password wouldn't offer protection against transmitting that password in clear text on a public Wi-Fi network.
&nbsp;
But all we have right now is speculation coupled with security experts and vendors crawling out of the woodwork to preach the same tired advice about complex passwords, password management tools, and two-factor authentication. Granted, all of those things have value and might play a role in providing better protection against getting hacked in general, but we have no idea whether any of those things played a part in this attack or could have helped prevent it.
&nbsp;
Don’t get me wrong— I am sure there are security issues in iCloud that savvy attackers can exploit. But no one will be served if we don't take the time to understand what really happened so we can have a rational discussion of tools or techniques that would actually have helped prevent this breach.
&nbsp;
Maybe iCloud is to blame. Maybe the celebrities used ridiculously simple passwords. Maybe the accounts were hacked because the victims logged in to sensitive accounts on a public network. Whatever happened, scapegoating iCloud will not solve anything or help avoid similar attacks in the future.
References
&nbsp;
Dave Lewis (2014); iCloud Data Breach: Hacking And Celebrity Photos; Available on: (http://www.forbes.com/sites/davelewis/2014/09/02/icloud-data-breach-hacking-and-nude-celebrity-photos/) (Accessed on: 15-Oct-2014)
&nbsp;
Peter M. Sandman (2014); Data Breaches: Managing Reputational Impact; Available on: (http://www.psandman.com/articles/breaches.htm) (Accessed on: 15-Oct-2014)

	              
	              Hi Dr. Anthony,
Many great lessons learnt this week. I found the TEDx videos very informative, I didn't know that much about the established hacker group and some of the state lead attacks that have been successful in compromising security and safety of Soverign States, a bit frightening. This exposure had we reading a whole lot of hacker related reports and getting to understand a bit more about some of the communities that exist. Truth be told vulnerabilities exists in all systems, some just aren't exposed as yet.
I never believe that network topology would have given me such a ride but this week was full with a lot of other activities for me as well. But it was a good week overall. i'm still working on trying to find the right fit and balance to study/work/life, I know I'll get it cause there ain't no giving up.
Thanks to my colleagues for discussion filled week, see you all in a few days as we look forward to next week and the challenges it will bring.

Regards,
Ricardo
	              
	              Hi Anthony,It is an inescapable fact that hackers are some of the master minds of the earth; they hack into large companies, clown credit cards, steal personal information, just to name a few. Additionally, quite a few high profile companies have been hacked; Google, Yahoo, Amazon and eBay to name a few. These companies may have believed that they are secure, that their employees are smart and are some of the best technologies in the world. However that did not stop them from being attacked. Because these events among millions of others, there has been much stigma surrounding what hackers do, society deeming them as criminals instead of the geniuses that they actually are, they force us to address the flaws in our systems.&nbsp;However, as many are aware not all hackers are bad, some hackers actually report security flaws to the relevant authorities. Perhaps what society should do is try to work with or have consultations with the hackers. That being said I believe that some form of reward can be given to hackers for discovering flaws in the systems, so as to deter them from committing further attacks. Another suggestion would be to hire them as well, to have them look into your system and examine it for loopholes. &nbsp;However because they are different kinds of hackers and each may have their own motivation and gain in mind, this seeming to be a good strategy to tighten security systems, can be a very risky as well. Based upon the offenses that the person may have committed it may not be a good idea to hire them or sometimes they may be wanted by the police, thus you cannot hire them. Additionally, persons who are aware that an entity has hired such an individual may lose trust in that entity. Based on his past hacking activities, you may also want to be skeptical of the information you give him access to. Take for instance a bank may not want to give a credit card hacker access to customers records. On the upside their ingenuity may uncover flaws in the system, which you couldn't have imagined.&nbsp;References:TED (n.d.) Keren Elazari: Hackers: the Internet's immune system, [2] [Online]. Available from:http://www.ted.com/playlists/10/who_are_the_hackers (Accessed: Octiober 10,&nbsp;2014).&nbsp;
Cole, E. (2001), Hackers beware: defending your network form wiley hacker, Pages 269-273, Publisher: New Riders Publishing. ISBN: 0-7357-1009-0 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
	              
	              Thanks for the summary, Terry. The list generated suggests that you had an interesting, albeit a demanding, study week :-).Regards,Anthony
	              
	              Hi Anthony.
The week 6 topics focused on network topology concepts as well as network security.
A review of network topologies helped distinguish between the bus and star network topologies and where each may be applied.
The security area delved deeper into the understanding of vulnerabilities in computer systems that are exploited through various forms of attacks. We reviewed issues to do with computer hacking and looked into major network security breaches that have occurred and the resulting impact and possible ways that the problem could have been forestalled in the first place.
The topics were as usual quite engaging and thought provoking with diverse and useful opinions in the DQ. The related videos on security issues on Ted Talks were also very informative on the level of exposure and potential threats that we face in computer security

Joseph

	              
	              Babatunde,
I understand your points, while some of individuals may not be interested in working for financial companies and so forth, there is also the option of consulting with them or just interviews with them to know how they think in order to get some further guidance. It would all depend on the type of hacker and past convictions. Additionally there will be issues of trust, if certain hackers are hired. Thus a lot of consideration, thought and vigilance needs to be in place if a hacker is to be hired.
Tanisha
	              
	              Hi All
An interesting article that is relevant to our discussons this week.
http://securityintelligence.com/heartbleed-and-shellshock-the-new-norm-in-vulnerabilities/#.VD_dPYvF_OA
Legacy code = a 'Hackers' treasure trove of vulnerabilities, until the security community catches up.
Best Wishes, Craig
	              
	              Hi Dr Anthony,

&nbsp;

This was a good week that dealt with revisits of Network Systems; covering network topologies (bus and star topology) in particular while focusing on security risks.

TED videos have really broadened my knowledge not only knowing more on hackers but in other technology areas as well. 

It was also very interesting reading through colleague’s discussions; understand their thoughts while contributing.

Another good insightful week

Best Regards, Babatunde
	              
	              Hi King Tan Yu
Very interesting, most news that I haven't heard before.
Br. Bo
	              
	              Thanks for the summary, Belinda. Glad to hear that you found the week's content interesting and useful.Regards,Anthony
	              
	              Thanks for the comments and summary, Numan. Yes, the various discussion were indeed illuminating, thanks to the many good contributions.Regards,Anthony
	              
	              Hello Anthony,
This week was quite interesting as a lot was learnt and are as follows:
network topology,
hackers and their likes,
worms,viruses,
computer security risk,
preventive measures etc

Regards, martins
	              
	              Thanks for the summary/comments, Martins. Glad to hear that you found the week's content interesting and useful.Regards,Anthony
	              
	              The DQ1 discussions centred on&nbsp;network systems security risks, including the security vulnerabilities commonly associated with such systems. &nbsp;We discussed the practice of Companies employing ex-hackers as security consultants,as well as high profile computer systems security breaches.
The posts also made reference to state-sponsored computer hacking, and how the use of VPNs impacted computer security.
Good effort all round with the discussions.
Anthony
	              
	              Dr. Anthony,
This week was good and informative as per norm. There was lots of information on hacking and security, which has some how gotten me more skeptical of what I do on the computer and the Internet. Thus, I am trying to secure my information. The was information on networks served both as a refresher and informative for me. I am looking forward to up coming weeks.
Thanks,
Tanisha
	              
	              Thanks for the comments, Craig. Yes, the TED videos were indeed quite interesting and thought-provoking.Regards,Anthony
	              
	              Thanks for the summary, Augusto. Maintaining a good balance between work, home and study is important - it often determines how well the student performs on the Master's program.Regards,Anthony
	              
	              Thanks for the summary, Chris. Glad to hear that you found the week's content interesting. &nbsp;The TED videos were also quite useful.Regards,Anthony
	              
	              Thanks for the summary, Bo. &nbsp;Yes, Token Ring networks were noted for being robust. &nbsp;I thought that the token-ring system used up and down -stream monitoring schemes to allow easy detection of network faults - does this not encompass defective token-ring cards?Regards,Anthony
	              
	              Thanks for the summary, Kharavela. Yes, the area of compter security is evolving rapidly, and we still have quite a way to go.
Regards,Anthony
	              
	              Thanks for the summary, Ricardo. Yes, the TED videos were indeed thought-provoking.Regards,Anthony
	              
	              Hi Anthony
I guess not for all instances - or perhaps I suffer from temporary memmory laps :-) ? I
think I have forgotte just have it worked in detail. But I recall it was mostly easy to find the errors, and I do remember it as you write, that if a machine had an error of some kind, the traffic would just turn around until it came back to the machine from the other side, and the turn around again etc. . But I also recall that in certain instances there were spcial problems with the network cards which could "kill" the entire network, and the only solution was to put in a new network card.
Br. Bo
	              
	              Thanks for the succinct summary, Joseph. Yes, it looks like it was quite a productive study week for the class :-).Regards,Anthony
	              
	              Thanks for the comments, Babatunde. Good to hear that the week was very productive.Regards,Anthony
	              
	              Thanks for the comments, Tanisha. Yes, it is a good idea to always secure personal data :-).Regards,Anthony
	              
	              One of the curious things about algorithms is while they are written to resolve a specific problem, there is no specific way they must be written. Think of it like cooking: If you want to prepare a certain meal, you have your own set of steps, utensils and ingredients that you use. If someone else makes the same meal, the nuances of what they use to create the meal can differ. Nevertheless, the result is still, roughly, the same meal.
The same basic principle applies to mathematical algorithms, too. Translate the objective of 'making a meal' to some other task, such as analysing a string of numbers or letters, or even deciding which kind of ice cream a group of people will buy. The information you start with are the 'ingredients', your 'steps' are each line of instructions and your 'utensils' are your preferred problem solving tools. Though the details vary, the objective achieved by the algorithm is the same. For this Discussion, you will write pseudo-code based on this very problem, and compare it with that of your colleagues. From this, you will gain a clearer picture on how much algorithms can vary.
This is a two-week Extended Discussion. In this Week, you'll write and post your own algorithm. In Week 8, you'll respond to at least one colleague, respond to feedback on your own algorithm, and revise it as needed.
To prepare for this Discussion

Review your weekly Learning Resources with a focus on interpreting and tracing algorithms and writing pseudo-code. 

To complete this Discussion Post: Choose one of the three following options and create an initial post in which you generate pseudo-code for an algorithm. Make your initial post by Wednesday (Day 7) of this Week.

Option 1: Numbers. Your algorithm should:


Search a string of at least five numbers (for example, 37540)
Identify all of the substrings that form numbers that are divisible by 3.
For example, applying the algorithm on the string 37540 should produce the following substrings&nbsp;(not necessarily in this order): 0; 3; 75; 54; 375; 540.


Option 2: Word Search.


Make a list of five words, 3-6 letters in length.
Create a string of approximately 30 letters containing some of the five words.
Your algorithm should identify all of the substrings of the longer string that match any of the five words you generated, and the number of times each one appears
For example: If you chose the words structure, such, system, blue, red, and your algorithm operates on the string jkdistructuredstrusyssystemoon, your algorithm should report that the string contains the words structure, red and system each one time, and the words such and blue zero times.


Option 3: Consensus Algorithm. Ten people need to decide which one flavour of ice they will order as a group. There are 3 types of ice cream from which to choose.


Design an algorithm which can survey and re-survey each person, with the goal of reaching consensus on one kind of ice cream. The algorithm can present answers to each person in the group until a consensus is reached.
This task is open-ended. You may make assumptions as needed. Explain these assumptions in your initial post.
Determine whether there are situations in which your algorithm may never result in an answer, and account for this when writing your pseudo-code.



Respond: By Sunday (Day 4) of Week 8, respond to at least one colleague. Address the following:

Interpret your colleague's algorithm.
Trace your colleague's algorithm for correctness.
Compare your algorithm to your colleague's algorithm.


Post your own algorithm in your response to provide context for your colleagues.
Focus on the differences between the effectiveness of the two algorithms.
Recommend improvements to your own algorithm. (You will post a revised version of your own algorithm on Monday of Week 8.)


Be sure to support any claims you make.

For all Discussions (unless stated otherwise):

Create a single document with your initial post.
Post the text of your document to the Discussion Board for this Week, and upload the document using the Turnitin submission link for this Discussion.
Do not submit your follow-up responses to Turnitin.

Click on the Reply button below to reveal the textbox for entering your message. Then click on the Submit button to post your message.
	              
	              


Option 1: Numbers. Your algorithm should:

Search a string of at least five numbers (for example, 37540)
Identify all of the substrings that form numbers that are divisible by 3.
For example, applying the algorithm on the string 37540 should produce the following substrings (not necessarily in this order): 0; 3; 75; 54; 375; 540.










def substrings_divisible_by_3(numberstring):
    result = set()
    for begin in range(0, len(numberstring)):
        for end in range(begin+1, len(numberstring)+1):
            subnumber = numberstring[begin:end]
            if int(subnumber) % 3 == 0:
                result.add(subnumber)
    return result








Blow are the tests I used to "drive" out and verify the above solution.









# 1-number string that is not divisible by 3 should return empty
assert substrings_divisible_by_3("1") == set()
# 1-number string that is divisible by 3 should return the number
assert substrings_divisible_by_3("3") == {"3"}
# 2-number string with 1 number divisible by 3 should return that number
assert substrings_divisible_by_3("62") == {"6"}
# 2-number string with both number divisible by 3 should return 3 numbers
assert substrings_divisible_by_3("69") == {"6", "9","69"}
# test the example
assert substrings_divisible_by_3("37540") == {"0", "3", "75", "54", "375", "540"}






	              
	              
Week 7 DQ - Designing an algorithm, part 1

1. Introduction
I’ve heard the term pseudocode before, but I have neither seen nor used it before – or?
Personally I’m not a programmer, though I have written a great deal of queries in IBM QMF (IBMDB2forzOS), but I’m not an expert. Normally I have been in either the project manager or the customer/user role. In these roles I have either, when necessary:

Re-written requirements specifications, in a pseudocode like style trying to translate the user’s requirements to the programmers.
Read/Interpreted the programmers code (especially SQL and DL/1 on mainframe) in order to spar with/help the programmer in analysing possible or found errors, and I have done so by either:
Translating their code to a pseudocode like style, or
Drawing process diagrams 


2. Abstract
Pseudocode seems like an excellent technique to use in the process of developing an algorithm. It allows expressing ideas in an informal way creating a logic overview of the planned code.
I know my code below could have been simpler, but it’s hard to stop when my primary interest is the final user program. Hence I’ve tried to make it user friendly, i.e. making a procedure that asks the user to write the initial number (i.e. 37540) and which number to divide by (i.e. 3). Then the algorithm calculates and finds the integer results from the calculation (e.g. 37540 / 3), and presents the integers on the screen.
The code could also be more complex/longer, e.g. the sub_procedure Generate(Init.List) should be improved, but that will obviously be done in the final algorithm.
3. Procedure Find.Integers
As mentioned, in my effort of being GUI minded, I have decided to define some sub procedures within the main procedure.
a. sub_procedur UserInputs(init_no; init_div)
define init_no = 0
define init_div = 0
init_no = integer value from&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; -&nbsp;- save the input into init_no
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; echo “Write the initial integer number”&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - - the user writes 37540
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; If (init_no is not an integer
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; then echo “Your input is not an integer, try again”
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; init_div = integer value from 
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (echo “Write integer number to divide by”)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - - the user writes 3
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; If (init_div not an integer
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; then echo “Your input is not an integer, try again”
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; else echo “Thank you. Calculation in progress”
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ) end if
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ) end if
goto sub_procedure Generate(Init.List)
b. sub_procedure Generate(Init.List)
define Init.List()
Init.List = (find number combinations from init_no) 
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; start with init_no(1st)number&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- - returns (3)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; then init_no(1st and 2nd)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- - returns (37)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; etc&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- - returns (375; 3754; 37540)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; start with init_no(2nd)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- - returns (7)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; then init_no(2nd and 3rd)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- - returns (75; 754)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; etc&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- - returns (5740; 5; 54; 540; 4; 40; 0)
echo “The initial combinations are: “&amp;init.list&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- - shows on screen all the numbers
goto sub_procure Calculate.Integers
c. sub_procedure Calculate.Integers
define result = 0 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; - - temporary container for calculated results
define v_no = 0 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;- - temporary container for count of values
define Integer.List()&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- - Integers will be stored here
v_no = Count(values in Init.List) +1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- - returns 16, i.e. 15 + 1
if (v_no &lt; 1
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; then goto sub_procedure Finished.List &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;- - The procedure prints an error description
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; else 
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; while v_no =&gt;1 do the following steps
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; v_no = v_no – 1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- - first number is 15 in first iteration, then 14 etc.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; result = Value(v_no) / init_div&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- - returns the 15th number, i.e. 0, then 40 etc.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; If (result is an Integer&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- - Integer is a boolean, returns True if integer
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; then add result to procedure Generate(Int.List)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- - the procedure adds and sorts values numerically
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; else goto sub_procedure Calculate.Integers&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- - tries next number
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ) end if
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; end while
) end if
d. sub_procedure Finished.List
echo “The final integers are: “&amp;Integer.List&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - - returns (0; 3; 75; 54; 375; 540)
End procedure Find.Integers
4. Users perspective
Seen from the users perspective, i.e. the GUI, the final result on screen will show:
&nbsp; &nbsp; &nbsp; Write the initial numer:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;37540
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Write the integer number to divide by: &nbsp; &nbsp; &nbsp; &nbsp;3
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; The initial combinations are: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 3; 37; 375; 3754; 37540; 7; 75; 754; 7540; 5; 54; 540; 4; 40; 0
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; The final integers are: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;0; 3; 75; 54; 375; 540
5. Conclusion
Especially for a beginner, but I imagine also for an experienced programmer, the use of pseudocode is an excellent technique to use, giving the necessary overview and a chance to optimize before starting, or even during, developing the final algorithm. Though I have not done so in this assignment, I would also recommend:

Using good old fashioned process diagrams (Brookshear, figures 5.8, 5.9, 5.23). 
Always focus on error handling as soon as possible rather than late, i.e. instead of accepting incorrect inputs one should rather handle the input right away and hereby, hopefully, it won’t be necessary with control routines.

6. References
Brookshear, J. Glenn, (2012). Computer Science An Overview. 11th edition. Boston Massachusetts: Pearson. Addison Wesley.
IBMDB2forzOS. (2014), QMF Best Practice: QMF 11 Classic...CURRENT EXPLAIN MODE”. [Online] Available from: https://www.youtube.com/watch?v=JbqQLAEkuEc. (Accessed 18-10-2014)
Johnson, Mary. (2013). “Pseudocode Tutorial”. [Online] Available from: https://www.youtube.com/watch?v=rSz7549WSjY. (Accessed 18-10-2014)
UoL. (2014). “Computer Structures — Lecture Notes Week 7: Algorithms”. &nbsp;[Online]. Available from: https://elearning.uol.ohecampus.com/bbcswebdav/institution/UKL1/201480_SEPTEMBER/MS_CKIT/CKIT_501/readings/UKL1_CKIT_501_Week07_LectureNotes.pdf. (Accessed 18-10-2014)
&nbsp;
Best wishes
Bo W. Mogensen

	              
	              Introduction
It has been said the most major part of a computer is not the hardware, but software; programmers adopt different computer languages to compile the software, known as programming. There are many classes of computer languages that the language programmers use has an impact on the manner who consider about a question, thus the more important is to grasp the algorithms rather than the language itself.
Algorithm
According to the definition of Anany V. Levitin (2011) that “An algorithm is a sequence of unambiguous instructions for solving a problem, i.e., for obtaining a required output for any legitimate input in a finite amount of time.” This refers to all of clear-cut computational processing which adopts certain values acting as input and brings about certain values acting as output. It is similar as a cookbook that an algorithm gives a step-by-step means for settling the computational issue. It is unlike of the programs that an algorithm is not based on a designated machine, compiler, system or programming language. It is mathematical entity that is able to be deemed as executing on certain kind of chimerical computer with a limitless word size and a limitless RAM. The design of algorithm is total regarding the mathematical principle and theory behind the layout of favorable programs.
Pseudocode
&nbsp;
Due to algorithm is not relied on any specific programming languages, generally, it is not taking advantage of a particular programming language to present the algorithm; but a fictitious and bogus one, called as Pseudocode. Pseudocode is a readable and detailed depiction of what the computer algorithm or program has to carry out, presented in an officially-styled human language instead of in a programming language. Pseudocode is occasionally exploited as a detailed procedure in the processing of a program development. It enables head programmers, designers or developers to present the layout in extremely detailed and offers developers an exhaustive template for afterward process of compiling code during a particular programming language. 
&nbsp;
Since pseudocode is readable and detailed, it is able to be examined by the team of programmers and designers as the approach to make sure that the practical programming is appropriate to conform design standards. Debugging at the pseudocode phase is more inexpensive than during the stage of development procedure. When the pseudocode is admitted, it will be substituted by the syntax and vocabulary of a programming language to rewrite.
&nbsp;
Pseudocode is not a real programming language that is unable to be compiled or truly run in the computer systems, as well as there is not any truly rules of syntax for formatting. The major advantage of pseudocode is to let programmer or designers to focus on the algorithms without concerning about all the syntax or formatting details of a designated programming language. The designers in fact are able to use the pseudocode even without the actual knowledge of the programming language which will be applied for the ultimate implementation.
&nbsp;
Word Search
&nbsp;
There are some ways to search a particular word during a given string, for instance, search the word "system", "blue" and "red" during the given string "jkdistructuredstrusyssystemoon", the simplest approach is that let the particular word ("structure", "such", "system", "blue", "red") using the "try and match" method to find the word out. As this approach is easy to understand and implement, but less efficiency. It is called "Naive algorithm".
&nbsp;
Naive algorithm
The algorithm is known as naive as it is straightforward, simple and easy to comprehend. But when it actually executing at computer environment, it does not present express a acceptable efficient standard which is usually requiring much time or RAM in spite of hunting for a proper answer or it does not hunt for a best answer to a optimization issue, and more advanced algorithms are able to be devised and achieved with more smart techniques and attentive thinking. Naive algorithm is liable to invent, always liable to confirm proper, and always evident to the debugging programmers. Even if Naive algorithm is inefficiency, it is usually as the steppingstone to more efficient algorithms, possibly even asymptotically the best algorithm, particularly once its efficiency is able to be bettered by selecting more suitable data architectures.
Adopting Naive algorithm for word search
&nbsp;
The naive algorithm for word search implies trying to match the given word at any probable location during the disorganized string, conducting a M (m) match at every step that n is the length of the given word as well as providing a M (mn) runtime that m is the length of the disorganized string. The detailed thinking of Naive algorithm for word search as the following: 

Let the given string S (such as "jkdistructuredstrusyssystemoon") as L1L2L3L4...Lm, where L is letters of the string S. 
Let the particular word W (such as "structure", "such", "system", "blue", "red") as C1C2...Cn, where C is the characters to constitute the particular word, as well as m &gt;=n and is constant .
Let W travel from the beginning of the string S (left hand side) to the end (right hand side) upon the S of character by character parallelly, simultaneously; compare the S and W regularly one by one. Once the matching is found, and then the search completed successfully (e.g. C1 = L3, C2= L4, C3= L5 … Cn = L (n+2)); otherwise, keep continuously move to right, until the last letter has been contrasted, then the word is not existing in the string.

The pseudocode of naive algorithm for word search (string matching algorithm) as following:
&nbsp;
Set string as S
Set Word as W
Set k as counter, where k is constant
Set m = length(S)
Set n = length (W)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Where m and n is constant and m &gt;= n
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; k=0 (set the counter to zero)
For k =0 to m-n
If W (1...n) = S (k+1…k+n)
Then print k (the matching is found, the search is successfully, then output the result of how many time to match (k))
Else
k=k+1 (continue to match, loop until k= m-n)
Print word is not found at the string (compare the entre string, but it is non matching)
&nbsp;
Use the naïve matching way to search the word, the average of comparisons can be easy to calculate that supposing the letters in S and W is randomly arising, for the finite letter set, assume which the number of x, then the times of comparisons should be:
(m-n+1)* (1-x^-n)/(1-x^-1)
= 2 (m-n+1)
&nbsp;
This is really inefficiency, thus there are some other smarter algorithm, such as Knuth–Morris–Pratt string searching algorithm (known as KMP algorithm).
&nbsp;
KMP algorithm
&nbsp;
The KMP algorithm was conceived in 1974 by Donald Knuth and Vaughan Pratt, and independently by James H. Morris. The three published it jointly in 1977. The KMP algorithm was envisaged by Donald Knuth and Vaughan Pratt as well as by James H. Morris alone during 1974. In 1977, they announced to publish it together. KMP algorithm make use of searching for positions of a given word in a particular string by utilizing the examination that once a non-matching arises, the word itself reflects enough message to decide where the next match would start, and then ignoring re-observation of before matched letters, thus improve the search efficiency.
&nbsp;
According to Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest and Clifford Stein (2009)’s citing, the pseudocode of KMP algorithm as the following:
&nbsp;
KMP-MATCHER(T，P) 
　　1　n ← length[T] &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
　　2　m ←length[P] 
　　3　π ← COMPUTE-PREFⅨ-FUNCTION(P) 
　　4　q ← 0　#Number of characters matched. 
　　5　for i ← 1 to n　#Scan the text from left to right. 
　　6　do while q&gt;0 and P[q+1]≠T[i] 
　　7　doq ← π[q]　#Next character does not match. 
　　8　ifP[q+1]=T[i] 
　　9　thenq ← q+1　#Next character matches. 
　　10　ifq=m　#Is all of P matched? 
　　11　then print “Pattern occurs with shift” i-m 
　　12　q ← π[q]　#Look for the next match. 
　　COMPUTE-PERFⅨ-FUNCTION (P) 
　　1　m ← length[P] 
　　2　π[1] ← 0 
　　3　k ← 0 
　　4　forq ← 2 to m 
　　5　do while k&gt;0 and P[k+1]≠P[q] 
　　6　dok ← π[k] 
　　7　ifP[k+1]=P[q] 
　　8　thenk ← k+1 
　　9　π[q] ← k 
　　10　return π
&nbsp;
Conclusion
&nbsp;
For the word search programming, there are many different algorithms to realize the goal. Such as FC-RJ, FLC-RJ and FMLC-RJ algorithms, those algorithms are quite efficiency that they enhances the execution time as compared to the Naive algorithm by 7.4, 16.2 and 20.6% (Rami H. Mansi and Jehad Q. Odeh (2009)). Although Naive algorithm is inefficient, it is obvious enough to comprehend and easy to find out the errors, this is important for the programmers to conduct debugging.
&nbsp;
References
&nbsp;
Anany V. Levitin (2011); Introduction to the Design and Analysis of Algorithms (3rd Edition); Available on :(ftp://doc.nit.ac.ir/cee/jazayeri/.../Algorithms/Books/Design%20&amp;%20Analysis%20of%20Algorithm.pdf) (Accessed on: 19-Oct-2014)
&nbsp;
Guy M.Haas (1999); Pseudocode; Available on: (http://www.bfoit.org/itp/Pseudocode.html) (Accessed on: 19-Oct-2014)
&nbsp;
H.W. Lang &amp; FH Flensburg (2013); Knuth-Morris-Pratt algorithm; Available on: (http://www.inf.fh-flensburg.de/lang/algorithmen/pattern/kmpen.htm) (Accessed on: 19-Oct -2014)
&nbsp;
lbackstrom (2014); The Importance of Algorithms; Available on: (http://community.topcoder.com/tc?module=Static&amp;d1=tutorials&amp;d2=importance_of_algorithms) (Accessed on: 19-Oct-2014)
&nbsp;
Leonard Soicher &amp; Franco Vivaldi (2004); Algorithmic Mathematics; Available on: (http://www.maths.qmul.ac.uk/~leonard/ambook.pdf) (Accessed on: 19-Oct-2014)
&nbsp;
Rami H. Mansi and Jehad Q. Odeh (2009); On Improving the Naïve String Matching Algorithm; Available on: (http://www.medwelljournals.com/fulltext/?doi=ajit.2009.14.23) (Accessed on: 19-Oct-2014)
&nbsp;
Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest and Clifford Stein (2009); Introduction to Algorithms (3rd Edition); Available on: (http://ldc.usb.ve/~xiomara/ci2525/ALG_3rd.pdf) (Accessed on: 19-Oct-2014)

	              
	              Week 7 Discussion Question - Designing an algorithm, part 1

Babatunde KOLAWOLE 


In this discussion I’d choose OPTION 3: Consensus Algorithm.

Pseudo-code 

Pseudo-code is a description of a computer programming algorithm that uses the structural conventions of programming languages, but omits detailed subroutines or language-specific syntax.&nbsp; (Wikipedia)

Designing an algorithm: Ice Cream (3 types) 

Assuming flavors of ICE CREAM are A, B &amp; C

Declare A as integer,

Declare B as integer,

Declare C as integer,

Declare HIGHEST as string,

Declare HIGHESTVALUE as integer


DIALOGUE:

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; A = 0

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; B = 0

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; C = 0

For each person

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Print “choose between A, B &amp; C”

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; If A is chosen, set A = A + 1&nbsp;&nbsp; 

If B is chosen, set B = B + 1

If C is chosen, set C = C + 1

Next

If A &gt; B then

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; HIGHEST = “A”

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; HIGHESTVALUE = A

Else

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; HIGHEST = “B”

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; HIGHESTVALUE = B

End if

If C &gt; HIGHESTVALUE then

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; HIGHEST = “C”

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; HIGHESTVALUE = C

End if

If HIGHESTVALUE &lt; 10 then

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; For each person

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Print “Most People Chose “&amp; HIGHEST

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Next

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; GOTO DIALOGUE

Else

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Print “Everybody wants “&amp; HIGHEST

End if


Conclusion

For consensus protocol, there must be an agreement: in which every correct process must agree on an equal value. Weak validity: in which if every correct process receives the same input value; they must all give same output of that value. Strong validity: in which on each correct process, its output must be the input of some correct process. And then Termination: in which all processes must decide on an output value.

&nbsp;

Reference:

Laureate Online Education. (2014). Computer Structures – Lecture Notes Week 7: Algorithms: [online]. Available from: https://elearning.uol.ohecampus.com/bbcswebdav/institution/UKL1/201480_SEPTEMBER/MS_CKIT/CKIT_501/readings/UKL1_CKIT_501_Week07_LectureNotes.pdf (Accessed on: October 17, 2014)

J.G. (2012) Computer science: An overview. 11th ed. Chapter 5, pp. 188-222. Boston: Pearson Education / Addison-Wesley. (Accessed on: October 17, 2014)

Guy M.H (1999). “Pseudo-code” [online] Available from: http://www.bfoit.org/itp/jargon/jargon_p.html#pseudocode (Accessed on: October 18, 2014)

Thomas H. C, Charles E. Leiserson, Ronald L. Rivest and Clifford S. (2009); Introduction to Algorithms (3rd Edition); Available from: http://ldc.usb.ve/~xiomara/ci2525/ALG_3rd.pdf (Accessed on: October 18, 2014)

Wikipedia free encyclopedia Consensus (computer science) [online] Available from: http://en.wikipedia.org/wiki/Consensus_%28computer_science%29 (Accessed on: October 18, 2014) 
	              
	              Hi Guys,
Just in case I missed something? The deadline to submit this is next Sunday right?
Tanisha
	              
	              Hi Tanisha,&nbsp; 
I think the week 7 DQ initial response is due by Wednesday [Day 7]

GOTO: Discussion Board --&gt; General Chat --&gt; Week 7 Announcement



Best Wishes, Babatunde
	              
	              Hi Tanisha
The deadline for this Initial Discussion Question is on Wednesday 22/10. Anthony has written a post 16/10 in the General chat, and he writes e.g. " You are expected to send in your initial DQ response via the Turnitin link and post this to the relevant DQ thread before the end of Wednesday (Day 7), for weekly feedback only".
https://elearning.uol.ohecampus.com/webapps/discussionboard/do/message?action=list_messages&amp;forum_id=_982597_1&amp;nav=discussion_board_entry&amp;conf_id=_455978_1&amp;course_id=_1487745_1&amp;message_id=_16519657_1
PS: I think were supposed to ask such questions in the General chat.

Br. Bo
	              
	              Bo,
I think I got mixed up with that the the response to your colleagues. I did plan on submitting before though. Thanks a lot guys.
Tanisha
	              
	              Designing an algorithm

&nbsp;

Choice

Option 1: Numbers. Your algorithm should: Search a string of at least five numbers (for example, 37540). Identify all of the substrings that form numbers that are divisible by 3.

For example, applying the algorithm on the string 37540 should produce the following substrings (not necessarily in this order): 0; 3; 75; 54; 375; 540.

Method

What I need to achieve, in a generic form, is: For all substrings of a Y numeric string of any length, if the substring is divisible by X, show the substring.

There are two problems to resolve: first is finding all the substrings of a string, second is finding whenever a string is divisible by X. For the second problem it exists already a well-known arithmetic operation that determines if a number is divisible by another: Modulo operation, in which the result of the operation is the remainder after one integer is divided by another (Daintith, Wright 2008b). Modulo operation can be found in any scientific pocket calculator, normally abbreviated “Mod”. Mod works as follows: 7 mod 5 = 2. Meaning that there is a remainder of 2 in 7÷5. When there is no remainder like in 75 mod 3 = 0, then we can deduct that 75 is divisible by 3. In fact, we can generalize this as: if Y mod X = 0, then Y is divisible by X.

The other problem to resolve is finding all the substrings of a string. For the sake of fully understanding the problem, I must first point out that a substring is a string inside S that occurs within S. For example a substring of 37540 is 37540 (754), while 37540 (354) is not a substring of 37540 (nor is 453) (Wikipedia. 2014).

I will take 37540 as an example to find its substrings. First, I need to create an index (NIndex) and position this index at the beginning of the string: NIndex ← 0. Having NIndex at the first integer of the string, the next logical substring is the NIndex+1 value appended to NIndex value (fist integer and second integer), and so on until I reach the end of the string. This way, for 37540, the first substring is 3, the second is 37 (7 appended to 3) and the last is the string itself (37540). Figure 1 is graphical representation of this logic.




Figure 1. Finding substrings in a string with an index.

As seen in Figure 1, this will only give me one set of substrings. How can I find the substring 54? If I look closely, all I need to do is move the index rightwards and repeat the whole process again of appending the NIndex values. Therefore, to find 54 (as shown in Figure 2), I do NIndex ← NIndex+2 (that will put my index at the number 5 in the 37540 string and then find the next logical substring: NIndex+1 value (4) appended to NIndex value (5), which will be 54. Thus, if I keep changing index for every time I reach the end of the string while the NIndex is less than the length of the string, then I will have all possible Substrings.




Figure 2. Moving the index to find other substrings.
 
&nbsp;

Let me put this in pseudo code, so it is easier to understand:





NIndex ← 0 while (NIndex ≤ stringOfNumbers length)  &nbsp;&nbsp;&nbsp; do ( &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; get NIndex value &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; append NIndex value to existing NIndex value &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; display NIndex value &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; )
&nbsp;





This will be my logic backbone to create the final algorithm. I will need to execute this logic every time the index is changed, so I need a procedure with parameters (such as the current NIndex) that I can use several times. I also need to properly define “NIndex value”, which is an arranged collection of the integers in our string (in our example: 3,7,5,4,0). Such collections are called Arrays (Daintith, Wright 2008a). Hence I will call the collection of ““NIndex values” MyArray[], where MyArray[0] is the NIndex0 value (in our example: 3) and MyArray[4] is the NIndex4 value and last integer (in our example: 0).

Note that I am starting the NIndex at 0, while the String has 5 elements, which one would normally count as 1 through 5, but because 0 through 4 contains five elements and NIndex0 is the first element, then NIndex4 is the fifth and last element of the string 37540.

Let me, then, write a more complete pseudo code with a procedure, when called this procedure will display the substrings, if called with different index positions, it will eventually return all the substrings:





procedure FindDivisiblesSubstrings (NIndex, MaxLength, MyArray, DivisibleBy) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //set a IndexNext variable and add 1 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; IndexNext ← NIndex + 1 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; If (IndexNext &gt; MaxLength) Then &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //this means we reached (at the last iteration) the end of the string. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (Exit the procedure) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; end if &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //create a temporary string where to append the substrings. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; TempString ← MyArray (NIndex) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //maxLength-1 is important as I will be passing the length as 5, but the index start as 0. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; while (a ← IndexNext ≤ MaxLength-1)  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; do ( &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; append MyArray (a) to TempString &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; display TempString &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; ) end procedure //for easier reading, adding end procedure
&nbsp;





&nbsp;And now I can implement the Modulo operation, this procedure will then display only the substrings that are divisible by the DivisibleBy value:





procedure FindDivisiblesSubstrings (NIndex, MaxLength, MyArray, DivisibleBy) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //set a IndexNext variable and add 1 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; IndexNext ← NIndex + 1 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; If (IndexNext &gt; MaxLength) Then &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //this means we reached (at the last iteration) the end of the string. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (Exit the procedure) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; end if &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //create a temporary string where to append the substrings. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; TempString ← MyArray (NIndex) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //maxLength-1 is important as I will be passing the length as 5, but the index start as 0. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; while (a ← IndexNext ≤ MaxLength-1)  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; do ( &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; append MyArray (a) to TempString &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; If (TemporaryString Mod DivisibleBy = 0) Then (display TemporaryString) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; ) end procedure //for easier reading, adding end procedure





&nbsp;

Result

At this point I am ready to write the complete pseudo code that will call this procedure to produce the list of substrings that are divisible by the Divisor variable. With this generic code I merely need to change the value of StringOfNumbers and the Divisor to find the answer the goal: “For all substrings of a Y numeric string of any length, if the substring is divisible by X, show the substring”.

The final algorithm is (with the example of 37540 and divisor 3):





procedure Main StringOfNumbers ← 37540 MyCounter ← 0 Divisor ← 3
//go through each integer in the StringOfNumbers and save each one as part of the array MyIntegerArray in the position MyCounter. Because the counter starts at 0 and ends with the last integer inside StringOfNumbers, the counter maximum value is always the length of the string. for each MyInteger in StringOfNumbers do ( &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; MyIntegerArray[MyCounter] ← MyInteger &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; MyCounter ← MyCounter + 1 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ) end For
//Set the Index to 0. MyIndex ← 0 repeat (
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //Execute the FindDivisiblesSubstrings procedure and move the index by 1 increment.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; FindDivisiblesSubstrings(MyIndex, Length of StringOfNumbers, MyIntegerArray[],Divisor) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; MyIndex&nbsp; ← MyIndex&nbsp; + 1 ) until (MyIndex = Length of StringOfNumbers) end procedure  
procedure FindDivisiblesSubstrings (NIndex, MaxLength, MyArray[], DivisibleBy) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //set a IndexNext variable and add 1 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; IndexNext ← NIndex + 1 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; If (IndexNext &gt; MaxLength) Then &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //this means we reached (at the last iteration) the end of the string. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (Exit the procedure) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; end if &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //create a temporary string where to append the substrings. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; TempString ← MyArray[NIndex] &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //maxLength-1 is important as I will be passing the length as 5, but the index start as 0. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; while (a ← IndexNext ≤ MaxLength-1)  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; do ( &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; append MyArray[a] to TempString
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //evaluate divisibility by DivisibleBy, If divisible display the number &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; If (TempString Mod DivisibleBy = 0) Then (display TempString) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; ) end procedure 





&nbsp;

Testing

To test the algorithm, since I know a little of VB.NET I created a quick console program. It is a little more complex because if the VB.NET syntax and to allow for user input and better console display, but it is basically a direct implementation of my final algorithm. Testing proved the algorithm to be correct, delivering results for 37540 but also for any other number of strings.

For example it delivered these correct results:

***** RESULTS *****

List of substrings in 7852140 divisible by 3: 0, 21, 5214, 52140, 78, 785214, 7852140, 852

*******************

I have attached (to the discussion forum) the VB.NET executable and source code for anyone that want to run the little program and have fun for it.

Attention: there is no error control in the program so using strings and divisors other than positive numbers may result in a crash of the program.

Attention: the attached program is limited (on purpose) to accept strings of Integers from 0 to 4,294,967,295 (32-bit integer or Interger32).

Further Considerations

While this algorithm solves the requested problem, there must be more elegant ways to resolve it. The major disadvantage of this algorithm is that when the string contains a big amount of integers (i.e. one billion integers) it will be very slow to process, because it goes and search every single occurrence of the substrings. Given enough time it will be fun to find an algorithm that performs better.

&nbsp;

References

Daintith, J. &amp; Wright, E. (2008a) 'Array' in Dictionary of Computing, 6th edn, Oxford University Press, Oxford Reference, .

Daintith, J. &amp; Wright, E. (2008b) 'Modulo Operation' in Dictionary of Computing, 6th edn, Oxford University Press, Oxford Reference, .

Wikipedia. (2014) Substring. Available at: http://en.wikipedia.org/wiki/Substring (Accessed: 18 October 2014).


ATTACHMENT
FindDivisibleSubstrings.zip&nbsp;

	              
	              Option 2

Algorithm that&nbsp;identifies&nbsp;substrings (RandomLetters)&nbsp;of the longer string that match any of the five words (subStringtomatch)&nbsp;you generated, and the number of times each one appears


Input Length_random&nbsp; RandomLetters = "jkdistructuredstrusyssystemoon"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;\* RandomLetter with lenght of Length_randominput Length_tomatch substringtomatch = "structure", "such", "system", "blue", "red"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \* Substringtomatch with Lengthttomatch
Counter set to zero&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \*Counter set to zero&nbsp;&nbsp;&nbsp;&nbsp; for i = 0 to (Length_random&nbsp; – Length_tomatch)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \*increment to start looping each character in the random letter string&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for j = 0 to (Length_tomatch)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\*increment the starts of the string to match&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if&nbsp; substringtomatch[ j ] = RandomLetters[ i +j ] then&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \*we have matched one character&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Counter +1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if&nbsp; Counter =&nbsp;length_tomatch then&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \* We matched the sub string pattern &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Print&nbsp; "Following word appear in the Random string" substringtomatch&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; next j&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; else&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; counter set to zero&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; next i

	              
	              Week 7 DQ - Designing an algorithm, part 1 For this week’s DQ, I chose option one.  String test=“37545" len=length(test) k=1 For(i=0 to len-1,i++)    While k&lt;=len String subString = substring of test(StartAti, StopAtk&nbsp;) Convert subString to int num If num mod 3 ==0 then  Print this string   If k ==5       k=i+1 Break While loop   Else       k++   End While   &nbsp; &nbsp; References: [Online]http://www.unf.edu/~broggio/cop2221/2221pseu.htm accessed October 19, 2014 [Online]http://www.maths.udsm.ac.tz/mujuni/mt512/Algorithms.pdf accessed October 20, 2014 [Online]http://en.wikibooks.org/wiki/Alevel_Computing/AQA/Problem_Solving,_Programming,_Data_Representation_and_Practical_Exercise/Problem_Solving/Pseudo_code accessed October 20, 2014 
	              
	              ---------------------------Begin-------------------------------------
Main Module
&nbsp;
Declare X as integer
Declare Y as Integer
Declare Z as integer
Declare final as string
Declare CF as integer
Declare IV as integer
&nbsp;
set X = 0
set Y = 0
set z = 0
final = ""
&nbsp;
Do While user wants to continue
Display Menu
Get Icream Value
Evaluate
Display Results
End loop
Print exit message
End Main Module
&nbsp;
Display Menu
&nbsp;
Declare continue as Boolean
Set continue = true
While continue = true
Display "Welcome to the Icrecream Consensus Algorithm&nbsp; program"
Display "Please make a selection"
Display "Available Icecream Types:"
Display "1: Choco"
Display "2: Mango"
Display "3: Vanilla"
Display "4: Reset Consensus"
Display "5: Quit"
Display "Enter a selection:";
&nbsp;
Input CF
&nbsp;
If CF &gt;= 1 AND CF &lt;= 3 then
Set continue = false
&nbsp;
else if CF = 4
Display "Resetting Concensus Data"
Set X = 0
Set Y = 0
Set Z = 0
final = ""
&nbsp;
else if CF = 5
Display "Quitting Currency Conversion"
continue = false
&nbsp;
else
Display ""Error 4: Invalid menu selection."
continue = true
end if
&nbsp;
end While
&nbsp;
End Display Menu
&nbsp;
&nbsp;
&nbsp;
&nbsp;
Get Icecream Value
&nbsp;
Declare value as integer
Declare continue as Boolean
Set continue = true
While continue = true
Display "Enter a Icecream value (positive number): "
Input IV
if IV &gt; 0 AND IV &lt;=3 then
continue = false
else
Display ""Error 1: Invalid input--Invalid Number"
continue = true
end if
end while
&nbsp;
End Get Icecream Value
&nbsp;
&nbsp;
&nbsp;
&nbsp;
Evaluate
&nbsp;
Select Case of IV
case 1:
Set X = X + 1
case 2:
Set Y = Y + 1
case 3:
Set Z = Z + 1
End Case
&nbsp;
IF X &gt; Y AND X &gt; Z then
final = "Choco"
End if
&nbsp;
IF Y &gt; X AND Y &gt; Z then
final = "Mango"
End if
&nbsp;
IF Z &gt; X AND Z &gt; Y Then
final = "Vanilla"
End IF
&nbsp;
End Evaluate
&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;
Display Results
&nbsp;
if final &lt;&gt; "" then
Display "Selected Icecream Flavor is ", final
else
Display "Error 5: Could Not make a Decision, Kindly select an Icream Flavor from the menu."
end if
&nbsp;
End Display Results
&nbsp;
&nbsp;
------------------------------------------END---------------------------------------------------
	              
	              Hello Class,As our usual way of rounding up the week's activity, please provide a short summary of the key 'Computer Structures' lessons learnt in week 7, from your perspective.Anthony
	              
	              Designing an Algorithm. &nbsp; We were given 3 optional scenarios to choose 1 from and I decided to look at this one. • Option 1: Numbers. Your algorithm should:  Search a string of at least five numbers (for example, 37540) Identify all of the substrings that form numbers that are divisible by 3. For example, applying the algorithm on the string 37540 should produce the following substrings&nbsp;(not necessarily in this order): 0; 3; 75; 54; 375; 540.  &nbsp; To create the pseudo code algorithm I first broke down the individual aspects to resolve in order.    Identify a set string of at least five numbers (example given, “37540”) Enter the divisible number (example given, “3”) Strip out the entire possible sub strings within that string. Check to see if each of the sub string values are divisible by 3 Report the remaining sub strings which are divisible by 3    Part 1 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; We are given the initial example as “37540” for the string value and a divisible number of “3”. However I will view it from the aspect that the user would be prompted to enter these details so they could be any value. &nbsp; The user would be prompted to enter an integer value of at least five numbers and the divisible number to test against. Procedure EnterData(entVal,divVal)( &nbsp; Define aVal as int; Define aCount as int; Define aDiv as int; Int aVal = 0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // value of the string for example 37540 initially set to 0 Int aCount = 0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // the string length count initially set to 0 Int aDiv = 0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // the string set initially to 0 &nbsp; If entVal.length &gt; 4 then &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; set&nbsp; aVal = entVal;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // set the entered value to aVal &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; set aCount = entVal.length+1;&nbsp;&nbsp; // set the string length count to aCount+1 Else Echo “sorry not enough numbers, you must enter 5 or more”; End; Endif &nbsp; If divVal &gt; 0 then&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //make sure a value is entered as the division &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; set aDiv = divVal;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // set the entered division value to aDiv&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Else Echo “sorry you need to enter a division value”; End; Endif ) Part 2 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; To strip out the possible sub strings we loop through each string up to the string count and then test to see if divisible by the divVal that was entered. Procedure stripString(aVal,aCount,divVal)( Define divisions[] as array;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // set an array to hold each of the substrings Define tempStr as string&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // temporary string Define X = int;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Define Y = int; Set X = 0; Set Y = 0; &nbsp; While&nbsp; (X &lt; aCount)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //&nbsp; loop through each iteration from x to aCount &nbsp; Do ( Loop until Y = aCount(&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // internal loop for internal Y position &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Set tempStr = aVal.SubString(X,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Y)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // set the temp string to start X and pos Y&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; If (tempStr is divisible by divVal )&nbsp; // check to see if value is divisible &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Then &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Set divisions[]&nbsp;&nbsp;&nbsp; = tempStr;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // save to array &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Else &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; End if &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Y = Y + 1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // increase Y by 1 each loop ) &nbsp; X = X +1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // increase X by 1 each loop ) &nbsp; Echo divisions[] &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // show the final string ) The above procedure stripString() simply takes the start and end position of the substring command in turn and then tests the substring to see if it divisible before storing it in the array. This is advantageous to do it this way as it is quicker and less I/O needed instead of saving all of the possible substrings first then checking each substring and then either saving again or displaying the values. If it is done in one go, only the result needed is saved. This would loop through as follows if the entered value was “37540” 3, 37, 375, 3754, 37540, 7, 75, 754, 7540, 5, 54, 540, 4, 40, 0   And the divisible numbers saved to the array if the divisible number was “3” would be in order: &nbsp; 3, 375, 75, 54, 540, 0 &nbsp; Conclusion &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; This whole algorithm can be more simplified in conclusion as seen below.     I prefer to show pseudo code as a flow diagram where possible as I like to visually see the data/process flow. I have used this technique often in the past when building algorithms of my own. &nbsp; &nbsp; References: Brookshear, G. (2011) ‘Computer science: An overview’. 11th ed. Boston: Addison Wesley/Pearson.  Chapter 5, ‘Algorithms,’ apart from the discussion on 'Efficiency and Correctness'   Section 5.1, ‘The Concept of an Algorithm’ Section 5.2, ‘Algorithm Representation’ Section 5.3, ‘Algorithm Discovery’ Section 5.4, ‘Iterative Structures’ Section 5.5, ‘Recursive Structures’    Calpoly.edu (2014) ‘Pseudocode Standard’ [http://users.csc.calpoly.edu/~jdalbey/SWE/pdl_std.html] Accessed: 18/10/2014. Laureate Education, (2014) Algorithms: Week 7 Lecture Notes [Video, Online], (accessed: 19/10/14) The Scratch project at MIT (n.d.) Scratch: Imagine, program, share [Online]. Available from: http://scratch.mit.edu (Accessed: 18/10/2014). UNF.edu, (2014) ‘Pseudocode Examples’ [http://www.unf.edu/~broggio/cop2221/2221pseu.htm] Accessed: 18/10/2014. Wikipedia, (2014) ‘Pseudocode’ [http://en.wikipedia.org/wiki/Pseudocode] Accessed: 18/10/2014.   
	              
	              Hi Anthony,
WOW. What an incredibly tough week! Well, for me personally anyway. 
I have found this week to be incredibly challenging having had no real programming experience. However, I have had a lot of fun! 
I had written some code back in the late 80’s and early 90’s, but that was for operating systems and I can only vaguely recall it, for example on backups, moving from one disk to another. 
I have honestly struggled with where to even start this week - however, no excuses, this is exactly why I wanted to do the Masters programme, to re-learn and re-discover Information Technology. 
This week has been really beneficial, to begin to understand the mindset and logic for algorithms, some concepts, and starting points for developing software. I’m not necessarily looking to take my career down the developer route, but I most definitely do want to understand it and be able to have meaningful dialogue relative to software engineering, as such this week has been great in that respect (and I expect next week also).
 
It has been interesting to use Scratch to see the practical side of programming, beyond that of documentation. 
My thanks to my team colleagues Augusto and Bram, for great teamwork, enlightening discussion and collaboration throughout this week.
Best Wishes, Craig
References
Laureate Online Education. (2014). Computer Structures – Lecture Notes Week 7: Algorithms [Online]. Available from: https://elearning.uol.ohecampus.com/bbcswebdav/institution/UKL1/201480_SEPTEMBER/MS_CKIT/CKIT_501/readings/UKL1_CKIT_501_Week07_LectureNotes.pdf (Accessed 16 October 2014)
Brookshear, J. G. (2012). Computer Science An Overview. 11th ed. Boston Massachusetts: Pearson. Addison Wesley
Meyer, H. B. (n.d.) Eratosthenes’ sieve [Online]. Available from: http://www.hbmeyer.de/eratosiv.htm (Accessed 17 October 2014)
	              
	               Option 1: Numbers. Your algorithm should:        Search a string of at least five numbers (for example, 37540) Identify all of the substrings that form numbers that are divisible by 3. For example, applying the algorithm on the string 37540 should produce the following substrings&nbsp;(not necessarily in this order): 0; 3; 75; 54; 375; 540.     Pseudo-Code:   Strat:   &nbsp;   Declare Input ç User Input String of at least five numbers   &nbsp;   Declare String Array[]   &nbsp;   Declare Index ç 0   &nbsp;   Function SubStrings (Input)   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; {   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Array[]ç Sub Strings found in Input string   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }   &nbsp;   Loop for traversing Array[]   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; {   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; If (Array[Index] % 3 == 0)   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Print Array[Index]   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Index++   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }   &nbsp;End: 
	              
	              What Is An Algorithm?
&nbsp;
An algorithm is a well defined series of steps or instructions which commands the computer to solve problems. With the help of algorithms, the computer performs its operations in a systematic manner.
&nbsp;
Algorithm
&nbsp;
I would like to choose first option Numbers for my algorithm;
&nbsp;


Start


Declare variable sNum


Declare counter variable i =0


Declare counter variable j =0


Accept string with at least 5 digit in length as an input and store in sNum


If length of sNum is less than 5 digits or sNum contains any alphabets then


Display Error Message 


goto Step 5



Loop until (j &lt; Length of sNum)


Let i = 0


Loop until (i &lt; (Length of sNum)-1)


Declare iTemp to store the individually extracted digits from sNum


Let iTemp = MID(sNum,i,(j+1))


If the remainder of iTemp divided by 3 equals 0, then print the value of iTemp


Increment the value of counter i by 1


Go back to Step 7.2



Increment the value of counter j by 1


Go back to Step 7



Stop


&nbsp;
Traces  
Lets use the given value 37540 as an example, and trace it;
&nbsp;

      



iNum


Loop index j value


Loop index i value


iTemp


Print Output




37540


0


0


3


3




37540


0


1


7


-




37540


0


2


5


-




37540


0


3


4


-




37540


0


4


0


0




37540


1


0


37


-




37540


1


1


75


75




37540


1


2


54


54




37540


1


3


40


-




37540


2


0


375


375




37540


2


1


754


-




37540


2


2


540


540




37540


3


0


3754


-




37540


3


1


7540


-




37540


4


0


37540


-





  
Conclusion  
The most important aspect of an algorithm would be its exhaustive coverage of all possible scenarios and a plain english description of the necessary steps. Then again, the flow of control is equally important, i.e., all mandatory steps are to be executed in a precise order as required. Algorithms, structured as they in a well-defined series of instructions, most definitely make life easier when it comes to computing and data processing.
&nbsp;
Reference
 UoL. (2014). “Computer Structures — Lecture Notes Week 7: Algorithms”. &nbsp;[Online]. Available from:https://elearning.uol.ohecampus.com/bbcswebdav/institution/UKL1/201480_SEPTEMBER/MS_CKIT/CKIT_501/readings/UKL1_CKIT_501_Week07_LectureNotes.pdf. (Accessed 18-10-2014)
Best regards, Kharavela
	              
	              Hi Dr. Ayoola,
This week has been quite interesting. I really the DQ. I find it complex to respond to others because it requires reading and understanding their algorithms carefully.
Key lessons learnt:

I discovered Scratch. It is really a nice piece of software, very focused on the logic. My wife likes it too :-).
It not always that I stop and think about algorithms in detail. It was nice to see a proper definition for it. I do not recall if during my old school days I looked at them in detail (but then again, things get forgotten).
This week made me reflect on problem understanding and problem-solving. It is important that we fully understand the problem before moving forward. For instance in the DQ, when I first read it I was doing in my mind the permutations of all the possible integers in the string rather than the substrings. It is good to stop and try to understand the problems before jumping into finding algorithms/solutions.
It was also good to work in a group. I am looking forward to other group works in the future.

Best Regards,
Augusto

	              
	              Hi Anthony
This has been very challenging for me, as will week 8 be. I neither am nor tend to be a programmer. Still, I always want to learn and understand, so I guess I just have to fight for it. Some of the literature is for me very complex even though I guess its pure logic to others, here I’m especially thinking about the algorithms which, sorry but honestly have to say, I don’t understand. But I do believe that I understand the principles, and that is the most important thing to me.
I do understand that while dealing with issues like algorithms, one has to:

Be able to think in abstracts
Have a systematic approach to the processes, i.e. remember to cover the four steps (Brookshear, p. 199)
Be open-minded for going back and forth in the process. This I think is necessary because if one is to chronologically structured, and then one could (will) miss out on potential good/better ways of handling a problem (Brookshear, p. 199), i.e. one should give time to the “incubation period” (Brookshear, p. 200).
Know about the algorithm characteristics, e.g. the efficiency an correctness of sorting algorithms (Brookshear, chap. 5.4-6)

Also it seems like especially newcomers, bus also others should know the following:

Pseudocode since it an excellent method to “express ideas in informal (human) way”.
Tools like e.g. Scratch, especially in the starting process since one can trains one’s logic thinking.

And last, but not least, starting the team collaboration process, learning in small steps how the asynchronous collaboration works. Here I learned:

When using a tool like scratch, the group (or perhaps Anthony?) should decide on which version to use. I downloaded Scratch 2.0 while Babatunde used 1.4 resulting in him not being able to open my Scratch file.
Collaboration is, as always, the best tool when searching for good solutions, thus I was able to get input from Babatunde to improve my Scratch program.
If/when we at some time want to chat, or perhaps use Skype, it’s important to know about the time zones in order to find a suitable time.

Best wishes
Bo W. Mogensen

	              
	              Week 7 DQ - Designing an algorithm, part 1
I've choosed Option 1:&nbsp; Numbers.
below is my algorithme:
// Declaration of variables

X int;

n int;

i int;

T[n] array;

// Enter a number of min 5 digits

echo "Please entre an integer number with min 5 digits :"

input number read (X); // ex. 37540

if length(X) ← 5

&nbsp; echo "Please enter a number with min 5 digits"

else

&nbsp; echo "OK! it's a correct number"

end if

// We will now intert each digit of the number provided into a array

n ← length(X);

T[n] ← X;&nbsp;&nbsp;&nbsp; // insert into T[n] as below

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // T[1] ← X1 (first digit)

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //&nbsp; T[2] ← X2 (Snd digit)

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //&nbsp;&nbsp; ...

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //&nbsp;&nbsp; T[n] ← Xn (last digit); n is the length of the number input

//

Begin

// first we will check each digit or each entry of the array and see which one is divisible by 3 the display it

&nbsp; i ← T[1];

&nbsp; for i = T[1] to T[n]

&nbsp;&nbsp;&nbsp; if T[i] mod 3 := 0

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; echo T[i]

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; echo ";"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; --// 3;0;

&nbsp;&nbsp;&nbsp; end if

&nbsp;&nbsp;&nbsp; i ← i+1;

&nbsp;&nbsp; end for

//&nbsp;Now we will check combination of 2&nbsp;substrings: 37; 75; 54 and 40
 

// will use concatenation function (cat)

&nbsp;&nbsp; &nbsp;i ← T[1];

&nbsp;&nbsp; for i = T[1] to T[n]

&nbsp;&nbsp;&nbsp;&nbsp; if cat(T[i]&amp;T[i+1]) mod 3 := 0

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; echo cat(T[i]&amp;T[i+1])

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; echo ";"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; --// ;75;54

&nbsp;&nbsp;&nbsp;&nbsp; end if

&nbsp;&nbsp;&nbsp;&nbsp; i ← i+1;

&nbsp;&nbsp; end for

// Now we will check combination of 3&nbsp;substrings:&nbsp;37; 75; 54 and 40 : 375; 754 and 540

&nbsp; &nbsp;i ← T[1];

&nbsp;&nbsp; for i = T[1] to T[n]

&nbsp;&nbsp;&nbsp;&nbsp; if cat(T[i]&amp;T[i+1]&amp;T[i+2]) mod 3 := 0

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;echo cat(T[i]&amp;T[i+1]&amp;T[i+2])

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; echo ";"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; --// ;375;540

&nbsp;&nbsp;&nbsp;&nbsp; end if

&nbsp;&nbsp;&nbsp;&nbsp; i ← i+1;

&nbsp;&nbsp; end for

&nbsp;&nbsp; .

&nbsp;&nbsp; .

&nbsp;&nbsp; . &nbsp;&nbsp;&nbsp;&nbsp;// will do it for 4,5,...,n (n is the last digit of the number input

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;and the length of that number

&nbsp;&nbsp; .

&nbsp;&nbsp; .

&nbsp;&nbsp; .

&nbsp;&nbsp; 

&nbsp;&nbsp; i ← T[1];

&nbsp;&nbsp; for i = T[1] to T[n]

&nbsp;&nbsp;&nbsp;&nbsp; if cat(T[i]&amp;T[i+1]&amp;T[i+2]&amp;...&amp;T[n]) mod 3 := 0

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; echo cat(T[i]&amp;T[i+1]&amp;T[i+2]&amp;...&amp;T[n])

&nbsp;&nbsp;&nbsp;&nbsp; end if

&nbsp;&nbsp;&nbsp;&nbsp; i ← i+1;

&nbsp;&nbsp; end for

End;
Reference:
Lecture Notes, (2014). &nbsp;Week 6: Computer networks [Online]. Available from: https://elearning.uol.ohecampus.com/bbcswebdav/institution/UKL1/201480_SEPTEMBER/MS_CKIT/CKIT_501/readings/UKL1_CKIT_501_Week08_LectureNotes.pdf. (Accessed: 22 October 2014)

	              
	              Week 7 Discussion Question – Designing an Algorithm (Part 1)
Craig Thomas – 22 October 2014
Task: To generate pseudo-code for an algorithm: 
Chosen: Option 1 – Numbers.
Your Algorithm should:

Search a string of at least five numbers (for example, 37540)
Identify all of the substrings that form numbers that are divisible by 3.
For example, applying the algorithm on the string 37540 should produce the following substrings&nbsp;(not necessarily in this order): 0; 3; 75; 54; 375; 540.

I have adopted a simple logical approach to the pseudo-code.
&nbsp;&nbsp;
I need to figure out how to be able to search a string, identify a substring from that, extract the substring, and search the original string again to the end without picking up substrings already identified. This includes being able to identify and not exceed the length of the string, and pick up valid substrings. 
Craig’s attempt at Pseudo-Code, please don’t laugh! 
BEGIN
//Setting counter and creating a list to store integers

SET “COUNTER” to “0” &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//counter is set to 0
CREATE LIST called “INTEGER LIST”&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//creating list of integers

//Generating list of 5 integers with repeat process/or loop

Ask User to INPUT a ‘single digit integer’ &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;//user inputs the number(s) 3 [loop, user then inputs 7,5,4,0]

&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;3.1&nbsp; SET “COUNTER” to “COUNTER” +1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;//counter is now set to 1 [loop then sets to 2,3,4,5]
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;3.2&nbsp; ADD ‘single digit integer’ to “INTEGER LIST”&nbsp;&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;//adds 3 to integer list [loop then adds 7,5,4,0]
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;3.3&nbsp; IF COUNTER &lt;5 THEN repeat step 3&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;//checks, and repeats if less than 5, else moves to step 4
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;ELSE go to step 4&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; //counter is 5
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//integer list contains 37540
&nbsp;
//Creating a final integer substring list that are divisible by 3
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 4. &nbsp;&nbsp;&nbsp;&nbsp; SET “COUNTER” to “0”&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//counter is reset to 0
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 5. &nbsp;&nbsp;&nbsp;&nbsp; CREATE LIST called “FINAL INTEGER SUBSTRING LIST”&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;//creates list for final substring integers
&nbsp;
//Finding first set of integer substrings (37540)
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 6.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; IF COUNTER &lt;5 THEN go to step 6.1 ELSE go to step 12&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //counter check to ensure less than 5 
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 6.1.&nbsp;&nbsp; SET “COUNTER” to “COUNTER” +1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//counter is set to 1
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 6.2.&nbsp;&nbsp; GET FIRST VALUE in INTEGER LIST and assign as ‘n’&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//gets 3, assigns as ‘n’
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 6.3.&nbsp;&nbsp; IF ‘n’ mod 3 = 0 THEN add ‘n’ to FINAL INTEGER SS LIST &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;//3 is divisible by 3, so adds 3 to ss list
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 6.4.&nbsp;&nbsp;GET SECOND VALUE in INTEGER LIST and assign as ‘n’&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //gets 7, assigns as ‘n’ [assume ‘n’ is superseded by next value]
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 6.5.&nbsp;&nbsp; IF ‘n’ mod 3 = 0 THEN add ‘n’ to FINAL INTEGER SS LIST&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;//7 is not divisible by 3, so doesn’t include
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 6.6.&nbsp;&nbsp; GET THIRD VALUE in INTEGER LIST and assign as ‘n’&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //gets 5, assigns as ‘n’ [assume ‘n’ is superseded by next value]
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 6.7.&nbsp;&nbsp; IF ‘n’ mod 3 = 0 THEN add ‘n’ to FINAL INTEGERS SS LIST &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //5 is not divisible by 3, so doesn’t include
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 6.8.&nbsp;&nbsp; GET FOURTH VALUE in INTEGER LIST and assign a ‘n’&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //gets 4, assigns as ‘n’ [assume ‘n’ is superseded by next value]
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 6.9.&nbsp;&nbsp; IF ‘n’ mod 3 = 0 THEN add ‘n’ to FINAL INTEGERS SS LIST &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //4 is not divisible by 3, so doesn’t include
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 6.10. GET FIFTH VALUE in INTEGER LIST and assign as ‘n’&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //gets 0, assigns as ‘n’ [assume ‘n’ is superseded by next value]
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 6.11. IF ‘n’ mod 3 = 0 THEN add ‘n’ to FINAL INTEGERS SS LIST &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //0 is not divisible by 3, but this included as answer is 0 (zero)
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 6.12. READ FINAL INTEGERS SS LIST and remove any single value&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //0 removed, as this is not divisible by 3
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;of 0 (zero)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//counter is 1
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //final integer ss list contains: 3
&nbsp;
//Finding second set of integer substrings (37540)
&nbsp; &nbsp; &nbsp; &nbsp; 7.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; IF COUNTER &lt;5 THEN go to step 7.1 ELSE go to 12&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //checking that the counter is less than 5&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
&nbsp; &nbsp; &nbsp; &nbsp; 7.1.&nbsp;&nbsp; SET “COUNTER” to “COUNTER” +1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;//counter now set to 2
&nbsp; &nbsp; &nbsp; &nbsp; 7.2.&nbsp;&nbsp; GET FIRST VALUE and SECOND VALUE in INTEGER LIST&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;//gets 3 and 7, assigns as ‘n’ [n=’37’]
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;and assign as ‘n’
&nbsp; &nbsp; &nbsp; &nbsp; 7.3.&nbsp;&nbsp; IF ‘n’ mod 3 = 0 THEN add ‘n’ to FINAL INTEGER SS LIST&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //37 is not divisible by 3, so doesn’t include it
&nbsp; &nbsp; &nbsp; &nbsp; 7.4.&nbsp;&nbsp; GET FIRST, SECOND and THIRD VALUE in INTEGER LIST&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;//gets 3, 7 and 5, assigns as ‘n’ [assume ‘n’ values are superseded] 
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;and assign as ‘n’&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
&nbsp; &nbsp; &nbsp; &nbsp; 7.5.&nbsp;&nbsp; IF ‘n’ mod 3 = 0 THEN add ‘n’ to FINAL INTEGER SS LIST&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //375 is divisible by 3, so includes
&nbsp; &nbsp; &nbsp; &nbsp; 7.6.&nbsp;&nbsp; GET FIRST, SECOND, THIRD and FOURTH VALUE in INTEGER&nbsp;&nbsp;&nbsp; //gets 3,7,5 and 4 assigns as ‘n [assume ‘n’ value is superseded]
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;LIST and assign as ‘n’&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
&nbsp; &nbsp; &nbsp; &nbsp; 7.7.&nbsp;&nbsp; IF ‘n’ mod 3 = 0 THEN add ‘n’ to FINAL INTEGER SS LIST&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //3754 is not divisible by 3, so doesn’t include
&nbsp; &nbsp; &nbsp; &nbsp; 7.8.&nbsp;&nbsp; GET FIRST, SECOND, THIRD, FOURTH and FIFTH VALUE &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;//gets 3,7,5,4,0 and 4, assigns as ‘n’ [assume ‘n’ is superseded]
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;in INTEGER LIST and assign as ‘n’&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
&nbsp; &nbsp; &nbsp; &nbsp; 7.9.&nbsp;&nbsp; IF ‘n’ mod 3 = 0 THEN add ‘n’ to FINAL INTEGER SS LIST&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //37540 not divisible by 3, so doesn’t include
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//counter is 2
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp;//final integer ss list contains: 3; 375
//Finding third set of integer substrings (37540)
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;8.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; IF COUNTER &lt;5 THEN go to step 8.1 ELSE go to 12&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //checking that the counter is less than 5
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;8.1.&nbsp;&nbsp; SET “COUNTER” to “COUNTER” +1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//counter is now set to 3
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;8.2.&nbsp;&nbsp; GET SECOND and THIRD VALUE in INTEGER LIST and &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //gets 7 and 5, assigns as ‘n’
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; assign as ‘n’
&nbsp; &nbsp;8.3.&nbsp;&nbsp; IF ‘n’ mod 3 = 0 THEN add ‘n’ to FINAL INTEGER SS LIST &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//75 is divisible by 3, so includes
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;8.4.&nbsp;&nbsp; GET SECOND, THIRD and FOURTH VALUE in INTEGER LIST&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //gets 7, 5 and 4, assigns as ‘n’ [assume ‘n’ is superseded]
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;and assign as ‘n’&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;8.5.&nbsp;&nbsp; IF ‘n’ mod 3 = 0 THEN add ‘n’ to FINAL INTEGER SS LIST&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //754 is not divisible by 3, doesn’t include
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;8.6.&nbsp;&nbsp; GET SECOND, THIRD, FOURTH and FIFTH VALUE in &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//gets 7,5, 4 and 0 assigns as ‘n’ [assume ‘n’ is superseded]
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;INTEGER LIST and assign as ‘n’&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;8.7.&nbsp;&nbsp; IF ‘n’ mod 3 = 0 THEN add ‘n’ to FINAL INTEGER SS LIST&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //7540 is not divisible by 3, doesn’t include
&nbsp;&nbsp;&nbsp;&nbsp; 
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;//counter is 3
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;//final integer ss list contains: 3; 375; 75
//Finding fourth set of integer substrings (37540)
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 9.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; IF COUNTER &lt;5 THEN go to step 9.1 ELSE go to 12&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //checking that the counter is less than 5
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 9.1.&nbsp;&nbsp; SET “COUNTER” to “COUNTER” +1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//counter is 4
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 9.2.&nbsp;&nbsp; GET THIRD and FOURTH VALUE in INTEGER LIST &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//gets 5 and 4, assigns as ‘n’ [assumes ‘n’ is superseded] 
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;and assign as ‘n’
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 9.3.&nbsp;&nbsp; IF ‘n’ mod 3 = 0 THEN add ‘n’ to FINAL INTEGER SS LIST&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //54 is divisible by 3, so includes
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 9.4.&nbsp;&nbsp; GET THIRD, FOURTH and FIFTH VALUE in INTEGER LIST&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;//gets 5, 4 and 0, assigns as ‘n’ [assumes ‘n’ is superseded] 
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;and assign as ‘n’&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 9.5.&nbsp;&nbsp; IF ‘n’ mod 3 = 0 THEN add ‘n’ to FINAL INTEGER SS LIST&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //540 is divisible by 3, so includes
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;//counter is 4
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;//final integer ss list contains 3; 375; 75; 54; 540
&nbsp;
//Finding fifth set of integer substrings (37540)
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 10.&nbsp;&nbsp;&nbsp; IF COUNTER &lt;5 THEN go to step 8.1 ELSE go to 12&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //checking that the counter is less than 5
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 10.1. SET “COUNTER” to “COUNTER” +1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//counter is now 5
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 10.2. GET FOURTH and FIFTH VALUE in INTEGER LIST&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//gets 4 and 0, assigns as ‘n’ [assumes ‘n’ is superseded]
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;and assign as ‘n’
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 10.3. IF ‘n’ mod 3 = 0 THEN add ‘n’ to FINAL INTEGER SS LIST&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //40 is not divisible by 3, so not included
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;//counter is now 5
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//final integer ss list contains 3; 375; 75; 54; 540
&nbsp;
//Show final list of Integers divisible by 3
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 11.&nbsp;&nbsp;&nbsp; SHOW FINAL INTEGER SS LIST &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //shows: 3; 375; 75; 54; 540
&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 12.&nbsp;&nbsp;&nbsp; END
END
&nbsp;
I recognise that this is an extremely inefficient and an especially inelegant method. I just need to figure out how it should look!
Best Wishes, Craig
References
Laureate Online Education. (2014). Computer Structures – Lecture Notes Week 7: Algorithms [Online]. Available from: https://elearning.uol.ohecampus.com/bbcswebdav/institution/UKL1/201480_SEPTEMBER/MS_CKIT/CKIT_501/readings/UKL1_CKIT_501_Week07_LectureNotes.pdf (Accessed 16 October 2014)
Brookshear, J. G. (2012). Computer Science An Overview. 11th ed. Boston Massachusetts: Pearson. Addison Wesley
Meyer, H. B. (n.d.) Eratosthenes’ sieve [Online]. Available from: http://www.hbmeyer.de/eratosiv.htm (Accessed 17 October 2014)
	              
	              Dear Dr. Ayoola,
This week's content was easy for me:-) I program for living. But that doesn't mean I could pass it without problems. Luckily, I had my group-mates to correct my errors in the code. I misread the "multiply" as "addition"! thanks, group Indigo. You rock!
Most of my struggles came from the bad Internet connection I had in Hangzhou, China. I had to wait and wait for a long time utill a button on LENS is clickable. I'm on the train from Hangzhou to Shanghai now. I will have much better connection there, built-in VPN in the office!
I'm not a big fan of pseudo-code. Actually, I almost never write pseudo-code, nor do I write code with a pen. I may explain why in week 8:-)
Programming Scratch has been fun. I like it. I'm not quite sure about option 2 and 3, but I'm sure option1 in DQ week7 can be solved by Scratch. I would like to challenge my colleagues in this class to post your Scratch solutions to these 3 problems in DQ week8:-)
The train always gave me a bit weird feeling...
br, Terry
	              
	              Hello Anthony,
I couldnt spend much time due to my tight schedule, but whatever time I spent it was worth revisiting my old days of algorithm writing!It was indeed good week for me, I thoroughly enjoyed it.The visual block representation of programming structure was really well presented. It eases the complexity of logic defining with drag drop blocks and redefines the way we learn programming. Really glad I tried it! :-)
Thanks and regards,Kharavela&nbsp;
	              
	              Week 7 Assignment - Option 1: Numbers
&nbsp;
The&nbsp; algorithm below searches a string of at least five numbers &nbsp;and identifies all of the substrings that form numbers that are divisible by 3. Lines that begin with the apostrophe are comments
&nbsp;‘ Start by declaring all the required variables to be used in the algorithm
&nbsp;‘Holds the input number
Dim MyStr As String &nbsp;&nbsp;&nbsp;
&nbsp;
‘Stores the numbers to be evaluated for divisibility by 3
Dim MyNum As Long
&nbsp;
‘Stores the final output
Dim MyResult As String
&nbsp;
‘ Variables to be used for the loop
Dim i As Integer
Dim j As Integer
&nbsp;
&nbsp; ‘Accept a 5 digit number
&nbsp;&nbsp;&nbsp; MyStr = Inputbox(‘Enter&nbsp; a number with at least 5 digits’)
&nbsp;
&nbsp;&nbsp;&nbsp; ‘ Check whether value entered is numeric
&nbsp;&nbsp; If &nbsp;not&nbsp; isnumeric(MyStr) then
&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;Msgbox “Invalid Number”
&nbsp;&nbsp;&nbsp;&nbsp; Exit sub
Else
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ‘ Check whether value entered has at least 5 digits
If len(trim(mystr)) &lt;5 then
&nbsp;&nbsp; MsgBox “Invalid number”
End if
&nbsp; End If
&nbsp;&nbsp;&nbsp;
&nbsp; ‘ Outer loop allows for iteration of each digit in the number
&nbsp;&nbsp;&nbsp; For i = 1 To Len(MyStr)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ‘ Inner loop allows for iteration of segments of numbers within the value entered
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; For j = i To Len(MyStr)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ‘ Extract number and compare whether divisible by 3
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; MyNum = Mid(MyStr, i, j)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; If MyNum Mod 3 = 0 Then
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ‘ For concatenation of the string result, stores the first number
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; If Len(Trim(MyResult)) = 0 Then
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; MyResult = Str(MyNum)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Else
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ‘ Add subsequent numbers obtained to result string and ignore any duplicate numbers
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; If InStr(1, MyResult, Str(MyNum)) = 0 Then
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; MyResult = MyResult + ", " + Str(MyNum)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;End If
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; End If
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; End If
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Next j
&nbsp;&nbsp;&nbsp; Next i
&nbsp;&nbsp;&nbsp;
&nbsp;‘Display result
&nbsp;&nbsp;&nbsp; MsgBox MyResult
&nbsp;
References
Brookshear, J. G. Computer Science: An Overview, 11th Edition. Pearson Learning Solutions. &nbsp;
Thayer, R. Visual Basic 6 Unleashed– September 11, 1998, SAMS
Week 7 Lecture Notes: Algorithms
&nbsp;
&nbsp;

	              
	              DQ week 7
Bram Tullemans
Option 2: Word Search

Make a list of five words, 3-6 letters in length.

-&gt; The list of 5 words: the, cat, loves, his, mat

Create a string of approximately 30 letters containing some of the five words.

-&gt; String of 30 letters: &nbsp;thecatisonthematoftheneighbour

Your algorithm should identify all of the substrings of the longer string that match any of the five words you generated, and the number of times each one appears

-&gt; Approach is to research the problem, define the 

For example: If you chose the words&nbsp;structure,&nbsp;such, system, blue, red,&nbsp;and your algorithm operates on the string&nbsp;jkdistructuredstrusyssystemoon,&nbsp;your algorithm should report that the string contains the words&nbsp;structure,&nbsp;red&nbsp;and&nbsp;system&nbsp;each one time, and the words&nbsp;such&nbsp;and&nbsp;blue&nbsp;zero times.

-&gt; The output of the programme is that: the (found 3 times), cat (found 1 time), loves (found 0 times), his (found 0 times), mat (found 1 time). This can trigger the question in the mind of a beholder if the cat loves the mat of the neighbour.
Finding the solution:
- The string is 30 letters long, there are 5 words to be checked and they can appear more times but they will not appear all the times.
A - Starting at the end identifying subtasks that need to be performed to reach this end point

Output of text whereby x is a variable showing how much times it appeared in the string.
Word 1 appeared x times 
Word 2 appeared x times
Word 3 appeared x times
Word 4 appeared x times
Word 5 appeared x times


x has a value 0, 1, 2, 3, 4, 5, 6, 7, 8 or9 (at least 2 words should appear and as they have a minimal length of 3 they should appear at least 1 and the other 9 and the other 3 not at all)

A recursive loop identifies how many times the words appear.
Adding &nbsp;1 to the value W1 to W5 when a word appears.
An recursive algorithm that checks if a word appears in the string

Input: 5 words are sets containing minimal 3 and maximal 5 letters
Input: 1 string contains 30 letters

B - For the recursive loop we use the Knuth-Morris-Pratt algorithm that works efficient with short strings. The following pseudo code (Simha 2014) finds if a word (pattern array) is found in the string (text array)
Algorithm: KnuthMorrisPrattPatternSearch ('text array', 'pattern array')
Input: 'pattern array' of length W, 'text array' of length T, textCursor position C
&nbsp;&nbsp;&nbsp;&nbsp; // Compute the nextIndex function, stored in an array: 
1.&nbsp;&nbsp; nextIndex = computeNextIndex (T)
&nbsp;&nbsp;&nbsp;&nbsp; // Start the pattern search. 
2.&nbsp;&nbsp; textCursor = C
3.&nbsp;&nbsp; patternCursor = 0
4.&nbsp;&nbsp; while textCursor &lt; T and patternCursor &lt; W
5.&nbsp;&nbsp;&nbsp;&nbsp; if text[textCursor] = pattern[patternCursor]
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // Advance cursors as long as characters match. 
6.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; textCursor = textCursor + 1
7.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; patternCursor = patternCursor + 1
8.&nbsp;&nbsp;&nbsp;&nbsp; else
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // Mismatch. 
9.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if patternCursor = 0
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // If the mismatch occurred at first position, 
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // advance the pattern one space. 
10.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; textCursor = textCursor + 1
11.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; else
&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; // Otherwise, use the nextIndex function. The textCursor 
&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;// already points to the next place to compare in the
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // text. 
12.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; patternCursor = nextIndex[patternCursor]
13.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; endif
14.&nbsp;&nbsp;&nbsp; endif
15.&nbsp; endwhile
16.&nbsp; if patternCursor = W
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // If there was a match, return 1 and the position of the 
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // textCursor to be able to check rest of 'text array'. 
17.&nbsp;&nbsp;&nbsp; return 1
18.&nbsp;&nbsp;&nbsp; return textCursor
19.&nbsp; else
20.&nbsp;&nbsp;&nbsp; return -1 and display[the 'pattern array' was not found in the 'text array']
&nbsp;&nbsp;
Output: 1 and textCursor if the inputted word is found, -1 if not found.
&nbsp;
C - Compiling the total programme
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; List (1 , the , 3, S), (2, cat, 3), (3, loves, 5), (4, &nbsp;his, 3), (5, mat, 3)
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Procedure: countWords
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Input 'list of pattern arrays and their length', N = 1, S = 0)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // algorithm as written in section B is started with input of first of 5 words (N = 1) 
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // and the length of the word W, &nbsp;while the score (S) and textCursor position &nbsp;is 0 
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;// as the counting begins.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1. &nbsp;&nbsp;if N &lt; 5 &nbsp;do Start Algorithm ( KnuthMorrisPrattPatternSearch) and
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;Input N (pattern array, 'text array') and Input (textCursor = 0)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // algorithm is started again when a pattern is found but now
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // with updated textCursor position and Score which is also displayed
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if return = 1 do Start Algorithm (KnuthMorrisPrattPatternSearch) and
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Input N(pattern Array, text array) and Input (textCursor ) and&nbsp; S = S + 1, 
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Display ['text array' appeared T times in 'text array']
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;// if there was no hit the next word needs to be selected and score set to 0
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 11.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;else N = N + 1 and S = 0 
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 12. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;endif
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 13.&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;else
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 14.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Display [Ready, but does the cat love the mat of the neighbour?]
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 15&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; endif
&nbsp;
References
Brookshear, G. (2012), Computer Science: An Overview, Pierson Education Inc. publishing as Addison-Wesley, Boston, United States, Version 11. ISBN 13: 978-0-13-256903-3. pp 188-222.
&nbsp;
Simha (2014) Course Materials; Module 5, Pattern search The George Washington University Available from http://www.seas.gwu.edu/~simhaweb/alg/lectures/module5/module5.html (Accessed on 22 October 2014)
&nbsp;

	              
	              Week 7 Initial Post  I. Introduction Every application II. Systems requirement Of the requirements given, the requirement chosen was, option one, which was to generate a pseudo-codethat will search a string of at least five numbers, “37540”, and identify all of the substrings within that string that will form numbers which are divisible by 3. Thus when the algorithm is applied on the string, “37540”, it should produce a output of these substring, 0; 3; 75; 54; 375; 540. Sample requirement, below: A. Search a string of at least five numbers (for example, 37540) B. Identify all of the substrings that form numbers that are divisible by 3. C. For example, applying the algorithm on the string 37540 should produce the follow ing substrings&nbsp;(not necessarily in this order): 0; 3; 75; 54; 375; 540.      III. Analysis For testing and verification purposes it was decided that the values presented in the requirement will be used in the analysis and development of the pseudo-code. Thus it was apparent that the following had to be done   Store the string, variable created as string astring   Rearrange the values in the string, producing all possible sub values.   Store the sub-values in a new string, variable created as string bstring   Create another variable to perform the computation on the string, variable of type integer num   Check all sub values to see if it is divisible by 3, that is sub-value mod 3 produces an output of zero.   If sub-value mod 3 produces an output of 0, print sub-value.   From looking at what is required, we can see that there will be several rearranging of values and computation will have to be performed on each rearranged value. Because there would be one specific computation that needs to be performed and one type of rearrangement method involved, it is suggested that we use an iterative structure in developing the algorithm be used. This structure include forand whilestatements. Here a series of statements are executed in a looping manner. Other conditional statements will also need to be included. Based on the requirements it was decided that a series of variables needed to be created. These were as follows:   Variable to store the initial string to be searched, which would be of type string; astring   Variable to capture substrings created, which would be of type string; bstring   Two variable for counting purposes, which would be of type integer; iand k.   Variable to compute the of type Integer, num.        3. Deliverables Thus the below pseudo-code is suggested. 1) Create String astring = "37540"; 2) Create int i =0; 3) Create int j=0; 4) for (i = 0; i&lt;=astring.length(); i++) {  for (j = i; j&lt;=astring.length();j++) {  String bstring = astring.substring(i, j); Integer number = ConvertToInteger(bstring);  if (number % 3 == 0)  System.out.println(bstring);   4. Extensions and future works No extensions were added to the original requirements, for future work and practice I would like to change the code so that it can do the following:   Asks the user to input a string, to be stored as astring.   Ask user to input another string which string should be divisible by, to be stored as bstring.   Produce an output in the similar manner as this application does, that is produce substrings dividable by the string entered, substring.      References:  Cormen T.H. et al. (2009)&nbsp;Introduction to algorithms. 3rd ed. Cambridge, Massachusetts: MIT Press.  Harel, D. &amp; Feldman, Y.A. (2004)&nbsp;Algorithmics: the spirit of computing.&nbsp;3rd ed. New York: Addison Wesley.  MIT (n.d.)&nbsp;Getting started with Scratch&nbsp;[Online]. Available from:&nbsp;http://info.scratch.mit.edu/Support/Get_Started, (Accessed: 27 April 2014).     
	              
	              Option 2: Generate pseudo-code for word search algorithm
&nbsp;

Make a list of five words, 3-6 letters in length.
Create a string of approximately 30 letters containing some of the five words.
Your algorithm should identify all of the substrings of the longer string that match any of the five words you generated, and the number of times each one appears
For example: If you chose the words&nbsp;structure,&nbsp;such, system, blue, red,&nbsp;and your algorithm operates on the string&nbsp;jkdistructuredstrusyssystemoon,&nbsp;your algorithm should report that the string contains the words&nbsp;structure,&nbsp;red&nbsp;and&nbsp;system&nbsp;each one time, and the words&nbsp;such&nbsp;and&nbsp;blue&nbsp;zero times.

&nbsp;
Start
fword ← “structure”
sword ← “such”
tword ← “system”
foword ← “blue”
fiword ← “red”
List&nbsp; ← fword, sword, tword, foword, fiword&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
String ← “jkdistructuredstrusysstemoon”
count ← 0;
procedure Search (String, List)
If (List empty)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;then
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (Declare search a failure)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; else
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (Select the first entry in list to be TestEntry;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; while (String ≠ TestEntry and there remain entries to be considered)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; do (Select the next entry in List as TestEntry);
If (fword = String)
&nbsp;then count=fword ← 1
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; else count = fword ← 0
&nbsp;
If (sword = String)
&nbsp;then count=sword ← 1
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; else count = sword ← 0
&nbsp;
If (tword = String)
&nbsp;then count=tword ← 1
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; else count = tword ← 0
&nbsp;
If (foword = String)
&nbsp;then count=foword ← 1
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; else count = foword ← 0
&nbsp;
If (fiword = String)
&nbsp;then count=fiword ← 1
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; else count = fiword ← 0
)end if
&nbsp;Print the values stored in count.
&nbsp;
End
&nbsp;&nbsp;
References:
Brookshear, J. G. Computer Science: An Overview XML Vital Source ebook for Laureate Education, 11th Edition. Pearson Learning Solutions. VitalBook file.

	              
	              Hi Dr. Anthony,
This week turn out to be really tough for me. The lecture notes, reading material and group engagement got me off to a good start and I believe I have grasp the concepts related to algorithm and pseudo-code. I enjoyed using the Scratch program, learning how it works and finally doing the assignment and working in the group was great.&nbsp;
When I got to the DQ I somehow got confused as I tried to apply what I understood form the text and was not able to complete the selected option satisfactorily. I need help going forward in this area.&nbsp;

Regards,
Ricardo.
	              
	              Hi Dr. Anthony,
My apologies for my previous post without talking about the key lessons learnt. So here goes...
First of all I learnt the formal definition for an Algorithm, the difference between the algorithm and the pseudocode. The representation of an algorithm and the various structures used in that representation.&nbsp;The art of problem solving really appealed to me as that is exactly the way I approach problem solving in my everyday routine, getting your foot in the door, evaluating &nbsp;a problem and possibly looking at solutions before the problem is defined. I also learnt the formal phases to problem solving and how to translate those phases in program development&nbsp;(Brookshear, 2011, p.199).
These are some of the key lessons learnt this week.

References:
Brookshear, J. G. Computer Science: An Overview XML Vital Source ebook for Laureate Education, 11th Edition. Pearson Learning Solutions.

	              
	              Hi

Week 7 was very interesting simply from the fact that we went from individual assignments to a group arrangement which I think fed in well with the type of assignment it was.
I learned about the scratch program and website which I hadn't heard of before, and from what I can see it looks like a useful little thing indeed. Ideal introduction to areas like this and I can see how schools and colleges could adopt it as a tool.
I expanded my knowledge of pseudocode and algorithms and was pleasantly surprised because even though there was only 3 choice situations to base the pseudocode on, that the wide variety of different angles and styles of pseudocode reported,&nbsp; all of which are correct and none are wrong was really interesting to see the differences that other students had produced.
Overall an interesting week.

Chris


	              
	               
DESIGNING AN ALGORITHM
 
&nbsp;CHOOSE OPTION 1
 
Create String test=”37545”
 
create Len=length 
 
K=constant, k=1
 
For (i=0&nbsp; to Len-1, i ++)
 
While k&lt;= Length
 
String substring=substring of test
 
Convert submit to int num
 
If( number mode 3 ==0), &nbsp;Then
 
Print this string
 
&nbsp;&nbsp;if k==5
 
&nbsp; if k = i +1
 
break while loop
 
Else&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
 
K++
 
End while
 
&nbsp;
 
 
DESIGNING AN ALGORITHM
 
&nbsp;CHOOSE OPTION 1
 
Create String test=”37545”
 
create Len=length 
 
K=constant, k=1
 
For (i=0&nbsp; to Len-1, i ++)
 
While k&lt;= Length
 
String substring=substring of test
 
Convert submit to int num
 
If( number mode 3 ==0), &nbsp;Then
 
Print this string
 
&nbsp;&nbsp;if k==5
 
&nbsp; if k = i +1
 
break while loop
 
Else&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
 
K++
 
End while
 
&nbsp;
  
DESIGNING AN ALGORITHM
 
&nbsp;CHOOSE OPTION 1
 
Create String test=”37545”
 
create Len=length 
 
K=constant, k=1
 
For (i=0&nbsp; to Len-1, i ++)
 
While k&lt;= Length
 
String substring=substring of test
 
Convert submit to int num
 
If( number mode 3 ==0), &nbsp;Then
 
Print this string
 
&nbsp;&nbsp;if k==5
 
&nbsp; if k = i +1
 
break while loop
 
Else&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
 
K++
 
End while
 
&nbsp;
 
	              
	              Thanks for the comments, Craig, and well done for hanging in there with the programming. As you correctly surmised, it is beneficial to approach algorithms and programming with the right mindset.Regards,Anthony
	              
	              Thanks for the comments/summary of topics covered, Augusto. &nbsp;It looks like you had a cohesive team for the group work, which makes the tasks less onerous.
Regards,Anthony

	              
	              Thanks for the comments, Bo, and well done for persevering with the programming/algorithmic work. The group work is designed to further foster team-based collaboration, and I am glad that you received additional input from Babatunde. &nbsp;
As regards ensuring that the versions of applications used are harmonized, we are a liitle relaxed with this, and avoid being overly-prescriptive about the versions - in future, you can negotiate with your colleagues to agree on which version the team uses for the week's work.Regards,Anthony
	              
	              Hi Anthony,
I have found this week to be interesting but though and challenging for me personally.
I learnt the formal definition for an Algorithm, the difference between the algorithm and the pseudo-code, the representation of an algorithm and the various structures used in that representation.
Though Scratch program has never been used by me before but I had fun and learned more while working with it. It was quite difficult at the start but following the steps, it turned out to be a little bit cool.
It was good working in a group. I am looking forward to other group works in the future.
&nbsp;
Best Regards, Babatunde
&nbsp;
References:
Laureate Online Education. (2014). Computer Structures – Lecture Notes Week 7: Algorithms [Online]. Available from: https://elearning.uol.ohecampus.com/bbcswebdav/institution/UKL1/201480_SEPTEMBER/MS_CKIT/CKIT_501/readings/UKL1_CKIT_501_Week07_LectureNotes.pdf (Accessed on: October 16, 2014)
Brookshear, J. G. (2012). Computer Science An Overview. 11th ed. Boston Massachusetts: Pearson. Addison Wesley
Learn Scratch [online] Available from: http://learnscratch.org/ (Accessed on: October 20, 2014)
	              
	              Hi Anthony
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; I will be honest Algorithm is not a subject I excelled in my previous academic experience. I struggled in comprehending the subject area. I did enjoy the Scratch assignment. I found out my son had used the software in the past and he was making fun of me for using it. I also found the Week 7 extension to Week 8 and deliverables timeline &nbsp;DQ/Assignment a bit confusing.
robin

	              
	              Dr. Anthony,
This week was interesting with the algorithms and the scratch program. I think it was a bit of a challenge for me as I tried to use my pseudo-code to do what it was expected to in Java. And I must say that I am a little bit disappointed in myself there. I have recently started a job in the line of programming and it has been challenging at work because I lacked the necessary experience have been exposed to it since university which was over 5 years ago.&nbsp;
The scratch software was good, I liked the interface and its user friendliness.
Overall I think it was a good week and as usual I am looking to see what comes next.
Tanisha
	              
	              Hi Babatunde
I love Ice cream, so I should have chosen that optionJ. Instead I chose the number optionL.
Well, were asked to check each other’s code, so because of the Ice cream, I chose to comment on your code, and I’ll check for correctness. As I’ve mentioned several times, I’m no programmer hence I will only give “simple” feedback. And yet, I have tried to come up with some more constructive suggestions, but I haven’t managed yet.
If I understand you’re code correctly, then I think there are 2 problems/challenges:

A tie, e.g. 5-5-0 or even 4-4-2:
If the voting is 5-5-0, then the first “if a &gt; b then” will go to Else defining B as highest even though A and B are qual. Then what?
Possible solution: Construct a test that finds these ties, and for example in the 5-5-0 you could print a message saying there is a tie, and let A and B choose again hoping someone will change their mind, and if so, then you have a winner.

If for example you get a 4-4-2 voting, then you could let the 2 vote over again and just be able to choose between A and B. If the voting ends with 6-4-0 or 4-6-0 then you have a winner.

Highestvalue always &gt; 10

If the value is less than 10, which it probably will, then this test will go to Dialogue. Dialogue starts by setting the ABC variables to 0, thus the process starts again. Of course you do inform every person about the highest score, thus there is a possibility that they will chose differently next time since they know they have to choose just one flavour. But perhaps you might think of another strategy.


A solution to this could be that the majority always wins:

Voting 2-3-5 = C is the winner
Voting 4-4-2: Let C chose again and hope for a winner. If still a draw then &nbsp;
It the voting goes to 5-5-0 then you probably have to do a loop until someone votes differently and you have a winner.




Another issue, if this was to be a finished program, then there should have been some error handling e.g. if a person returns 1 instead of A. And what about asking the person if he/she is sure, e.g. in case of mis-typing.
Best wishes
Bo
	              
	              hi Anthony,
This week task was a bit challenging for me as programming has been an issue with me. learning scratch new to me but it was good and I liked the interface. I had always wanted to learn more on &nbsp;pseudo-codes and Algorithm and and am glad it was a good start.
regards
martins

	              
	              Hi Augusto
Your code looks great! I just wish I could give it the appropriate justice in my review.
It looks like you have adopted a much better and elegant method for looping, which occurs once you have identified substrings that are divisible by 3.&nbsp;
I haven't fully figured it out, but it looks like you have created an index, which you perform +1 each time you loop, and the loop is only executed while you have an index that is less than or equal to the max length of the string (it is -1 as you start at 0, and only want to review a string of 5 which is your string length), else the program ends.
I believe your algorithm/principle is what I was hoping to acheive in my pseudo-code, however I just wasn't able to figure out how to do it. Unlike my pseudo-code, which executes a set of actions sequentially, which are all hard coded, your pseudo-code adopts a loop, executing the same set of actions until a certain index parameter is reached. Nice job, very elegant!
I'm going to try to adopt this same approach and re-post my revised algorithm (on Monday of week 8, which is as per Week 7 DQ pt 1 under Respond:)&nbsp;
Best Wishes, Craig
	              
	              Thanks for the comments, Terry. &nbsp;Is the internet connection in&nbsp; Hangzhou, and similar parts of China, slow all the time? &nbsp;I imagined that this was an aspect of China's infrastructure that had been revamped.
Regards,Anthony

	              
	              Thanks for the comments, Kharavela. &nbsp;Glad to hear that you had a good week, with the course topics.
Regards,Anthony

	              
	              Thanks for the comments/summary of topics covered, Ricardo. &nbsp;Don't worry that not all the topics have been fully mastered - the important ones will be revisited multiple times in future Master's modules, to ensure that you gain sufficient understanding of these.
Regards,Anthony

	              
	              This week’s activity entailed generating pseudocode to implement specific algorithms. &nbsp;The follow-on discussions about this will take place in the next study week.
Anthony
	              
	              Hello Anthony,
This weeks assignment and discussion needed more of my time than earlier subjects. Scratch is fun to play with and pseudo coding is very practical as it is language agnostic.
In general I learned about how to analyse a project in order to build an algorithm, how you can find different versions and why some of them are better than others. The text about tools like loops and recursion&nbsp; was a contribution also.
Working together in a team is great. It was fun to see how we all contributed to the solution by sharing our thoughts and get to know some class members.
I hope somebody will give me feedback on my pseudo code. For all assignments I miss a practical level of feedback&nbsp; that goes beyond grading and actually helps me understand what I do wrong or right. But I also understand this is perhaps hard to organise.
Greetings from Bram

	              
	              Hi Anthony,
The network infrastructure in China is not bad. It's at least much better than in the US as far as I know, since there are lot more new infrastructure in China. I experienced slow Internet is because my VPN didn't work (that means no Google and Facebook) and LENS seemed to be very slow without a VPN.
br, Terry
	              
	              Yes, that's true - the vpn tools do make a difference!
Regards,
Anthony
	              
	              Thanks for the comments/summary of topics covered, Chris. &nbsp;
It looks like you had a useful and productive week all round.
Regards,Anthony

	              
	              Thanks for the comments, Babatunde. &nbsp;Yes the topics were useful and Scratch programming was quite interesting.
Regards,Anthony

	              
	              Dear Tresor,

In your algorithm before starting for loop, you are initializing integer i with value of T[1], which can produre wrong output. 

If we test your algorithm for value 37540

In this case T[1] = 3, T[2] = 7, T[3] = 5, T[4] = 4, T[5] = 0 

i ← T[1]; // i &lt;- 3

&nbsp; for i = T[1] to T[n] //for i=3 to i=0

&nbsp;&nbsp;&nbsp; if T[i] mod 3 := 0 // here value of i is 3 and it will check if T[3] is divisible by 3 or not skipping checking T[1] &amp; T[2]

	              
	              Hi Terry,I was looking at your first submitted pseudo-code and to be quite honest it looked a bit difficult to follow, perhaps this is because I am not sure of the language it is expressed in. However I noticed that you resubmitted but it was the quite similar to your first submission. Note, I am commenting on the submission where you selected Option 1: Numbers.“In general,&nbsp;a pseudocode is a notational system in which ideas can be expressed informallyduring the algorithm development process.”, Brookshear, (2011).Prehaps if this program is placed in the appropriate programming application it will run, which I believe is one of the reasons that it does not meet the requirements of a pseudo-code. I think that you should try to re-submit your pseudo-code keeping the following pointers in mind.1) Pseudo-codes are an informal way of representing an algorithm.2) Pseudo-codes should not be executable.3) Certain details are omitted from pseudo-code, e.g. variable declarations.4) Pseudo-codes usually take the pattern of English like statements, which makes them easier to read and understand compared to the actual programing language.However, there you did seem to have initialization, modification and termination components in your code.See an example of a pseudo-code below, which was taken from Brookshear, (2011).if (not raining) then &nbsp;(if (temperature = hot)  then (go swimming) else (play golf))else (watch television)&nbsp;I think that it would be really commendable, if you can explain your code in an easier to understand manner, that is using &nbsp;pseudo-code.References:Brookshear, J.G. (2011) Computer science: An overview. 11th ed. Pages 192 and Pages 195 &nbsp;Boston: Pearson Education / Addison-Wesley.&nbsp;Computer Structures — Lecture Notes, Week 7: Algorithms [Online]. Available at:https://elearning.uol.ohecampus.com/bbcswebdav/institution/UKL1/201480_SEPTEMBER/MS_CKIT/CKIT_501/readings/UKL1_CKIT_501_Week07_LectureNotes.pdf (Accessed on October 25, 2014)&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;
	              
	              Thanks for the comments, Robin. &nbsp;
Yes, thinking algorithmically does take some getting used to - but I am sure you will enjoy the process :-).
Regards,Anthony

	              
	              Thanks for the comments/summary, Tanisha. &nbsp;
Yes, I agree - the Scratch software was quite useful in encouraging algorithmic thinking.
Regards,Anthony

	              
	              Hi Tanisha,
Thanks for your comments:-)
I'm sorry for writing code that is not expressive enough. I should have done it better.
Yes, you are right. My code is not "pseudo-code", it's actually Python and you can run it in any Python environment. In my case, it runs directly in my editor. But that doesn't make an execuse for my code not being expressive. All programming languages support some level of abstraction. I should have done it with better abstraction.
On the other hand, I don't like pseudo-code, and I won't write pseudo-code.
br, Terry
	              
	              Hi Bo,
Thanks for your reply, this will be addressed in the ENHANCED version of my Pseudo-code and will be posted By Day 5 (Monday, 27/10) into the "Week 7 DQ continued" thread.
Best Regards, Babatunde
	              
	              Hi Chika,

I love Ice cream, that’s why I chose this option.

Well, we were asked to check each other’s code by Sunday (Day 4) (Today) of Week 8, so because of my likeness for Ice cream, I chose to comment on your code, and I’ll check for correctness. Although I’m no programmer hence I will only give “simple” feedback. And yet, I have tried to come up with some more constructive suggestions.

If I understand your code correctly, then I think there are 2 problems/challenges:

Firstly, your ‘do while user wants to continue’ would mean that if do not wish to continue a consensus cannot be reached. Secondly, after ‘Error 5’ the process doesn’t repeat. It simply terminates. This also could result in not having consensus.

Suggestions


Remove the ‘5: Quit’
After ‘Error 5’ repeat the process from the point where the system displays available options.


Best Regards, Babatunde.
	              
	              Hello King Tan Yu,

Besides having chosen the same problem statement (finding words in a string of letters) we also looked in the same direction for a solution (The Knuth-Morris-Prath algorithm). In the end we found some different references and therefore diverted a bit in our proposed solution. 
&nbsp;
In your proposal I found the COMPUTE-PERFⅨ-FUNCTION (P) an interesting function as it looks if the pattern consists of returning patterns itself. If this is the case this information can be used to skip some positions in the investigated text string. I think this variant is also known as the Boyer-Moore string search algorithm. In my Text-string example with my chosen set of words this would not have made a difference, but still this a great optimalisation when word search inputs do have repeating patterns as the Knuth-Morris-Prath (KMP) algorithm check every single letter one by one.
&nbsp;
I wondered how you would apply your proposal with different sets of words that needed to be checked. I changed the KMP solution of Simha (Simha, 2014) in order to deploy it in my overall procedure I was set to create for the 5 words and the string. One change I had to make is line 18 to return the textCursor position in order to let the procedure not stop when one word was found. Instead it starts the matching exercise again for the first letter after the match. This is also part of your solution by line 12. q ← ∏[q]. 
&nbsp;
The main procedure countWords consists of a recursive 'if-then-else' operation selecting the words one by one, start the KMP-solution to check how much they occur, until all 5 are checked. The KMP-algorithm gives back when the text is not found, the procedure counts how many times occurrences are found. This is still a weak point of my programming as one gets too many negative messages. The reporting has to be done in the procedure and I have to add that if the KMP-algorithm is not repeated a word was not found. 
&nbsp;
Both Knuth-Morris-Prath as Boyer-Moore optimised string search algorithm are ideal for single pattern searches. For our multiple word assignment we could improve our algorithm by using&nbsp; hashing for shifting substring search as worked out in the Rabin-Karp algorithm. In this method the hash value of the words are determined and look for these pattern values in the total word string. The hash values have to be created only ones and can be applied to the whole string instead of looking at every letter in a string every time (or skip if the search word has repeated pattern). But this is something I will try for tomorrows exercise. 
&nbsp;
List (1 , the , 3, S), (2, cat, 3), (3, loves, 5), (4, &nbsp;his, 3), (5, mat, 3)
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Procedure: countWords
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Input&nbsp;'list of pattern arrays and their length', N = 1, S = 0)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;// algorithm as written in section B is started with input of first of 5 words (N = 1)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // and the length of the word W, &nbsp;while the score (S) and textCursor position &nbsp;is 0
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;// as the counting begins.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1. &nbsp;&nbsp;if&nbsp;N &lt; 5 &nbsp;do&nbsp;Start Algorithm (&nbsp;KnuthMorrisPrattPatternSearch)&nbsp;and
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;Input N (pattern array, 'text array')&nbsp;and&nbsp;Input (textCursor = 0)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // algorithm is started again when a pattern is found but now
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // with updated textCursor position and Score which is also displayed
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;return = 1&nbsp;do&nbsp;Start&nbsp;Algorithm (KnuthMorrisPrattPatternSearch)&nbsp;and
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Input&nbsp;N(pattern Array, text array)&nbsp;and&nbsp;Input (textCursor )&nbsp;and&nbsp; S = S + 1,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Display&nbsp;['text array' appeared T times in 'text array']
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;// if there was no hit the next word needs to be selected and score set to 0
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;else&nbsp;N = N + 1&nbsp;and&nbsp;S = 0
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;endif
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5.&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;else
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;6.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Display&nbsp;[Ready, but does the cat love the mat of the neighbour?]
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 7.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;endif
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;8.&nbsp;&nbsp;&nbsp; endif
Algorithm: KnuthMorrisPrattPatternSearch ('text array', 'pattern array')
Input: 'pattern array' of length W, 'text array' of length T, textCursor position C
&nbsp;&nbsp;&nbsp;&nbsp; // Compute the nextIndex function, stored in an array:
1.&nbsp;&nbsp; nextIndex = computeNextIndex (T)
&nbsp;&nbsp;&nbsp;&nbsp; // Start the pattern search.
2.&nbsp;&nbsp; textCursor = C
3.&nbsp;&nbsp; patternCursor = 0
4.&nbsp;&nbsp;&nbsp;while&nbsp;textCursor &lt; T&nbsp;and&nbsp;patternCursor &lt; W
5.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;text[textCursor] = pattern[patternCursor]
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // Advance cursors as long as characters match.
6.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; textCursor = textCursor + 1
7.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; patternCursor = patternCursor + 1
8.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;else
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // Mismatch.
9.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;patternCursor = 0
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // If the mismatch occurred at first position,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // advance the pattern one space.
10.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; textCursor = textCursor + 1
11.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;else
&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; // Otherwise, use the nextIndex function. The textCursor
&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;// already points to the next place to compare in the
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // text.
12.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; patternCursor = nextIndex[patternCursor]
13.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;endif
14.&nbsp;&nbsp;&nbsp;&nbsp;endif
15.&nbsp;&nbsp;endwhile
16.&nbsp;&nbsp;if&nbsp;patternCursor = W
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // If there was a match, return 1 and the position of the
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // textCursor to be able to check rest of 'text array'.
17.&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;1
18.&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;textCursor
19.&nbsp;&nbsp;else
20.&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;-1&nbsp;and&nbsp;display[the 'pattern array' was not found in the 'text array']
&nbsp;&nbsp;
Output: 1 and textCursor if the inputted word is found, -1 if not found.
&nbsp;
&nbsp;
References 
Simha (2014) Course Materials; Module 5, Pattern search [Online]. The George Washington University. Available from http://www.seas.gwu.edu/~simhaweb/alg/lectures/module5/module5.html (Accessed on 22 October 2014).
Wikipedia. (2014). Boyer-Moore string search algorthm [Online]. Wikipedia Foundation Inc.. Available from: http://en.wikipedia.org/wiki/Boyer%E2%80%93Moore_string_search_algorithm. (Last accessed 26 October 2014).
Wikipedia. (2014). Rabin-Karp algorithm [Online]. Wikipedia Foundation Inc.. Available: http://en.wikipedia.org/wiki/Rabin%E2%80%93Karp_algorithm. (Last accessed 26 October 2014).

	              
	              Hi Craig,
I see that from your response to my original post you were trying to find out how to create the index. From your pseudo code above I can see that you have one index, but in fact, you need to identify the position of your index. Without going into too much detail, a variable with an index is called an array, something like this is an array: CarColors() = Blue,Green,Black,Yellow. And the car color Black is the CarColor(3), where 3 is the postion of the index. In this notation we do not define the index, the mere parentheses () indicate that this variable has an index.
I guess the simplest case would be something like this:
 

MyString() ← A,C,K,M&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // MyString(1) = A, MyString(2) = C, etc X ← 1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;// This is the Index position, which is also a counter
while&nbsp;(X &lt; 5) do&nbsp;( &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Display MyString(X) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;X ← X + 1 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;) )
This will display:
A &nbsp;//at the first iteration&nbsp;MyString(X) is&nbsp;&nbsp;MyString(1), which is equal to A) C K M&nbsp;&nbsp;//at the last iteration&nbsp;MyString(X) is&nbsp;&nbsp;MyString(4), which is equal to M)
Your algorithm works, it is just not that efficient. But I hope that with this you will be able to improve your algorithm.&nbsp;
	              
	              Hi Numan, Just a detail: although you mention a loop for traversing the Array[], it seems it's missing a recursive structure. If I follow your logic that part will only be executed once. I know it needs to be repeated, but if I show this to my wife I bet she will only do it once :). As an example: while (Index is &lt; than Input lenght)  do ( your logic&nbsp;)   You leave open to interpreation how to find the substrings of Input. Once can argue that there are already algorithms for that, but it might be good to have that logic included :). Best Regards, Augusto.
	              
	              Hi Bo,

Reading that you said you're not a programmer you did very well with how you structured your work this week with the code.
The plus points I found with your work is that it was clear and easy to follow and instructional. You set it out well to make it easy to read and the notes are a good addition.
The only area i'd suggest improving is from a processing speed and efficiency perspective to help you. Please note there is nothing wrong with your work here but more a suggestion to make it a tad cleaner and quicker.

Where you have section "B" procedure the initial generation of the list and then section "C" procedure taking that list and then removing the values divisible by 3 is where I think can be improved. If you make these into 1 procedure and check whether the value is divisible by 3 after each number is initially recognized it would not only reduce the need for a second procedure but also reduce the I/O read/write and thus speeding up the process. Because essentially what you're doing is saving a full list of values and then re-opening the list and then saving again for the ones divisible by 3. If you combine them to imediately check if it is divisible by 3 and only write to the final saved list the numbers that are needed for the final output.
I hope this helps.
Chris


	              
	              Hi&nbsp;Kharavela,
Your algorithm is very simple and easy to follow. I was able to follow it properly, until 7.2, where I hit a problem with&nbsp;&nbsp;MID(sNum,i,(j+1)) .&nbsp;If I am at the first run I end up with this: MID(37540,0,1), I suppose MID somewhat tells me that position 0,1 is equal to 3 (of&nbsp;37540), is that correct? I just don’t know what MID stands for.&nbsp;What is exactly MID?
Best Regards,
Augusto.
	              
	              Thanks Augusto
I will look to re work it tomorrow and re post - hopefully more efficient, which is my objective. I really appreciate your help. Thanks.
Best Wishes, Craig

	              
	              Hi Bo.Just looking at your algorithm, I first see that&nbsp;you have tried to make it generic by allowing&nbsp;the user to define both the numerator value and&nbsp;the divisor.
Probably at the point you are checking to see&nbsp;whether the value entered is an integer a&nbsp;provision to exit the sub procedure would be&nbsp;useful.
This will ensure that the sub procedure 'Generate' will only be called up &nbsp;if both input&nbsp;values meet the required criteria of being&nbsp;integers.&nbsp;
It was not initially clear to me how are you&nbsp;linking or referencing values in your sub&nbsp;procedures. For example the original values&nbsp;input in the first sub procedure. That's the&nbsp;challenge sometimes with using sub procedures, control of execution is sometimes difficult to&nbsp;follow through the use of sub procedures and the&nbsp;goto statement.
In the sub procedure Calculate.Integers, what's&nbsp;the rationale behind the line v_no = Count&nbsp;(values in Init.List) +1 &nbsp;? It would appear to&nbsp;me then that at no time would the value of v_no&nbsp;be &lt;1 as tested in the next line. I believe the&nbsp;Count of values in Init.List would be 0 or a&nbsp;number.
Also in your loop, I notice that you start by setting v_no=v_no-1 and then use the same v_no as the numerator in obtaining the result. I believe this is a mistake because the value you are checking to establish whether it is divisible by init_div ought to be those obtained from your Init.List. I think you intended to use the v_no to pick the ordinal position of the numbers from the initial combinations that you had extracted.
I didn't understand the lines that follow:&nbsp;If (result is an Integerthen add result to procedure Generate(Int.List)&nbsp;My understanding is that Init.List holds the initial combinations that you want to check so when you say add result to procedure Generate(Init.List) I get lost.
The purpose of the next statement (else goto sub_procedure Calculate.Integers) is also not clear to me because my thought would have been to use the loop you had initiated to iterate through the values that you are checking. Calling this sub procedure at this point in the way it is stuctured could result in an endless loopGenerally I would hesitate to use sub procedures within a main procedure as it more often than not complicates the flow of execution and control and may not be the best standard practice to adopt as it is less compact.
Below is the algorithm I developed for this same problem for comparison :
The&nbsp; algorithim below searches a string of at least five numbers &nbsp;and identifies all of the substrings that form numbers that are divisible by 3. Lines that begin with the apostrophe are comments
‘ Start by declaring all the required variables to be used in the algorithm
&nbsp;
‘Holds the input number
Dim MyStr As String &nbsp;&nbsp;&nbsp;
&nbsp;
‘Stores the numbers to be evaluated for divisibility by 3
Dim MyNum As Long
&nbsp;
‘Stores the final output
Dim MyResult As String
&nbsp;
‘ Variables to be used for the loop
Dim i As Integer
Dim j As Integer
&nbsp;
&nbsp; ‘Accept a 5 digit number
&nbsp;&nbsp;&nbsp; MyStr = Inputbox(‘Enter&nbsp; a number with at least 5 digits’)
&nbsp;
&nbsp;&nbsp;&nbsp; ‘ Check whether value entered is numeric
&nbsp;&nbsp; If&nbsp; not&nbsp; isnumeric(MyStr) then
&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;Msgbox “Invalid Number”
&nbsp;&nbsp;&nbsp;&nbsp; Exit sub
Else
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ‘ Check whether value entered has at least 5 digits
If len(trim(mystr)) &lt;5 then
&nbsp;&nbsp; MsgBox “Invalid number”
End if
&nbsp; End If
&nbsp;&nbsp;&nbsp;
&nbsp; ‘ Outer loop allows for iteration of each digit in the number
&nbsp;&nbsp;&nbsp; For i = 1 To Len(MyStr)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ‘ Inner loop allows for iteration of segments of numbers within the value entered
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; For j = i To Len(MyStr)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ‘ Extract number and compare whether divisible by 3
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; MyNum = Mid(MyStr, i, j)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; If MyNum Mod 3 = 0 Then
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ‘ For concatenation of the string result, stores the first number
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; If Len(Trim(MyResult)) = 0 Then
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; MyResult = Str(MyNum)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Else
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ‘ Add subsequent numbers obtained to result string and ignore any duplicate numbers
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; If InStr(1, MyResult, Str(MyNum)) = 0 Then
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; MyResult = MyResult + ", " + Str(MyNum)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; End If
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; End If
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; End If
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Next j
&nbsp;&nbsp;&nbsp; Next i
&nbsp;&nbsp;&nbsp;
&nbsp;‘Display result
&nbsp;&nbsp;&nbsp; MsgBox MyResult



	              
	              Thanks for the comments, Martins. &nbsp;
Good to hear that the Scratch software was a useful programming tool.
Regards,Anthony

	              
	              Hi Chris
Thank you for your kind words.
Yes, your advice help and you are completely right about step b and c. These could and should have been one procedure. I just didn't know how to do it even though it was somewhere in my head - like when you try to say something, and it's on you tongue, but the words just don't come out.
Bo
	              
	                   Terry, Your ending statement seemed very brash, given the fact that we are in a learning environment. Additionally the requirement was to write pseudo-code. I think the overall objective of the assignment was basically to submit your pseudo-code, something that would appear easy to follow and understand, thus others can learn from your work. You seem like a very smart individual and as such I assumed that your code would work. At one point I actually, said to myself, "Let me see what Terry did", going to the discussion board and looking for your submission. But to my disappointment, after looking at it several times I still did not understand it. Forgive me, but you must know that we are all from different backgrounds and as such everyone here is not a programmer. Nevertheless I have seen that you have made multiple submissions on different options, which is very commendable as is, in my opinion, your usual submissions. Kind regards, Tanisha  
	              
	              Hi Tanisha,
I'm really sorry about my very brash ending statement. I was in a hurry closing everything before going to bed.
I don't write pseudo-code is because I want my representation of the algorithm to be readable both by a human and a computer. So that I can communicate the idea with other people, without being too abstract from the necessary details. Obviously, I wasn't very successful in achieving the goal of making it readable by a person. If I got time, I would re-submit my code with better abstraction and try to do it better:-)
br, Terry
	              
	              Thanks for the comments, Bram. &nbsp;
Yes, it is a little difficult to give very detailed critique of your pseudocode within the limited time available for grading. &nbsp;Sometimes, in those cases, your peers do a better job with this as they post comments and feedback on your initial DQ post, in the forums.
Regards,Anthony

	              
	              Hi Terry,
Its okay, no problem.
Tanisha
	              
	              Code may seem complex, but much of it can be explained using real-world examples and analogies. For example, consider the following scenario: You take a holiday to a city you have never visited before. You know someone there, and decide to pay this friend a visit. You hire a car, but now you need directions. You have two options. (1) You could use your mobile phone and get live help (directions) from your friend during the entire drive. (2) You could use a GPS device to give you both directions and traffic reports. However, you remember that GPS calculates directions based on traffic conditions, and updates on said conditions usually lag about 10 to 15 minutes behind a traffic incident (e.g., traffic jams and accidents). If you translated these choices into programming counterparts, they become quite reflective of compiled and interpreted programming languages.&nbsp;
Both compiled and interpreted programming languages have their merits and weaknesses. When you think of them simply as categories of languages, those differences might be hard to recognise. However, it is likely that the driving analogy immediately triggered thoughts of pros and cons. You quickly evaluated each option based on your past experiences. This is the power of making real world analogies centred on programming concepts. For this Discussion, your challenge is to draw an analogy between these two driving choices and their programming language counterparts, and assess each choice.
To prepare for this Discussion:

Review your Weekly Learning Resources with a focus on compiled vs. interpreted code.
Identify the pros and cons of compiled and interpreted code.
Reflect on ways to explain compiled and interpreted code concepts using real world examples and analogies.

To complete this Discussion: Post: Create an initial post in which you draw an analogy from the alternatives of receiving live navigation directions [for driving] versus using GPS, to the case of a compiled versus an interpreted code. Address the following:

Analyse the pros and cons of each driving navigation alternative.
Analyse the pros and cons of compiled vs. interpreted code.
Fully state and justify any choices, assumptions or claims that you make using the suggested Learning Resources for this Week and/or your own research.

Respond: Respond to your colleagues. Address the following:

Identify and analyse additional pros and cons that your colleague may have overlooked, based on their drawn analogy.
Offer different pros and cons, for both the navigation choices and programming languages that might arise should you look at the choices using a different analogy. Be sure to include the different analogy as context.
Be sure to support any claims you make.

For all Discussions (unless stated otherwise):

Create a single document with your initial post. Your document should be 350-500 words, though you will be marked based on the quality of your writing, not on the number of words.
By Sunday, post the text of your document to the Discussion Board for this Week, and upload the document using the Turnitin submission link for this Discussion.
By Wednesday, make 3–5 substantial follow-up responses to your colleagues. These can include responses to your colleagues’ initial posts, as well as responses to colleagues who responded to your own initial post. Your total Discussion Board participation must occur on at least 3 individual days during each week. Follow-up responses should be significant contributions to the Discussion. Do not submit your follow-up responses to Turnitin.
In general, online discussion is best when you:


Ask insightful questions.
Extend the discussion into new but relevant areas.
Model or promote critical reflection.
Support your arguments with citations and references from the assigned Learning Resources and other literature, using Harvard Liverpool Referencing Style.



Ensure that you spread your discussion posts across at least three separate days of each week. This will help maximise the value of your discussion with colleagues and serve to meet the learning objectives for each activity.
Click on the Reply button below to reveal the textbox for entering your message. Then click on the Submit button to post your message.
	              
	              The development of new programming languages is a continuous endeavour to better represent binary machine code as natural human thought. However, due to the ambiguity of thought (assumptions, inferences, sub-conscious actions, etc.), no ideal solution has been found for emulating real human thinking. Nevertheless, computer scientists have made vast improvements in programming languages over the years.
One of the newer paradigms of programming languages is ‘object-oriented programming’ (OOP). Unlike the hierarchal nomenclature of structured programming, OOP focuses on describing objects (chunks of data) and combining them with functions that manipulate data.
Is OOP the next step in mimicking human thought? What are the advantages and disadvantages of OOP versus structured programming? For this Discussion, you will analyse these nuances as you attempt to explain OOP to those of a non-technical background.
To prepare for this Discussion:

Review your Weekly Learning Resources with a focus on objected-oriented and structured programming.
Research the usages of objected-oriented and structured programming today.
Reflect on how you think and how well objected-oriented and structured programming imitate that thinking.
Identify examples of OOP and structured programming.
Identify at least five key OOP concepts.
Reflect on how to explain OOP concepts to a non-programmer.

To complete this Discussion: Post: Create an initial post in which you compare object-oriented programming to structured programming. Address the following:

Summarise the key concepts behind OOP and structured programming languages as if you were explaining them to a non-programming, non-computing individual. Use real-world analogies (e.g., you could say that the technique of inheritance is analogous to the inheritance of biological traits from parent to child in that certain characteristics and physical structures carry from one generation to the next).
Analyse OOP and structured programming languages.


Identify the advantages and disadvantages of each language type.
Explain each advantage and disadvantage.
Use examples of OOP and structured programming languages to support your analysis.


Explain the perceived preference of OOP languages over structural programming languages in the computing industry today.
Evaluate whether OOP languages better reflect the human way of thinking and conceptualising. Explain your reasoning.
Fully state and justify any choices, assumptions or claims that you make using the suggested Learning Resources for this Week and/or your own research.

Respond: Respond to your colleagues. Address the following:

Debate your colleague’s position on the advantages and disadvantages of OOP and structured programming languages.
Support or refute your colleague’s position on whether OOP languages better reflect the human way of thinking and conceptualising.
Be sure to support any claims you make.

For all Discussions (unless stated otherwise):

Create a single document with your initial post. Your document should be 350-500 words, though you will be marked based on the quality of your writing, not on the number of words.
By Sunday, post the text of your document to the Discussion Board for this Week, and upload the document using the Turnitin submission link for this Discussion.
By Wednesday, make 3–5 substantial follow-up responses to your colleagues. These can include responses to your colleagues’ initial posts, as well as responses to colleagues who responded to your own initial post. Your total Discussion Board participation must occur on at least 3 individual days during each week. Follow-up responses should be significant contributions to the Discussion. Do not submit your follow-up responses to Turnitin.
In general, online discussion is best when you:


Ask insightful questions.
Extend the discussion into new but relevant areas.
Model or promote critical reflection.
Support your arguments with citations and references from the assigned Learning Resources and other literature, using Harvard Liverpool Referencing Style.



Ensure that you spread your discussion posts across at least three separate days of each week. This will help maximise the value of your discussion with colleagues and serve to meet the learning objectives for each activity.
Click on the Reply button below to reveal the textbox for entering your message. Then click on the Submit button to post your message.
	              
	              


Interpreted Language and Compiled Language
Terry Yin October 31, 2014




1 Interpreted language 
!turn left 
/bin/sh: turn: command not found

!go left 
/bin/sh: go: command not found
!find direction




find: direction: No such file or directory

!make ourway 
make: *** No rule to make target ‘ourway’.  Stop.

Now you are stuck in the middle of your way to your friend’s place in a foreign city. And you are getting frustrated with this unix shell (/bin/sh) based vehicle, Wikipedia (2014c). It doesn’t seem to understand your command. But at least the shall program tried to interpret your command immediately and give you some feedback.
!find grocery_store




&nbsp; &nbsp; grocery store






Ok, it seems the smart vehicle has found the grocery store you pointed to it. It’s a good start. Now you've learned more commands you can use on the vehicle. You can even program the vehicle with multiple commands:
!echo "trip start..."!if find grocery_store; then echo "I saw it"; else echo "what?";fi!echo "thanks for using our service."
&nbsp; &nbsp;trip start...&nbsp; &nbsp;grocery store&nbsp; &nbsp;I saw it&nbsp; &nbsp;thanks for using our service. 
The way computer runs the above shell script program is the same as the interactive commands we use in the beginning. The shell (here it’s the interpreter) first read one line from the script, next interpret it to understand your intent for the computer, then do it, and last it provides the feedback. The same process will continue until the end of the script. Wikipedia (2014b) 
2 Compiled language 
You might be a person who likes to plan your trip up-front. You are not satisfied with finding that your smart car doesn’t understand your command ‘turn left’ only when your trip started. You want to make sure the car understands all your commands and knows the direction already before you start the journey. 
Then you’d better compile all your instructions before you run the program by translating all the instructions to the basic commands smart cars can understand. It’s just like using a compiled language Wikipedia (2014a), like C: 
%%writefile trip.c #include "stdio.h" int main() {&nbsp; &nbsp; printf("start trip\n");&nbsp; &nbsp; if( find("grocery_store", F_OK ) != -1 ) {&nbsp; &nbsp; &nbsp; &nbsp; printf("I saw it.\n");&nbsp; &nbsp; } else {&nbsp; &nbsp; &nbsp; &nbsp; printf("what?\n");&nbsp; &nbsp; }&nbsp; &nbsp; printf("thanks for using our service\n");}
Overwriting trip.c

The above C code is “equivalent” to the shell script code in the previous section. But you cannot use it directly. You need to compile it first. Let’s compile it using the GCC compiler: 
    !gcc trip.c -o trip






trip.c:4:9: warning: implicit declaration of function ’find’ is invalid in C99 [-Wimplicit-function-decl if( find("grocery store", F OK ) != -1 ) {&nbsp; &nbsp; ^trip.c:4:31: error: use of undeclared identifier ’F OK’if( find("grocery store", F OK ) != -1 ) { &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;^1 warning and 1 error generated.
As you can see, the compilation wasn’t successful. The compiler told me that it doesn’t understand my function find and it doesn’t understand what F OK means. The good news is, at least I don’t have to be stuck in the middle of my journey, I know there’s problem already. 
Let’s fix the C code by including the necessary header file and using the correct function name. Then try again: 
%%writefile trip.c #include "stdio.h" #include "unistd.h" int main() { 
        printf("start trip\n");
        if( access("grocery_store", F_OK ) != -1 ) {

            printf("I saw it.\n");
        } else {

            printf("what?\n");
        }

        printf("thanks for using our service\n");
    }

Overwriting trip.c

Then run the GCC command again:
!gcc trip.c -o trip
 This time gcc didn’t say anything. It’s because gcc follows the good old Unix philosophy – no new is&nbsp;good news, Gancarz (2003).
The above command compiled the C code into a binary file trip, which can be executed directly on the machine. If it’s on a Microsoft Windows system, the name of the output binary file might be something like trip.exe. We can execute the executable binary file directly. Like: 
!./trip 
start trip
I saw it.
thanks for using our service






3 The mix of compiled and interpreted langauges 
So, which way of making the trip to your friend’s place do you like? Probably, you will do both. You might try your best to plan the direction before you start the journey, but you would also like to get ready to adjust for the situation you’ll meet on the go. 
Programming languages are the same. Nowadays, most languages have a little bit of both. Java is mostly considered a compiled language, but it compiles only to the bytecode that the JVM (Java Virtual Machine) could interpret. Python is considered an interpreted language. But it also compiles the source code to its own bytecode before execution. And interpreted programming languages like JavaScript has the technology called Just-In-Time (Brookshear 2011, JIT, p.271) compilation. JIT translates the script into machine code at run-time. That’s why JavaScript is so fast these days. On the other hand, the traditional low-level programming language like C are now using technology called LLVM, which is originally known as the Low Level Virtual Machine. So it might not always compiles to machine code, Lattner (2008). 
4 Conclusion 
It has been a while that new languages are mostly interpreted language, like Python, Ruby, JavaScript. But the most recent noticeable new language are compiled languages again, like the Go language by Google and the Swift language by Apple. That means the two kinds might just co-exist. And we want to take advantage from both of them. 
* I’m using OS X Yosemite. The GCC compiler I used is actually Clang. 
References 
Brookshear, J. G. (2011), Computer science: an overview, Paul Muljadi. Gancarz, M. (2003), Linux and the Unix philosophy, Digital Press. Lattner, C. (2008), Llvm and clang: Next generation compiler technology, in ‘The BSD Conference’, pp. 1–2. 
Wikipedia (2014a), ‘Compiled language — wikipedia, the free encyclopedia’. [Online; accessed 30-October- 2014]. 
URL: http: // en. wikipedia. org/ w/ index. php? title= Compiled_ language&amp;oldid= 619022934 
Wikipedia (2014b), ‘Interpreted language — wikipedia, the free encyclopedia’. [Online; accessed 30-October- 2014]. 
URL: http: // en. wikipedia. org/ w/ index. php? title= Interpreted_ language&amp;oldid= 628934282 
Wikipedia (2014c), ‘Unix shell — wikipedia, the free encyclopedia’. [Online; accessed 30-October-2014]. URL: http: // en. wikipedia. org/ w/ index. php? title= Unix_ shell&amp;oldid= 629756474




	              
	                 A Duck Tour Of Object-Oriented Programming Terry Yin October 31, 2014        Welcome on board our programming duck tour! But, wait a minute, just what is a duck? 1 The Object-Oriented Tour 1.1 Class and method    Well, duck is a general kind of bird. In object-oriented programming term,&nbsp;Duck&nbsp;is a&nbsp;class.&nbsp; Python is a programming language that supports multiple&nbsp;paradigms, like structured programming and object-oriented programming,&nbsp;Wikipedia&nbsp;(2014b). I will use Python 2.7 as the introduction language here. This is how you define a class in Python:            class  Duck  :         pass          The keyword&nbsp;pass&nbsp;here means to skip the content definition of the class.       What does a duck do? It&nbsp;quacks and&nbsp;walks. So, let's define these behaviours of the duck class:            class  Duck  :         def  quack  (  self  ):             print  "quack"         def  walk(    self  ):             print  "walk"         1.2 Inheritance   But&nbsp;Duck&nbsp;is still too general. There are more specific types of ducks. For example,&nbsp;DomesticDuck&nbsp;is a type of duck.&nbsp;MandarinDuck&nbsp;is another type of duck. The "is a" relationship in OO programming is called&nbsp;inheritance.             In Python:            class  DomesticDuck  (  Duck  ):         pass       class  MandarinDuck  (  Duck  ):         pass          Then we can say A&nbsp;DomesticDuck&nbsp;is a&nbsp;Duck, because&nbsp;DomesticDuck&nbsp;is&nbsp;derived&nbsp;from the&nbsp;Duck&nbsp;class. This is especially important for&nbsp;strong typed&nbsp;language, e.g. Java &amp; C++. Because in strong typed language, only the instance of the&nbsp;Duck&nbsp;class or its derived classes are considered duck. Not matter how similar it is to a duck, as long as it doesn't have the class&nbsp;Duck&nbsp;in its inheritance hierarchy, it's not a duck. However in a&nbsp;dynamic programming language, like Python, as long as the object has the&nbsp;quackmethod and&nbsp;walk&nbsp;method that we need, we can consider it a duck without needing inheritance. Or "if it walks like a duck and quacks like a duck, it is a duck." This is called&nbsp;duck typing,&nbsp;&nbsp;Wikipedia&nbsp;(2014a).     1.3 Object     So an instance of a class could have other values as its member. We just used the term&nbsp;instance. It also has another name,&nbsp;object. A domestic duck is an instance of the&nbsp;DomesticDuck&nbsp;class. Let's create two&nbsp;DomesticDuck&nbsp;instances:             jimmy  =  DomesticDuck  ()      tommy  =  DomesticDuck  ()        1.4 Polymorphism       Although we didn't define method&nbsp;quack&nbsp;for the class&nbsp;DomesticDuck, it inherits the method from its&nbsp;parent class&nbsp;(another name for derrived class)&nbsp;Duck. We can also override the method in a&nbsp;subclass. Let's say the&nbsp;DomesticDucks are noisier than other ducks:             class  DomesticDuck  (  Duck  ):          def  quack  (  self  ):              print  "quack-quack-quack"          Now the&nbsp;DomsticDuck&nbsp;shares the same&nbsp;interface&nbsp;quack&nbsp;as the more generic&nbsp;Duck, but has different behaviours. This is called&nbsp;polymorphism.             jimmy  =  DomesticDuck  ()      romeo  =  MandarinDuck  ()      jimmy  .  quack  ()      romeo  .  quack  ()             quack-quack-quack     quack             1.5 Composition    A domestic duck unlike the wild ducks, they have owners. So a&nbsp;DomesticDuck&nbsp;has an&nbsp;owner. In Python, that could be expressed by:             class  DomesticDuck  (  Duck  ):          def  __init__  (  self  ):              self  .  owner  =  "the nature"          Then we can access the&nbsp;method&nbsp;and&nbsp;property&nbsp;we defined for the class:             jimmy  .  owner  =  "Terry"      print  "Jimmy duck belongs to"  ,  jimmy  .  owner             Jimmy duck belongs to Terry           The&nbsp;owner&nbsp;information is a field composes the&nbsp;DomesticDuck&nbsp;instances. For this kind of&nbsp;has a&nbsp;relationship, we call it&nbsp;composition. There's another type of composition. Considering ducks could have 0 to n children.             class  DomesticDuck  (  Duck  ):          def  __init__  (  self  ):              self  .  owner  =  "the nature"              self  .  children  =  []  # a list of 0 to n other ducks          The children ducks are not information that composes a (mother) duck, but only related to the (mother) duck. This is also a&nbsp;has a&nbsp;(or, has many) relationship. We call it&nbsp;aggregation.    &nbsp;       1.6 Encapsulation     Children ducks follow mother duck. So the behaviour of a duck could be:             class  DomesticDuck  (  Duck  ):           def  __init__  (  self  ):              self  .  owner  =  "the nature"              self  .  children  =  []  # a list of 0 to n other ducks           def  walk  (  self  ):              for  child  in  self  .  children  :                  child  .  walk  ()          Now when you call the&nbsp;move&nbsp;method on a mother duck,&nbsp;motherDuck.move()&nbsp;all its chidren, if there is any, will also move. We group the children information and the operation on the information together in the&nbsp;DomesticDuck&nbsp;class. This is called&nbsp;Encapsulation.     1.7 Information hiding     I think that's enough about duck. Let's talk about the duck tour. A "duck tour" is a sightseeing tour on a special vehicle that goes on both land and water, which can be found in cities like Singapore and San Francisco. But let's make a simple version of "duck tour", which is the tour of a happy duck.             def  duck_tour  (  duck  ):          duck  .  quack  ()          duck  .  walk  ()          duck  .  quack  ()          duck  .  quack  ()          duck  .  walk  ()          duck  .  quack  ()          duck  .  quack  ()          duck  .  quack  ()          duck  .  walk  ()          In the&nbsp;duck_tour&nbsp;function, it takes one parameter&nbsp;duck. What duck is it? This function doesn't care. It only "talks" to the duck using the interface&nbsp;quack&nbsp;and&nbsp;walk. The detailed implementation of&nbsp;quack&nbsp;and&nbsp;walk&nbsp;is hiding from function&nbsp;duck_tour. This is called&nbsp;information hiding,&nbsp;Colburn &amp; Shute&nbsp;(2007).             duck_tour  (  jimmy  )             quack quack quack walk quack quack quack quack quack quack walk quack quack quack quack quack quack quack quack quack walk            2 Structured programming       The previous&nbsp;duck_tour&nbsp;function doesn't belong to any class or object. It's a set of statements that is set aside from the states it operates. We call this the&nbsp;structured programming. Our function could be improved to have better structure by introducing a loop:             def  duck_tour  (  duck  ):          for  i  in  range  (  1  ,  4  ):              for  _  in  range  (  i  ):                  duck  .  quack  ()              duck  .  walk  ()          The duck tour story is just an analogy!     You might like the duck tour story and examples because it helped you to understand the object-oriented programming concepts (hopefully). But it's important to remember:&nbsp;The duck story is not OO programming. It's only an analogy of OO programming. OO programming is NOT to model the real world. It's a programming paradigm that lowers the coupling in the program by creating cohesive objects. So it rather depends on how the information inside a program related to each other and communicate with each other. It's just sometimes coincidentally similar to the concepts we see in a real world. 4 Conclusion I couldn’t found any evidence that the OO programming along really improved anything overall, comparing to structured programming. It is true that I know quite many good OO design principles and patterns that could make very good OO design. But I might be able to make the similar quality and maintainable design with structured programming style as well. And what really happened is program written in traditional OO language like C++ is sometimes even harder to be maintained than in simple structured programming language like C. Becuase traditional OO language is usually harder to master than structured language. A Peking Duck is also called a duck, but it doesn’t quack and it doesn’t walk. It tastes good though. This will confuse a lot of so-called OO programmers.   Does that mean we should abandon OO programming? I don’t think that’s going to happen. My personal opinion is, if you are using an OO language, you need to do proper OO design. Otherwise, things only get worse. If you are using a more flexible language, As Kevlin Henney said, the programming paradigms are&nbsp;probably not that important. We may have a hybrid of all different styles, Henney (n.d.). We can borrow things from any paradigm as long as it’s good.       This paper is created using IPython Notebook, P ́erez &amp; Granger (2007). References Colburn, T. &amp; Shute, G. (2007), ‘Abstraction in computer science’, Minds and Machines 17(2), 169–184. Henney, K. (n.d.), ‘C++ stylistics’, https://www.youtube.com/watch?gl=SG&amp;hl=en-GB&amp;v=zh8W4ZglOlw. Accessed: 2014-10-30.   P ́erez, F. &amp; Granger, B. E. (2007), ‘IPython: a System for Interactive Scientific Computing’, Computing in Science &amp; Engineering 9(3), 21–29. URL: http://ipython.org.   Wikipedia (2014a), ‘Duck typing — wikipedia, the free encyclopedia’. [Online; accessed 31-October-2014]. URL: http: // en. wikipedia. org/ w/ index. php? title= Duck_ typing&amp;oldid= 631664731 Wikipedia (2014b), ‘Python (programming language) — wikipedia, the free encyclopedia’. [Online; accessed 31-October-2014].   URL: http: // en. wikipedia. org/ w/ index. php? title= Python_ ( programming_ language) &amp;oldid= 631855502     6    
	              
	              Week 9 DQ1 - Structured and object-oriented programming
&nbsp;




Object Oriented Programming (OOP)
&nbsp;


Structure Programming




OOP is a formal programming approach where logical structures known as objects are built from data and associated actions/methods. These objects are reusable because of the underlying concepts of OOP, which comprise objects, classes, inheritance, interfaces and packages.
&nbsp;
Object:
An Object in OOP is a self-sustainable “thing” or logical structure that can perform a set of related activities which define that object’s behavior, and so an object has a state and a behavior e.g. a dog has states that are its name, its color, its name, and breed as well as behaviors such as barking, wagging its tail, and eating.
&nbsp;
&nbsp;
Class:
A class is a blueprint from which objects are created which contains an object’s details thus an object is an instance of a class. A class is comprised of a name, an attribute and operations. Using the dog object example, in the real world there are many dogs, possibly hundreds of thousands that are of the same breed and colour and each dog has the same attributes of that type of dog, in OOP this can be translated to that, that one dog is an instance of the class of objects of dogs.
&nbsp;
A class is also comparable to a chocolate cake recipe, where the recipe itself is not a cake and cannot be eaten but if the recipe instructions are followed (instantiated), the end result will be a chocolate cake which can be eaten(the object-instance of class). The same chocolate cake recipe can be used to make multiple cakes but what may distinguish each cake from the other could be the name on the cake.
&nbsp;
Inheritance:
Using the dog example where different kinds of dogs usually have some commonality with each other regardless of the breed, all dogs usually share some characteristics such as number of legs, number of eyes, they all have tails, however, each breed has some additional distinguishing characteristics associated with that breed e.g. body size, tail length, and head shape. Thus dog is a super class of specific breed, were specific breed inherits the common states and behavior of dog. &nbsp;In OOP, inheritance is where a new class is created from an existing class, and the new class ultimately inherits the super class’s attributes and methods. However, the new class (subclass) can also have its own new attributes and methods; basically this child extends the parent’s class state and behaviors.
&nbsp;
Interface:
These can be an object’s exposed methods through which the object interacts with the outside world, for instance the power button on a TV which allows a user to send commands to the wiring to switch on the TV.
&nbsp;
Package:
A package is a folder or namespace that basically sees to the logical organization of related classes and interfaces. This facilitates easier management of large software projects.
&nbsp;
&nbsp;
&nbsp;


Structured programming is also known as modular programming &nbsp;is a top down programming approach which uses a hierarchy of modules where each module has&nbsp; a single point of entry and a single exit point and &nbsp;control is passed down through this top down structure. For example how to eat an elephant. We need to plan how we will go about it: we will break it up and go at it one bite at a time.
&nbsp;
Instead of this:

&nbsp;
Structured programming takes this approach to solving problems:
&nbsp;

&nbsp;
The key concepts of structured programming are top-down analysis approach when solving problems, modularization for program structure and organization and structured code for the individual modules.
&nbsp;
Top-down Analysis:
This is where the overall program structure is mapped out into separate sub sections; where every necessary step required to solve the problem is stated from the beginning. With this approach, a large problem is subdivided into several smaller tasks.
&nbsp;
Modular programming:
This is a method whereby large programs are broken down into small, separate components known as modules, where each module has a specific function to perform.
&nbsp;
Structured coding: 
This describes the way in which instructions within a module are grouped; a control structure. This represents a unique pattern of execution for the instructions. In a control structure, the component statements are executed in one of three ways: sequentially, conditionally or repetitively. 
&nbsp;




&nbsp;
&nbsp;
&nbsp;

Analyse OOP and structured programming languages.

Structured programming is based on to top down method where:

A problem is broken down into small tasks and the subsequent tasks are viewed as new problems that are further broken down into smaller tasks and the process is repeated with the resulting subsequent tasks until a problem/task cannot be broken down further. 
Structured programming works on producing the problem.
Structured programming uses modularization where a module is a component of a system that performs a specific task and interacts with the rest of the system in a simple and well-defined manner. The notion is that a module can be plugged into a system. 
In a control structure, the component statements are executed in one of three ways: sequentially, conditionally or repetitively. 





Sequence&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
b = 2 e = 4&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  l = b + e WriteLine(b)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
&nbsp;


Repetition
While condition &nbsp;&nbsp;&nbsp; action End While
Example:
b = 2 While b &lt; 10 &nbsp;&nbsp;&nbsp; WriteLine(b) &nbsp;&nbsp;&nbsp; b = b * b End&nbsp;&nbsp;
&nbsp;


Selection 
If condition Then &nbsp;&nbsp;&nbsp; action End If
Example:
b = ReadLine() If b Mod 3 = 0 &nbsp;&nbsp;&nbsp; WriteLine("this is an odd number") End If
&nbsp;




&nbsp;
 While the OOP approach:

Begins by identifying a problem’s objects &nbsp;and identifying the messages that the objects will respond to
these objects can be reusable in other programs
Each object has the necessary data and details of what to do with the data it contains

&nbsp;




Employee class is created:
&nbsp;
public class Employee{
&nbsp;&nbsp; String name;
&nbsp;&nbsp; int age;
&nbsp;&nbsp; 
&nbsp;&nbsp; // This is the constructor of the class Employee
&nbsp;&nbsp; public Employee(String name){
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; this.name = name;
&nbsp;&nbsp; }
&nbsp;&nbsp; // Assign the age of the Employee&nbsp; to the variable age.
&nbsp;&nbsp; public void employeeAge(int employeeAge){
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; age =&nbsp; employeeAge;
&nbsp;&nbsp; }
}
&nbsp;


employeeOne is an object instantiated from Employee:
&nbsp;
public class EmployeeTest{
&nbsp;
&nbsp;&nbsp; public static void main(String args[]){
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // Create employee object using constructor 
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Employee employeeOne = new Employee("Belz");
&nbsp;
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // Invoking methods for object created
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; employeeOne.empAge(26);
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }
}
&nbsp;




&nbsp;
&nbsp;
&nbsp;




OOP


Structured programming




Advantages

OOP improves software development productivity because objects can be reused within applications as well as across applications, objects can also be extended and because objects are self-sufficient entities they each have their own specific duties and functions.
OOP improves maintainability of software- because of the self-contained objects, maintaining is easier as parts of the system can be easily modified without affecting other parts
OOP facilitates faster &nbsp;and reduced cost development because of its reusability capability as well as because of the rich libraries that usually come with OOP languages 
OOP ensures higher quality software because less time and money is spent developing software, effort can then be directed to improving the quality of and verification of the software.

Disadvantages

There is a steep learning curve because of the thought process required for OOP which may be a challenge for individuals not accustomed to it
It results in larger programs which usually contain more lines of code compared to structured programs
It may result in slower programs because of the above i.e. there are more instructions to execute
OOP is not suitable for all types of problems

&nbsp;
&nbsp;
&nbsp;


Advantages

Structured programming is less complex as it is easy to write – a programmer is forced to look at the bigger picture from the onset and refine to smaller details later
Allows multiple programmers to work on the big problem at the same time by working on the different modules which are then plugged in
Less time is involved in writing the programs as they are less complex
Structured programming also facilitates usability as&nbsp; modules can be used across applications if they are to perform the same task
They are easy to debug as each module has a specialized task and easy to modify

&nbsp;
Disadvantages

Structured programming lacks encapsulation which means that the same code or similar code is duplicated in multiple locations, this also means the probability of program errors will increase and testing will take longer
there is lack of information hiding- information hiding is the process whereby a program’s design decisions that could change are isolated. This provides stability and control when those elements do change

&nbsp;
&nbsp;




&nbsp;
&nbsp;

Explain the perceived preference of OOP languages over structural programming languages in the computing industry today.

&nbsp;
OOP is most likely preferred because of its key principles and what they provide for instance the use of classes where elements belonging to the same family are grouped, and objects are created from these classes. These objects are created with the ability to know how to interact with other objects and respond to messages. OOP also promotes information hiding, high cohesion and low coupling which allow for easier component debugging and modification.
&nbsp;

Evaluate whether OOP languages better reflect the human way of thinking and conceptualising. Explain your reasoning.

&nbsp;
I think OOP reflects human way of thinking as related items are grouped together as belonging to the same family in the form of classes and that is how humans normally associate things. Also, instead of tasks, in OOP it is all about finding “things” (objects) that interact with one another and each of those objects have states and behaviors and hold information. The concept of inheritance in OOP reflects humans thinking, in fact it models humans’ essence of life-where children inherit their parents’ DNA, traits and yet still come into their own with their own additional traits and attributes. 
&nbsp;
References
&nbsp;
[Online]http://www.adobe.com/devnet/actionscript/learning/oop-concepts/objects-and-classes.html accessed 30 October 2014
[Online]http://whatis.techtarget.com/definition/class accessed 31 October 2014
[Online]http://java.about.com/od/objectorientedprogramming/a/inheritance.htm accessed 1 November 2014
[Online]http://targetstudy.com/nature/animals/dog.html accessed 1 November 2014
[Online]http://www.merckmanuals.com/pethealth/dog_basics/description_and_physical_characteristics_of_dogs/description_and_physical_characteristics_of_dogs.html accessed 1 November 2014
[Online, Richard P. Gabriel]http://www.dreamsongs.com/Files/Incommensurability.pdf accessed 1 November 2014
&nbsp;
&nbsp;

	              
	              Hi Belinda,
I think you did a great job describing both programming paradigms. I enjoyed reading it. I especially like the expression of the 3rd elephant. I guess it said:"Ouch, that one hurts!"
I like the Java-ish example code (you didn't say it's Java). It will be better if the test really "test" something.
I like the "disadvantages" of OOP you listed. During my HIA study, I came across this paper to back up my idea of OO has a steep learning curve,&nbsp;Meyerovich &amp; Rabkin (2013). It can be accessed via the university library.
I cannot agree that structured programming doesn't have encapsulation. Encapsulation isn't unique to OOP. It's just modularization in the end.&nbsp;
The last "human essense" analogy could be dangorous. A parent-children relationship is a too strong relationship (you cannot change it, ever). That's exactly what we want to avoid most of the time when designing software. We want loosely coupled things. Why would a children need to do what the parent do? Why don't just use the parent to do the parent job and use children to do the children job? If the parent can breath the children doesn't have to, right? I know I sound very "inhuman". That's exactly my point -- the analogy of parent/children analogy is very often wrong.
br, Terry

References:
 



Meyerovich, L. A. &amp; Rabkin, A. S. (2013), Empirical analysis of programming language adoption, in ‘Pro- ceedings of the 2013 ACM SIGPLAN international conference on Object oriented programming systems languages &amp; applications’, ACM, pp. 1–18. 



 
	              
	              Comparing Structured to Object-Oriented Programming(OOP). &nbsp; Structured programming is an application or piece of software that has been designed in an orderly structured whole logical block which is processed in the order that it is programmed. They generally have a known start and end and follow a clear path using ‘loops’ and ‘goto’ statements, Wikipedia, A (2014). Object-oriented programming however is a programming paradigm that is again an application or piece of software much the same as structured programming but is specifically designed into individual objects and methods/data within. These are all separate to each other but can work together and communicate between each other in an abstract way, Wikipedia, B (2014). OOP Concepts, Oracle (2010).  Objects Classes Inheritance Polymorphism Interface Package/Encapsulation  To help describe what this means I’m going to explain it in terms of a human family. We all have a family, some large some small in size, we all have parents and some of us have brothers and sisters and even children. A human would be a Class, which is basically a blueprint or prototype from which an Object can be made. There are many humans in existence and we all share this same class and the components within that class such as sex and physical attributes, and in object oriented terms a person would be an instance of the class. In Java this class can be represented like this: Class Human { Int head = 1; Int arms = 2; Int legs = 2; Int heart = 1; &nbsp; void createHead(int newValue) { &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; head = newValue; &nbsp;&nbsp;&nbsp; } &nbsp;&nbsp;&nbsp; void createArms(int newValue) { &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; arms = newValue; &nbsp;&nbsp;&nbsp; } &nbsp;&nbsp;&nbsp; void createLegs(int newValue) { &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; legs = newValue;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; } &nbsp;&nbsp;&nbsp; void createHeart(int newValue) { &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; heart = newValue; &nbsp; &nbsp;&nbsp;} } An Object would be an individual person. Now Inheritance is the objects relationship to the class and the specific elements it has inherited from that class to be created, much like a child sharing the same DNA as the parents. Therefore an object would simply be represented as: Class person extends Human{ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // and the details of that person would go in here for example name, sex, etc } Now there can be many different types of person and this is called Polymorphism, because they are still both a person object and have the inherited class of Human and share it’s attributes but could still be uniquely different. People are a great example of this, we are all human beings and share the same genetic attributes and codes but still unique in our own way and as such we are our own object. This can be seen like this: Class createPerson { public static void main(String[] args) { &nbsp; Person person1 = new Human();&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //create person number 1 Person person2 = new Human();&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //create person number 2 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; person1.arms(2); person1.legs(2); person1.sex(male); person1.name(John Smith); &nbsp; person2.arms(2); person2.legs(2); person2.sex(female); person2.name(Jane Doe); &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; } } From this you can see that 2 people are created, each of them is an Object which is inherited from the Class of Human but each is different due to Polymorphism. Both are human and have the same number of legs and arms etc but one is male and the other female. Objects can also inherit from other objects to create a Package of objects and such an example may be an object of person 1 and object of person 2 having a child object that shares the same human classes but also shares DNA of the two parent objects.    Now this Package or Encapsulation example shows a child object of John Smith and Jane Doe so the child will share similar traits such as hair colour, eye shape, skin colour even how they walk and speak. In OOP this can be the same where a new object may want to share the attributes and traits of other objects so instead of creating a whole new object you would simply create an object that has a parent object so the child may take the hair colour of Parent A and the eye colour of Parent B. &nbsp;So because the child has also inherited these attributes, for argument sake if Parent A’s hair colour changed then the child would change to match because it is an inherited colour of hair. Also we know as humans we have the inherited class from the human class of DNA, however the child object doesn’t need to know this as its DNA is inherited from the parent objects. The Interface is the connection between the object class and the outside world so when a class or object uses and implements an interface it is acting like human behaviours such as speaking and communication. An interface in the OOP world may simply be a monitor and visual display or sound. Interface HumanBeing{ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // speak a language &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Void voiceCommunication(string newValue); &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // movement &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Void walkForward(int newValue); &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Void runFast(int newValue); } In structured programming all of the above would all be repeated into individual whole logical blocks of programs. There would be no inheritance in this way but more of a structured defined human from start to finish. Advantages of Object-Oriented Programming (OOP)  A clear modular program structure good for defining abstract datatypes. Far easier to maintain Easier to make changes Re-usability, provides a good framework for sharing code libraries Simplicity, complexity reduced when modelling real world objects.  Disadvantages of Object-Oriented Programming (OOP)  OOP tends to be slightly slower over structured but it depends on both programs you compare. Effort to create requires far more thinking of how to build them OOP programs tend to be larger in size  Advantages of Structured Programming  Quicker to both program and in operation Modularity to tackle logical problems in order Smaller programs  Disadvantages of Structured Programming  Harder to change after Difficult to share common code to other programs Code repetition to create similar programs.  &nbsp; Despite the above advantages and disadvantages, in my experience with programming you can pick a direction depending on what you actually need to accomplish. There isn’t a right or wrong between OOP and Structured but more what is the best approach to do the job and what technique would work in this situation. Example if you’re project is to build a library of different sharable mathematical classes then OOP approach may be best simply because you can create individual classes for algebra, geometry, arithmetic, probability etc and then if you need a specific function you can pick the right class that it is in a and incorporate it into your new program and you won’t have to reinvent the wheel. Alternatively if you are designing a simple and onetime program to specifically do a unique task then there may not be a current sharable library or class available and you won’t need to share it to others so a structured quicker approach would be best. In industry however I’ve found that on most projects a hybrid style is used most often where you will use or have a set of low level base OOP libraries and classes but then build on them in a structured way. &nbsp; Conclusion I believe OOP has improved the way we think about the efficiency of development as it helps structure how you approach a problem by thinking if you already have part of the solution elsewhere or long term thinking that we may not have the solution now but what if we need some of this functionality in the future in which case should we save effort in the long term by creating OOP classes for reuse later. Then there is the portability factor and ease of modification of OOP, so it makes sense to take the principles of OOP to any project. In the end of the day it also comes down to what tools you have to do the work and what the project needs. If you have OOP languages, then use OOP to the best of its ability but if you use structured languages then develop in a structured way. My personal preference is to develop an agile hybrid approach and embrace what OOP aspects you can and focus on the parts that are better developed in a structured approach. &nbsp; &nbsp; References: Brookshear, G. (2011) ‘Computer science: An overview’. 11th ed. Boston: Addison Wesley/Pearson. Neely, A (2010) ‘Structured vs. Object-Oriented Programming: A Comparison’ [http://www.brighthub.com/internet/web-development/articles/82024.aspx] (accessed: 29/10/2014) Oracle (2010) Java™ tutorials: Lesson: Object-oriented programming concepts [Online]. Available from: http://download.oracle.com/javase/tutorial/java/concepts, (Accessed: 29/10/2014). The Scratch project at MIT (n.d.) Scratch: Imagine, program, share [Online]. Available from: http://scratch.mit.edu (Accessed: 29/10/2014). Wikipedia,A (2014), ‘Structured Programming’ [http://en.wikipedia.org/wiki/Structured_programming] (Accessed: 29/10/2014). Wikipedia,B (2014), ‘Object-Oriented Programming’ [http://en.wikipedia.org/wiki/Object-oriented_programming] (Accessed: 30/10/2014). &nbsp; 
	              
	              Hi Chris,
A few things that are not correct.
1. Your example about inheritance is a bit weird. Is there any human that is not a person?
2. Your example for polymorphism is wrong. You cannot have polymorphism with just one class. What you called polymorphism is just the state of the object.
3. I believe the package in OOP is something else.
4. The parent(s) analogy is ... well, wrong in many ways. But you might want to get the class/object thing right first, then I guess this would become clear:-)
br, Terry
	              
	              If you want to read the better format, I'm attaching the pdf here.
	                Attachment: &nbsp;A Duck Tour Of Object-Oriented Programming.pdf (331.719 KB)
	              
	              Terry,

Are you ok,?
You seem to be very negative with people recently. I noticed at the end of last weeks work you were having a go with other students as well.
1) my example with inheritance is perfectly fine as far as I'm concerned. your right there is no person who isn't a human but this proves my point because there isnt inheritance without a class to inherit from. It may sound weird to use this as an example but no more weird that using a duck! It may be the terminology that I'm using but i'm happy with how I have described it.
2) My example for polymorphism is my interpretation of it. I've never read anywhere that says you need more than one class for polymorphism ? Even you're example of polymorphism only uses one class of Duck so if i'm wrong then your wrong also. So I don't know what you mean by "You cannot have polymorphism with just one class". Even you describe polymorphism as overriding the inherited method in a subclass by saying domestic ducks are noisier than other ducks. Which in my mind is no different to inheriting the fact that all humans have a sex some are male some are female being the override because not all humans are male are they.?
3. Your right a package is something else but I placed a package with encapsulation for this example instead of splitting them up because the description of a package is placing all of the classes and objects that are linked in some way into the same package. Hence because a child is linked to its parents which use the class of human this could be seen as 1 package or at least that's how I relate to it.
4. I'm not sure what you mean by the parent analogy being wrong in many ways which I feel is a little bit rude to be honest.

Terry I learned a good saying a long time : "if you haven't got something nice to say then don't say anything at all" . It has stuck with me in my time.

So thanks for your feedback but I'm quite happy with my work
Ta
Chris

	              
	              Week 9 DQ1 – Structured and Object-Oriented Programming

Craig Thomas – 1 November 2014

In Computer Science, a number of methods have evolved in the computer programming process, and are commonly referred to as ‘Programming Paradigms’. Each of these has fundamental differences and affects the software development lifecycle entirely. As such, a more appropriate description would be “Software Development Paradigms” (Brookshear, 2012, pg. 244). 

This paper will look to outline the key concepts and summarise the advantages and disadvantages associated with the Structured programming and Object-Oriented programming paradigms.

Structured Programming

Structured programming language is a high level imperative programming language, and includes Pascal, Algol and Ada although, many of the newer Procedural programming languages, which are derivatives of the Structured programming languages, such as C and BASIC, also include the base principles and features of Structured programming languages.

Structured Programming languages are based on the use of routines and sub-routines. This extends to block routines, as well as ‘for’ and ‘while’ loop statements.

Corrado Bohm and Giuseppe Jacopini, who developed the Bohm-Jacopini theorem in 1966, and as described by Kozen and Tseng (n.d.), assert that any deterministic flowchart program is equivalent to a while program. 

They further demonstrate that there are three key controls, or building blocks, within the program structures, including: i) Sequence, which coincidentally executes sub programs in sequence, ii) Selection, which executes sub programs depending on the conditional state or value and includes ‘Boolean’ instructions such as ‘If’, ‘Then’, ‘Else’, ‘Do’ statements, and iii) Iteration, which executes until the program reaches a certain point or value, and includes statements such as ‘While’, ‘For’, Repeat’, ‘Do’ and ‘Until’ (Brookshear, 2012, pg. 254).

As a result, there is no need for the program to include ‘go to’ statements, which could prove to be difficult and complex to interpret, and to follow and maintain within the program structure. A key advantage of this reduced complexity enables the program to be developed in a way that is logical and modular. This would also enable the workload to be divided amongst programmers, who could develop certain aspects of an overall software design, or resolve certain conditions within sub routines, in isolation of other sub routines.

The Structured programming paradigm includes strong elements of the ‘Imperative’ (also ‘Procedural’) programming paradigm, in that it essentially represents a traditional programming style of a sequence of commands, to provide a desired result. 

The program paradigm is required to, similarly to the ‘Functional’ programming paradigm, retrieve, input or output data, and handles this in the immediate sense depending on the execution sequence and status.

In summary, this programming paradigm is analogous to driving a car from A to B (executing in sequence), turning left or right (If, Then, Else) depending on the location (state or value) and keeping the car moving forward, whilst (While) the traffic allows, and stopping for example if you arrive at traffic lights, or your destination (Do Until). 

Object-Oriented Programming

Object-Oriented programming is also a high level imperative programming language and includes C++, C# and Java, and are all derivatives of C. Object-Oriented programming adopts a similar characteristics of an imperative language – in essence, it is a collection of short imperative programs. 

Object-Oriented programming paradigm is essentially viewed as a collection of units, referred to as Objects. These objects are able to execute actions that are immediately related to themselves, as well as requesting actions from other objects through interfaces, and together they interact to solve problems. These objects contain Methods, which are the procedures that describe how the object is to respond to events. The object also contains certain data attributes, relative to the object. Once the objects are defined, with the data attributes and methods, together these properties are called a Class, which can be applied anytime an object with those characteristics is needed, as well as being used to build subsequent objects from the same class (Brookshear, 2012, pg. 247). In contrast to the ‘Imperative’ programming paradigm whereby the algorithm or program performs the task, the program would ask the object to do it itself (Brookshear, 2012, pg. 247).

Object-Oriented programming languages have high levels of abstraction, and are able to exhibit features that are essential, without knowing or including other features, details or explanations. An example of this would be to describe someone as having brown hair, but not providing any additional information such as height or gender. 

Functions and data in a single class are referred to as Encapsulation, and data can only be accessed by other functions that are in the same class. 

However, there is a process by which objects can obtain the attributes of other classes, which is called Inheritance. This provides major advantages of portability/usability, whereby you are able to add features without modifying the class. This is possible through the creation of a new class, and building on the same analogy, this could be like adding a pair of glasses without changing anything else. The new class would have the attributes of both classes. 

Another way to describe this would be Polymorphism, in that the objects may require to be subtly different, perhaps the glasses have short-sighted lenses, and a new object now requires glasses with long-sighted lenses, depending on what it is being asked to do – for example read a book, alternatively read the registration number on a car 45ft away. As such, polymorphism is a construct of inheritance. 

There are strong organisational attributes to Object-Oriented programming. The code is easier to read, translate and maintain, as well as having strong re-usability characteristics. Fig. 1 shows a simple comparison between both paradigms, relative to a software development lifecycle as adapted from an article by Sumit Sircar (2001, pg. 461).





Fig. 1
&nbsp;


Structured


Object-Oriented




Lifecycle


Sequential with iterations permitted


Iterative and incremental




Single Unifying concept across lifecycle


None


Object




Analysis


Data models, process models


Object models (class diagrams) collaboration diagram




Design


Structure chart, detailed program specifications, database schema


Design class specification, sequence diagrams




Implementation


Program implementation, database implementation, integration and testing


Class implementation, database implementation for persistent objects, integration and testing





Evolution

Nirpjeet Kaur et al (2011, pg. 112) provide a useful description (Fig. 2) of the historical aspects and evolution in the development of programming paradigms and shows the successions of the paradigm developments, with key attributes that are relative to the previous historic paradigm. In addition to the evolution from Structured to Object-Oriented programming, it also highlights the suggestion of another paradigm evolution as a natural successor to Object-Oriented programming. 





Fig. 2
&nbsp;


Machine Language


Structured Programming


Object-Oriented Programming


Agent-Oriented Programming




Structural Unit


Program


Sub-routine


Object


Agent




Relation to previous level


&nbsp;


Bound unit of program


Sub-routine and persistent local state


Object and independent thread of control and initiative





Conclusion

Structured programming languages break the tasks into a collection of variables, data and sub-routines. Object-Oriented programming breaks the tasks into objects that exhibit behaviour (methods) and data attributes using interfaces.&nbsp;
Ultimately, both programming paradigms yield their complexities in different ways. Procedural code is flat with functions, and Object-Oriented code is hierarchical with inheritance (Boronczyk, 2009).

There are a number of common themes between both paradigms, however my reading suggests this is merely evolutionary and not necessarily transformative although the Object-Oriented architecture appears to provide additional functional and flexible capabilities at the Analysis and Design stages of development. As such, this may serve to present certain preferences to developers as their programming paradigm of choice, given its readability and organized design methodologies (Brookshear, 2012, pg. 256) to meet its requirements specification.&nbsp;On the flip side, I do wonder however whether there is significant duplication in relation to Object-Oriented programming, in that there could be many different objects and methods, which are essentially duplicated?

Best Wishes, Craig

References

Laureate Online Education. (2014). Computer Structures – Lecture Notes Week 9: Programming Languages [Online]. Available from: https://elearning.uol.ohecampus.com/bbcswebdav/institution/UKL1/201480_SEPTEMBER/MS_CKIT/CKIT_501/readings/UKL1_CKIT_501_Week09_LectureNotes.pdf (Accessed 30 October 2014)

Brookshear, J. G. (2012). Computer Science An Overview. 11th ed. Boston Massachusetts: Pearson. Addison Wesley

Wikipedia. (2014). Structured Program Theorem [Online]. Available from: http://en.wikipedia.org/wiki/Structured_program_theorem (Accessed 30 October 2014)

Bohm, C. &amp; Jacopini, G. (1966). Computational Linguistics – Flow Diagrams, Turing Machines and Languages with only two Formation roles [Online]. Available from: http://eds.b.ebscohost.com.ezproxy.liv.ac.uk/eds/pdfviewer/pdfviewer?sid=b48c556b-f066-4311-b3e1-51c627e9db20%40sessionmgr114&amp;vid=10&amp;hid=113 (Accessed 1 November 2014)

Kozen, D., and Tseng, W. L. D. (n.d.). The Bohm–Jacopini Theorem is False, Propositionally [Online]. Available from: http://www.cs.cornell.edu/~kozen/papers/bohmjacopini.pdf (Accessed 1 November 2014)

Kaur, N. et al (2011). Agent Oriented Software Engineering and Other Programming Paradigms. pp. 112. [Online]. Available from: http://eds.a.ebscohost.com.ezproxy.liv.ac.uk/eds/pdfviewer/pdfviewer?vid=3&amp;sid=1a8cb221-c21e-47bb-9a43-96fbaada2d87%40sessionmgr4003&amp;hid=4113 (Accessed 1 November 2014)

Sircar, S. et al (2001). Object-Oriented &amp; Structured systems development MIS Quarterly. Vol. 25. No. 4. pp. 461. [Online]. Available from: http://content.ebscohost.com.ezproxy.liv.ac.uk/ContentServer.asp?T=P&amp;P=AN&amp;K=6406420&amp;S=R&amp;D=bth&amp;EbscoContent=dGJyMMTo50Seqa84zdnyOLCmr0yep7VSs6m4Sa6WxWXS&amp;ContentCustomer=dGJyMOzprkmvqLJPuePfgeyx43zx (Accessed 1 November 2014)

Boronczyk, T. (2009). What’s wrong with OOP [Online]. Available from: http://zaemis.blogspot.co.uk/2009/06/whats-wrong-with-oop.html (Accessed 1 November 2014)
	              
	              Hi All
Here's the word doc too.
Week 9 Discussion Question 1 - Craig Thomas - Structured and Object Oriented Programming 1 Nov 2014.docx&nbsp;
Best Wishes, Craig
	              
	              Hi Terry
The first thing that immediately springs to mind is: …what, no references to Jeff Dean? J&nbsp;
I have just reviewed your paper and enjoyed the simplicity of the way you have written and described SP and OOP – providing a really useful explanation and analogies. This has helped me crystalise my overall understanding, thank you.
I can’t really comment too much on your content, as this subject, as you know, is very new to me, however I was pleased to see that I had arrived at similar conclusions, particularly around the discipline of software design.
Did your research (or your knowledge already) come across Agent-Oriented programming language? I haven’t researched this specifically, just wondered whether you know much about it?
I like duck too, very crispy with peking sauce please…
Best Wishes, Craig

	              
	              Hi Belinda&nbsp;
This was a brilliant read, and a great way of explaining both paradigms, thank you for this.
You referenced the steep learning curve for Object-Oriented programming – I certainly recall seeing some reference to this in my research too. In fact, I recall reading somewhere that there is a general surprise that OOP is difficult to learn, given that it is a step closer to mimicking how Humans interact with a higher level of abstraction (you also allude to this in your closing statements).
I also like your terminology of ‘blueprint’ for the class of which objects can be created.
I was however disappointed with your starting point for eating an elephant. I feel there could be enormous improvements in taste, by changing the object method so that it doesn’t start at the back end! J
Best Wishes, Craig
	              
	              Hi Chris,&nbsp;
I enjoyed reading your paper, and I am pleased that I have similar findings.
Do you have a view of why OOP tends to be slower over SP? I have arrived at a conclusion that there are perhaps many duplications and I was wondering whether your perspective of speed, or inefficiency perhaps, could be a result of OOP needing to handle duplicated objects and methods, or is it down to size, as you also refer to OOP programs being larger than SP. I’m not suggesting this is an architectural flaw, just trying to understand why perhaps OOP is slower?
Also, I appreciate your closing comments regarding the uses of both paradigms – the way I have interpreted it, is that you are suggesting that a complex hierarchy would be better placed with OOP, and a more simple or non-complex need may be better suited to SP. However, your personal preference would be an agile hybrid model, taking preferred aspects from both paradigms.
Good read, thanks Chris
Best Wishes, Craig

	              
	              &nbsp;
Structured and object-oriented programming
&nbsp;
Structured programming and objected-oriented programming (OOP) are two programming paradigms. Structured programming aims to divide a solution into smaller chunks or tasks (Bookshear 2011a, Dijkstra 1983). A real world example could be an organization for a dinner for 300 people. The idea would be to break the tasks to complete the dinner in several smaller tasks and execute them in a structured way to complete the dinner. The smaller tasks could be something like hire personnel, define a menu, buy groceries, cook starters, etc. (and even those can be broken in smaller tasks). The main features of this approach are functions and procedures (the smaller tasks). Thus, we will have a function (that’s it: a set of instructions) to call the chef, one to go buy groceries and one to clean the kitchen.
The OOP insists on handling and manipulating Objects and their properties to create programs. Each Object is a collection of data, attributes and procedures. For a dinner of 300 hundred people, the idea would be that each step is represented by actors or objects. These objects could be something like Chef, Menu, Kitchen, Cook, Cleaner, etc. Each of these objects will have its characteristics. For example, the Cook object could have a procedure to prepare the entrée (i.e.: Cook.PrepareEntree).
Let me take the example of a cat (let us assume that a cat is an OOP object) to describe some main features of OOP’s (Pokkunuri 1989, Bookshear 2011b):

Encapsulation: Cats have a strongest smell than humans, as humans we cannot access in any way the smell capabilities of a cat by owning or using a cat. We could say that only a cat can access its smell capabilities. That’s it: smell is encapsulated within the cat itself. In OOP, objects can restrict access to internal properties so that only the object itself can access them. 
Data Abstraction: when we call a cat name, the cat will come to us. We are not concerned about how the cats interpret our calling. We just focus on the what: calling the cat. In the same way, external entities from an OOP object are not concerned about how stimuli are processed by the object, they only focus on the object itself (the what). 
Inheritance: The cat that we have at home is only one of many types of cats. All cats share a common biological definition based on certain characteristics (for example all cats have four legs). Different types of cats have different characteristics a top of the ones inherited from the generic cat definition. For example, a Bengal cat has short hair with rosette patterns, while a Maine Coon cat has large hair with different combination of colors. In OOP, the generic cat object is called a super-class (Cat), while sub-definitions of the cat object are called sub-classes (Bengal Cat).

The greatest advantage of structured programming is that it allows for an easy and quick, logical organization of the program (Dalbey no date). By using the procedures and functions, I can easily build an algorithm that can be followed logically. However, structured programming has two main disadvantages: lack of encapsulation and code repetition. Encapsulation provides the means to reuse code without the need to type it again several times. Consider, for example, how to make a cat smell: every time we need to make the cat smell we will need to type the relative code or at least call that particular function. But think about a Cat object: it smells when there is an external stimulus that trigger this behavior (function/procedure). Thus encapsulation is an advantage in OOP’s, which have three other main advantages (Pokkunuri 1989):

Re-usability: classes inherit code from their super-classes, avoiding unnecessary repetition of code.
Modularity: objects are autonomous; different objects are independent of other objects. It results that the program is easier to maintain and develop. Changes only need to be done on a particular object(s) or relative functions (inside the object).
Conceptual pleasantness: as we define objects in a program, we relate them to real world objects. We define a Cat object as closely as we define a real world cat.

OOP’s, however have some disadvantages: size and effort. Since objects are defined closely to real world ones, it results that object size can be quite big. Consider an Object called Galaxy, just as a real world Galaxy is huge, the equivalent in OOP can have a big size. The effort to create such an object is also significant as the object internal functions and procedures must be meticulously planned. This effort can be considered daunting, but it does payoff. Once our Galaxy object is defined we can simply create billions of those objects (in a very fast timeframe) and assemble them in a program called Universe.
It is clear then than OOPs languages reflect better the real world and the way we humans imagine things (Pokkunuri 1989). For example, we can easily imagine a bicycle as a generic object with certain characteristics such as wheels, gears, brakes, speed, and pedal cadence (among others). In an OOP, we would be able to translate this by creating a Bicycle object with procedures such as, but not limited to: Bicycle.GetGear, Bicycle.GetSpeed, Bicycle.ChangeGear, Bicycle.Brake and Bicycle.ChangePedalCadence.
 References
Bookshear, J. G. (2011a) '6&nbsp;Programming Languages' in Computer Science: An Overview, 11th edn, Addison-Wesley, pp. 239-244.
Bookshear, J. G. (2011b) '6.5 Object-Oriented Programming' in Computer Science: An Overview, 11th edn, Addison-Wesley, pp. 276-277.
Dalbey, J. (no date) Structured Programming. Available at: http://users.csc.calpoly.edu/~jdalbey/308/Resources/StructuredProgramming.pdf (Accessed: 1 November 2014).
Dijkstra, E.W. (1983) 'Go To Statement Considered Harmful', Communications of the ACM 23(1), pp. 73-74.
Pokkunuri, B.P. (1989) 'Object oriented programming', ACM Sigplan Notices 21(11), pp. 1 November 2014-97-100.
&nbsp;

	              
	              Hi Terry,
&nbsp;
I would like to thank you for explaining object oriented programming concepts in such simple way with crispy example. My findings are the same that you can do almost anything that you can do in OO programming in structured programming. I agree that OOP approach is difficult to master, I still remember during my graduation, OOP was considered to be very difficult course and we spent a lot of time discussing the trying to understand its concepts that you have explain so simply.
&nbsp;
Regards,,,Numan &nbsp;

	              
	              Hi Belinda,
Couldn't agree more. Very nice reading. I liked your Dog example, I went with a Cat, but almost did the dog too, I am glad I did not because your examples are great.
Also, now that we have an elephant in the fridge, we could have a way to eat it via structured programming. It is funny but this is a good example that, in my opinion, illustrates using structured programming for certain things is better: how to eat an elefant? I am sure there is a an&nbsp;object-oriented programming way on solving this problem but at first glance it might be over complicated for the task at hand.
Best Regards,
Augusto.
	              
	              Hi Chris,
Wow. I'm really sorry. It was a gift from me:-)
br, Terry
	              
	              1 Introduction   Programming paradigms and languages continue to evolve. What was good enough yesterday, can be further improved today! And why do we keep improving? Well, there are too many reasons to discuss here, but I believe it’s because, as always, we want to do more, cheaper, faster and better.   Abbreviations used below:    SP&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Structured Programming OOP&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Object Oriented Programming 1GL&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1st generation languages: Machine code 2GL&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2nd generation languages: Assembly code 3GL&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3rd generation languages: SP and OOP like C, and C++, Java and C# 4GL&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4th generation languages: OOP like C++, C#, Java, SQL 5GL&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 5th generation languages: Not clearly defined. E.g. declarative languages like HTML and SQL    As far as I have been able to research there is no clear list of which programming languages belong to which generation, and especially I have found several articles opposing each other regarding 3GL and 4GL, hence I have defined the object oriented languages in both 3GL and 4GL.   Examples of code (Asmasum, 2014):       Executable machine code          Low level program code          High level program code            2 Abstract   In this assignment I will do my best to explain structured programming (SP) and object oriented programming (OOP) and in way that non-technical people will understand. I will do so by using parallels to objects we know in our everyday life. &nbsp;   3 SP and OOP descriptions   3.1 SP   SP was created to address the complexity of former programming languages. SP is sometimes referred to as modular programming, and by some it’s referred to as the way out of the complex GOTO programming (Cal Poly). &nbsp;Here you structure the entire code in smaller modules (e.g. base cases) and it is much more readable (Brookshear, p256) and understandable (Yuen, 2005) than former languages.   3.2 OOP   OOP is a step further from SP. It is still structured, but at a higher abstraction level.   Objects are everywhere in our daily life, e.g. television, computer, car, dog. And these objects have both a state and behaviour (Oracle, 2010) and these can be of different complexity. Here are two examples:       Object   States / Variables   Behaviour / Methods / Functions     Television   On and Off   Show films and pictures. Different channels. Volume can be adjusted.     Desk   On and Off   Elevation up or down. Static.       &nbsp;   With OOP the programmer can abstract from most details, and just focus on the skills needed to understand the classes and objects, how these work. This makes it much   3.2.1 Encapsulations   “Users see only the services available from an object, but not how those services are implemented” (Virginia Tech).   Imagine an online shopping site:    Buyers view: The buyer only sees the interface, i.e. an item and a buy button. The buyer can abstract from everything that happens in the code, e.g. how the button is supposed to react when clicked on, how it finds the correct item to put in the shopping basket, etc. Programmers view: The (GUI) programmer only as to apply a “buy” object to the buy button, and inform the object about which item to fetch, and where to put it. The programmer can abstract from having to program code which checks the stock, the price, etc.    3.2.2 Polymorphism   Polymorphism is the “ability to take more than one form”. (AASD, 2010, p. 35).   So basically this is all about something changing form. This could be illustrated whit a ball as the object. A ball can take many forms and sizes. So through the interface the methods can make the ball change into e.g. a handball, football, basketball etc.   Another illustration is a cursor which we know from computers. A cursor will change form depending on where it is. If were using a text processing program, then the cursor is normally a vertical line, but when moved over a menu or a function then it will change to an arrow. In all these cases there is one object taken different shapes.   3.2.3 Data abstraction   “Abstraction means ignoring irrelevant features, properties, or functions and emphasizing the relevant ones”. (AASD, 2010, p. 27).   If we are to focus on all details every time, then we will “never finish”, it will become too complex. Abstraction lets us focus on the things relevant to us in the given situation. Let’s take the car as an example, and specifically the gear. There are a lot of data regarding a gear, like turn into 1st by moving the lever to the far left and then forward. With abstraction we don’t have to focus on what really happens from the lever and all the way through the mechanics etc.   3.2.4 Inheritance   “Defining new classes to be extensions of previously defined classes.” (Virginia Tech).   The web page of Virginia Tech has an excellent example where the first class defined is a Figure from which other classes can inherit the parents attributes. Further the child classes can have attributes of their own, and these classes can again be instantiated to new child classes.   4 Comparing SP and OOP   A way of comparing SP and OOP is by these six software quality characteristics (Virginia Tech).       Characteristics by Virginia Tech   SP   OOP     Maintainability: The ease with which changes can be made to satisfy new requirements or to correct deficiencies.   Even though SP is considered more readable, it’s still harder to maintain.   Easier to maintain changes     Correctness: The degree with which software adheres to its specified requirements   You can test each module, but it’s easy to change things that will influence the program thus also the correctness.   Largely more correct because of the reuse of code that has been tested before.     Reusability: The ease with which software can be reused in developing other software.   Yes, SP code can be reused, but normally you will not reuse between programs and even not within a program.   OOP is very good at reusability it that of classes and inheritance.     Reliability: The frequency and criticality of software failure, where failure is an unacceptable effect or behavior occurring under permissible operating conditions.   Depending on the design, then SP of course can be just as reliable as OOP.   OOP could be considered more reliable because it’s easier to maintain.     Portability: The ease with which software can be used on computer configurations other than its current one.   Portability is not paradigm dependent, but rather dependent on the specific language used.   Though the very idea of objects to me speaks for portability.     Efficiency: The degree with which software fulfills its purpose without waste of resources.   Since SP is closer to machine logic, and normally more structured, it’s faster to execute.   OOP is more demanding because of inheritance, and typically more lines of code, etc. hence considered slower.        Other ways to compare:       &nbsp;   SP   OOP     Learning curve   Because everything is broken down to small pieces/routines, it’s easier to learn.   Even though OOP has a more human approach, it’s still considered to have a steeper learning curve because it can be hard to truly understand and master e.g. polymorphism.     Readability   The code is code is considered to be more readable because of it’s structured apporach   It’s not at readable e.g. because of the abstraction level.     Reusability   Lower sinc   Higer because off standard classes that can be reuse in many programs     Dev. Time   Longer time   Shorter time since reuse shortens the time.     Structure   Sequence of instructions (Routines, Subroutines) Variables   Classes Objects Methods Interfaces     Cost   Longer development time leads to more cost.   Reuse leads to less cost       &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; * (Saylor)   5 Perceived preference for OOP   It is my understanding, that the industry turns more (Lecture Notes, p7) towards OOP. I believe this is happening because of the ever increasing demand for rapid development and better programs.   Let’s take the gaming industry as an example. To keep attracting gamers the vendors have to keep creating new exciting features to the games, thus the programmers need to create solutions faster. This can be done with several “independent” teams, where one teams programs the classes, and other teams can the abstract from the details and just focus on creating objects and “configure” them (e.g. a gun with the object variables size, speed, colour, shape).   If the programmers were to use SP instead of OOP, then it would be more complex, take longer time, and require more resources (Garg, 2013) because SP is a lower level and more complex programming language. Yes I know, there are different opinions on this. But imagine you have good developers of each language, then I would imagine that OOP should do a “better” job. Although my research has also shown that SP is the correct language for specific tasks. &nbsp;   6 Is OOP more human?   OOP compared to lower level programming languages, to me, reflect much better the way humans think. I’ll illustrate this with a car. Today we don’t normally think that a car has about 30.000 parts (Toyota), we just think of it as an object. Were more or less just interested in if the car has e.g. automatic or manual gear shifts, what colour it is. Hence, overall, the car is just one object. But then again, we do go one step further and think of a few more encapsulated objects, like gear shift, GPS, security, seating’s, how many doors, etc.   7 Conclusion   I will start my conclusion with this quote:&nbsp; “If experienced developers believe so strongly in the advantages of OO systems development, why is there such controversy about the value of using it? (Johnson, 2000)   My research has shown that OOP is winning grounds, and even though I’m not a programmer, I understand the OOP principles thus understand why programmers turn to OOP. I hope this analogy says why: In my daily work I’m faced with a lot of technical issues that I don’t know the details about, but I understand the business needs and I’m able to find enough knowledge to make the (hopefully J ) right decisions. Hence I’m thinking about the objects and what they can do for me, and I’m abstracting from the details.  PS: Sorry for the length of the document&nbsp;    8 References   AASD. (2010). Bulgarian Association Of Software Developers. “Object-Oriented Programming Concepts”. [Online]. Available from: &nbsp;http://www.slideshare.net/bgjeecourse/objectoriented-concepts-5576132?next_slideshow=1. (Accessed: 01-11-2014).   Brookshear, J. Glenn, (2012). Computer Science An Overview. 11th edition. Boston Massachusetts: Pearson. Addison Wesley.   Cal Poly. “Structured programming”. [Online]. Available from: http://users.csc.calpoly.edu/~jdalbey/308/Resources/StructuredProgramming.pdf. (Accessed: 01-11-2014)   Garg, V. (2013). “Introduction to programming languages”. [Online]. Available from: http://www.slideshare.net/VarunGarg7/lect-1-introduction-to-programming-languages. (Accessed: 01-11-2014).   Johnson, Richard A. (2000). “The Ups and Downs of object-oriented systems development”. [Online]. Available from: http://eds.b.ebscohost.com.ezproxy.liv.ac.uk/eds/pdfviewer/pdfviewer?vid=4&amp;sid=0b5840c5-f808-4604-a2c3-44241ef35608%40sessionmgr111&amp;hid=120. (Accessed: 01-11-2014).   Oracle (2010) Java™ tutorials: Lesson: Object-oriented programming concepts [Online]. Available from: http://download.oracle.com/javase/tutorial/java/concepts. (Accessed: 01-11-2014).   Saylor. “Advantages and Disadvantages of Object-Oriented Programming (OOP)”. [Online]. Available from: http://www.saylor.org/site/wp-content/uploads/2013/02/CS101-2.1.2-AdvantagesDisadvantagesOfOOP-FINAL.pdf. (Accessed: 01-11-2014).   Toyota. Children’s Question Room. “How many parts is each car made of?”. [Online]. Available from: &nbsp;http://www.toyota.co.jp/en/kids/faq/d/01/04/. (Accessed: 31-10-2014)   Virginia Tech. (n.d.) “Comparison of Paradigms”. [Online]. Available from: http://courses.cs.vt.edu/~csonline/SE/Lessons/Comparison/index.html. (Accessed: 01-11-2014).   Wikipedia. (2014). “Object-oriented programming”. [Online]. Available from: http://en.wikipedia.org/wiki/Object-oriented_programming. (Accessed: 01-11-2014).   Yuen, S. Y. (2005). “Structured programming: A refresher”. City University of Hong Kong. [Online]. Available from: http://search.cityu.edu.hk/search?q=cache:VWejp2QJEiMJ:www.ee.cityu.edu.hk/~syyuen/Structured%2520Programming.ppt+structured+programming&amp;site=default_collection&amp;client=cityu&amp;output=xml_no_dtd&amp;proxystylesheet=cityu&amp;ie=UTF-8&amp;access=p&amp;oe=UTF-8. (Accessed: 01-11-2014)    Best wishes   Bo W. Mogensen 
	              
	              Hi Craig,
Good job:-)
It's my first time seeing the "Agent Oriented Programming" and I was bit surprised. I was surprised is because when programmers say "AOP" they usually mean something else. So I followed your reference and read the paper , then it says:



"Agent-oriented software engineering is currently one of the most interesting research fields in the computer science community and represent an exciting new means of analyzing, designing and building complex software systems."
I thought to myself, "I must have missed a big party." But after reading it, I didn't find anything interesting. It's just OOP, but with very narrow space of application.
If you are interested, you might want to look at the more popular "AOP", Aspect-Oriented Programming, which has been quite popular and has a lot of applications in many domains.
br, Terry
Aspect-oriented programming. (2014, October 14). In&nbsp;Wikipedia, The Free Encyclopedia. Retrieved 23:51, November 1, 2014, from&nbsp;http://en.wikipedia.org/w/index.php?title=Aspect-oriented_programming&amp;oldid=629599378

&nbsp;



	              
	              Hi Terry
First I would like to complement you on your clear and short description. I guess it shows that this is an area very much within your comfort zone. And your animal examples are again entertaining.
I do agree with your (Henney's) conclusion, that the paradigm is not the most important. I you are better at something else than OOP, and you have a good manageable code, then by all means stick to that. But then I would imagine that you will also be forced to choose which company to work in - or? I say this because the places where I have been, the companies try to have just one, or maybe 2 or 3 languages they stick to and have their internally defined ways of design, develop&nbsp;and explaining the code
Bo
	              
	              Hi Craig,
I'm glad you like them (the paper and the duck). I'm trying to improve my technical writing in English and communicate ideas. So if you have any finding in those areas, please feedback as well. Or simply let me know which part sucks. That would also help.
Regarding the Agent-Oriented porgramming, as I replied to your previous post, this is my first time heard about it. I don't think there's any programming language directly using it, but it's implemented in a few "frameworks". I don't think this "AOP" is interesting.
I would recommend you to read about the other AOP, the Aspect-Oriented Programming. I believe you must have read critics about OOP that it "overemphasizing one aspect of software design". Then it would be interesting to read about what AOP want to achieve.
The sliced duck need to be wrapped by a think piece of bread. I hope you remember that too:-)
br, Terry
	              
	              Hi Numan,
I'm glad that you like it and finding it simple.
I've seen books that trying to teach kids OO programming including inheritance and multiple-inheritance. I was really sorry for those kids who read the book.
br, Terry
	              
	              Hi Bo,
Good job!
The "5GL" (fifth generation programming language) is something like smart programming. With the 5GL you only need to define the problem and give the constaints, then the machine will generate the program for you.
Japan invested heavily in 5GL during the 80s.
I'm very happy that they didn't make it and eventually gave up. If 5GL were really invented and implemented, most programmers like me would lost our jobs to computers.
br, Terry
References:
Fifth-generation programming language. (2014, September 12). In&nbsp;Wikipedia, The Free Encyclopedia. Retrieved 00:46, November 2, 2014, from&nbsp;http://en.wikipedia.org/w/index.php?title=Fifth-generation_programming_language&amp;oldid=625281028
	              
	              Hi Bo,
You are right, I've spent fairly large part of my life in this.
A lot of companies choose Java/C# as their development language, because of 3 reasons 1)Existing codebase; 2)Existing competence; 3)Free resources, (Meyerovich, 2013). But I think programmers still need to learn and use more languages, especially new ones. Because a lot of good design ideas could be brought back to programs that's written in old programming language. As Henney in his speech suggested, with C++ you can also do functional style programming, you can also implement aspect-oriented programming.
br, Terry




 



Meyerovich, L. A. &amp; Rabkin, A. S. (2013), Empirical analysis of programming language adoption, in ‘Pro- ceedings of the 2013 ACM SIGPLAN international conference on Object oriented programming systems languages &amp; applications’, ACM, pp. 1–18. 



 



	              
	              Absolutely Terry, a form of tortilla... with shreadded cucumber, shreadded spring onions, fresh chillis. Yum Yum
I'll take a look at Aspect-Oriented Programming
Best Wishes, Craig
	              
	              1 Introduction

Imagine you are late and driving to the airport just relying on your GPS. Now imagine there is a car accident ahead on the chosen road, and suddenly you are stuck with nowhere to go. What can you do? Nothing – right! Well, if you are lucky, then you have an online GPS system which also receives traffic information’s. But be aware, no matter how online your system is, it all depends on the messages being sent, e.g. how fast will the traffic incident be signalled.

Short description of different types of code:





Compiled code


Static code, but generally runs faster. A compiler converts the source code into machine code, an executable file


C, C++, Objective-C




Interpreted code


Dynamic code, i.e. it can modify it self. Often used in test environments since you don’t have to compile every time a change has been made.


PHP, JavaScript




Hybrid


The programmer creates an intermediate code a close to machine code as possible to reach multiple platforms


Java, C#, VB.NET, Pyton





&nbsp;

Abbrevations:


GPS&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Global Positioning System
WAAS&nbsp;&nbsp; Wide Area Augmentation System. P.t just in the US. (Garmin).


Combination of satellites and ground stations.

2 Abstract

In this assignment I will explain compiled code versus interpreted code in as plain English using the GPS as a real world example. Also I’ll describe the pros and cons of these.

3 Car GPS examples





Examples


Comments




Example 1 – In-precision
I was driving in Auckland city back in 2008. Driving away from the harbour the GPS told me to turn left. But my natural driving instincts told me there is something wrong. I looked at the traffic, looked and the signs, and realised that I was not allowed to turn left. I had to drive further 200 meters before there was any way of turning left.


Reason
I am not sure of the reason, but probably it was because of signal interference.
&nbsp;
Solution
A hybrid solution being able to receive live information from several systems, e.g. from the local transport authorities.




Example 2 – Outdated information
In Denmark (I don’t recall where) a friend of mine came to a crossroad. The GPS told him to drive straight forward. But looking ahead, there was a brick wall. What would you do?


Reason
Compiled code with old maps.
&nbsp;
Solution
Interpreted code being able to receive, in advance, new map information for the local area.




Example 3 – Lack of signal
When driving in areas with high mountains or buildings you will most probably experience lac of signal.


Reason
In this case, there is no difference whether it’s compiled or interpreted code. Although you could run offline with compiled code.
&nbsp;
Solution
Implementing the WAAS technology would probably help.




Example 4 – Car accidents
You’re driving on a road but there is a car accident ahead blocking and stopping all traffic, and you are either not informed or you get the information too late to be able to adjust your course.


Reason
Compiled code with no or to slow on line information.
&nbsp;
Solution
Hybrid code which, in good time, receives live information of the traffic.





&nbsp;

4 Compiled and interpreted code - Pros and cons in general





Code type


Pros


Cons




Compiled code


Ready to run.
Runs faster – typically.
You keep your source code.
&nbsp;


Platform dependent.
Compiling needed for every test.
&nbsp;




Interpreted code


Platform independent.
Dynamic programming.
Test and debug “on the fly”


Slow because of the interpretation.
Locally installed interpreter needed.





&nbsp;&nbsp;

5 Conclusion

Imagine a GPS system with interpreted code and being able to adapt to the real surroundings getting information from many different systems, thus the GPS system would update itself well in advance and my friend (example 2) would not be faced with a brick wall in front of him. And imagine every car having a crash reporting system which automatically and immediately would inform the GPS system.

I trust my GPS very much, but there is a limit. I know by experience that today’s systems are totally reliable thus I always combine GPS with radio information’s, experience and real time surroundings.

My opinion is that there is not a final “one or the other” choice. Thus you will always have to look at the given situation, and then choose the appropriate type of language.

6 References

Arafa, Mostafa. (2012). “Compiled and interpreted languages”. [Online] Available from: &nbsp;http://www.youtube.com/watch?v=qaj7nO1HUqA. (Accessed 02-11-2014).

Brookshear, J. Glenn, (2012). Computer Science An Overview. 11th edition. Boston Massachusetts: Pearson. Addison Wesley.

CurvusGPS. (n.d). “Beginners’ guide”. [Online] Available from: http://corvusgps.com/content/pages/home/gps_tracking_beginners_guide.pdf. (Accessed 02-11-2014).

Garmin. (n.d.). [Online] Available from: http://www8.garmin.com/traffic/. &nbsp;(Accessed 02-11-2014).

Garmin. (n.d.) “What is GPS?”. [Online] Available from: http://www8.garmin.com/aboutGPS/. (Accessed 02-11-2014).

UoL. (2014). “Computer Structures — Lecture Notes Week 9: Programming languages”. [Online] Available from: https://elearning.uol.ohecampus.com/bbcswebdav/institution/UKL1/201480_SEPTEMBER/MS_CKIT/CKIT_501/readings/UKL1_CKIT_501_Week09_LectureNotes.pdf. (Accessed 02-11-2014)


Best wishes

Bo W. Mogensen

	              
	              Week 9 DQ1 – Compiled versus Interpreted Code
Craig Thomas – 2 November 2014
Computer machines are ubiquitous in our way of life, but they are not able to work on levels of abstraction – there is a very clear need for the computers to execute instructions which precisely and unambiguously describe the commands to be executed. This is achieved by operating under instructions in binary, low-level machine language. 
Although machine language is precise and unambiguous, it is also, by all accounts, a language that is not intuitive or easily understood by humans. As such, high-level languages have been developed to introduce various forms and generations of abstraction, which enable programmers to produce programs in languages that are more easily understood. 
However, for these high-level programs to be executed by the computer, they still need to be converted into the low-level machine language that the computer understands. 
There are two principle methods used for this conversion process, which is either through a compiler or through an interpreter. In summary, a Compiler compiles (converts) the high level language into low-level machine language, and stores it ready for future execution by the computer. An Interpreter essentially does something similar, however it executes the program at the same time it converts it, rather than storing it ready for future use (Brookshear, 2012, pg. 242).
This paper will look to draw an analogy between those two methods, and as described in the DQ2 of driving a car with directions being provided either by GPS or receiving live instructions from a friend.
I have decided to use the GPS as my primary choice for providing directions to my friend’s location. In this analogy and scenario, this would be comparable with a compiler, and using the option of having my friend provide live directions would be comparable with an interpreter.
Compiler/GPS. I have just made a quick call to my friend, to advise that I’m setting off now on my journey to meet up and I expect to be with my friend in less than half an hour. The GPS had calculated and mapped out the route, and advised that my journey will take me 25 minutes. The GPS had analysed the traffic patterns, and will continue to do so, and has accounted for known road works, by also providing me with an alternative route to the route that my friend had suggested. 
Interpreter/Friend. My friend suggested the preferred and most common route, and estimated that this would take approximately 20 minutes. This would not be the end result, with my arrival being significantly delayed should I have taken this preferred route. My friend was not aware of the road works, so would not have been able to advise me of this in advance, as such, I would have hit the traffic and with there being no exit roads, I would have had no alternative but to just wait until it cleared. 
Compiler/GPS. I am now driving, 15 minutes into the Journey when I start to slow. The GPS simultaneously advises there has been a report of an accident on the same road I am on, I immediately realise this is the reason for the congestion as I come to a complete stop. The GPS is recalculating directions. 
Interpreter/Friend. In parallel I call my friend to advise of the situation. He suggested that I turn right after the petrol station, and begins to outline a number of other directions when I interrupt him, and advise I had already past the petrol station 3 miles ago. He then explained that I was stuck until the next exit road, which was in another 2 miles. If I had been on the phone during the journey, I may have been able to react to the new directions, and make the right turn after the petrol station.
GPS/Compiler. At the same time, the GPS recalculates and suggest the same exit road. Typically the compiler would spot this run error and the programmer could then make the necessary adjustments before the program is executed, or before you start your journey. The time lag in the GPS could be compared to compilers, in terms of run time speed being slower than machine language. I turn down the volume on the GPS, just as it is also suggesting a secondary alternative route, which is just after the first exit road. 
Interpreter/Friend. I stay on the phone (hands-free!), and after about 10 minutes, the traffic congestion slowly begins to subside. I see the exit on the right that my friend had described, so I take it. I ask my friend to stay on the phone, to help provide directions from this point onwards. After about 1 mile using this new route, the traffic again starts to slow. I mention this to my friend, and my friend advises me that this leads onto the original preferred and most common route, and that it had slipped my friends mind about the road works that I mentioned earlier. I am now not that far away from my friend’s location, but stuck in traffic again!
GPS/Compiler. I look down at the GPS and can see that the secondary alternative route is still highlighted on the GPS map, I turn up the volume and I am given directions to take the next left, which joins up with the highlighted route. This route takes me all the way to my friend’s location. The journey had taken 60 minutes in total, 35 minutes longer than the GPS initially advised and approximately 40 minutes longer than my friend had advised.
Summary advantages and disadvantages
The GPS/Compiler was able to advise of any potential traffic delays and also provided an alternative suggestion but was slower in real time. In a compiler environment, this would identify any obvious mistakes or run errors, which could enable the programmer to then put them right in advance, and thus the programmer would not be required on hand during the execution. In an interpreter environment, the execution would simply stall/or fail and would not be corrected until a programmer was able to do so. Thus, the programmer would need to be on hand to fix it.
It is worth noting, and as I alluded to in the analogy, the compiler would introduce a small level of delay in automatic execution, in contrast to programs that are hand-coded in machine language (Mogensen, 2010, pg. 2) albeit a compiler is not as slow as an interpreter. The interpreter may need to process and generate code from the same syntax tree many times and as such, is typically slower (Mogensen, 2010, pg. 3). However, the interpreted code could also react to different situations and call on new information to present an output or outcome. The compiler would continue to operate in exactly the same way it was compiled.
It is suggested by Mogensen (2010, pg. 3), that writing an interpreter is often simpler than writing a compiler, and an interpreter is also easier to move to a different machine and concludes that, interpreters are often used where speed is not critical. A good example would include a development environment – one of the advantages of an interpreter is that they generally do less on the program before execution starts, and are therefore able to start running the program more quickly (Mogensen, 2010, pg. 4).
Conclusion
It is clear that there are benefits of applying the different conversion methods, depending on your situation. 
I would subscribe to the suggestion of Mogensen (2010, pg. 3), in which a combination of conversion methods could be used to implement a programming language. A compiler could be used to provide code at an intermediate level, which could then be interpreted, rather than compiled, to produce machine language. This could be exploited even further, to keep some parts of the code under a compiler conversion method, and others under an interpreter conversion method, again depending on your situation. 
The choices taken would introduce different levels of compromise on complexity, efficiency, space, and so requires good planning and decision-making to establish the most appropriate approach. Each conversion method provides good options under different circumstances.
Best Wishes, Craig
References
Laureate Online Education. (2014). Computer Structures – Lecture Notes Week 9: Programming Languages [Online]. Available from: https://elearning.uol.ohecampus.com/bbcswebdav/institution/UKL1/201480_SEPTEMBER/MS_CKIT/CKIT_501/readings/UKL1_CKIT_501_Week09_LectureNotes.pdf (Accessed 30 October 2014)
Brookshear, J. G. (2012). Computer Science An Overview. pp. 242. 11th ed. Boston Massachusetts: Pearson. Addison Wesley
Mogensen, T. (2010). Basics of Compiler Design [Online]. DIKU, University of Copenhagen. Available from: http://www.diku.dk/~torbenm/Basics/basics_lulu2.pdf (Accessed 1 November 2014)
Taral, P. (2014). Compiler vs Interpreter: Difference [Online]. Programming Tutorials. Available from: http://www.c4learn.com/c-programming/compiler-vs-interpreter/ (Accessed 1 November 2014)
Wallace, M. (n.d.). hi – hmake interactive – Compiler or Interpreter? [Online]. Available from:ftp://ftp.cs.york.ac.uk/pub/malcolm/ifl00.pdf (Accessed 1 November 2014)
	              
	              Hi Terry
Just a little question to start with. You mention Python to be an interpreted language - and that's also what I find on their web page. But in my research I found a YouTube video from Mostafa Arafa (2012) stating that Python is a hybrid/intermediate language. What's your comment on this?
References
Arafa, Mostafa. (2012). “Compiled and interpreted languages”. [Online] Available from: &nbsp;http://www.youtube.com/watch?v=qaj7nO1HUqA. (Accessed 02-11-2014).
Python. (n.d). "The Python Tutorial".&nbsp; [Online] Available from:&nbsp;https://docs.python.org/3/tutorial/index.html. (Accessed 02-11-2014)
 

Bo
	              
	              Hi Bo,
Python is mostly still considered an interpreted language, although it does compile to byte code first before running, but

The compilation is not transparent to user. So if you run a Python script a.py for the first time, Python will generate a a.pyc file in the same folder, but you don't have to know that. If you run the script again, and Python found the a.pyc is up-to-date, it won't compile again. But if you delete the a.pyc, Python will just regenerate the .pyc byte code without complaining to the user.
A lot of type information will still need to be decided at run-time. So the compilation only solve a small part of the problem.
Python can be run in an interactive way. If you run "python" without any paramemter on a system that Python is installed, you will enter the Python interactive interface. Typically, only interpreted language can do that.

Python has many braches in it's family, besides the classic Python, there's also Jython, Cython, Pypy, etc.
CPython - is the default, or classic Python. It's written in C. And I've talk about it already. It's mostly an interpreted language.
Jython - compiles to JVM byte code. So you can say it's a hybrid.
Cython - it translate the Python code to C, so it's a compiled langauge.
PyPy - it's a faster version of Python, interpreted language (like CPython), but has technology like Just-in-time compilation to make it faster.
I just realized that the Python family itself has all the variants:-)
br, Terry
	              
	              Hi Craig,
Thanks for the good job. I think everything is nicely explained.
I was a bit hesitant to use the GPS story because I wasn't so sure about what is a GPS... OK, so now I realized I've made it to too complicated in my mind. It's just the map+route plan+real-time location infomation+traffic situation reporting system we sometimes call, well, GPS. And I sometime also call it mobile phone. But my mobile phone has been used for calling my friends...
br, Terry
	              
	              Hi Belinda
It’s a very good piece you have written with clear analogies.

In the section “Advantages and disadvantages” you mention in the SP advantages that several programmers can work at the same time. I used the same argument; just I used it for the OOP. Do you agree that it’s also possible for OOP, e.g. having one group focussing on the bigger class problems/descriptions while others focus on developing the necessary objects?

Bo

	              
	              Hi Terry,

Thanks for explaining OOP concept in a simple way,

Python seems all the rage these days, and not undeservingly - for it is truly a language with which one almost enjoys being given a new problem to solve. But, as a wise man once said (calling him a wise man only because I've no idea as to who actually said it; not sure whether he was that wise at all) to really know a language one does not only know its syntax, design, etc., advantages but also its drawbacks. No language is perfect, some are just better than others.

So, what would be in your opinion, objective drawbacks of Python?

Note: I'm not asking for a language comparison here (i.e. C# is better than Python because ...)&nbsp; - more of an objective (to some level) opinion which language features are badly designed, whether, what are maybe some you're missing in it and so on. If must use another language as a comparison, but only to illustrate a point which would be hard to elaborate on otherwise (i.e. for ease of understanding)

Best Regards, Babatunde.
	              
	              Hi Craig
That was a nice, well written and realistic story.
You mention in your summary’s first paragraph, that “In an interpreter environment, the execution would simply stall/or fail”. I’m not sure if I’m misunderstanding something, but I don’t understand why that should happen. Wouldn’t the reason be because of bad design rather than because of interpretation? I mean, to me it’s logic that a good design would just receive the new live information about the accident and change some variables/methods, and the return any possible solutions. Could you please comment on that?
Thanks
Bo

	              
	              Hello Class,The greater propensity for code reusability is often put forward as a key advantage of OOP over Structured programming. Some however think that this aspect of OOP is overhyped, and that in fact a number of software design problems may arise from over-stressing code reusability. Do you feel that this view has any merit?Anthony
	              
	              Hi Everyone,One of the apparent problems with interpreted code is that it is more easily reverse-engineered, since it is typically delivered to target platforms at, or close to, source code form.So, others with mal-intent may discern its inner workings and exploit its vulnerabilities much more easily than with compiled code. Software piracy is also therefore more likely with interpreted code, unless specific measures are taken.What methods could be used to secure/protect interpreted code to minimize or mitigate these issues?Anthony
	              
	               
Hi Augusto
Nice post. I think your explanation is great.
I particularly like the way you described encapsulation. 
I want to make doubly sure I understand all of this. 
If I could just paraphrase this please – in that, a human (object A) is not able to access the cat (object B) smell senses (property A) as they are not part of the same object, and that the cat is encapsulated (private).
However, just for clarity (mine!...as this is confusing and could be extremely complex), I am assuming that it is still feasible that both objects (A – human, and B – cat) could perhaps still be an instance, or to have inherited properties from the same class (or superclass as you also refer), i.e. a mammal class?
Therefore, objects A and B both have the property of smell senses, but characteristically have different strengths, perhaps due to Polymorphism, which enables a subtly different interpretation or variation.
I hope you understand what I am saying here – I think I am now confusing myself!?
Best Wishes, Craig
	              
	              I.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Introduction
"Computer language" is a language for describing all consecutive actions a computer must be running. A computer language is thus a practical way for us (humans) to give instructions to a computer.
The language used by the processor is called machine language. These are data as they come to the processor, constituted by a sequence of 0 and 1 (binary data). Machine language is not comprehensible by a human that is why intermediate languages have been developed. Written code in this type of language is converted into machine language to be usable by the processor.
Computer languages have many advantages:

It is easily understandable than the machine language;
It permits greater portability, that is to say a greater adaptability to different types of machines;

There are usually two main types of programming languages, depending on the manner in which instructions are processed:
•&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; The Procedural or structured programming language
•&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; The Object-Oriented Programming language (OOP)
II.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Structured programming language
The conventional or procedural programming language treats programs as a set of data on which act procedures. Procedures are important active assets and the data becoming passive elements that cross the tree of procedural programming as an information flow.
This way of designing programs remains close to Von Neumann machines and is ultimately treated independently data and algorithms (translated by procedures) without taking into account the relationships between them.
To solve a problem, algorithms are designed and we need to found the proper way to store data. Data structures are often chosen for ease of handling data with respect to functions implementing the algorithms selected. The programs are thus decomposed as a hierarchy of atomic functions and data.
&nbsp;
II.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1.&nbsp; Advantage of Procedural Programming
Procedural Programming languages are:
•&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Centered on the procedures (operations)  1.&nbsp;&nbsp; Decomposition of functionality from a program to procedures which will be executed sequentially 2.&nbsp;&nbsp; The programs are thus decomposed as a hierarchy of atomic functions and data.
•&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Coupling procedures / data
1.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Data are independent of procedures
2.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; The data to be processed are passed as arguments to procedures
•&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; We can reuse the same piece of code in several places in the same program without being forced to write it again.
•&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; The execution of a procedural program is simple to follow
•&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Algorithmic view of a procedural program is very close to the machine language
•&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; This approach is consistent and intuitive
II.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.&nbsp;&nbsp; Disadvantages of Procedural Programming
•&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; The maintenance and adding new features require modifying or inserting sequences in what already exists
•&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Can become complex very quickly and results in a complex upgrade in case of change or evolution and makes adjustment difficult
•&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Makes the job in team very difficult (not modular), so the quality of code is suffering
III.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Object-Oriented Programming (OOP)
In object-oriented programming, computer programs are interacting of "objects". In a game program, there could be an object called "spaceship", which has instance variables as fuel. One method could be "torpedo fire." The spacecraft could belong to a "class" of objects called "moving objects." Asteroids also belong to this category. The implementation of the program consists of objects send "messages" to another.
Some important definition:
What is an object?
An "object" is a representation of a material or immaterial thing to which the real property and activities are associated.
For example: a car, a person, an animal, a number or a bank account can be seen as objects.
What is an attribute?
The "attributes" are the specific characteristics of an object.
A person, for example, have different attributes as its own name, first name, eye color, gender, hair color, size...
&nbsp; &nbsp; &nbsp;What is a method?
The "methods" are applied to an object action.  A person object, for example, has the following: eating, sleeping, drinking, walking, running...
What is a class?
A "class" is a data model defining the structure common to all objects that will be created from it. More concretely, we can see a class as a mold through which we create as many objects of the same type and the same structure as desired.
For example, to model any person, we could write a class one in which we define the attributes (eye color, hair color, height, sex ...) and methods (walking, running, eating, drinking...) common to all human beings.
What is an instance?
An instance is a particular representation of a class.
When creating an object, we realize what is called an "instance of the class." That is to say that the mold is extracted from a new object that has its attributes and methods. The object created will like the name of the class.
For example, objects Tresor, Romain, Nicolas Daniel are instances (objects) of the class Person.
III.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1.&nbsp; What are the advantages and disadvantages of object approach?
Object-oriented programming provides many benefits. Among them:
• The OOP gradually replace the procedural in large programs because it has enormous advantages: ease of organization, re-use, more intuitive method, possibility of inheritance, ease of correction, projects easier to manage. The main advantage of OOP is that we no longer described by the code of actions to achieve linear but coherent sets called objects.
• The OOP is easy to imagine because it describes entities as they exist in the real world. Thus, the Car objects that implements the method Car.ride and has property Car.Cylinder is easily conceivable and can be used -for example- in a racing simulation where you will interact several Car objects. The object models were created to model the real world. "In an object model, any real-world entity is an object, and vice versa, any object represents a real world entity."
• The OOP allows somehow factor the code into logical sets. From the point of view of programming, OOP allows you to write programs easily readable with minimal experience, minimum size and easy correction. These programs, in addition, are often very stable.
There are some drawbacks to the use of object-oriented programming:
• A poorly conceptualized object-oriented application will be difficult to maintain and modular.
• The object-oriented programming usually requires more time and resources implementing
IV.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Conclusion
What are the tangible benefits of procedural programming versus object-oriented programming? What drives some professional programmers to use the procedural while most seem to speak of the object oriented as more advantageous?
One could therefore conclude that it is more a school of thought question, or vision of programmer? So, could we objectively say that one paradigm is actually better than the other?
Object-oriented programming is not synonymous of good programming. There are excellent developers capable of producing good procedural code, maintainable, structured and modular. But there are also developers in object-oriented programming that produce the wrong result code to a lack of conceptualization
V.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Reference:
J. G. (2011) Computer science: An overview.&nbsp;11th ed. Boston: Pearson Education / Addison-Wesley., Section 6.1-6.5.
Lecture Notes, (2014).&nbsp; Week 6: Computer networks [Online]. Available from: https://elearning.uol.ohecampus.com/bbcswebdav/institution/UKL1/201480_SEPTEMBER/MS_CKIT/CKIT_501/readings/UKL1_CKIT_501_Week09_LectureNotes.pdf. (Accessed: 02 November 2014)
Wikipedia Inc. (2014) Interpreter_(computing) [Online]: Wikipedia Inc. Available from: http://en.wikipedia.org/wiki/Interpreter_(computing) (Accessed: 02 November 2014)
www.cct.lsu.edu (2014) Compilation and Interpretation [Online]: www.cct.lsu.edu. Available from: https://www.cct.lsu.edu/~kosar/csc4101-spring06/Lectures/02-Compilation.pdf (Accessed: 02 November 2014)
Torben Mogensen. (2014) Basics of Compiler Design [Online]: Torben Mogensen DIKU, University of Copenhagen, Universitetsparken 1. Available from: http://www.diku.dk/~torbenm/Basics/ (Accessed: 02 November 2014)
&nbsp;

	              
	              I.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Introduction
A computer actually understands only machine language, made of binary code unreadable even by a computer specialist. Thus we must find a way to understand instructions in a language designed by humans. In theory the two techniques are:

Interpretation: the computer reads the instruction, and depending on what he reads he deduces which functions must be called with which parameters. It performs these function then proceeds to the next instruction.
The compilation: translating once and for all to the machine language, it saves it in a file that can be run as many times as you want.

II.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Interpretation
In the technique called interpretation, the interpreter software must be used whenever you want to run the program. In this technique, in fact, each row of the analyzed source program is translated to machine language instructions, which are then directly executed. No object program is generated.
The interpretation is ideal when you are in the process of learning programming language, or being tested a project. With this technique, one can indeed immediately test any changes to the source program without going through a phase of compilation that always takes time.
Interpreters Language: Basic, Ruby, Python, Perl, PHP, etc.
Based on the case presented in the DQ, where you hire a car and need direction to go to a city that you never visit before, the approach (1) is exactly corresponding with interpretation language.
Use your mobile phone and get live help (directions) from your friend during the entire drive, this is like when interpreter analyze the source code line by line, step by step. If there is an error he stop if not continue with the next instruction.
Advantages of interpreted languages 

The interpreter stops when an error occurs on the wrong line; the "debugging" is easier.
The type of machine that interprets the file is not necessarily the same as the one on which we typed the source file. We just need to have the proper interpreter on the destination machine. Code written in interpreted language program is "portable".

Disadvantage of interpreted languages 

The Checking line by line of source code causes slowness on execution.

&nbsp;
III.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Compilation
The compilation is a translation of all the source text once. The compiler software reads all lines of source program and produces a new sequence of codes called object program (or object code). It can now be run independently of the compiler and retained as such in a file ("executable").
When a project involves complex functionality that must run quickly, the compilation is better: it is indeed clear that the compiled program will always run much faster than its counterpart interpreted, since in this technique the computer doesn't need to (re) translating each instruction into binary code before it can be executed.


Compilation is working exactly like GPS when you need to pay a visit to your friend in a city where you didn’t go before (ref. DQ), so option (2) is corresponding to compilation when using GPS, in fact all the information about the direction, traffic condition, accident, etc are treated at once and calculated by your GPS devise. It’s the same principle with compilation system, all the source code is read and analyzed once.
Advantages and disadvantages of the compilation
Advantages: 

The program is translated once and for all at compile time.
Once the program is compiled, its execution is very fast
A compiler can compile itself

Disadvantages of compiled languages
&nbsp;The development of programs is difficult (if it crashes during execution, where is the error?)
•&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; The compiler depends on the type of computer you are working on.
•&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; The object code is dedicated to the type of machine on which it was compiled.
&nbsp;
IV.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Conclusion
In practice, we are now seeing more and more intermediate situations. First, most of the time an interpreter does not load instruction by instruction, it will load everything and build a collection of objects called syntactic tree, which is faster to explore. This is the case in Lisp, Smalltalk, Perl, and many others. Then we tried to do better, Java in particular was first popularized the concept of virtual machine (it existed before but it is Java that gave it a commercial reality). In this type of system we "compiled" into an intermediate code easier to interpret than Java. But that intermediate code is not a machine code, it should be interpreted.
Then Java machine of Sun popularized the concept of compilation just in time: after reading through this intermediate code, it does not interpret but it translated into machine code on the fly. Then remains whether the time of this compilation on the fly will be offset by the gain in execution speed? Sometimes yes, sometimes no. Currently the Sun machine implements the two techniques.
In all cases a compiled program is always faster than the best interpreters, even with a compilation Just-in-time. But by compiling we lose a feature that may be important in some cases: portability.
It means that the compiled code is executable in the same environment (same type of processor and operating system). And we want to gain speed, the less the program will be "portable", it probably will not be compiled into a different environment, at least not without some modifications due to differences between the systems. An interpreter has in principle no such problems, it is he and not the programs that must be compiled interpreted in several environments.
V.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Reference:
J. G. (2011) Computer science: An overview.&nbsp;11th ed. Boston: Pearson Education / Addison-Wesley., Section 6.1-6.5.
Lecture Notes, (2014).&nbsp; Week 6: Computer networks [Online]. Available from: https://elearning.uol.ohecampus.com/bbcswebdav/institution/UKL1/201480_SEPTEMBER/MS_CKIT/CKIT_501/readings/UKL1_CKIT_501_Week09_LectureNotes.pdf. (Accessed: 01 November 2014)
Wikipedia Inc. (2014) Procedural programming [Online]: Wikipedia Inc. Available from: http://en.wikipedia.org/wiki/Procedural_programming (Accessed: 01 November 2014)
Torben Mogensen. (2014) Basics of Compiler Design [Online]: Torben Mogensen DIKU, University of Copenhagen, Universitetsparken 1. Available from: http://www.diku.dk/~torbenm/Basics/ (Accessed: 01 November 2014)
&nbsp;

	              
	              Hi Anthony
If I could just compare this to a business evironment, and specifically at the standardisation of products.
I come from a service environment, and one of the key success factors in my line of business is being able to standardise and repeat the service offering in large volumes without any need for additional engineering or bespoke processes. This enables you to hone your skills in service delivery, but also develop a predictable and assured environment.
Of course, as a business you may want to extend this, and provide a more specialised solution for some customers. The key is to keep the management and the operational aspects seperate, even though you may still share (or not) some of the underlying resources or infrastructrure. And if you do, you may want to scale accordingly.
If we compare this with your question, I see no reason why you would not want to be able to exploit OOP reusability and, if necessary scale accordingly. &nbsp;
It will be interesting to see responses from our colleagues who are actually programmers, to get a real situation analysis.
Best Wishes, Craig

	              
	              Hi Guys,
Just on the question of languages, or opinions of: This may be a dumb question....is it perfectly feasible for a business application to be constructed entirely from many many different programming languages i.e.) C, C++, C#, Python, VB, Java, cobol, fortran etc which are all used in unison for the application to function, or it is typically just one or two which provide discrete functionaliy.
I have never really been in a situation to need to understand the ins and outs of a business application, my focus has been more on people, strategy, leadership, service performance, outcomes, budgets etc, so just keen to get a perspective on this. If it is feasible, are there any interoperability issues or is it generally truely transparent.
Best Wishes, Craig
	              
	              Hi Bo
Good question
My perspective was that an interpreter deals with the conversion there and then, and then executes it. Whereas a complier is done in advanced, and then stored for future use.
So, assuming this starting point is correct, my view here would be that the programmer had not accounted for the traffic jam, and in the same way the interpretation of the program then fails/or stalls as it is waiting on an input of some kind that just isnt there, until the programmer (or as per my analogy, my friend or the GPS system) provides a new input. Equally, the input may not be possible because it needs that from something else that hasn't completed (similarly to the jam, which until it moves on I am stuck). Hence, the fail/or stall.
Therefore, you could argue this is a design flaw. Following my reading, I believe a design flaw would be picked up during compilation and as such, a programmer could then resolve this before sending off the program for execution, and then forgetting about it. However, for the interpreter scenario, if the programmer sent the program to be run (to be interpreted and then executed at the same time) and then forgotten about it, my understanding is that the interpreting would fail/or stall, until the programmer returned to realise the issue was occuring, and then could do something to put it right.
Unconsciously, I think this is perhaps one of the reasons why I concluded that a combination or hybrid model of both conversion methods could be useful.
Best Wishes, Craig
	              
	              Week 9 DQ 1- Structured and object-oriented programming


Babatunde KOLAWOLE – 2 November 2014


Object-Oriented Programming 

Object-Oriented Programming (OOP) is a decorative or created theory which uses a distinguished pattern of Software developing languages than old procedural programming languages (C, Pascal,). 

In Object Oriented Programming, everything is grouped as self sufficient "objects".

The “hand”, which is a vital part of the human body, should be taken as an example in order to clearly understand the object orientation. The “human hand” is a class which has two objects of hand types: the left and right hand and their main functions are being controlled/ managed by a set of electrical signals sent through the shoulders which is an interface the body uses to interact with the hands. The hand is a structured class being re-used to create the left hand and the right hand by imperceptibly changing the attributes.

Simply put, Object Oriented Programming (OOP) is an Abstract Data Type which corresponds to things found in the real world and administered by sending messages to objects. To understand this, the concept of Object has to be clearly stated. 



What Is an Object?


For purposes of this discuss, Objects are keys to understanding Object-Oriented technology. The set of activities that an object performs defines the object's behavior. This refers to a particular instance of class, where the object can be a combination of variables, functions and data structures. 

The object encompasses a set of commands, each performing its specific action. An object asks another to perform an action by sending across a message. The requesting (sending) object is being referred to as sender and the receiving object is termed the receiver.








&nbsp;
&nbsp;



Control is therefore given to the receiving object until the command is completed and as well returns to the sending object. 

For example, a School object asks the Student object for its name by sending it a message asking for its name. The receiving Student object returns the name back to the sending object.








&nbsp;
&nbsp;



Also important is the argument in the message which contains information the sending object needs to pass to the receiving object. A receiving object always returns a value back to the sending object which may or may not be useful to the sending object.

An instance could be a decision by the School Object to effect a change in the student's name. This it does by sending the Student object a message to set its name to a new name. The new address is then passed as an argument in the message. Hence, the School object does not care about the return value from the message. 





&nbsp;



&nbsp;





Sequential Operation


Other messages are being sent in the cause of a particular message on either ways for a task to be completed. This is called sequential operation. The original sending object loses control until all other messages have been completed. For example, in the following diagram, Object A sends a message to Object B. For Object B to process that message it sends a message to Object C. Likewise, Object C sends a message to Object D. Object D returns to Object C who then returns to Object B who returns to Object A. Control does not return to Object A until all the other messages have completed. 

The process of accessing another object's data by sending it messages is called encapsulation and assures that there is a secure process for getting to an object's data&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 





&nbsp;



&nbsp;




Method


Codes are attached to each message and executed once a message is received by an object. This therefore means that an object’s behavior is determined by these messages. How the object carries out each message is determined by the code. A Method therefore, is the associated code in each message.


Advantages of Object Oriented Programming:



Code Reuse: Objects created for OOP can easily be reused in other programs.
Design Benefits: it is very difficult to write large programs. Designers pass through lengthy steps in Object Oriented Programs which results to proper creations with lesser mistakes. Object Oriented Programs are less difficult once a program gets to a particular stage than non-Object oriented ones.
Software Maintenance: Software developing languages are not expendable and source codes must be dealt with constantly for improvement (for a new category of an existing piece of software) or made to work with newer computers and software. An Object Oriented Program is much easier to make alterations, re-cast and maintain than a non-Object Oriented Program.
Encapsulation: This is to gather data, which is supported by using classes, which is an attribute of object design. Hence Encapsulation is including something within another and that which is included is not clearly visible or understood.


So far, there have been several disadvantages of Object Oriented Programming based on size, effort, and speed which made it unclear over the years. 


Size: Object oriented programs are much larger than other programs and typically involve more lines of code. In the early days of computing, space on hard drives, floppy drives and in memory was at a premium. Today we do not have these restrictions.
Effort: Object oriented programs require a lot of work to create. Specifically, a great deal of planning goes into an object oriented program well before a single piece of code is ever written.
Speed: Object oriented programs are slower than other programs, as they typically require more system resources, thus slowing the program down.



Structured Programming

Structured programming is a procedural programming subset that reduces the need for GOTO statements. In many ways, OOP is considered a type of structured programming that deploys structured programming techniques. Certain languages – like Pascal, Algorithmic Language (ALGOL) and Ada – are designed to enforce structured programming.


The structured programming concept was formalized in 1966 by Corrado Böhm and Giuseppe Jacopini, who demonstrated theoretical computer program design through loops, sequences and decisions.


Object oriented programming vs. Structured programming



Object oriented programming simply means that it is drawn around a set of values of qualitative or quantitative variables in programming, combination of variables, data structures and functions are used.


There are no variables in structured programming. Programs run from in a well prepared form.


Structured programming is a programming perspective targeted at enhancing the clarity, quality and development time of a computer program.
Object-oriented programming is discipline imposed upon indirect transfer of control.


This means approximately "manage code dependencies by using jump tables (polymorphism) instead of conditionals (if-/switch-statements)".


Structured programming introduced functions with parameters and local variables. I.e. gives more preference to functions and control flow, and global data can be used by any functions. So it is hard to do debugging 
Object oriented programming give more preference to data we can make data private or public. So it helps in debugging and maintainability in large programs



References:


Practical Approach to Computer Systems Design and Architecture [online] Available from: http://www.codeproject.com/useritems/System_Design.asp (Accessed on: October 30, 2014)


Introduction: What is Object-Oriented Programming? [Online] Available from: http://www.inf.ufsc.br/poo/smalltalk/ibm/tutorial/oop.html (Accessed on: October 31, 2014)


Object-Oriented Programming Concepts [Online] Available from: http://docs.oracle.com/javase/tutorial/java/concepts/index.html (Accessed on: October 30, 2014)


CS 164: Fall 2012 - Introduction to Computer Science Object Oriented Programming: Disadvantages of OOP [online] Available from: https://www.cs.drexel.edu/~introcs/Fa12/notes/06.1_OOP/Disadvantages.html?CurrentSlide=2 (Accessed on: October 31, 2014)


Cory Janssen. Techopedia Structured Programming [online] Available from: http://www.techopedia.com/definition/16413/structured-programming (Accessed on: October 31, 2014)


Wikipedia, the free encyclopedia Structured program theorem [Online] Available from: http://en.wikipedia.org/wiki/Structured_program_theorem (Accessed on: October 31, 2014)


Nirosh L.W.C (2014) Introduction to Object Oriented Programming Concepts (OOP) and more [online] Available from: http://www.codeproject.com/Articles/22769/Introduction-to-Object-Oriented-Programming-Concep#OOP (Accessed on: October 31, 2014)
	              
	              Introduction
The procedure of enabling from a primitive formulation of the computing issues to the executable programs is known as Computer programming. Computer programming relates a lot of activities such as understanding, analyzing, considering and generic problem-solving during the algorithm. 
The basic pattern of computer programming is called programming paradigm which is the approach of establishing the elements and structure of computer programs. Styles and capabilities of different programming languages are delimited by which supported paradigms; certain languages are supported by multiple paradigms, while others only abide by one paradigm. Programming paradigms which are usually divided comprise functional, imperative, structured, declarative, symbolic, logic and object-oriented programming. Here I will focus on the structured and object-oriented programming, as well as try to look into their features.
Structured Programming
The definition of structured programming is a programming paradigm which abides by a top-down design conception with block-oriented structured. The features of this type of programming paradigm is when the programmers tend to compartmentalize whose source code into logical block-structured that would generally comprise the statements of conditional, loops and logic blocks. Structured Programming has the performance of the program source code being executed in the sequence during which bits of the code has been inputted. The structured program can be simply expressed as the following figure: &nbsp;
&nbsp;

&nbsp;
Object-Oriented Programming
The simplest terminology of the definition of the object-oriented programming is the programming paradigm where there is an interactivity between objects or self-contained sub-programs among the main-program. In other words, object-oriented programming is able to be called as the procedure of adopting some classes to express various domains of data objects or functionality during the applications. As well as there are functions and data fields within the data objects which carry on the data fields. The three major features of Object-Oriented Programming are polymorphism, encapsulation and inheritance. Normally, objects would comprise text inputs, windows, icons, menus and so on. And there have to be procedures to operate these objects. The Object-Oriented Programming concept can be simply implanted by the following figure:

&nbsp;
Comparison of the both programming
Structured Programming is able to economize energy and time when compiling uncomplicated programs which achieve complex functions or classes as would be the instance during object-oriented programming. In most instances, simplified and straightforward source code could accomplish the job well. Structured programming is perfect for the developing of small-scale programs due to it is able to beat the purpose expending excessive energy and time to design classes when a whole development process would be exploited during the equal time span.
Since the small-scale programs are liable to perform modification, allowing more sense for the programmers to visualize the source code. It enables structured programming will be appropriate for small projects such as small or medium scale website that could perhaps not demand too many maintenance.
Structured programs are convenient to comprehend and modify that programmer is only required simply abiding by the source code on the file which make sure there are not any deviations or jumps to other sections of code in other files. However, this does not imply such program could certainly be simpler to maintain; that way object-oriented programming sometimes are more suitable in other circumstances.
The main merit of Object-oriented programming is its applicability for supporting huge-scale projects of web and software development. This is a quite better choice than adopting structured programming when there are numerous source code bases. The sheer property of object-oriented programs makes the programmers to economize much energy and time during development as the elements of the programs are in the pattern of objects that are able to be inserted into the program wherever the objects are demanded. Therefore possessing an application with hundreds buttons achieved as objects could be quite simple to maintain, for instance, in case the programmer needed to modify the behavior or style of all the buttons, it is simply to change a single object that determines anything regarding the button and this could alter any case of the button object.
Although object-oriented programming is favorable for large-scale project and enduring maintenance of web projects, this approach of programming brings out its own cons as object-oriented programming is often more intricate than structured programming due to there is many decisions of layout that require to be made and therefore the whole work of controlling the project is possible hard to the unskilled programmers.
Stand at a user point, the discrepancy between structured and object-oriented programming for performance is perhaps trivial. But during certain situations, such as in some WebPages, the object-oriented programs are actually slower because there is additional task the interpreter must go via the compiler the classes as against the structure approach of executing in a top down order.
&nbsp;
Conclusion
&nbsp;
In short, summarize the features of the Structured and Object-Oriented Programming as following: 
&nbsp;
Structured Programming 
&nbsp;

It is able to process complex programs, even the programs are unable to be processed by Procedural Programming
Support for standalone subroutines
Support for local variables
Support for rich control structures
"Code acting on Data"

&nbsp;
Object- Oriented Programming  

Programs are organized around Data
The evolution of Object Oriented Approach
Three major features are polymorphism, encapsulation and inheritance.
"Data being acted upon by Code" 

References
&nbsp;
Asagba, Prince Oghenekaro (2008); A Comparative Analysis of Structured and Object-Oriented Programming Methods; Available on: (http://www.ajol.info/index.php/jasem/article/viewFile/55217/43688) (Accessed on: 02-Nov-2014)
&nbsp;
Dr K R Bond (2000); Structured Programming versus Object-Oriented Programming; Available on: (http://www.educational-computing.co.uk/OOPSample.doc) (Accessed on: 02-Nov-2014)
&nbsp;
John Minor Ross and Huazhong Zhang (1997); Structured Programmers Learning Object-Oriented Programming; Available on: (http://bulletin.sigchi.org/1997/october/papers/ross/) (Accessed on 02-Nov-2014)
&nbsp;
Motaz Saad (2010); Structured vs. Object Oriented Analysis and Design; Available on: (http://www.slideshare.net/mksaad/structure-vs-object-oriented-analysis-and-design) (Accessed on 02-Nov-2014)
&nbsp;
Peter Wegner (n.d.); Concepts and Paradigms of Object-Oriented Programming; Available on: (https://analysis-of-persistence-tools.googlecode.com/files/p7-wegner.pdf) (Accessed on 02-Nov-2014)

	              
	              Structured and object-oriented programming
&nbsp;
Introduction
&nbsp;
Computers can only understand positive and negative voltages, i.e., 0s and 1s; while the most common form of communication for humans is the English language. Writing programs, in plain english language format, in order to command and direct the computer to recognize the end-user’s intentions and to perform the necessary tasks, is known as High-Level Language Programming. These high level programming languages are used to communicate algorithms to the computer; wherein the compiler/interpreter would convert the high level program into a machine-readable “executable” format in order for the computer perform the defined set of tasks and thus resolve real-time problems.

Structured Programming
 Structured programming is follows a top-down approach to resolving problems, which means, all the algorithm logic and controls are defined in a specific order of steps; and the execution of the algorithm will have to take place in the exact same sequence.

Object Oriented Programming
 OOP follows the problem-solving approach by applying the algorithms on specific “objects” rather than structures. This gives more flexibility in developing and analysing the code. The objects defined in OOP will contain some particular “attributes/properties” and we can define “functions” which can then perform necessary and/or specific actions on these properties.
&nbsp;
If we take an example of the most commonly used Phone as an “object”; we can define its properties such as follows:
&nbsp;
object Phone


property _Manufacturer Name


property _Model Number


property _IMEI


property _Type


property _Cost


function getModelDetails(){


  display all the attribute information
}


function getAdditionalFeatures(){


 display additional feature details
}


function setModelDetails(Name, Model, Type, Price){


  set _IMEI = generate unique number
set _Manufacturer Name = Name
set _Model Number = Model
set _Type = Type
set _Cost = Price
}
&nbsp;
The Phone is a generic object with certain attributes and “functions/methods” defined to it; and we can utilise this object to define OOPs concepts.

Class and MembersClass defines the structure of an object. Each of these objects will contain attributes or properties and methods or functions. The “Phone” is a Class and its attributes and methods are called members of the Phone class.

Encapsulation
Encapsulation is nothing but maintaining all members of the class, i.e., the attributes and methods together in a single structure, which also helps protect information from being corrupted. In the above example the attribute IMEI is hidden and protected from getting modified by end-user. Each time a new phone is defined using the “Phone” object, it will automatically create an IMEI value.

InheritanceInheritance in OOP means combining features of two different objects into one and referring both the features of those objects using one single object. To demonstrate Inheritance, with the above example itself, we could create another class called Company as follows:
&nbsp;
class Company
 _Name
 _Brand
function setDetails(Name, Brand){
  set _Name = Name
  set _Brand = Brand
}
&nbsp;
Now we can refer the company information of the “Phone” directly by inheriting the “Company” class.
&nbsp;
Ex : 
First we need to inherit
class Phone() : Company
_Model Number
_IMEI
_Type
_Cost
&nbsp;
function getModelDetails(){
  display all the attribute information
}
function getAdditionalFeatures(){
 display additional feature details
}
function setModelDetails(Name, Model, Type, Price){
  Company.setDetails(Name,Model)
set _IMEI = generate unique number
set _Manufacturer Name = Name
set _Model Number = Model
set _Type = Type
set _Cost = Price
}
&nbsp;
Then, define an instance of the Phone class, 
set Nokia &nbsp;= new Phone()
&nbsp;
Using the instance of the “Nokia”, we can access the “Company class members;
&nbsp;
Nokia.setModelDetails(“Nokia”,”Lumia 1020”,”Bar”,”480 GBP”)

PolymorphismPolymorphism is an ability to perform more than one operation with the similar or different data type(s). (Wiki, Computer science).
&nbsp;
Referring to the “Phone” class again, I am going to add another method:
&nbsp;
function getModelDetails(){
  display all the attribute information
}
&nbsp;
function getModelDetails(Type){
  display all the attribute information which is of given argument “Type”
}
 This is called as method overloading, where same function is used twice with and without arguments. This helps programmer to retrieve either all the information or specific Type information.
&nbsp;
Conclusion
Even though OOP provides a mechanism for relating to real-time scenarios with “objects”, it sometimes becomes very difficult to map the same, due to varying attributes and multiple inheritances etc. When observing from a bird’s eye view, OOP itself slightly follows the structured programming to resolves certain problem areas. We cannot always justify which of the two completely reflects the human way of thinking; as it is almost completely dependant on the nature of the problem, the resolution algorithm and the algorithm design; which in turn helps determine whether to apply OOP or Structured Programming. Having said that, OOP is still the most widely used programming approach.
&nbsp;
References
Wikipedia, Structured Programming - http://en.wikipedia.org/wiki/Structured_programming &nbsp;(Accessed: 02/11/2014).
Wikipedia, Object-Oriented Programming - http://en.wikipedia.org/wiki/Object-oriented_programming (Accessed: 02/11/2014).
Wiki, Computer science - http://en.wikipedia.org/wiki/Polymorphism_(computer_science) 
Best regards,Kharavela
	              
	              Hi Anthony,
&nbsp;
We can make interpreted code to be extra-hard, but not impossible to reverse engineer to break. Here are few thing that we can try.

Implementing critical parts in some compiled language. 
Use hardware dongle which can implement some calculation inside it. So it's almost impossible to reverse it back.
Offer upgrades and enhancements that make any reverse engineering a bad idea. When the next release breaks their reverse engineering
Offer software as a web service. SaaS involves no downloads to customers

&nbsp;
Best Regards,
Numan
	              
	              1.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Introduction
Software engineering is concerned with developing reliable and robust computer programs. With the increase of computer use, software engineering discipline has wide spread. Catastrophic issues related to large software projects have been resolved efficiently by writing thousand lines of codes involving multiple teams. Different programming languages are used in software industry to write computer programs. Two major categories of programming languages are structured programming and object oriented programming.
2.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Structured Programming
A software engineer solves a problem by using a particular method or group of methods, this is known as methodology. During 1970 and 1980s, structured programming was the basic software engineering methodology. Programs developed using structured programming use following methodology.

Solve a large problem by breaking it into small problems. Work on each small problem separately.
Solve each small problem as a new problem. Further break down problems into smaller problems.
Repeat the above process until each problem can be resolved without further decomposition

This approach is also referred as top-down program design. Consider following example of structured programming.
Write a program that displays average of two numbers that are entered through keyboard connected through computer. The average is to be displayed on monitor connected with computer. According to top-down program design, following solution is provided.
Top level:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1. Display average of two numbers entered through keyboard
&nbsp;
Bottom level:&nbsp; 2.1. Get and parse two number entered through keyboard.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.2. Calculate average of these two numbers
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.3. Display average on monitor screen
&nbsp;
The bottom level can be implemented using any structured programming language such as Pascal.
&nbsp;
3.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Object Oriented Programming (OOP)
&nbsp;
In early 1980s, information hiding becomes common in programming languages. This concept developed further and lead to a new programming platform known as object oriented programming (OOP).
&nbsp;
Object is the main concept of OOP, which is a sort of module containing subroutines and data. Each object has its own internal state (data) and can respond to other subroutines (messages).&nbsp; Object oriented programming to software engineering involves following method
&nbsp;

Identified all objects involved in the problem
Identify all messages that those objects have to respond

&nbsp;
End product of this method is a set of objects with each object having its own responsibilities and data. Objects can interact with each other by exchanging messages.
&nbsp;
For examples, a student record object contains detail of all registered students. If a new student arrives, its registration information is send to record object. Record object will respond by adding information of new student in registered students. If a message is send to record object to print itself, it will display registration information of all students on screen.
&nbsp;
4.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Analysis of structured programming and OOP
People who are used to structured programming, at first find OOP hard. However people who have expertise in OOP claim that OOP can better model real world examples. They assert that OOP solutions are


Easier to model
Easier to write
Easier to understand
Lower error rate
Reusability


&nbsp;

Following example illustrates how OOP can better model the way world itself works?
Consider exchange of messages between two persons.
&nbsp;
First Person:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;Numan, are you thirsty?
Second Person:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Yes I am, Ahmad.
&nbsp;

&nbsp;
Modelling this in object oriented terms, two messages are being exchanged between two objects. These objects are Numan and Ahmad. These messages are “are you thirsty?” and “Yes I am”. Numan knows how to respond to messages either negative or positive. That is Numan will respond according to internal state of his stomach.
&nbsp;
Top down programming approach which is used by structured programming is useful but it has certain limitation.
1.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Entire focus is on providing s a set of instructions to solve a problem. Designing data structures is not under the scope of top- down programming design.
2.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Reusability is difficult to achieve in structured programming. By using top down approach leads to a design that is unique to only that problem. A lot of time and effort is required to adapt an existing programming piece.
3.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Top down program design does not properly fit for some problems. It is not possible to express their solution in a particular sequence of steps.
&nbsp;
5.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Conclusion
&nbsp;
Because of these limitations OOP gains more focus and importance with its evolvement. OOP allows development of new components from pre-existing reusable components.&nbsp; This prevails a culture among programmers develop reusable software.
&nbsp;
&nbsp;
References:
Dr. K R Bond (2000), http://www.educational-computing.co.uk/OOPSample.doc [Online], (Accessed: 01 November 2014).
http://www.educational-computing.co.uk/OOPSample.doc [Online], (Accessed: 01 November 2014).
http://en.wikipedia.org/wiki/Structured_programming [Online], (Accessed: 25 October 2014).
	              
	              Compiled versus interpreted code
&nbsp;
Compiler - is the program that translates the high-level programming language to low-level or machine-understandable code. Compiler first parses and analyses the program to determine if all the syntax and semantics of the particular program is met. If so, then the code will be converted to machine executable format.
&nbsp;
Interpreter - &nbsp;is also the program that translates the written program to machine executable. The only difference here would be that, when the interpreter starts executing the program, it will read the program statements line-by-line (one after another) and then convert to machine code, and if any error is encountered, the program execution will be halted.
&nbsp;
&nbsp;
Difference between Compilers &amp; Interpreters (Compiler vs Interpreter, Programiz 2014):
&nbsp;
&nbsp;




Interpreter


Compiler




Executes program and translates one statement at a time.


Processes the entire program and translates it as a whole into machine code.




Analysis of the source code is very fast but the overall execution time is high.


Time taken to analyze the source code is high but the overall execution time is relatively low.




No intermediate object code is generated, hence Interpreters are memory efficient.


Generates intermediate object code which further requires linking, hence requires more memory.




Continues translating the program until the first error is met, in which case it stops. Hence, debugging is easy.


It generates the error message only after scanning the whole program.




Programming languages like Python, Ruby use interpreters.


Programming language like C, C++ use compilers.




&nbsp;

&nbsp;
&nbsp;
&nbsp;
Conclusion
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; The Car GPS example described in DQ 2, requires both Compiler and Interpreter programs. The main reason is that GPS devices are usually not updated with the latest map information, which will cause trouble if there is even a slight change in road plans, or if some road construction work is underway etc.,. In such scenarios, we need our GPS device to process the information as an interpreter and update the details. Having said that, if we design the GPS device only to interpret then, the GPS device would not be able to plan the route and direction and give us the required travel time in the first place. In a nutshell both Compilers and Interpreters play their roles and thus help in obtaining the better output from the program.
&nbsp;
&nbsp;
References
 Compiler - http://whatis.techtarget.com/definition/compiler (Last Accessed, 02-11-2014)
Compiler vs Interpreter - http://www.programiz.com/article/difference-compiler-interpreter (Last Accessed, 02-11-2014)
&nbsp;

Best regards, Kharavela
	              
	              Week 9 DQ2 - Compiled versus interpreted code
Live navigation
Pros

Up to date instructions- it offers real-time navigation services
Dynamic typing-user can change destination on the fly
If there’s an error, this is immediately picked up- when there is an error, the interpreter picks it up and its reported to the user immediately and further execution of the program is stopped
It is easy to learn and use

&nbsp;Cons

Slow, as it interprets line by line
Because it’s in real time, user needs an internet connection, if using a smart phone for instance, the connection is via the network provider’s towers and there could be latency issues in some areas where the towers are too far (rural locations)

&nbsp;
&nbsp;GPS
Pros:

It is s faster as the overhead to read and go through the code is only encountered once; at the beginning
User only needs to install updates on e.g. a weekly basis and thus they are able to use it even without an internet connection

&nbsp;
&nbsp;Cons:

If there’s an error it is only revealed at the end as compilers run the analysis reports at the end
There is no dynamic editing

&nbsp;
&nbsp;&nbsp;




Compiled code

Advantages

Compilers are faster compared to interpreted code
Stand-alone executables can be distributed
Compiled code is more secure
&nbsp;

&nbsp;
&nbsp;
Disadvantages

More complex 
requires a compiler
takes longer to edit and deploy the code compared to interpreters
it is difficult to write compilers



Interpreted code

Advantages

Relatively easy to learn and use as instructions are typed into a text file which the interpreter then runs without requiring a linker
Fully interpreted languages are more secure
It is platform independent
dynamic typing
easy to debug as it easier to is easier to get source code information in interpreted languages
the programs are smaller and flexible as there is an option to choose the instruction 
error reporting is instantaneous and this makes debugging and problem fixing easier

&nbsp;
&nbsp;
Disadvantages:

Poor performance as each line of code is interpreted one at a time
Presence of an interpreter is required on the system at run time

&nbsp;
&nbsp;




&nbsp;
References:
[Online] http://www.programmerinterview.com/index.php/general-miscellaneous/whats-the-difference-between-a-compiled-and-an-interpreted-language/ accessed 1 November 2014
[Online] http://www.codeproject.com/Articles/696764/Differences-between-compiled-and-Interpreted-Langu accessed 2 November 2014
[Online] http://www.dsbscience.com/freepubs/start_programming/node6.html accessed 2 November 2014
&nbsp;

	              
	               
Hi Craig,
 
Good observation! I get what you are saying. And you are right, both can have a property for smelling but that works differently. It really comes down to how you design your Object or Class, as I mention in my post the effort to design a class could be considerable. I could define a Class Cat, but if I design it more generic and detailed at the same time then perhaps I should define a Class Mammal. In terms of design you are free to choose what you do. However, I would say that if you are coding a program to help people choose what type of cat they like, you wouldn’t necessarily need to define a Class Mammal, the Cat one should suffice.
 
But to your point, I know that you are not familiar with any specific language, so let me create a simple class in pseudo code for mammal and build two objects as you described:
     



 
Class Mammal {
 
&nbsp;&nbsp;&nbsp; Private Function Smell (SensedMolecule) { &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (Split SensedMolecule &nbsp;and then Send the left brain the resulting stimuli) &nbsp;&nbsp;&nbsp; }
 
&nbsp;
 
&nbsp;&nbsp;&nbsp;&nbsp; Public Procedure Sense (MyMolecule) { &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ( &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Do very complicated stuff to sense… &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Smell(MyMolecule) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; )
 
&nbsp;&nbsp;&nbsp; } 
 
}
 
 Class HomoSapiens extends Mammal {
 
&nbsp;&nbsp;&nbsp; Public Procedure Handle (MyObject) { &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (do some handling with hands)
 
&nbsp;&nbsp; &nbsp;}
 
}
 
 Class Cat extends Mammal {
 
&nbsp;&nbsp;&nbsp; Overrides&nbsp;Private Function Smell (SensedMolecule) { &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (Send SensedMolecule to left brain, wait, then send brain response to the right brain) &nbsp;&nbsp;&nbsp; }
 
}
 
&nbsp;
 
MyBioProgram {
 
&nbsp;&nbsp;&nbsp; MyMolecule ← FishSmellMolecule
 
&nbsp;&nbsp;&nbsp; OneMan ← New HomoSapiens
 
&nbsp;&nbsp;&nbsp; MyCat ← New Cat
 
&nbsp;&nbsp;&nbsp; OneMan.Sense(MyMolecule)
 
&nbsp;&nbsp;&nbsp; MyCat.Sense(MyMolecule)
 
}
 



 
&nbsp;
 
The example is very simplistic, but it should give the proper idea. As you can see both Man and Cat have a Sense procedure, they are both equal (they sense the same way), but they have different Smell functions which are private (encapsulated). In the pseudo code I put the keyword Overrides to make it simpler to understand that we can have a function with the same name that overrides the inherited one. In some programming languages you do not need to state it explicitly.
 
In this scenario the encapsulation can be seen by looking at the objects OneMan and MyCat. OneMan cannot access the smell function of MyCat: you cannot do something like OneMan.MyCat.Sense nor something like OneMan.Sense(MyCat.Smell(MyMolecule)), that will generate an error as the Smell function is private. At best,&nbsp;you could see that as&nbsp;man "forcing" a cat smell something. But that looks better, albeit still not 100% correct, this way: OneMan.Handle(MyCat.Sense(MyMolecule)). In fact the MyBioProgram does not even know that OneMan has a different Smell function than MyCat. In conclusion they both can smell, but they smell differently and they cannot access each other smell functions.
 
Best Regards,
 
Augusto.
 
	              
	              Pros and Cons of Compiled vs. Interpreted Languages
&nbsp;
Anything that can be done using compiled language can be done through interpreted language. However both of them have advantages and disadvantages.
&nbsp;
1.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Compiled Languages
In a compiled language, a program once compiled can be expressed as target machine instruction set. For example, a multiplication “*” operation in source code can be translated as “Multiplication” instruction in machine code. Figure 1 illustrates process of compilation of high level languages
&nbsp;

Figure 1: High level language compilation
Advantages of Compiled Higher Languages are 

Hardware details abstraction.
Easily portable to other platforms.
Tailored for specific types of applications e.g. logic and web programming.
Achieve faster performance by directly using target machine native code.

•&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Powerful optimizations during the compile stage.
&nbsp;
2.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Interpreted Languages
In interpreted language, target machine does not execute instructions directly but some other program is responsible for reading and execution. This other program is written in native machine language. For example, “*” operator will be recognized by interpreter at run time which then calls its function “multiple(x, y)” with arguments which will then execute machine code instruction “ADD”.
Advantages of interpreted languages are

Easy to implement.
Compilation stage is not required.
Faster program development but messy because of on the fly creation and un typed variables.
Code portability
More powerful

Disadvantages of interpreted languages are
1.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Slow execution
2.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Not appropriate for large software projects. Generates linking issues.
3.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Comparison between compiled and interpreted languages
&nbsp;
Figure 2 illustrates diagrammatically the difference between compiled and interpreted languages.

Figure 2: Difference between compiled and interpreted languages
However, there is always tradeoff between advantages and disadvantages of compiled and interpreted languages.

Figure 3: Programming Tradeoffs
Driving navigation example:
You take a holiday to a city you have never visited before. You know someone there, and decide to pay this friend a visit. You hire a car, but now you need directions.
Option1: You use your mobile phone and get live help (directions) from your friend during the entire drive.
Pros:

You get correct address from your friend even if he is relocated to some other area.
Your friend can guide you about the shortest path as per his experience.
He can inform you about alternate paths if road is closed for maintenance.
If you are not able to find address, you can tell him the landmarks near you and he can take you from there.

Cons:

You can hit traffic if there is road accident recently and your friend is not updated.
There are chances that you are not able to explain correctly about your current location thus leading towards wrong direction.

Option2: You use a GPS device to give you both directions and traffic reports. However, you remember that GPS calculates directions based on traffic conditions, and updates on said conditions usually lag about 10 to 15 minutes behind a traffic incident (e.g., traffic jams and accidents).
Pros:

Your current location is correctly identified.
Visibility to alternate path in case of traffic on road or accident.
It can inform you about alternate paths if road is closed for maintenance.

Cons:

Face problem in case the map is not upgraded.
You can still hit traffic if the gps connectivity is slow.
You can miss turns if you are not used to gps system.

&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;
References:
http://en.wikipedia.org/wiki/Interpreted_language [Online], (Accessed: 02 November 2014).
http://nosleeptilbootcamp.com/compiled-vs-interpreted-languages-pros-and-cons/ [Online], (Accessed: 01 November 2014).
http://birg.cs.wright.edu/cs480/slides/02%20Compilation.ppt [Online], (Accessed: 01 November 2014).
	              
	              Hi Craig
I read you / understand.
Thanks for answering.

Bw Bo
	              
	              Introduction
Programming languages means a set of instructions with logic structure. They are applied for developing software and control hardware. Generally, Programming languages are categorized into two major kinds, which are compiled languages and interpreted languages. The accurate difference between the both languages is usually unclear and ill-defined, where certain languages are able to be compiled and interpreted, as there are existing grey areas that are admixture of the compiled and interpreted. All programming languages in the end are converted into machine code, which is the fundamental instruction set adopted by hardware.
Overview
Programmers adopt programming language to write the programming code, what is known as source code. The source code will finally be converted into a specification suitable for the software or hardware settings, what is termed machine code. Compilation is often particular to a kind of hardware construction, that implies the code will merely process on the hardware class for what it was compiled. Once a language has been compiled, the source code is probable kept as-is. Or compiled partially; merely when the source code is really run is it interpreted. This procedure is also known as just-in-time compilation.
Merit of Compiled Languages
The major merit of compiled languages is speed performance. Since the programming code have always been converted into a specification that is favorable for the particular software or hardware settings. Less procedure is demanded to implement the application, and thus the code can be executed with more efficient. The instances of the most general compiled languages are C++ and C.
Merit of Interpreted Languages
One of the main merits of interpreted languages is the capability of this type of languages to be implemented in spite of the hardware construction they are running on. For example, these languages are able to be neglected whether they are running on what the operating system is. One of the well-known interpreted languages is JavaScript, which is commonly applied to develop WebPages, is always able to execute the same no matter what Internet browser or operating system is being utilized. 
The comparison 
As the exact discrepancy of compiled languages and interpreted languages is tiny, to compare the both languages is most relied on the particular implementation of the compiler and interpreter, there are only able to compare the compiled languages and interpreted languages in an extremely common terms; even if the exceptions will be existed in some special cases, but the following are true in general: 

Compiled languages possess the advantage of speed as they converted directly into the machine code of the particular software or hardware settings.
Interpreted languages are often more portable as well.
Interpreted languages are familiarly simpler to develop due to compose compilers is quite troublesome.

Conclusion
Interpreted languages are the computing languages its source code is handled by a software application known as an interpreter which loads and interprets during the text and straightway conducts upon the commands instructed by the text. Compiled languages are the computing languages its source code is handled by a software application known as a compiler which compiles the source code into a machine code that is able to execute by a computer operating system directly or indirectly. Moreover, there is also existing admixture of both languages, known as hybrid languages. Such as Java, which is able to be compiled into portable code that then have to itself be executed by an interpreter known as a virtual machine. Because Java source code itself have no an interpreter, it is no doubt to deem Java to be a compiled language. Conversely, Python source code is also able to be compiled into portable code to be interpreted by a virtual machine. But Python source code is also able to be read by an interpreter. It is thus certainly to deem Python as an interpreted language. 
&nbsp;
References
&nbsp;
Harold Abelson and Gerald Jay Sussman with Julie Sussman (1996); Structure and Interpretation of Computer Programs (second edition); Available on: (http://deptinfo.unice.fr/~roy/sicp.pdf) (Accessed on: 02-Nov-2014)
&nbsp;
Manikandan10 (2013); Differences between compiled and Interpreted Languages; Available on: http://www.codeproject.com/Articles/696764/Differences-between-compiled-and-Interpreted-Langu) (Accessed on 02-Nov-2014)
&nbsp;
Tevfik Koşar (2006); Programming Language; Available on: (https://www.cct.lsu.edu/~kosar/csc4101-spring06/Lectures/02-Compilation.pdf) (Accessed on: 02-Nov-2014)
&nbsp;
Stephen Olivier (2009); Compilation and Interpretation; Available on: (http://www.cs.unc.edu/~olivier/comp524/Lecture02.pdf) (Accessed on: 02-Nov-2014)
&nbsp;

	              
	              Hi Numan
Did you forget to insert the figures via the "Insert/Edit Image" button above?

Br. Bo
	              
	              Compiled versus interpreted code. &nbsp; Programming languages in general fall into two distinct categories of ‘Compiled’ and ‘Interpreted’. Compiled language is code that has been converted and saved as machine language in opposition to interpreted language which is saved in the format that it was originally coded or entered as. “An interpreted language is a programming language for which most of its implementations execute instructions directly, without previously compiling a program into machine-language instructions. The interpreter executes the program directly, translating each statement into a sequence of one or more subroutines already compiled into machine code.” Wikipedia, A (2014). &nbsp;“A compiled language is a programming language whose implementations are typically compilers (translators that generate machine code from source code), and not interpreters (step-by-step executors of source code, where no pre-runtime translation takes place).” Wikipedia, B (2014). Some commonly known programming language examples of these can be seen below:     Interpreted   Compiled     Java   Ada     Perl   Basic     Python   Cobol     Ruby   Delphi     VBScript   Fortran     &nbsp; So the example shared to us, taking a trip on holiday to a place you have not visited before and remembering that a friend now lives here can be discussed. Without selecting a specific programming language I will describe the difference as pseudo code style numbered bullet point steps. I’m also using an old GPS which doesn’t have any AI or traffic updates. I still own one of these early GPS systems and all it does is tell you the route to take and does not know about traffic jams etc. &nbsp; &nbsp; Compiled  Start from current location A Continue forward onto road 1 for 1 mile Turn left at junction onto road 2 and continue for 2 miles Turn right at roundabout onto road 3 and continue for 0.5 miles Final location B on right.     You can see from above that to get from start location A to finish location B it is very specific and rigid because it is already compiled as machine code and is simply executed. It knows your start location it knows the connecting roads to the final location so sticks to the steps it has been programmed to follow, but what if there is an accident on road 1 blocking the road or what if there is road works at the roundabout not allowing anyone to turn right onto road 3? Because the journey was from A to B it already has its steps planned to be followed. This type of system can be beneficial at times because it is based on sound pre programmed and known solutions but it doesn’t allow for any flexibility. What if the date and time the code was programmed the roads was fine but now they have built a new highway in the last few weeks and blocked the old road? The system won’t know of it because it is already previously programmed months or years earlier based on old data. &nbsp; Interpreted &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; You don’t use a satellite navigation system but instead make a phone call to your friend to ask for instructions as you are driving.  Hi John can you tell me the best way to get to your house. Where are you now? I’ve just come off the ferry with my car so I’m at point A Ok, so continue forward onto road 1 for about 1 mile until you reach a junction. Ok John I’m half way down the road but there is a delay or accident ahead. Ok take the side street to your left now road X then follow the road around to the next junction. Ok I’m at the junction now Turn left this has cut out the incident and your now back onto road 2. Continue ahead up to the roundabout but because there are road works you can’t turn right onto my road. Ok I’m at the roundabout now Go straight over for 100 yards and take your next right onto road Y Ok I’ve just turned onto road Y Continue ahead for 0.5 miles and turn right at the end my house will be 100 yards in front of you with the red door. Ok thanks   &nbsp;  The reason I’ve highlighted it this way is because interpreted code as it does each task it then compiles to machine code in real time the next task or step. So instead of simply having a set numbers of steps and directions it can adjust due to stimuli along the way. So you can have many alternative routes programmed in or in this example talked through on the phone with your friend. Interpreted systems as an example will be slower and may not be specifically exact. What if your friend sends you down the wrong route or you don’t hear his instructions clearly enough or turn left instead of right. &nbsp; &nbsp; Conclusion For this real world example you’d probably do a combination of both. No doubt you would know beforehand that you were going to this place and however long before you started the trip would plan in some way or another the locations you are going to visit. Satellite navigation is also far more intuitive now with systems that can give traffic updates and know alternative routes that you can select but for this example we used it as a set route compiled example. Maybe your friend comes to meet you at the station to help you navigate the traffic if they know it’s a difficult route to take, and maybe the journey is more or less difficult to simply wing it on the fly. The main thing to remember is that compiled code tends to be quicker simply because it is already in machine code and is read to simply execute and interpreted while slightly slower in operation because it compiles and executes at real time. Interpreted code tends to be more flexible and agile allowing for real time abstraction if you will. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Many systems use both, were you will have compiled set ready to run events with the ability to use interpreted optional functions. &nbsp; References: Brookshear, G. (2011) ‘Computer science: An overview’. 11th ed. Boston: Addison Wesley/Pearson. Vanguard , ‘Compiled vs Interpreted Language’ [http://www.vanguardsw.com/dphelp4/dph00296.htm] (Accessed: 30/10/2014) Varoon, S ‘Compiled versus interpreted language’ [http://www.programmerinterview.com/index.php/general-miscellaneous/whats-the-difference-between-a-compiled-and-an-interpreted-language/] (Accessed: 30/10/2014) Wikipedia,A (2014), ‘Interpreted Language’ [http://en.wikipedia.org/wiki/Interpreted_language] (Accessed: 30/10/2014). Wikipedia,B (2014), ‘Compiled Language’ [http://en.wikipedia.org/wiki/Compiled_language] (Accessed: 30/10/2014). 
	              
	              Compiled versus interpreted code

 Let me first summarize the cons and pros of the driving navigation alternatives in Table 1.





&nbsp;


Pros


Cons




Friend Help


Easy to follow  (your friend will direct you properly as he knows the city, you interpret his directions). Can be followed independently of the type of car.


It could be slow to perform (step by step, it may run the full journey, you may execute instructions multiple times).
Not easy to optimize  (you depend on your friend optimization, have to translate instructions from your friend to the real landscape you are viewing; for another journey the instructions will need to be repeated and interpreted again).




GPS Help


Fast execution  (Select destination, press a button and follow).
Already Optimized (best route is calculated based on traffic conditions from the past 10 minutes and it is optimized for the target City)


Require a particular device (GPS).
Editing or changing the GPS specifications is harder  (imagine the GPS map has a wrong road direction, getting that corrected could be hard since you do not know the city, or the GPS). &nbsp;&nbsp;
&nbsp;





Table 1. Driving navigation alternatives pros and cons.

I will argue here that these Pros and Cons are exactly the same as the ones of compiled vs. interpreted code.

But first, let me define these types properly. Compiled code is code written in a high-level programming language that is translated into low-level machine language (Mogensen 2010a). Once the code is compiled the program is tied to that specific machine hardware. Interpreted code may also be written in a high-level programming language, but instead of compiled code, and the interpreter evaluates/executes at run time the written statements and/or expressions (Mogensen 2010b).

The first thing that I see here is that compiled code is ultimately tied to a particular machine while interpreted one could run on any computer as long as there is an interpreter. Thus interpreted code can be executed independently of the hardware. The second thing that I notice right away is that compiled code requires a particular compiler (so that it can compile for that device).

During the interpretation of the code, there could be sections that may be executed multiple times. For example, the body of a recursive structure (while ... do ...), this eventually lead to slower performance in interpreted code. Whereas compiled code optimizes the code so that it gets executed more efficiently, thus compiled code also executes faster. Thus interpreted code is not easy to optimize since it will always need to be translated into machine language at every execution.

Compiled code is not easily readable (since it is compiled to be read by a machine), it will not run until it is compiled; hence it is not easy to debug or test. If I wanted to debug a compiled program I would need to go back to the source code, change it and compile it again. I will need to keep doing that until the program works as intended. Hence editing or changing the compiled code is harder, and it requires access to the source code. In contrast, managing interpreted code is an easier task as all I need to do is change the source code and execute it (since interpreted code is never compiled, I have access to the source code).

As I have shown, with the aid of the colored words, the driving navigation example pros and cons are the same as the compiled and interpreted code ones. Hence, this is a good analogy to explain these types of codes.

I would like to add here another analogy, which is very easy to understand: imagine you are at the United Nations, and you have to talk with a representative of another country. If you can speak in that person, native language (required knowledge of it), working with this representative would be much faster and efficient than rely on the services of a translator (interpreter). Funny note: you cannot easily change that language (that is: the encoding of the language is fixed). However, if you do not speak that language, an interpreter would be your easiest way to go (albeit, slower). You can also leverage different interpreters’ services to talk with someone else in another unknown language (independency).

Best Regards,

Augusto Schoonewolff.

References

Mogensen, T. Æ (2010a) '1.1 What is a compiler?' in Basics of Compiler Design, 2010th edn, Department of Computer Science, University of Copenhagen, pp. 1-2.

Mogensen, T. Æ (2010b) '1.3 Interpreters' in Basics of Compiler Design, 2010th edn, Department of Computer Science, University of Copenhagen, pp. 3-4.

&nbsp;

&nbsp;

	              
	              STRUCTURED AND OBJECT-ORIENTED PROGRAMMING
INTRODUCTION-
The use of the term object and oriented was first used in late 1950’s and early 1960’s at massaccusette institute of technology (MIT). Toney Hoare’s record classes in the programming concept of simula 67 created in 1967 was the main influence to the use of object-oriented programming. The language used an automated garbage collection which was inverted earlier for the functional programming language LISP. Simula 67 later influenced many programming languages such as c++, object pascal, smalltalk etc.
On the other hand, structured programming provides a theoretical basis of structured programming. It state that three ways for combining programs- selection, sequencing and iteration are good to express any computable function.
DEFINITIONS-&nbsp; &nbsp;
OBJECT-ORIENTED PROGRAMMING - This represents the concepts of objects which have datafields and associated procedures called methods. This objects that are abstracts data types are used to interact. Object oriented programming integrates codes and data using the concept of objects. An object has two states which are data and behavior code. Examples of object-oriented programming are: c#, c++, object-c, smalltalk, Delphi, java, python, perl, ruby, php
STRUCTURED PROGRAMMING &nbsp;This is a software application technique which follows a top down design approach with block oriented structures. This type of program has the implementation of the source code being processed in the order in which bits of the code have been typed in. However, a structured program is made up of three elements namely: selection, sequence and iteration.&nbsp; 
KEY PROPERTIES /CONCEPT (OOP)-
The key concepts for object-oriented programming include:

Objects – This are the class instances which are used for real functionality.
Class – This can be defined as simple building block which has a single entity and has both data and operation.
Data Abstraction – This is a method that allows us to isolate how a compound data object is used. It gives us details of how it is constructed from its primitive data objects. Data abstraction is a simplifying complex reality by modeling classes appropriately to the problem and working at the most appropriate level of inheritance for particular type of the problem.
Encapsulation- this involves joining data and codes together in a place and hiding the data from outside sources hence forcing anyone that wants that information to get it from the related code.
Inheritance- Here, an existing class has the ability to create new classes. The name given to this existing class is called the base class.
Polymorphism – This means one name, many forms. Here, objects of different types respond differently to the same function call.

KEY PROPERTIES / CONCEPT (STRUCTURED PROGRAMMING)-

Selection – Here, a number of statements is executed depending on the position of the program. Example of key words include: else, if , what etc.
Sequence – This are subroutines executed in a sequence.
Iteration – Here a statement is executed until the program reaches a certain state. 

ADVANTAGES OF OBJECT-ORIENTED PROGRAMMING–

Object oriented programming gives a good framework for code libraries where supply software components can be easily adapted and modified by the programmer. (useful for developing GUI)
In Object oriented programming it is easy to maintain and adjust existing codes as new objects can be created with small differences to existing ones.

DISADVANTAGES OF OBJECT-ORIENTED PROGRAMMING-

It emphasizes one aspect of software design (data and object) leaving out other aspects (algorithm/computation)
It lacks reusability and modularity.

ADVANTAGES OF STRUCTURED PROGRAMMING-

It is easy to modify and easy to debug
Less time is involve in writing the program
Different programmers can work on different modules at the same time.

DISADVANTAGES OF STRUCTURED PROGRAMMING-

Data types cannot be created easily.
Large programs become very complex no matter how it is being structured.

Moreover, OOP can be compared to structured programming in that later is suitable for sustaining large software and web projects while the former involves massive codes. OOP is good for large projects and long term maintenance hence it tends to be more complex than structured programming.
In terms of performance, OOP programs are slower than structured programming. Also, in object oriented programming the interpreter has to be taught how the compiler works but in a structured program there is a structured method of a top down sequence. 
Furthermore, it is easy to read and understand the source code files in the structured programming because it is written in a straight forward manner while in the OOP it is difficult to understand. In the OOP it takes more time to implement the classes or complex functions but in the structured programming it saves time when writing simple programs.
CONCLUSION-
Object-oriented program is distinct from normal procedural programming because it has its own up’s and down likewise structured programming but it is more preferred because it is useful in making big modular programs but this programs will have to be structured in a very similar way to OOP structure.
REFERENCE:
Andrew, H (2000) what is object oriented programming[Online] Available from:https://www.duramecho.com/computer information/whatisobjectorientedprogramming.html. (Accessed 01 Nov, 2014)
Asagba, P. &amp; Ogheneovo, E. (2007) A comparative Analysis of structured and object-oriented programming methods [Online]Available from:https://www.ajol.info/indexphp/jasem/article/view/55190. (Accessed 01 November 2014)
Obbayi, S. (2010) structured vs. object-oriented programming: A comparison[Online] Available from:https://www.brighthub.com/internet/web-development/articles/82024.aspx. (Accessed 2 November, 2014)
Shiow-yang, w.(n.d)object-oriented concepts[Online] Available from:https://www.csie.ndhu.edu.tw/showyang/00P2012/0200concepts.pdf. (Accessed 2 November 2014)
www.wikipedia.org/wiki/object-oriented_programming
www.wikipedia.org/wiki/structured_programming
&nbsp;

	              
	              hi Terry,
you mentioned you had seen books who try to teach kids how to learn OOP and that you were sorry for them- why?
Regards
martins
	              
	              
Object Oriented Programming (OTT) describes objects act as different units that interact with others. The data layer is abstracted and an implementation in it simples form involves messaging between sending and receiving objects. Data abstraction, inheritance, encapsulation and Polymorphism are the paradigms of OTT. 

In the OTT approach there are classes of objects which can inherit behavior and attributes when they are spawned (inheritance). Objects are closed entities and keep their code and data secret (encapsulation). It only knows how to interface with each other using their messages to send or receive commands or return values. Two or more objects can respond to the same message (Polymorphism). 

For example a School object asks a Student object for a name by sending a message asking a name. The Student object can return that name. The object School does not know what the object Student consists of only that it can ask the question. The object Student can be copied to cover more students. The object school can send the question to more objects 

There are multiple perceived preferences for using the object oriented method with relation to structural language approaches. This is not a fixed list as protagonists of the other approaches tend to have a different view. In the following we will discuss some potential advantages and some counter arguments. 

Potentially OOP can reduce maintenance and reliability. Encapsulation of processes ensures that objects can be reused without the danger for overlapping naming or data exchange. Operations are defined stand alone and when applicable it makes it easier to change things in one branch as the alteration does not conflict other branches. Also in structural language approaches one can make sure that a copy does not create a conflict by for example renaming all variables. 

But one could say that it is easier to reuse code in OOP which makes it also more flexible to use. The inheritance attribute of OOP, as explains above, allows to copy data attributes and characteristics when a new object is spawned from existing classes or when it participates in a superclass. Furthermore polymorphism allows the reusability of messages as two or more objects can respond to one message which improves. The modular approach of creating objects and being able to all and address them at any time improves flexibility. 

Even if OOP is build around objects one could easily say reusability of code is as easy in traditional systems. Even though one can re-use for example classes one can also reuse algorithms in descriptive languages. 

OOP work well for dynamic, interactive environments and less in big information system applications. Design systems for example benefit from and OOP approach but it is questionable if for example CRM or accounting software developments are helped with it. In consequence OOP is not accepted by major vendors.

OOP is probably the best approach for modeling as it's ontology consists of objects instead of data and processing. Like one would do building a model in the real world, also using OOP a model is build by describing the possible classes of objects and their behaviours. 

Therefore, in relation to natural language, it could be said that OOP is easier to use as a model and makes the object and messages approach aligns with many aspects of the language. Prolog on the other hand is the language of choice for a logical linguistic representation. In the end all sentences can be translated into logical representations. 

On the other hand natural language uses ambiguous terms all the time. Logical linguists and Artificial Intelligence programmes try to mimic this ambiguity by following for example brain function resemblances. Like our brain cells are thought of performing in a distributed manner of synapses firing signal to other cells, one could also set up a self learning programming model containing sets of 'beliefs' and relations between them. 

As we have seen OOP performs well as model building language with messages moving from sending objects to receiving objects. These easily copyable objects represent individual cells that all behave the same way and have the same attributes but contain a different dataset. The dataset can be changed by the receiving messages and in such a programme linguistic ambiguity and it's defining self-learning component could be modeled using unambiguous languages.

Brookshear, G. (2012), Computer Science: An Overview, Pierson Education Inc. publishing as Addison-Wesley, Boston, United States, Version 11. ISBN 13: 978-0-13-256903-3. pp 244-288

&nbsp;

Kohr, K, Chavis, N, Lovett, S, White, D (2014). What is Object-Oriented Programming? [Online]. IBM Available from http://www.inf.ufsc.br/poo/smalltalk/ibm/tutorial/oop.html. (Accessed on 01 November 2014).

Burleson. (2014). Advantages and Disadvantages of Object-Oriented Approach [Online]. Burleson Consulting.. Available from: http://www.dba-oracle.com/t_object_oriented_approach.htm. (Last accessed 26 October 2014).

&nbsp;

	              
	              Hi Babatunde,
Python is a simple yet powerful language. In my opinion, there are a few things Python doesn't handle "very well".
Explicit: There's a set of Python language design philosophy, called the "Zen of Python". One of the "Zen" is "Explicit is better than implicit". This for sure makes things very clear, it's easier to trace what is from where. It especially makes the life of new programmers easier, because there is less "magic". But explicit also introduce some verbosity. E.g. The class mthod definition:
class A:&nbsp; &nbsp; def a_method(self):&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; self.member = 1&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; return self.member
In the above code, although a_method doesn't have any real parameter, you still need to pass the self and there's no implicit memeber access. Talking about implicit, in very implicit language like Ruby, you don't even need that return in the code! Ruby would assume you want to return the result of the last calculation - otherwise why do you bother calculate it.
Too easy to create a mess: Python as a very dynamic language, is very easy to create a big amount of mess quickly. One of my colleagues used to ask a startup company:"Is 'go fast, and die young' a cultrue of Python?" Another piece from the Zen of Python is "There should be one-- and preferably only one --obvious way to do it." And I think that one is a "lie" most of the time. I've often seen people using many ways to do the same thing, and most of them are wrong.
Package management: The pakcage management is quite "foreign" compare to the rest of the world. The others like Ruby Gem, NodeJS NPM, etc all follow the similar style. But Python has it's unique way of doing it.
Version 2 and 3: Python 2.7 was very successful, and many people don't like version 3. If you go to a Python conference, you can feel that people are very emotional about this. This actually confused people who want to learn Python. "Should I learn 3 or 2?" I would say 3, although I use 2 all the time and only run test in 3 to check the compatability.
This list could go on and on and into some details, I feel you might be bored with it. So I just stop. Hopefully, this doesn't decrease your interest in Python:P
br, Terry
	              
	              Hi Martins,
It would be very useful if the kids learn the "object-based programming", because that's the easy, fundamental and useful part of most general-purposed dynamic programming language.
OOP has a lot more than object-based programming, and things like multiple inheritance, polymorphism, etc, etc, are very hard to understand even for collage students. And the benefit you get from these are very limited.
There are many things you can teach a kids in programming, OOP is nearly at the end of the list.
br, Terry
	              
	              Dear Dr. Ayoola,
Yes, I'm one of them:-) I think there should be "No Reuse Before Use".
Kvlin Henney in his&nbsp;Simplicity Before Generality, Use Before Reuse (MH, 2009)

"People do not on the whole pay for (or need) generality: They tend to have a specific situation, and it is a solution to that specific situation that has value."

Speculative generality will introduce unnecessary complexity to the system. Good design should "emerge" from the actual use. Just ask Kent Beck suggested: "make it work, make it right, make it fast".
If you have something that works to serve the specific needs well, you may then see duplicates in the solution. Extracting the duplication will result in reusing some of the code. And a good software design should follow the DRY, or Once And Only Once principle.
So the so-called software reuse should be just the side-effect of proper software use.
Back to the topic of OOP. Object-oriented design should represent the coupling of the working software. It's quite often counterintuitive if you "listen to the source code" rather than "listen to your speculation".
Finally, as master Yoda used to say: "Luke, use the source!"
br, Terry

References:
http://www.slideshare.net/TerryYin/no-reuse-before-use
Monson-Haefel, R. (Ed.). (2009).&nbsp;97 things every software architect should know: collective wisdom from the experts. O'Reilly.
	              
	              Week 9 DQ2 – Compiled versus Interpreted Code
Babatunde KOLAWOLE – 2 November 2014
Compiled vs. Interpreted Languages
Compiler:&nbsp;Compilers are used to convert high level languages (like C, C++) into machine code.
Example: gcc, Microsoft Visual Studio
Interpreter:&nbsp;An interpreter is a computer program which executes a statement directly (at runtime).
Examples: python, LISP
Compiler converts source code to some kind of intermediate form. For static language, a compiler usually converts the source code to assembly, which usually did not get stored to disk, then the assembler is invoked to convert the assembly to binary code, which is usually stored as object file(.o or .obj suffix usually), then linker is invoked to link object file(s) to binary executable. Also it is common to refer to this whole process of compiling, assembling, linking as compiling.
 

A GPS requires manual input of destination, this simply means that nothing would work perfectly until you key in decide your desired destination.
A GPS gets you to your destination quicker without any disturbance. Sometimes, without technical help, I get lost quickly. But my Navigation System gets me to my destination quicker without the stress asking a friend for direction or trying to figure it out on my own.
A GPS gives steady feedback on your progress. With my navigation system, I always know the street I am on at a particular moment, how far I must travel to the next turn, and how far to my major destination.
A GPS helps in getting someone back on track should in case you get off. Just for the fun of it, ii sometimes take occasional wrong turn. But the GPS system never chides me. It alerts me on what I need to do to get back on track.
A GPS send along a different route when there’s a roadblock. It is incapable of being avoided that you will encounter obstacles on the way to the destination you are aimed at. A good GPS is able to adjust on the fly and recalculate the route.
A GPS is not always conforming exactly accurate. This is not surprising. It’s a challenge for GPS databases to keep up with all the changes: new roads, closed roads, traffic accidents, etc.

Advantages of compiled languages:

Complied languages have faster performance by directly using the native code of the target machine
Complied languages have the opportunity to apply potency or effectual optimizations during the compile stage.

Advantages of interpreted languages:

Interpreted languages are easier to implement unlike writing good compilers which is very hard.
Interpreted languages can execute codes directly without running a compilation stage
Interpreted languages can be more suitable for dynamic languages

Compiler vs. Interpreter (Differences)

Compiler take the whole program as input while, Interpreter takes single instruction as input
(if any) errors are displayed at the end of the program check in compiler while, Interpreters errors are displayed for every instruction interpreted
Since object code is generated in compiler, memory requirement is more while, Interpreter memory requirement is less
Conditional control statement in complier are executed faster while, otherwise for Interpreter

Compiler vs. Interpreter (Explained)

Complete program (in human readable format) is given as an input to the compiler
These readable format undergoes several passes and phases of compiler (execution) and at the end it is converted into the machine readable format
However if errors were found during this process where interpreter take single line of code as input at a time, it will terminate the execution
In case of interpreter, when no object code is created, memory requirement is less.

I am involved in a GPS sport called "Geocaching" (www.geocaching.com).&nbsp; Essentially a high-tech treasure hunt, the gist of it is you go online, find a 'cache' you want to hunt, get the coordinates (Lat. /Long.), enter them into your GPS, and away you go!
The trick to geocaching, however, is to be able to observe your surroundings, and understand what it is you're looking at.&nbsp; Geocaches are sometimes very small, and at least after you've been geocaching for a while--you begin to see certain aspects of locations that may be giveaways to the cache location. There’s bump on a branch that looks a little out of place (may be a sub-micro cache), consisting of a mini-cylinder with a magnet, attached to the tree via a thumbtack.&nbsp; (These actually exist; a friend of mine has found 1 before)
Being Observant in Life is the metaphor here.&nbsp; As in Geocaching, being observant of one's surroundings, and knowing, from experience, where to look for one's own personal treasures, is the key to finding fulfillment, joy, and happiness! :-)
Geocaching gives me great joy, as it allows me to be outside and breathing fresh air, while using my brain a bit, as well.&nbsp; I find this combination suitable for my kind of happiness:&nbsp; athletic/cerebral.
I wish everyone here the joy in finding your own true joys and fulfillments!
References:
Stackoverflow; Compiled vs. Interpreted Languages [online] Available from: http://stackoverflow.com/questions/3265357/compiled-vs-interpreted-languages (Accessed on: October 31, 2014)
Stackoverflow; Pros and Cons of Interpreted Languages [online] Available from: http://stackoverflow.com/questions/1610539/pros-and-cons-of-interpreted-languages Accessed on: October 30, 2014
Compiler vs. Interpreter: Difference [Online] Programming Tutorials. Available from: http://www.c4learn.com/c-programming/compiler-vs-interpreter/ (Accessed on: October 31, 2014)
Laureate Online Education. (2014).&nbsp;Computer Structures – Lecture Notes&nbsp;Week 9: Programming Languages&nbsp;[Online]. Available from: https://elearning.uol.ohecampus.com/bbcswebdav/institution/UKL1/201480_SEPTEMBER/MS_CKIT/CKIT_501/readings/UKL1_CKIT_501_Week09_LectureNotes.pdf (Accessed 30 October 2014)
Brookshear, J. G. (2012).&nbsp;Computer Science An Overview.&nbsp;pp. 242. 11th&nbsp;ed. Boston Massachusetts: Pearson. Addison Wesley





	              
	              Hi Dr. Ayoola,
You could hire me. There has been several colleagues in our class complained about my code not being very readable:P
I'm kidding, of course. I should feel shame about it.
There's a few methods in my mind:

Put it only on your own server. If you only deploy the code to your own server, nobody else can see it.
Compile to byte-code. As Java and Python can be compiled to byte-code. You may deploy the byte-code only. But the byte code is still relatively easier to be reverse-engineered.
Uglify it. There's code that you have to share with the public, like JavaScript running in the client broswer. You can uglify it to make it harder to read. Another benefit of uglification (is that Engllish?) is to shrink the size of the code and save some band-width.
Open source it. As we talked before, it's a world of fast fish eat slow fish. Sharing doesn't hurt you, slowing down hurts you.

br, Terry

	              
	              Hi Craig,
IMHO, "many many" probably no. Most businesses have one or two major language, and probably another supporting language. E.g. C++ as major language, Python in testing.&nbsp;
But as the interface between languages are getting more and more flexible, collaboration between different languages became easier. E.g. the REST API doesn't specify any implementation language but talking to each other with HTTP protocol.
It's not really "transparent". Switch from language to language is very costly, probably the most costly change in software. But if your software design has built-in flexibility for replacement e.g. with REST, SOA, PaaS, etc, the cost might be significantly lower.
br, Terry
	              
	              Compiled code is a translated language to machine code that is specific for a type of computer. It a implementation into a low level language with codes that can be processed immediately by the target machine. In this process the source code tokens generated by the Lexical Analyser is pushed by the Parser using parse trees to the Code generator into the Object programme.
An Interpreted code is also an implementation of a language but the source code is not directly run on that machine but read and executed by another programme. Normally this involves a conversion into an intermediate code that is translated by the interpreter into machine specific code. 


Interpreted code example

We are driving through town and getting live commands and updates by an interpreter.

1 - Asking where to go (address)

2- Receiving answer (address)

3- Say where you are (address)

4- Receive directions&nbsp; (direction)

5- Progress report (address, state of traffic)

6- Receive directions (updated direction)

- 5 and 6 are repeated until location is reached

&nbsp;

Compiled code example

We are driving through town using a GPS navigation device

1 - Typing in destination Address (address)

2 - Object programme calculates OptimalRoute (list of connected points)

3 - Object programme receives UpdateTraffic (value) and calculates if change of OptimalRoute (list of connected points)

4- Object programme generates outputs as commands for the driver UseRoute(print screen) plus NexStep(play voice commands when street has to be changed) 

5- Object programme receives UpdateLocation(GPS location) and calculates if change of OptimalRoute(list of connected points)

Step 4 and 5 are repeated until a new UpdateTraffic &nbsp;is received then 3 is triggered again

&nbsp;

The result should be the same (arriving to the destination address) but most likely the route and the time it took to arrive are different. Differences are subject to the specific implementations of the interpreter or compiler. In our example the friend can give inside knowledge of the town or be a lousy advisor when it comes to driving through town. The compiler give accurate and quick responses to new information (GPS location and traffic info) but cannot negotiate quickly outside the parameters of the machine code. 

&nbsp;

More general differences are:

- Compiled implementations are faster because they translate directly to the native code of the specific machine and not as interpreted which runs every line one after the other; as in our example one does not have to wait for the interpreter to process new information as it is almost immediately available.

- Interpretive languages can be more portable; Perhaps the person next to you friend is much better at giving directions.

- You need a local interpreter installed: You need a friend to call...

- Interpreter languages are easier to write / understand of due to the programming language (Syntax and smaller executable programme size) and debugging process (Reflection, Dynamic Type Checking and Scoping) &nbsp;while as writing a compiler tends to be difficult. The phases of compiling involve lexical and syntax analysis, time checking, intermediate code generation, register allocation, machine code generation, assembly and linking.

&nbsp;

There are also hybrid uses of both Compiles and Interpretative languages. To overcome the restriction of slow programme execution of Interpretative languages 'Just in Time' compilation can be used by compiling frequently used sequences. 

&nbsp;

Brookshear, G. (2012), Computer Science: An Overview, Pierson Education Inc. publishing as Addison-Wesley, Boston, United States, Version 11. ISBN 13: 978-0-13-256903-3. pp 244-288

Mogensen, T. (2010) Basics of compiler design [Online]. DIKU, University of Copenhagen. Available from: http://www.diku.dk/~torbenm/Basics/, (Accessed: 01 November 2014).

Horton, I (2004). Ivor Horton's Beginning ANSI C++: The Complete Language [Online]. Google eBook. Available from http://books.google.ch/books?id=JrZwBeXMkg4C&amp;dq=compiled+vs+interpreted+programming+languages&amp;source=gbs_navlinks_s (Accessed on 01 November 2014).

Wikipedia. (2014). Interpreted language[Online]. Wikipedia Foundation Inc.. Available from: http://en.wikipedia.org/wiki/Interpreted_language. (Last accessed 01 November 2014).

	              
	              Hi guys,
Thanks Kharavela for using&nbsp;"method overloading" as the example of polymorphism. And that leads to an interesting question:
Is method overloading considered polymorphism? Or it's just a syntactic sugar?
br, Terry
References
Syntactic sugar. (2014, October 31). In&nbsp;Wikipedia, The Free Encyclopedia. Retrieved 03:15, November 3, 2014, from&nbsp;http://en.wikipedia.org/w/index.php?title=Syntactic_sugar&amp;oldid=631893281
	              
	              

I. Introduction
Both structured and object oriented programing (OOP) languages have been in existence for some time and continue to be in use today. 
This paper aims to examine structured and object oriented programing (OOP) languages. 
II. Key concepts of OOP and structured programming languages
Structured programming language, a third generation language introduced in 1962-1970, is based on a concept of providing sequence of instructions that are seemingly easy to follow and Pascal, C, are examples of programs that use structured programming techniques. The “supports modular programming” with each module consisting of subprograms. [1] 
With the type of programming an algorithm is developed to solve a problem, it uses a top-down approach to problem solving, breaking problems down into subproblems solving one problem at a time. This programming type has the following features:
 A. There is scope and visibility of data
 B. It supports nesting of subprograms
 C. Each procedure has its own local data and algorithms and is independent of other procedures
 D. There is passing of parameters involved
 E. There is a lot of control structures.
 F. “Procedural abstractions or functions abstractions are achieved, yielding abstract operations” [2] 
Object-Oriented Programing is a programming concept that focuses on the creation of objects rather than a series of instructions. It uses the bottom up approach to problem solving with each object having its own data structures and operation procedures. [3]
There is more emphasis on data than there is emphasis on algorithms as in structured programming. “Procedural abstractions is complemented by data abstractions” [4].
An object is an instance of a class.
An example of structured programming:
For (i=1, i&gt;=10)
(Action)
End for.

An example of OOP:
Void main ()
{
 For (i=1, i&gt;=10)
 (Action)
 End for.
}

III. Analysis OOP and structured programming languages.
Features of OOP
 A. Encapsulation: with this feature the details of a component within a class can be hidden from the rest of the system.
 B. Inheritance: just as a child inherits certain properties from his or her mother of father classes can be based on or can inherit or have properitie of other classes.
 C. Polymorphism: with this feature different functions can share a single feature or behaviour. For example a method to bake something can be applied to bake chicken, cake and steak which are objects.
 D. Delegation: this allows objects to delegate a method to another for example a child “sending requests to parent” [6]
Other features includes genericity, event handling, multiple inheritance, messaging passing and extensibility.

IV. Advantages and disadvantages of OOP and structured programming languages.
Structured programming languages is more suited for building small applications while OOP is suited for larger applications. While structured program languages may offer a ease of understanding and readability and may be easier to implement, OOP offers some of the following advantages over structured programming languages:
 A. Provides greater efficiency in writing code
 B. Reduces the chances of errors
 C. Because of its structure, it caters better for expansion as compared to structured programming, the structure is also more flexible as objects and their functionality can be easily added without the need to disrupt the entire program or code.
 D. With structured programing language data is executed in sequence while with OOP this is not the case.
 E. With OOP you can reuse data while with structured programming languages you cannot do this.
 F. It is possible to hide data in OOP while with structured programming languages this is not possible. OOP also provides greater security and ease of maintenance. 
These along with its added features, may be so of the reasoning behind the preference of OOP language over structured programming languages in the computing industry. 



References: 

Buyya, R (2009) Object-oriented Programming with Java: Essentials and Applications. Tata McGraw-Hill Education. [1] page 5, [2] page 6, [Online]. Available from:&nbsp;http://books.google.gy/books?id=rXGMFYXFDwMC&amp;pg=PA17&amp;dq=object+oriented+programming+and+structured+programming+language&amp;hl=en&amp;sa=X&amp;ei=JMpWVJj8CsLnsAT7wIFA&amp;ved=0CDMQ6AEwBA#v=onepage&amp;q=object%20oriented%20programming%20and%20structured%20programming%20language&amp;f=false (Accessed: 02 November 2014).


Puntambekar, A. A.(&nbsp;2008&nbsp;) Advanced Data Structures and Algorithms Pages 1-6 [Online]. Available from:http://books.google.gy/books?id=9mBJ0CpKXdkC&amp;pg=SA1-PA1&amp;dq=difference+between+object+oriented+programming+and+structured+programming+language&amp;hl=en&amp;sa=X&amp;ei=ScdWVK_2LfCQsQSy04KQBQ&amp;ved=0CD0Q6AEwBg#v=onepage&amp;q=difference%20between%20object%20oriented%20programming%20and%20structured%20programming%20language&amp;f=false (Accessed: 02 November 2014).


	              
	              Hi Augusto,
Excellent example for overriding and polymorphism. You example is actually using a Design Pattern, named Template Method Pattern. Did you do that on purpose? :-)
One minor problem is smell cannot be a "private" method. In most OO program implementation a private method is invisible to any other class, including child. What you need is probably a "protected" method, which is visible to its children.
br, Terry
Reference:
Template method pattern. (2014, October 19). In&nbsp;Wikipedia, The Free Encyclopedia. Retrieved 03:41, November 3, 2014, from&nbsp;http://en.wikipedia.org/w/index.php?title=Template_method_pattern&amp;oldid=630220248&nbsp;
	              
	              Bram,
OTT? I'm a bit confused. Could please explain what is it?
br, Terry
	              
	              
Week 9 Initial Post 2

Compliers are used to compile code and was mainly used to compile machine instructions to get a desired output while interpreters are used to interpret high level programing languages that we use today in many popular programing software.

The main difference between a complier and an interpreter is the fact that a complier may be designed to work with a specific machine architecture and if placed on another may give different results as such, there may be some level of tuning needed to get the compiler to give desired outcome. While an interpreter an perform effectively on any machine architecture.

Thus used in the analogy of a GPS system, an interpreter may work better as it can better understand the high level programming language use and can suit any environment. While the complier which may need tuning to get the right results may perform inefficiently.




References: 

Brookshear, J.G. (2011)&nbsp;Computer science: An overview.&nbsp;11th ed. Pages 241-242 Boston: Pearson Education / Addison-Wesley. 











	              
	              A structured programming language can be viewed as a logical procedure which operates on a sequence of steps to produce some result at the end. While an object-oriented programming language organizes objects rather than actions which can be developed into modules which do not need to change and can be called repeatedly. In this discussion we'll look at the subtle differences between the two languages with explanations and analogies to real world examples.
In summarizing the key concepts behind Object-oriented programming and structure programming languages we will list some of these concepts and provide examples where possible using real world analogies to best explain each of the concepts listed. Concepts behind object-oriented programming includes: &nbsp;objects, classes, Data abstraction &amp; encapsulation, inheritance and interface. &nbsp;While in structured programming the concepts includes: sequence, selection and repetition.
Object-oriented programming language concepts:

Object: &nbsp;An object is that thing that performs a set of related activities which defines its behaviour, in other words they have both state and behaviour. An object in this sense can be likened to a real-world object like a radio, which has states of "on" and "off",&nbsp; "volume control" and "tuning " to name a few.
Class: This is considered the mould or blueprint from an object is created. A good example is a new housing development. A model of the houses to be built is normally completed with all the amenities to give a perspective buyers a sample of the finish product. This model house would have all the features and attributes of the other houses will be built with.
Data abstraction &amp; encapsulation: &nbsp;This is the access restriction of to an objects internal properties which primarily is the methods and data . This means they are private and can only access by the object itself.
Inheritance: This is where attributes such as states and behaviours commonly used&nbsp; in a class is passed down to subclasses. Like in a family specific genes are inherited from one generation to the next, which is identified by the same attributes.
Interface: This is the contract between a class and the outside, through this medium interactions are defined.&nbsp; An example is the radio, you use the tuning knob to request of the radio a change in stations.

Structured programming language concepts:

Sequence: This describes ordered statements or subroutines the executed line by line.
Selection: This involves the usage of branching statements into some structured form. Statements such as the if(.....), then(.......), else(.......) or case.
Repetition: this is the iterative statements which defines recurring controls such as loops; for, while, repeat , etc. It executes until the program reaches a certain state.

&nbsp;
In analysing the object oriented programming&nbsp; (OOP)&nbsp; and structured programming languages we look at the advantages and disadvantages&nbsp; with some explanation and the use of examples.
"Structured programming language is laid out in a series of explicit statements and does not generally contain commands which redirect the execution of statements to other areas of the program. In comparison, object-oriented languages often contain a set of reusable modular components, which can be called to execute at various points throughout the program, without having to be completely rewritten. Some of the more popular object-oriented languages include Java and C++ ".
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (Janea Taylor - Computers are Fun!, 2005)
OOP is close to natural language and therefore is easier to implement when design programs using tools of this paradigm. While structured language can become complex with sequential statements which only execute one after the other. This attribute can be beneficial to simple programmes and the structure orderly approach provided by the structured language. In C and other second generation languages large programs can be broken up into smaller components and assigned to different programmers to complete and test with data then the pieces are put together to form the complete code. OOP has a similar advantage with its reusable objects such as functions and classes which allow for modularity and can be called at different time throughout the program, this attributes also allows OOP to have faster execution time in larger projects over the structured languages. OOP can be time consuming to write classes over structured&nbsp; languages and this makes it rather complex. When classes and functions aren't required, a simple orderly structured piece of code to execute the specific command will be of great advantage. 
Object-oriented languages has developed considerably and most of today's programming language or built on this paradigm. Lets' consider a complex database system for document management, it would be better if an OOP language is used to design such a system which will require complex integration of various objects with similar characteristics to be called and pass information from one object to another. This must be done at very fast speed keeping in-line with today's hardware capabilities and the ability to process information quickly and produce results timely.
Object-oriented languages does better reflect the human way of thinking. Let's examine problem solving techniques, in which a human may break up the problem into small pieces trying to understand it, work out solutions to those pieces and reassemble pieces into a solution for the problem. In the same way OOP allows a programmer to develop objects to solve a problem and allow characteristics to be inherited from one class to another to develop a solution, which can possibly be used to solve other similar problems. In essence, a logical, step by step approach with dependencies on objects already established.
In this discussion we identified the key concepts of both structured programming and OOP languages, the advantages and disadvantages of each language. We provided examples of OOP languages and explained the perceived preferences over structured programming languages.
&nbsp;
References:
Janea Taylor - Computers are Fun! (2005) Structured VS. Object-Oriented Programming [Online]. Available from http://janeataylor.wordpress.com/2005/12/02/structured-vs-object-oriented-programming/ (Accessed: 2 November 2014).
Brookshear, J. G. Computer Science: An Overview XML Vital Source ebook for Laureate Education, 11th Edition. Pearson Learning Solutions.
&nbsp;
Oracle (2014)&nbsp;Java™ Tutorials: Lesson: Object-oriented programming concepts&nbsp;[Online]. Available from: http://download.oracle.com/javase/tutorial/java/concepts, (Accessed: 2 November 2014).
www.codeproject.com (2014) Introduction to Object Oriented Programming Concepts (OOP) and More [Online]. Available from http://www.codeproject.com/Articles/22769/Introduction-to-Object-Oriented-Programming-Concep (Accessed: 2 November 2014).
Computer Structures - Lecture Notes (2014) Programming languages [Online]. Available from https://elearning.uol.ohecampus.com/bbcswebdav/institution/UKL1/201480_SEPTEMBER/MS_CKIT/CKIT_501/readings/UKL1_CKIT_501_Week09_LectureNotes.pdf (Accessed: 2 November 2014).

	              
	              In this discussion we'll identify the pros and cons of compiled and interpreted code. Using these concepts I'll attempt to provide an analogy to alternative situations, where I'm visiting a friend and driving directions are needed. In the first situation, driving instructions to your friend's house is being provided via telephone conversation while driving and in situation two, you acquire a GPS that would provide both directions&nbsp; and traffic updates.
Situation 1:
The pros associated with this situation includes:
Real-time support will be on the phone providing turn by turn directions.
It should be easy to follow instructions.
No prior knowledge of the area required. &nbsp;&nbsp;
&nbsp;
The cons associated includes:
Time consuming to be on the phone for the length of the journey.
Costly to facilitate phone call.
Misinterpretation of instructions possible.
Slow travel time as best route may not be readily known.
Person giving directions may have little knowledge of traffic conditions.
&nbsp;
Situation 2:
The pros associated with this situation includes: 
The driver would be presented with a complete instructions, with best route and estimated time to friend's house.
Changes in traffic condition will be automatically provided.
&nbsp;
The cons associated includes:
&nbsp;GPS might not get traffic updates in-time to allow for a route change if there is a traffic challenge on selected route.
&nbsp;
As with the advantages of using compiled code it can be agreed that the use of a GPS would provide cost effective efficiency in driving to my friend's house.
&nbsp;
In our attempt to analyse the pros and cons of compiled code as against interpreted code we'll look at the advantages and disadvantages of using both codes and compare them.
Compiled code has several advantages which includes:&nbsp; 
Compilers can produce much more efficient object code.
Ready to go or execute.
Faster execution than interpreted code.
Native applications are secured and source code is kept private.
Large software programs are design using compiled languages.
&nbsp;
The cons associated with compiled code include: 
Editing and deploying the code is a lot slower than interpreter.
Time consuming error correction is a feature of the compiler code.
Inflexible.
Not compatible across different platforms.
&nbsp;
As for interpreted code the pros are:
Easier to test.
"Easy to learn and use". (www.sqa.org.uk, 2007)
Suitable for scripting.
Cross platform support.
Can be developed in small executable packages.
Easy to debug and is ideal for development in scientific and mathematical computing.
&nbsp;
The cons associated with the interpreted code includes: 
Interpreter is required to run the code.
The interpreter may analyse the same program statement multiple times if a loop is entered.
Often 2 to 10 times slower than compiler code.
Source code is public.
&nbsp;
Both the compiler and interpreter can be combined to for an intermediate approach. Where the code will be compiled part of the way to an intermediate language which takes it far along the way to machine code as possible while still being able to cross platforms. This is then sent to who needs to run the code and the receiver takes it the last step of the way on their computer to machine code&nbsp; which is sometime known as bytecode. Some examples include; Java, C#, VB.NET, Python. (www.youtube.com, 2012)
&nbsp;
References:
Brookshear, J. G. Computer Science: An Overview XML Vital Source ebook for Laureate Education, 11th Edition. Pearson Learning Solutions.
www.codeproject.com (2013) Difference between compiled and Interpreted Languages [Online]. Available from http://www.codeproject.com/Articles/696764/Differences-between-compiled-and-Interpreted-Langu (Accessed: 2 November 2014).
Computer Structures - Lecture Notes (2014) Programming languages [Online]. Available from https://elearning.uol.ohecampus.com/bbcswebdav/institution/UKL1/201480_SEPTEMBER/MS_CKIT/CKIT_501/readings/UKL1_CKIT_501_Week09_LectureNotes.pdf (Accessed: 2 November 2014).
www.sqa.org.uk (2007) Characteristics of Interpreted Languages [Online]. Available from http://www.sqa.org.uk/e-learning/ClientSide01CD/page_13.htm (Accessed: 2 November 2014).
www. youtube.com (2012) Compiled and Interpreted Languages [Online]. Available from http://www.youtube.com/watch?v=qaj7nO1HUqA (Accessed: 2 November 2014).
&nbsp;

	              
	              Object Oriented and Structured Programming
Structured Programming
Structured programming is a methodology used in computer programming that uses a top down analysis for problem solving, modularization for program structure and organization and structured code for individual modules.
To illustrate the concept of top-down, a large problem or task may be broken down into several pieces and each piece worked on separately. Each piece is then treated as a new problem and broken down further. &nbsp;This follows a sequential path up to the point where a problem cannot be further broken down. Each problem piece is then modularized or placed in specific routines that can be called up within the program. Prior to structured programming, programmers used the goto statement to transfer execution of the program to another area but this made the code complicated to read and follow. (spaghetti code). In structured programming, the modules or routines are organized within various control structures such as linear sequence, loops and conditional statements
Object Oriented Programming
Object oriented programming is a programming model in which objects bearing certain characteristics are used and manipulated to achieve desired programming results
Object
An object in the real world represents physical or abstract things that we interact with through our senses including seeing, touching, feeling, hearing or imagining. Real world objects also have state and behavior. State in the case of a Student would refer to properties such name, date of birth, student id, gender and other student details. The student would also be able to do things such as register for courses, submit assignment, attend class, change course &nbsp;etc.&nbsp; An object in object oriented programming can be modeled to reflect real world entities and contain data and routines that respond to calls from other objects.
Class
Before creating an object, a template for what the object contains is first defined. This is called a class. A class contains the properties of an object and all the functions that the object can perform (methods). A class is a blue print of an object and contains functions that can be used to set and retrieve properties of the object. An object is created as an instance of a class. The process of instantiation is when you actually create the object from its blueprint (class). A class is reusable by the program in which it was created as well as other object oriented programs
&nbsp;Interface
Once an object is created, you interact with the object through the particular methods or functions that it has made visible or exposed. These methods are the objects interface with the outside world. For example the power button on your television set is the interface between you and the electrical wiring that executes an instruction.
Inheritance
Inheritance in OOP is the concept by which a new class is created from an existing class and the new class contains the same attributes and functions of the parent class, but it may have additional attributes and functions unique to itself. For example we can have a main class called bicycle which is inherited by 2 other sub classes – racing bike, mountain bike each with their own properties in addition to the main ones inherited from bicycle.
Polymorphisms
An analogy of polymorphism to daily life can be envisioned in how students respond to a school bell. The ringing of the bell has its own interpretation to each student depending on what activity they are required to perform next and each one may respond differently to it.
Polymorphism allows a sending message to communicate with difference objects in a consistent manner without regard to the different implementations. Two or more objects can respond to the same message
Encapsulation
This is the idea that an objects data and logic is hidden from other objects and that it is only through messages that one object is able to interact with another. The interface encapsulates the objects code and data

Advantages and Disadvantages
Structured Programming




Advantages


Disadvantages




Easy to write and use


Can become too big to handle when things become more complex




Require less time to debug and test


Use of nested structures may be difficult to follow




Easy to maintain


Code can be replicated in several places within a single program




Programs are simpler and less complex


&nbsp;




&nbsp;
Object Oriented Programming




Advantages


Disadvantages




Reusability: Allows for software reuse to solve new problems


Learning curve can be steep due to its complexity




Better security of data due to encapsulation, information hiding


May take a longer time to write, great human effort to produce minimal results




Code is highly flexible due to ability to inherit from other objects


May not be ideal for all types of problems




Leads to faster development time


Lead to larger programs that may be slower due to the size




&nbsp;
Preference for OOP languages over structural programming languages in the computing industry today is high due particularly due to the reusability of the code. In instances where a particular problem has been solved through a one object, it makes it easier for others to inherit that object and focus their attention to other problem areas. This reduces development time and therefore lowers the total overall cost.
OOP languages reflect the human way of thinking and conceptualizing to some limited extent when it comes to performing basic standard operations. However in more complex operations, the operations&nbsp; of the human mind are much more difficult to mimic. For example a program to throw a button up in the air and catch would be difficult to program using OOP because of the many decision points and judgment in relation to the object in the air. Determining the right sort of balance required would be difficult and expensive.
References
&nbsp;

Brookshear J, 'Computer Science - An Overview', Programming Languages, 11th Edition, Pearson
Berard E. 'Basic Object-Oriented Concepts', Available at&nbsp;

http://www.ipipan.gda.pl/~marek/objects/TOA/oobasics/oobasics.html, Accessed 02/11/2014

Programming – Slideshare , Available at&nbsp;http://www.slideshare.net/vanesa4ab/programming-1189841, Accessed 02/11/2014
Structured Programming, California Polytechnic State University, Available at&nbsp;

http://users.csc.calpoly.edu/~jdalbey/308/Resources/StructuredProgramming.pdf, Accessed 02/11/2014

Rouse,M. object-oriented programming (OOP), Available at&nbsp;http://searchsoa.techtarget.com/definition/object-oriented-programming, Accessed 02/11/2014
Collins, J. 'A Beginner's Crash Course into Object Oriented Programming, OS News, Available at

http://www.osnews.com/story/6788/A_Beginner_s_Crash_Course_into_Object_Oriented_Programming&nbsp;, Accessed 02/11/2014
&nbsp;
&nbsp;

	              
	              Thanks for this Terry, good insight.
Best Wishes, Craig
	              
	              Hi Augusto
Thank you for taking the time to respond to this.
I appreciate your help with the principles of this - thanks
Best Wishes, Craig
	              
	              Hi Anthony
You could use obfuscation. 
This is a form of ‘program compiling’ that takes input from a program and outputs an obfuscation program, which is difficult to read and understand, but is still executable. 
According to Boaz Barak (n.d.), obfuscators can be very useful for many applications, in particular software protection and digital rights management whereby “the code of the obfuscated program should be very difficult to understand and presumably, any hacker looking at the code of the obfuscated program should gain nothing more than a headache”.
It is important to ensure that the obfuscated program operates with the same functionality and efficiency as the original input program.
Best Wishes, Craig
References
Barak, B. (n.d.). Can we Obfuscate Programs? [Online]. Available from: http://www.math.ias.edu/~boaz/Papers/obf_informal.html (Accessed 3 November 2014)
	              
	              Hi Craig,
Thanks for the obfuscation! I was thinking that "ugilification" doesn't sounds like good English. Now I know it should be obfuscation:-)
Another friend of mine also named Craig told me to "eschew obfuscation" many years ago, and I was very confused...
br, Terry
	              
	              Hi Craig and Terry

I agree with Terry. It was put into perspective and made that much easier to understand. We use all these things everyday and we don't know what drives them, and when you get to the core of it you realise you've interacted with it at one point or another.
Thanks again Craig.

kind regards,
Belinda
	              
	              Hi Terry
Thanks for the compliment. I’m very self-judging, and therefore I always have a hard time accepting compliments, since I always believe it could and should be better. I’m tolerant with others - I hope :-)
My guess as to why 5GL still isn’t invented fully is that it probably has too many ambiguous sentences, and nobody has cracked that nut/code yet. What do you think?
And regarding the risk of losing the job because of 5GL, I believe/hope you wrote that with a smile. There is always a risk of losing one’s job, but we are still here even though a lot of development has happened in our lifetime, we just keep improving our self’s or perhaps just “go with the flow”. There will always be new challenges. So – no worries J
Bo
	              
	              Thanks for this Terry, good insight.

Best Regards, Babatunde
	              
	              Hi Belinda
I noticed in your table comparing compiled and interpreted code, that there is an overlap in the advantages where you state that both are “more secure”. Is this a mis-typing? Or could you clarify?
And another question: There are those (I came across this in my research but unfortunately don't remember where)&nbsp;&nbsp;who claim that speed is not an issue anymore since computers are so fast today. What are your thoughts on this?
Br. Bo
	              
	              Hi Craig,Thanks for your reply and i'm glad you picked up on this part of my assignment because I wanted to go into it more but had to cut it down to try to keep my response down to manageable readable size and in line with the word count.You have answered part of your first question yourself really well, I certainly think what you said is correct. OOP programs do tend to handle more in the way of individual objects and the access to those files over 1 complete structured program. Granted these access times may be insignificant upon measuring them in real time, possible sub seconds differences but still a slightly larger workload for OOP. Obviously it would also depend on the sizes of each program your comparing but if you take it as similar large projects one OOP designed with numerous objects and classes vs a large structured program of equivalent scale.Then as you say OOP programs tend to be slightly larger in code size because you want to design them correctly and cover all aspects when distributing shareable classes. So for example if you were designing a class of say mammals. Then you would have :
Group I: Afrotheria&nbsp;&nbsp;&nbsp; Clade Afroinsectiphilia&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Order Macroscelidea: elephant shrews (Africa)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Order Afrosoricida: tenrecs and golden moles (Africa)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Order Tubulidentata: aardvark (Africa south of the Sahara)&nbsp;&nbsp;&nbsp; Clade Paenungulata&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Order Hyracoidea: hyraxes or dassies (Africa, Arabia)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Order Proboscidea: elephants (Africa, Southeast Asia)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Order Sirenia: dugong and manatees (cosmopolitan tropical)Group II: Xenarthra&nbsp;&nbsp;&nbsp; Order Pilosa: sloths and anteaters (neotropical)&nbsp;&nbsp;&nbsp; Order Cingulata: armadillos and extinct relatives (Americas)Group III: Boreoeutheria&nbsp;&nbsp;&nbsp; Clade: Euarchontoglires (Supraprimates)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Superorder Euarchonta&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Order Scandentia: treeshrews (Southeast Asia).&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Order Dermoptera: flying lemurs or colugos (Southeast Asia)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Order Primates: lemurs, bushbabies, monkeys, apes, humans (cosmopolitan)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Superorder Glires&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Order Lagomorpha: pikas, rabbits, hares (Eurasia, Africa, Americas)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Order Rodentia: rodents (cosmopolitan)&nbsp;&nbsp;&nbsp; Clade Laurasiatheria&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Order Erinaceomorpha: hedgehogs&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Order Soricomorpha: moles, shrews, solenodons&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Clade Ferungulata&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Clade Cetartiodactyla&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Order Cetacea: whales, dolphins and porpoises&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Order Artiodactyla: even-toed ungulates, including pigs, hippopotamus, camels, giraffe, deer, antelope, cattle, sheep, goats&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Clade Pegasoferae&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Order Chiroptera: bats (cosmopolitan)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Clade Zooamata&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Order Perissodactyla: odd-toed ungulates, including horses, donkeys, zebras, tapirs, and rhinoceroses&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Clade Ferae&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Order Pholidota: pangolins or scaly anteaters (Africa, South Asia)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Order Carnivora: carnivores (cosmopolitan), including cats and dogsWikipeadia (2014).

And you would include all of the specific mammalian attributes for each mammal in question above. Then you would have a good distributable mammal class.Now if you were then producing a program for creating a dolphin, you would import this mammal class and then just reference the part for dolphin to create your object. In contrast to this if you were doing this same program structurally then you would just code the individual aspects of the mammal dolphin directly into the program because thats all you need. Arguably you could say well just create a class for the dolphin part of the mammal class which you could well do but you then have to ask yourself is this the best way to do it because if all you need is the dolphin then isn't structured the best way forward anyway.?So because of this robust and complete mammal class inherently it is far larger and hence slower to access/store. But your right again it's not because of an architectural flaw it's and inherent result of the offset to producing OOP properly. So now onto your last point, you read it exactly right. It's all down to what program or job you are trying to accomplish really plus what you already have in place. At the inception stage when you may have a business plan set out and you get to the point of what way to design your new program do we go down the OOP route or the structured route.! If you're a software house producing complex and detailed programs week upon week with a massive staff base then OOP would definitely be advantageous in not reinventing the wheel on each and every project you undertake so getting the core classes right initially will work well going forward. In flip to that if your a company who rarely do development work and have a small staff base and the project is a known one off simple program to do a small task then you really don't need to overdevelop the solution in an OOP way, a good structured approach is fine in this case.What I found over time certainly by my experience is that most places i've either worked or been involved in they do a bit of both. They may have a range of OOP style classes and setup but the build on them in structured way. so they kind of use the best approach of both paradigms. They understand the benefits of redistributable classes and code but also appreciate the speed, cost, efficiency of more structured ways to finish it off. So they base there work on imported classes that they can use what they need but then build the rest of the program in a structured way. What they can and feel needs to be available to others is done so but anything seen to be unique stays that way. So in essence you have a layer of OOP design and then a layer on top of structured which is what I also prefer.&nbsp;&nbsp;&nbsp; Thanks
ChrisReferencesWikipeadia (2014) 'mammals' [http://en.wikipedia.org/wiki/Mammal] (accessed: 3/10/2014)
	              
	              Hi Bo,

I just simply wanted to say I liked your report this week, I liked how you structured it and your real world examples to back up your viewpoints.
And I agree with you on your final observations and conclusion.
Good report

Chris


	              
	              Hi Terry
Uglification - I think that translates into the 'Obfuscation' that I have researched and posted previously. Your wording is easier to understand, even though it is about not being able to understand it! Ha!
Best Wishes, Craig
	              
	              Ha, yes, to avoid understanding! Well funny. Thanks Terry
Best Wishes, Craig
Sometimes I cannot but avoid understanding! :-)
	              
	              Hi Terry and Belinda
Thanks for your feedback. It also helped me put clarity to the subject matter, by comparing and contrasting with an everyday situation.&nbsp;
I'm gonna get the train next time though!
Best Wishes, Craig
	              
	              Hi Chris
Fantastic! Thank you so much for such a comprehensive reply! I am pleased my understanding is valid and in line with the structure. I can see that one of the key challenges is deciding which way to approach the creation of a new object. Thanks again Chris.
Best Wishes, Craig
	              
	              Compiled versus Interpreted Code
Introduction
In order to execute its program instructions, a computer program needs to be compiled or interpreted. Compilation is the process of translating human readable source code into computer executable machine code. This is done by a compiler program.
Interpreters convert a source program and execute it at the same time allowing for &nbsp;interactive debugging and testing. Diagnosis of a program is made easier than through the use of a compiler but its disadvantage is that it runs much slower than a compiled program
Analysis of analogy
In the example given, receiving live navigation directions is akin to the application of interpreted code. One does not have to know precisely where they are going but will interact with the other person for step by step instructions. In case of any problem, they are able to clarify the problem and instruction right there.
The use of GPS is akin to the application of compiled code, one starts by defining precisely where they are going and the GPS will then map out the direction to take. The directions are laid out on the map and the driver simply follows the directions given by the GPS.
Pros and cons of live navigation




Pros


Cons




Interactive –driver is able to describe their position


Easy to follow wrong instruction and not realize until it’s too late




Faster – Driver is able to get specific instructions and respond accordingly


May not have visibility for alternative routes in case of traffic congestion




Maneuverability – driver is able to get alternative routes based on traffic situation


Can be slow because the complete route is unknown




&nbsp;
Pros and cons of GPS




Pros


Cons




Route is established at the onset


Can easily get stuck. Once you get going, it may be hard to




All parameters are known in advance e.g. distance and expected time of arrival


Driver may not be able to know status and routing information such as traffic congestion




Possible to know and set alternative routes to destination


Not interactive – GPS program may be static in its presentation and not able to obtain feedback on a situation




Redirection to alternative routes is possible


The GPS follows its predefined mapping&nbsp; and may not have taken into account recent status changes such as road repairs




&nbsp;
Pros and cons of compiled vs. interpreted code




Compiled Code


Interpreted Code




Pros


Pros




Executes faster as the instructions are already translated to machine readable format


Allows for easy debugging and testing




Occupies less space as the resultant code is compact


Development is quick as




Well optimized during compilation to avoid extra code at run time


Interactive – allows you to interrupt it while its running, change variables and restart procedures




&nbsp;


&nbsp;




Cons


Cons




Hard to debug as problems are picked up at program execution


Slower in execution as every line of code has to be re-read and processed




Program has to be recompiled to make changes


Not suitable for large programs




&nbsp;
References
1) Brookshear, J. G. Computer Science: An Overview, 11th Edition. Pearson Learning Solutions
2) Alkhamese,A. Interpreter versus Compiler, Available at,&nbsp;http://www.slideshare.net/Ayayakoutabas/compiler-vs-interpreter,&nbsp;Accessed 2/11/2014
3)&nbsp;C4LEARN, Compiler versus Interpreter, Available at&nbsp;www.c4learn.com/c-programming/compiler-vs-interpreter/, Accessed 2/11/2014
&nbsp;4)&nbsp;Yao, G. 'Advanced Compiling Techniques', School of EECS, Peking&nbsp;University, Available at&nbsp;sei.pku.edu.cn/~yaoguo/ACT09/slides/lect1-intro.ppt, Accessed 2/11/2014
&nbsp;

	              
	              Hi Anthony
My personal (not a programmer) opinion is that OOP definitely has its place here. But I’m also convinced that it’s not correct for every solution, so you should only use OOP if there is a business case behind it, be I profit or values.
Well, I think I will let you specialists/programmers chat about this. But I have come across an interesting blog by Dahan, Udi (2009) which I guess Terry will like.
http://www.udidahan.com/2009/06/07/the-fallacy-of-reuse/
&nbsp;
Br Bo

	              
	              Hi Everyone,Are there any types of applications for which the use of Object-Oriented Programming (OOP) for software development would be inadvisable or detrimental? In other word, when would you prefer not to use an OOP approach?Anthony
	              
	              Hello Class,Following on from my earlier question on ways for securing or protecting interpreted code, has anyone in class used .NET Reactor or Javascript Obfuscator schemes? Are there any performance, or other problematic, issues from using these schemes?Anthony
	              
	              Hi Terry and Craig,
As a term to use, "uglification" definitely sounds more interesting than "obfuscation" :-). &nbsp;On a more serious note, yes - whilst not being fully tamper-proof, obfuscating javascript code definitely helps to secure it.
My follow-on post on another sub-thread poses additional questions relating to the efficacy of obfuscation schemes.
Anthony
	              
	              Hi Terry,
I am afraid that is a result of my fingers remembering what they type a lot. Instead of typing OOP, I typed another 3 letter acronym starting with an O. OTT stands for Over The Top, as a consumer service for professional produced video content using internet for delivery instead of third party video distribution networks.&nbsp;
Sorry I lost you at my first sentence.
Greetings from Bram
	              
	              Hello all,
Just jumping in the discussion. Could we say that OOP was needed conceptually to use a more language agnostic approach for software architectures? As Terry sais REST APIs do not specify an implementation language and using APIs to communictate between different implementations makes a modular approach possible. Perhaps the paradigm shift to make the data layer more abstract was needed to come to these kind of current popular approaches?
Greetings from Bram
	              
	              Hello Bo,

Indeed a nice DQ. You triggered another thought in my head.
I drive a car from 2008 with a build in navigation which I do not trust at all. In a few years the situation on the road has changed that much I must always double check what the navigation proposes and also ignore it when driving on freshly paved new road. Updating the code is not possible: it is what it is. You can send in the hard drive to the fabric and receive a new version back.&nbsp;
I guess it must be a compiled version in my car and they can only replace it with another compiled version. Perhaps using an interpreted version would have made it more easy to replace the software?
But then again I guess one could change the cards and traffic details if you implemented an update system in the compiled version also. The code what to do with those 2 data sets could stay the same.&nbsp;
Would it be easier to update as system that uses interpreted code that compiled code?
Greetings from Bram
	              
	              Hi Anthony,
What is actually the reuse of code?
At first glance, this concept refers to the reuse of code, classes and models of existing program rather than create new ones.
What are the problems associated with the reuse of existing code?

Identify the reusable parts of a code is difficult;
Lack of documentation if reusable items are not documented, their use becomes more complex;
Developers do not take the time to look for reusable code;
A reusable component can alter the design model, which implies a benefit / cost ratio unfavorable.
Writing code on non-reusable way.

Reusability is fine, but it's not as easy to identify duplicate blocks of code and extract it into new methods / classes, or say: that thing there vaguely resembles with what we need and can use it.
Reuse requires a lot of work on abstraction level, cutting and naming. We must learn and understand the function of a block of code to put a specific and appropriate name above and give it a clear appearance, possibly distinguish its various responsibilities to extract them in separate elements ... It's one of the things most complicated in programming.
Even once you do that, reusing a component is useless if we know that it could change completely the reasons beyond our current needs and become inconsistent, or force us into a mold that will bring unnecessary complexity or unwanted dependencies to other components.
And if we add to that the problems of performance, increased coupling, subtle differences in behavior, etc., anything that seems interesting to generalize it is not necessarily.
&nbsp;
Reference:
Wikipedia Inc. (2014) Code_reuse [Online]: Wikipedia Inc. Available from: http://en.wikipedia.org/wiki/Code_reuse (Accessed: 03 November 2014)

	              
	              hi BO,&nbsp;
LOVE your DQ especially the conclusion how you end it. Also, I liked the real world experiences, they were really cool.
GOOD work.
Regards, martins&nbsp;
	              
	              hi TERRY,
Good reply and i agree with you. Initially, I was thinking you were refering to something different but when I read your reply it well fits. Kids could learn from OOP but for their age i bet they will really get confused. Like you mentioned, polymorphism, encapsulation etc are too very broad for kids to grasp at a tender age.&nbsp;Even grown up in universities find this very difficult talk less of kids.
Thanks for the reply
REGARDS
martins&nbsp;
	              
	              Hi Bram&nbsp;
Based on my learnings so far, this makes sense to me.
Moving the modularity debate onwards, and following Terry's suggestion, I've also been reading up on Aspect-oriented programming, which provides additional object-based modularity in that you can apply 'aspects' as a special standalone modules which allow you to alter properties to classes or objects, in which are completely independent of any inheritance hierarchy. This enables a detachment from cross-cutting concerns, thus not affecting the abstractions within the primary functions of the program.&nbsp;
I like the idea of a modular approach. Makes alot of sense to me.
Best Wishes, Craig


	              
	              Dr. Ayoola,
thanks for the tip. I will start to use the term obfuscation.
br, Terry
	              
	              Hi Martins,
Teaching kids OOP might not be very useful either. OOP is
	              
	              Hi Bram,
You are right about the good data abstraction contribute to the popularity of models like RESTful. But we don't have to give OOP all the credit.
Modularization, Data Abstraction, and even Object-Based programming, do not need to be part of OOP. OOP includes them all, though, and a lot of other "nice-to-have" and "hard-to-master" stuff.
br, Terry
	              
	              Hi Bo,
I believe the thing 5GL want to achieve is simply too hard. If the 5GL compiler is so intelligent to understand the intent of the programmer, I think it must be smart enough to know "oh, I can just hire a human to program for me." It feels like the "government layer" above the 7 OSI layer.
I don't know which one will come first, human invent 5GL or computers conqor human.
br, Terry
	              
	              Hi Craig,
Right, train! Usually, trains have highly compiled schedule and routines.
br, Terry
	              
	              Hi Bo,
You are right, I like it very much. Thanks for sharing.
"stay away from reuse" the blogger said. I would recommend the same.
"Remove the duplicates", I would suggest instead.
br, Terry
	              
	              Dr Ayoola,
I'm using javaScript obfuscator UglifierJS. In Ruby gem its called Uglifier. I only apply it in the production, not development environment.
It makes the JavaScript load faster, since the file is much smaller.
Now and again we need to debug JS in production, and that's very hard.
But now and again I find myself reading obfuscated JavaScript...
br, Terry

https://github.com/mishoo/UglifyJS
	              
	              Bram,
I thought you were using someone's&nbsp;Dvorak keyboard... :-)
br, Terry
	              
	              Dr. Anthony,Over three months ago I started a new post as an Analyst Programmer, the position continues to be much of a challenge because I do not have much programming experience. Most of my experience is that from university which was over five years ago.At work we use Java and PL SQL for our major Payroll application. Over my period working there the application present several challenges in terms of functionality as queries of different sorts may arise. In addition to this there is a long list of changes of fixes or patches which has to be addressed that was brought to the attention by users to management. This is a fresh list in addition to the perhaps thousands that were already dealt with in the past. A plethora of times what I have to look at previous code and copy it and reuse it and make minor changes as needed to suit what I need to do. Which is even a challenge for me since I am no programmer really.While being there, my question from time to time, has been, why or how come so many fixes? Does it ever end? Over the past couple of months it has really been frustrating having to deal with this type of scenario.While code reusability may pose as an advantage, in Object Oriented Programming (OOP), over time it poses as a disadvantages, as hinted in the scenario described above, where there are constant fixes to bugs or patches for the code. As time progresses the application will lose its original design, “As a result, the&nbsp;program drifts away from the original design and becomes harder and harder to&nbsp;modify.”, Harel. D, 2004. And it will be more difficult to fix eventually. At this stage you need to look at the overall functionality and design of the application instead of looking at fixing existing code.Allow me to compare this analogy to that of a beautiful dress that has a hole. You stitch that hole and it gets another, you stitch that as well. As the dress continues to get holes you do the same and may even apply patches of cloth to cover the holes. As time goes by the dress will no longer look the same, but may simple resemble a patched garment. Thus, you will need to explore the option of getting another dress.Perhaps, this drawback is one of the reasons the concept of concept of 'Extreme programing' was introduced. Extreme Programming is a agile software development model that focuses of simplicity. A simple application is initially created and as time goes by newer versions are created. This type of model focuses on frequent testing and analysis of the application as a whole.References:Harel, D. &amp; Feldman, Y.A. (2004) Algorithmics: the spirit of computing. 3rd ed. Pages 350, Pages 351, New York: Addison Wesley.Brookshear, J.G. (2011) Computer science: An overview. 11th ed. Pages 308 &nbsp;Boston: Pearson Education / Addison-Wesley.&nbsp;
	              
	              Hi Terry,
Thanks for the simple presentation, this has surley helped me in understanding OOP much better. I believe OOP languages is very powerful and that is why most of today's high level languages like C#, Java and python incorporate object-oriented paradigm in its design.
Regards,
Ricardo
	              
	              Hi Dr. Anthony,
I believe the code reusability feature of OOP makes it ideal for high level programming languages that would need to call the same object numerous times and that is the basis on which GUI based softwares are design using languages of OOP paradigm. Therefore in my view OOP is not overhyped but has it purpose in the prpgramming languages.&nbsp;
	              
	              Hi Terry,
I don't think languages are the issue personally. I've always thought that knowing particular languages is far less important than knowing how to program. If you know how to program, then learning a new language is usually very easy. If people have trouble transferring from one language to another, they're probably not very good programmers.
Rather more time-consuming is learning frameworks - e.g. Cocoa, .NET, J2EE. While most languages have the same basic concepts, the functionality provided by frameworks and the way they do things differ dramatically.
&nbsp;
Best Regards, Babatunde
	              
	              Colleagues,
Thanks for sharing the concept of "uglification" and "obfuscation", up until it was mentioned here I was not aware of the concept. I can now recall receiving a communication that was rather confusing and I could not determine the message being conveyed. I can now safely say it was "obfuscated" to protect the contents.
Regards,
Ricardo
	              
	              Hi Dr. Anthony,
The only solution that comes to mind apart from "obfuscation", is to employ an intermediate approach for distributed programs. Where the source code is partially compiled which makes it private and adds some level of security. Then the program can be distributed as a just in time compilation.
Regards,
Ricardo
	              
	              Hi Craig,

Thank you for your feedback:-)
About the elephant, the beauty of it is that you can start from anywhere as long as it's one bite at a time:-):-):-)

kind regards,
Belinda
	              
	              Hi guys,
&nbsp;
Using method overloading as the example of polymorphism? To me it seems like simply the differentiation of methods with the same name and different parameters.
So stuff(Thing t) and stuff(Thing t, int n) are entirely different methods as far as the compiler and runtime are concerned.
It creates the illusion, on the caller's side, that it's the same method that acts differently on different kinds of objects - polymorphism. But that's only an illusion, because actually stuff(Thing t) and stuff(Thing t, int n) are completely different methods.
&nbsp;
Br, Babatunde
Reference
Wikipedia the Free Encyclopedia Polymorphism (computer science) [online] Available from: http://en.wikipedia.org/wiki/Polymorphism_(computer_science)
	              
	              Hi Terry, Thank you for your comments. i agrre that good code is loosely coupled. In terms of inheritance in OOP my&nbsp;view is that parents and children are are not handled as logical parents and children, they are handled more like biological parents and children. It is best to think of the parent/child relationship in OOP as a concept of “inheritance” rather than a concept of containers and contained elements.  The child while a product of the parent can still exist on his own, with his own individual attributes, no two child elements of a parent are alike. I'll give you an exmaple that I use in my automation testing framework: I have a maven "parent project" called QA-Tests, in its pom i have child modules for my different sub projects (maven modules) e.g belindaTests is one of the children. In the parents pom, i have a selenium dependency, when I created belindaTest from QA-Tests, i didnt need to add the selenium dependecy because belindaTest inherited that dependency automatically because of being a child of QA-Tests. However, belindaTest has her own additional dependecies in her pom not related to parent, that only belindaTest uses. I extend this to classes and the benefit of this is that the child class(project in my example) doesnt have to redeclare and redfine all the memebrs it inherits forn the parent class(project in my example). Best regards, Belinda   
	              
	              Hi Augusto,
While I love dogs, during this assignment, I realised I know little about them. I had to google dog characteristics. hahaha.
THank you for your feedabck.
I'm sure in OOP there are ways to solve the elephant problem too:-)
kind regards,
Belinda

	              
	              Hi Bo,

Absolutely, I think that it is possible. While doing my research on the evolution of OOP, I came accross a suggestion that&nbsp;Object-oriented Paradigm is an evolution of Structured approach in&nbsp;Programming.&nbsp;http://ecet.ecs.uni-ruse.bg/cst06/Docs/cp/SII/II.6.pdf&nbsp;
I am inclined to aggree, I believe OOP was introduced to improve what was already there, taking some of the good bits of sstructured programming and repackaging them to OOP to include additional desirable features. And that is why some traits apply in both cases.

kind regards,
Belinda
	              
	              Hi Bo,

I absolutely agrre with Terry. Excellent job! while conducting my research for this, it was hard to lock down some traits to one paradigm beacause some traits just arent paradigm specific as you clearly pointed out.
Your post definitely calmed the confusion that was raging in my head.hahaha

best regards,
Belinda
	              
	              
Hi Dr Anthony,
Yes code re-use is achieved in OOP but it is also achieved in functional programming. In layman's terms; anytime you take a block of code and make it callable by the rest of your code such that you can use this functionality elsewhere is code re-use.
kind regards,
Belinda

	              
	              Tanisha,
You have a very interesting job.
The "inventor" of Extreme Programming, Kent Beck, suggested "Make it work, make it right, make it fast".
So "making it work" or having the fix is only the first step. We need to make it right again after that.
br, Terry
	              
	              Hi Babatunde,
I won't say languages are not the issue. Yes, frameworks are very important and probably time-consuming after you've master a language. And learning a new language is relatively easier if you already master another one. But there's still quite different design philosophy behind each language, and usually that's a quite deep issue.
I remember read somewhere that "if the programming language doesn't influence the way you think about design, you haven't fully master the language." Just cannot find where it's from. Probably because of my bad English used to retell it.
br, Terry
	              
	              Hi Babatunde,
I call that "trying to be cute":-) They functions could just have different names.
br, Terry
	              
	              Hi Bo,

THanks for pointing that out.
That definitely doesn't belong there:-)
I came accross an interesting read on this topic. Its a bit outdated (2006), titled &nbsp;"The Day peformance didnt matter anymore",&nbsp;so I can't rule out your statement about speed not being an issue anymore. See some extracts form the said article:

"OSNews published a&nbsp;nine-language performance roundup&nbsp;in early 2004. The results are summarized here:



&nbsp;
int
long
double
trig
I/O
&nbsp;


Visual C++
9.6
18.8
6.4
3.5
10.5
48.8


Visual C#
9.7
23.9
17.7
4.1
9.9
65.3


gcc C
9.8
28.8
9.5
14.9
10.0
73.0


Visual Basic
9.8
23.7
17.7
4.1
30.7
85.9


Visual J#
9.6
23.9
17.5
4.2
35.1
90.4


Java 1.3.1
14.5
29.6
19.0
22.1
12.3
97.6


Java 1.4.2
9.3
20.2
6.5
57.1
10.1
103.1


Python/Psyco
29.7
615.4
100.4
13.1
10.5
769.1


Python
322.4
891.9
405.7
47.1
11.9
1679.0



It's not a very practical benchmark, but it does tell us a few things. It's no surprise that C++ is at the head of the pack. But the others aren't terribly far behind. What I find really interesting, though, is how&nbsp;most of the languages clump together in the middle. There's no significant performance difference between Java and .NET if you throw out the weirdly anomalous trig results.
However, there is one language definitely bringing up the rear –&nbsp;Python. That's because it's an&nbsp;interpreted language. This is explained in&nbsp;Code Complete:

Interpreted languages tend to exact significant performance penalties because they must process each programming-language instruction before creating and executing machine code. "&nbsp;
The write goes on to say:
"Clearly, the performance penalty for interpreted languages is extreme. How extreme? If you have to ask,&nbsp;you probably can't afford it.&nbsp;One of the biggest stigmas for early Visual Basic developers was that our code wasn't compiled. It was interpreted. Interpreted executables were yet another reason so-called "professional" developers didn't take VB seriously. It was too slow. This finally changed when we got&nbsp;compiled executables&nbsp;in 1997 with VB 5. The most commonly used interpreted language today, however, is JavaScript. And&nbsp;JavaScript is the very backbone of Web 2.0. How is this feasible if JavaScript is a hundred times slower than Java? Consider this&nbsp;ancient 1996 JavaScript benchmark page&nbsp;:



&nbsp;
1996
2006
&nbsp;


primes
0.15
0.02
8x


pgap
3.13
0.06
52x


sieve
5.05
0.02
252x


fib(20)
2.15
0.03
72x


tak
10.44
0.08
131x


mb100
8.4
0.2
42x



In ten years,&nbsp;JavaScript performance has improved a hundredfold. But so what, right? Computers get faster every year. Well, our computers are now so fast that – with very few exceptions –&nbsp;we don't care how much interpreted code costs any more.
What many pundits don't realize is that&nbsp;the viability of interpreted JavaScript for mainstream applications is a relatively recent development."
I also read somewhere where someone implied that&nbsp;performance doesn't matter too much anymore from a language standpoint, and these days it's all about scalability.
Thanks for bringing that up and allowing me to look at it from a broader more updated perspective.
Referrences:
http://blog.codinghorror.com/the-day-performance-didnt-matter-any-more/ &nbsp;
Best Regards,
Belinda



	              
	              Hi Belinda,
Your work looks so interesting to me:-)
And this answered a question I asked you several weeks ago (and you didn't reply me ). So now I know you are not using Calabash, you use Selenium.
Let me explain more on why I don't favor the parent/child analogy.
In OOP, usually, when we talk about inheritance, we are talking about types (or classes), not instances. Parent and child are not types, it's just relationship between human instances. Even when we say "Parent" is a type of people, and "Child" is another type of people, there won't be any inheritance relationship between the two types. like:
class Human {&nbsp; &nbsp; eat();&nbsp; &nbsp; sleep();}class Parent {&nbsp; &nbsp; worry();&nbsp; &nbsp; blame();}
class Child {&nbsp; &nbsp; spend();&nbsp; &nbsp; play();}
And then, you can have a specific, yet still a type, of people:
class PeopleLikeTerry : extends Human, Parent, Child {&nbsp; &nbsp; keepAskingStupidQuestions();}
Then, I'm just an instance of my type:
terry = new&nbsp;PeopleLikeTerry();
--------------------------
However, there's another type of inheritance, that is copying behavior directly from another object (or instance). That is called Prototype-based programming. I won't consider maven a programming language. But if it is, it's sort of a prototype-based language. The maven project you defined is the instance. And the inheritance is actually copying behavior from an existing project, rather than from a type. Prototype-based programming is also considered a special type of OOP. But I would say it's quite different than the usual OOP was normally talk about.
We'll continue to call them "parent class" and "child class". I don't think there's any problem. They are parent and child in their own way.
br, Terry
Reference:
Prototype-based programming. (2014, October 29). In&nbsp;Wikipedia, The Free Encyclopedia. Retrieved 06:49, November 4, 2014, from&nbsp;http://en.wikipedia.org/w/index.php?title=Prototype-based_programming&amp;oldid=631562891
	              
	              Hi Terry,
Don't worry about the English, it's understanding that matters.....i understand all you write.
Thanks, Babatunde
	              
	              Hi Colleagues,


To answer this, you first need a definition for "syntactic sugar." I'll go with&nbsp;Wikipedia's: In computer science, syntactic sugar is syntax within a programming language that is designed to make things easier to read or to express. It makes the language "sweeter" for human use: things can be expressed more clearly, more concisely, or in an alternative style that some may prefer.

Specifically, a construct in a language is called syntactic sugar if it can be removed from the language without any effect on what the language can do

So, under this definition, features such as Java's varargs or Scala's for-comprehension are syntactic sugar: they translate into underlying language features (an array in the first case, calls to map/flatmap/filter in the second), and removing them would not change the things you can do with the language.

Method overloading, however, is not syntactic sugar under this definition, because removing it would fundamentally change the language (you'd no longer be able to dispatch to distinct behavior based on arguments).

Best Regards, Babatunde

References

Wikipedia the Free Encyclopedia Syntactic sugar [online] Available from:&nbsp; http://en.wikipedia.org/wiki/Syntactic_sugar 
	              
	              Hi Belinda,
This is really a good reading. Thank you very much.
JavaScript is quite fast and getting faster. Interestingly, the Google JavaScript engine and Firefox JavaScript engine are all developed in C++. The Google JavaScript engine V8 is really nice implemented. Anybody who learn C++ could learn something from the V8 project.
So OOP can be really good if doing right.
br, Terry
	              
	              Hi Babatunde,
Thanks for answering my question:-)
As you said: "you'd no longer be able to dispatch to distinct behavior based on arguments."
But if you use different name for each function with different arguments, it will be the same. Everything is determined already at the moment you write it, right?
I'm not sure about Java, but at least for C++, as you compile the code, each overloaded funcitono actually has a unique name in the compiled output. So it seems just a sugar?
br, Terry
	              
	              Hi Belinda, Bo and Terry
Really interesting to read your threads.
Just for my clarity, are we concluding that performance is&nbsp;a potential issue...but because of the speeds and costs of technology, we dont care anymore?
Best Wishes, Craig
	              
	              Hi Craig,
In year 2006, Amazon found that "Every 100ms delay costs 1% of sales" on their web page.
Do you think people don't care speed any more?
br, Terry
Reference:
http://sites.google.com/site/glinden/Home/StanfordDataMining.2006-11-28.ppt
	              
	              Hi Guys

You could use your GPS on the train …..(Give it a try someday you may be willing to….it’s FUN)


Using a GPS receiver on a train is so that you can find out where you are and when you are going to get to a certain point. I use mine all the time to track the trains’ progress. I believe that using my GPS is more accurate in a way. Using a GPS, you can notify people when you are going to arrive and be very accurate.


Items:

Use a GPS receiver (the smaller the better). Garmin Venture/Etrex work best. They are the size of a cell phone and are easy to work with. You will also need a laptop with good battery-life (for real time mapping)

Setup:


Find a window seat on the train. Turn on your GPS and point the antenna towards the sky. This is a little difficult to do but if you have a newer model GPS it will work.

After you receive a satellite signal you can then put the GPS by the window and it should be fine. Lay the GPS by the window facing towards the sky. The GPS will not receive a signal if you do not have it by the window. I have attempted to use several GPS receivers and have found none that will receive from the interior of the train without being by a window.




For real-time mapping:


Set up your GPS as explained above. Make sure you have a good signal. You might have to change your interface settings; check your owner’s manual for this. Example: Street Finder by Rand McNally works with the GPS set on NMEA In/NMEA Out @ 4800 band rate.
Plug in the GPS cable into the back of your laptop. Turn on the laptop and make sure the laptop accepts the new plug-in device (consult your owner’s manual for this).
Start the mapping program you are using. You can try (Rand McNally's Street Finder Deluxe).
Click on the GPS option and you should start to receive images that show your train’s current location. This location is refreshed by the second. If you are using the National Geographic program (my friend does) it refreshes every 5 seconds.



Results:

You can track your train’s progress and see on a map your current location down to the street address! 


Have fun with your system.

NB: There are some live train apps on Google Play
 

Best Regards, Babatunde

Reference:


LiveViewGPS Amtrak Passengers Can Now Track Trains Thanks to GPS Tracking [online] – Available from: http://www.liveviewgps.com/blog/amtrak-passengers-track-trains-gps-tracking/#sthash.V40uYD4h.dpuf (Accessed on: 3 November 2014)

Live train map [online] Available from: https://play.google.com/store/apps/details?id=com.esri.android.VRMobile&amp;hl=en (Accessed on: 3 November 2014) 

http://www.gps.gov/applications/rail/ (Accessed on: 3 November 2014)

http://trn.trains.com/railroads/abcs-of-railroading/2006/05/gps-helps-you-find-the-trains (Accessed on: 3 November 2014)
	              
	              Hi Anthony, 


There are few methods I can offer:




Use appropriate privileges: don't connect to your database using an account with admin-level privileges unless there is some compelling reason to do so. Using a limited access account is far safer, and can limit what a hacker is able to do. 
Keep your secrets secret: Assume that your code &nbsp;is not secure and act accordingly by encrypting or hashing passwords and other confidential data including connection strings. 
Don't divulge more information than you need to: hackers can learn a great deal about database architecture from error messages, so ensure that they display minimal information. Use the "RemoteOnly" customErrors mode (or equivalent) to display verbose error messages on the local machine while ensuring that an external hacker gets nothing more than the fact that his actions resulted in an unhandled error.



Best Regards, Babatunde
	              
	              Hi Babatunde, I'll give that a go next time.
Best Wishes, Craig
	              
	              No Terry,
I believe performance is and will always be an issue, irrespective of technology speeds and costs. I was asking what your own respective conclusions are?
Best Wishes, Craig
	              
	              Hi Craig,
I would say not every software requires the same performance. And how to get that performance is a technical question.
By saying it's a technical question, I mean it's not about being responsible or lazy, it's about how to more effectively improve the performance. Oh, yes. And that leads back to the good old 20/80 rules again:-)
And allow me to speculate a little about what is important to your role in your work. I guess it's important for you to know that there's a cost for getting the performance (and not getting the performance). And it doesn't make sense to blindly rely on "every engineer try their best". You need to know your requirement.
Did I speculate too much?
br, Terry
	              
	              Precisely, thanks Terry
Best Wishes, Craig
	              
	              Hi Anthony It's an interesting question.As discussed by others already I would look at Obfuscation, I have read a very interesting journal paper by Marius Popa (2011) at the Department of Economic Informatics and Cybernetics in Romania."The paper investigates the most common obfuscation techniques for software program source code. Engineering elements of the compiling and interpreting processes are presented form the most widely used programming language based on Java Development Kit and .NET Framework."He goes on to say : "Obfuscation is the process which transforms the source code or intermediate code to make it more difficult to be decompiled and analyzed. Thus, the reverse engineering of software is more difficult to be implemented."
The obfuscation process is implemented in automatic tools called obfuscators.As obfuscation techniques, the following must be considered :

Name obfuscation - the process of replacing identifiers within the code with meaningless strings
Data obfuscation - the process of changing the way the data is stored in memory
Code flow obfuscation - the process of changing the control flow so it is more difficult to analyse
Incremental obfuscation - the process of providing code consistency in software updates to previous versions
Intermediate code optimization - the process of optomisation in the code
Debug information obfuscation - the proces of removing debug information in the code.
Watermarking - the process to embed information in the software application.
Source code obfuscation - the process to hide the source code meaning when this code is disclosed to a third party to test or maintain it. This technique supposes renaming of the program identifiers and removing the comments

Other ways you could look at the problem is through source code encryption, certainly this way would help for simple aspects such as the deploying of code and securing on portable devices. But there are techniques to encrypt code before compilation at runtime. Programs such as the 'Rijndael encryption algorithm' for C# does just this. The only issue is that because it creates temp files during runtime the code could still be accessed as it is stored in memory but it's still an extra level of security.
Maybe in certain situations the locking down of portable devices such as Sat Nav's. Not a practicle solution granted because people want the ability to plug the Sat Nav or other devices into laptops etc to do updates but you could in certain situations make access to the code physically impossible.
Or how about cloud based code. It would inherently make the device or system slower due to the need to access the internet in real-time but you could have a device which doesnt have any form of code on it. It simply accesses a cloud based or website with the code ready to execute. It knows who you are as you will have a licence or subscription and it does the execution online live then simply sends the results back to your device. No way to get the code unless you hacked the site itself directly.
I don't think there is ever going to be a complete solution to solve the problem but like you said to minimize and protect it as much as possible.ReferencesMarius Popa (2011) 'Techniques of Program Code Obfuscation for Secure Software' [http://www.google.co.uk/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=8&amp;cad=rja&amp;uact=8&amp;ved=0CFMQFjAH&amp;url=http%3A%2F%2Fwww.jmeds.eu%2Findex.php%2Fjmeds%2Farticle%2Fdownload%2FTechniques-of-Program-Code-Obfuscation-for-Secure-Software%2F55&amp;ei=qeNYVJSxKqfIsASArIDQCw&amp;usg=AFQjCNHJSCb7w4HT4iD6Ypo6rCy9aapM0A&amp;bvm=bv.78677474,d.cWc] (accessed 4/10/2014)
Tabbia, B (2013) 'Encrypted code compiled at runtime' [http://www.codeproject.com/Articles/531028/Encrypted-code-compiled-at-runtime] (accessed: 4/10/2014)
	              
	              Hi

I thought of another after posting my last response.
Imagine a world where nobody steals anything!
We could all engage in a global "moral crusade". I don't know about anyone else but I don't steal or hack anything.
We could make sure that everyone in the world is somehow raised correctly so that code theft and hacking etc is a thing of the past.. Problem solved... Everyone is a goody two shoes!&nbsp;

ta
Chris

	              
	              H Belinda
Thanks for the clear answer.
Also, I'm not an expert in these matters, but I think you are right about the evolution part.
Br. Bo
	              
	              Maybe someday we will find out, and say/wonder, "hey, what just happend"&nbsp;
	              
	              I alway happy to calm somebody in rage&nbsp;
	              
	              You're welcome&nbsp;
	              
	              Hi Anthony.
I concur with the above view on code reusability being a key advantage of OOP over structured programming. When you consider the whole notion of open source software that has led to the creation of a whole developer community, the ability for code reusability stands out as one of the key aspects that have led to the growth and development of various software. The fact that a piece of code defined as a class or method and made to do just one thing and do it right, has made it easy to extend the functionality of software without the need for a user to rethink and recreate a specific function. ('If it aint broke, don't fix it')
The effort to create a new program from scratch with budget and timeline constraints is a severe hurdle and OOP makes code reuse possible so that for specific functions that you need to incorporate in your program,you would simply embed that object and utilize. This may happen at the expense of code design but its worth considering what the payoff is and the constraints you are working under
On a macro scale, I would say that the cost-benefit of this is far much more than the hiccups and constraints that may be posed against it.
Reuse has long been&nbsp;recognized as crucial to overcome the “software crisis” (Naur and Randell, 1968), as&nbsp;it allows for more efficient and more effective development of software of higher&nbsp;quality (Krueger, 1992; Kim and Stohr, 1998)
References
1) Taibi,F. 'Reusability of open-source program code: a conceptual model and empirical investigation,'ACM Digital Library, Available at&nbsp;http://dl.acm.org/citation.cfm?id=2492276, Accessed 04/11/2014
2) Sojer, M. Henkel, J. ‘Code Reuse in Open Source Software Development: Quantitative Evidence, Drivers, and Impediments’, Available at&nbsp;http://papers.ssrn.com/sol3/papers.cfm?abstract_id=1489789, Accessed 04/11/2014
Joseph
	              
	              Hi Belinda.
On this one, I would want to look at it more from the broader perspective because usage of a specific function within your program is part and parcel of fundamental program design principles. Question is how portable can you make that piece of code? can it be packaged in such a manner as to make it wholly available for use in other progams and this is where I believe OOP comes in. A good example is the use of Microsoft Foundation Classes which make it possible for you to enhance your application through the use of standard windows APIs exposed through a wrapper object in MFC.
Joseph
Reference
Microsoft, 'MFC Desktop Applications', Visual Studio 2013, Available at&nbsp;http://msdn.microsoft.com/en-us/library/d06h2x6e.aspx , Accessed 04/11/2014
	              
	              Hi Chris,

The word Imagine made me remember Imagine song by (John Lennon) I’d want you to listen to that track 

Init ….he wrote:

Imagine there's no hell below us above us only sky Imagine all the people living for today

…..Imagine there's no countries it isn't hard to do nothing to kill or die for and no religion too Imagine all the people living life in peace …..Imagine no possessions I wonder if you can No need for greed or hunger a brotherhood of man Imagine all the people sharing the entire world….And the world will live as one&nbsp; 
Regards, Babatunde 
	              
	              Hi Chris,
Nice post.&nbsp;I have also come across this - Computer Scientists achieve 'iron wall' software obfuscation top protect IP.
Available from: http://www.wired.co.uk/news/archive/2013-07/31/software-obfuscation&nbsp;
No immediate plans to commercialise this, but steps being taken which could make it more widely available.
Best Wishes, Craig
	              
	              Hi Terry,
I did not know about the Template Method! That was a good read in Wikipedia. I realized I do that automatically although it seems to be similar/related to the principle of normalizing database tables, which I am a bit more familiar with, perhaps that's why I just do that ;).
But it also seems that the&nbsp;Template Method is kind of a "structured programming" way of approaching OO coding. One could argue that this equates to split the problem in smaller tasks.
Best Regards,
Augusto
	              
	              Hi Anthony,

I have used .NET 1.0 libraries in different version of framework (anything between 1.1 to 4.0) and didn’t experience any issues. &nbsp;I sometimes use SQLDataProvider which I think was release a bit after .NET 1.0.

Back with .Net 1.1 obfuscation was essential: decompiling code was easy, and you could go from assembly, to IL, to C# code and have it compiled again with very little effort.

Best Regards, Babatunde
	              
	              Hi Terry,
Have you tried firebug? …could be useful (Suggestion)
Firebug integrates with Firefox to put a wealth of web development tools at your fingertips while you browse. You can edit, debug, and monitor CSS, HTML, and JavaScript live in any web page
Firebug includes a powerful JavaScript debugger that lets you pause execution at any time and have look at the state of the world. If your code is a little sluggish, use the JavaScript profiler to measure performance and find bottlenecks fast.
Best Regards, Babatunde
References:
JavaScript Debugger and Profiler about firebug [online] Available from: http://getfirebug.com/whatisfirebug (Accessed 4 November 2014)
JavaScript Debugger and Profiler Learn more [online] Available from: http://getfirebug.com/javascript (Accessed 4 November 2014) 
	              
	              Hi Antony,
Difficult question, if I look even at the scripting languages, many of them support heavily Object-Oriented Programming (OOP). If we look at PowerShell basically all the commands handle objects. Although that doesn’t mean I would necessarily code a full-fledged OOP script with PowerShell. If I were to create a script to remotely change the password of the local administrator user on the windows servers in a network then I would simply use a simply use structured programming as I really need is some while … do and if … then statements. So I will assume that for scripting overall, you do not really need to use OOP.
It’s a guess here, but I could not find much information about this but I think that OOP may not work well if you are actually coding a compiler.
There is of course perhaps the best place where not use OOP: for functional programming. As this paradigm is aimed at mathematical functions (Bookshear 2011), then OOP does not really fit well. I cannot possibly imagine how would we define E = mc^2 as objects in a program (or at least define the objects precisely to be able to make calculations).
Also, if I were to program an air drone with capabilities of auto-adjusting its hovering based on current wind, I think I will not use objects for the majority of that. It would be hard for me to define input objects such as wind, or sensing capabilities as objects. For me is easier to express the wind speed in a function or equation rather than an object. Also defining up or down as an object could be hard. If I were to code a simulation of an air drone inside a software (such as a video game) then OOP would make more sense, but for an actual real air drone I will find another way (perhaps that is better suited for functional or imperative programming).
Best Regards,
Augusto.
References
Bookshear, J. G. (2011) '6.1 Programming Paradigms' in Computer Science: An Overview, 11th edn, Addison-Wesley, pp. 245-246.
&nbsp;

	              
	              Hi Colleagues
I have a question for you.... Do you think we are close to a time when we may see forms of automatic self-creating application software programming on an enterprise wide scale?
Similar to "hello Google, please tell me what the weather will be like today"? It generates the algorithm to go get the information and present it to you...

Could we see application capabilties whereby we have application administrators who are able to design and construct applications, such like "hello application creator, please create baseline GUI with screen icons to view customer details, sales records, service records, last bill, next bill, last contact record, last purchase, similar products you may like".
Immediately the application administrator is provided with a baseline GUI (already with a pre-determined and branded wallpaper, that was built and designed using similar methods) that has been built and coded by the application creator, and then enables a drag and move facility to enable repositioning of the icons in a standardised position for general user release.&nbsp;Thus, the application creator produces the code and once authorised through some form of click/check by approved and authorised personnel, the application creator sends the program for compilation and then releases through a sequence of steps into production envrionments?&nbsp;
Yes, this requires strong management, governance and oversight, and a completely different way of working, but does anyone ever envisage a time when we may see this?&nbsp;What generation of software programming language could this be, 5GL, 6GL?...
We have seen major shift towards digitalisation (I know, we've been talking about a paperless office for decades now, but aren't we truely on the verge of achieving this now) and dark computer sites with automation and remote management have taken on new levels of existence. Isn't it now time we saw a shift of similar proportions in software development? This is not about removing the 'programmer', this is about Building Capabilties for 'businesses' to then produce application functionality without needing an IT programmer. This, allowing our 'programmers' to focus on building the future capabilities and exploiting technology advancement to its fullest potential.
Also, let me compare (loosely!) this with IT strategy (simple phases) over the last 20 years or so, whereby: Phase 1 was perhaps reacting to business needs and building big bang business applications and infrastructure to support those needs. Once delivered, already out of date. Then we moved through phase 2 for example, which was being more joined up and responsive to the business and together, working out a strategy which enabled smaller increments of IT change, but still retrospective to meeting timescales for take to market differentation. Then perhaps phase 3, which is now all about IT being business leaders and constructing strategies for Building Capability proactively and ahead of business needs, thus when the business needs to realise their strategy, the IT Capability is already there to refine, customise and deploy.
I'd be delighted to hear anyones thoughts on this.
Anthony, also just wondering whether you have a perspective on this?
Best Wishes, Craig
	              
	              Hi Anthony,
The most common technique is obfuscator, using an obfuscator, it scrambles the source code and compile anyhow (this is the goal) with references on memory segments to complicate the task of the cracker, by cons, the flip side is that the code can be slower on execution (must choose between maximum speed and maximum security).
So an obfuscator will rename all variables, remove all the comments, put jumps everywhere, replace strings by their ASCII values, etc .....
We must notice that an obfuscator is not a miracle solution, because there is no miracle solution. If your computer can interpret any program, with a little patience a cracker can also do it.
The obfuscator makes the read operation of the disassembled code more tedious, which discourages the small malignant, but someone motivated enough can still to break the code.
Regards,
Reference:
Wikipedia Inc. (2014) Obfuscation_(software) [Online]: Wikipedia Inc. Available from: http://en.wikipedia.org/wiki/Obfuscation_(software) (Accessed: 04 November 2014)

	              
	              Hi Augusto
Great reply - I have been thinking a little about the question too and based on my interpretation of the various different paradigms, I agree that real time aerospace type systems may well not be best suited to OOP. I agree, it 'feels' like these would be more aligned with imperative paradigm, which, again based on my learnings this last few week, is much closer to machine language.&nbsp;
Also, the simple calculator or other homebased appliance types - I would doubt these operate with OOP.
Best Wishes, Craig
	              
	              Hi Everyone,Despite its many benefits, some view the OOP paradigm as often being unnecessarily complicated, with a large hardware footprint, and generally requiring more elaborate preparatory planning.Any tips on techniques that may be used to minimize or eliminate some of these inherent OOP disadvantages?Anthony
	              
	              Hello Class,One often-quoted advantage of interpreted programs is the ease of updating the code. Can you however think of situations where this feature could also be a disadvantage?Anthony
	              
	              Hi Bo
Yes, nice job. I particularly liked the way you described the different levels of abstraction in relation to the structure of OOP.
Best Wishes, Craig
	              
	              Dr. Anthony,

Applications not suited for Object-Oriented Programming (OOP) may fall into specific grouping satisfying a particular set of criteria. This group includes distributed, web, mobile and utility applications.
Today's apps for smartphones, tablets and other mobile devices. Web applications, dynamic websites, diagnostics tools and utilities require significant flexibility in their design.
OOP would not be used for any application that is small and has a simple design with little to no complexity. OOP would also be inadvisable for applications that are designed for distribution over the internet and needs to be executed on different computing platforms. Applications like those listed above falls into this category. 
Many web applications are design using a structured approach as the conceptual benefits of the language are suitable to their purpose and operation. On the other hand application/software that are large scale with multiple complex structures will benefit from an OOP approach. Software such as operation systems, GUI intense games and database related production software, to name a few.
&nbsp;
References:
Brookshear, J. G. Computer Science: An Overview XML Vital Source ebook for Laureate Education, 11th Edition. Pearson Learning Solutions.

	              
	              Hi Tanisha, Hi Terry
Tanisha - I'm sure you'll get around to mastering the discipline. The fact that you are embarking on a masters programme shows you have ambition and a drive to learn, so I have every confidence you'll achieve your goals.&nbsp;
I thought your peice was interesting (and to link to Terry's points also, around 'making it right after that'), isn't this a question of 'making it right first time round', and removing the need for what seems to be a somewhat&nbsp;disproportionate amount of 'fixes' ? In my opinion, your peice seems to 'imply' that there needs to be an end-to-end review relative to the design stages, and validating that with the requirements specification. In addition, I would also be questioning the quality of the testing phase before putting the application live into production?
Coming from a service background (I recognise I dont have the full background here), this sort of stuff drives me mad and frustrates me so much as it feels like a 'quality issue' somewhere in the discipline of software development.&nbsp;
Best Wishes, Craig
	              
	              hi Babatunde,
Good work here!
liked your illustration diagrams, it made the content settle well on my head.
Regards
martins
	              
	              Hello Belinda,
I must agree with the other reactions: It is a good read.
Now I understand why a lot of programmers were complaining about JavaScript some years ago while it is use nowaday a lot again. I wonder how this speed is realised as I cannot imagine it is computerspeed only. This would not be realy scalable if speed can only be reached by throwing more boxes at it. Has experience learned to code more optimal or is it the interpretation process that has been improved?
Greetings from Bram

	              
	              Hello Christopher,
Your pictures help to understand better the more real time adjustability of the interpreted approach. But you also need to poll much more times for getting an updated answer. Would the difference not disappear if the GPS would get a more regular update (as much polls as the interpreted version)?

Greetings from Bram
	              
	              hi Craig, Terry, Babatunde,
Good work craig.&nbsp;
Babatunde I like your reply and am planning to try it someday. It sounds interesting and will like to see the outcome myself. I had issues ones locating my route on a train and seriously it wasnt funny. Eventually, I got back on track but it would have been a lot easier if i had tried this before that trip.
Regards, martins
	              
	              Thanks for this Tanisha,
May I please just clarify something, for my understanding. You mention that "with OOP you can reuse data while with structured programming languages you cannot do this". I just want to clarify whether this is absolutely accurate - in structured programming languages, could you not retrieve and simply store the data, in say a temp file, or a perm file for that matter, for use then, but also later on for a sub routine perhaps?
Or is that not the way you would achieve this, in that it is not best practice, and that you would simply go get the data again for a sub routine (possibly because you want most uptodate data)?
I have no programming experience whatsoever, so just looking for clarity and to maximise my learning. Thanks.
Best Wishes, Craig


	              
	              Hello Anthony,
This would be the situation where a problem on one end is solved quickly and checked on internal consistency only, but would lead in practice to non-optimal performance when run on a device(s). This could of course also happen with compiled code but the compiling process protects the hardware deployment more.
Bram
	              
	              HI Anthony.
It appears that OOP may not be suitable in instances that are better implemented through functional programming such as those involving conversion of inputs to outputs,&nbsp;command line programs, compilers, numeric calculations, engines of antivirus programs, web server side scripts.
For a compiler, I guess its because it strips down code segments to their lowest denominator and therefore its design does not require the complexity of OOP in its design.&nbsp;A compiler goes through the stages of completed these stages of Lexical Analysis (reads character streams from source code file), Syntactical Analysis (analysis and determination of validity of input) and then final generation of machine code. This process is more logic based as opposed to being data focused hence not suited for OOP
Reference
Stackoverflow 'Programmers - What is Object Oriented Programming Ill Suited for', Available at&nbsp;http://programmers.stackexchange.com/questions/52608/what-is-object-oriented-programming-ill-suited-for, Accessed 04/11/2014
Java,&nbsp;Software Development and Object-Oriented Programming Paradigms, Available at&nbsp;http://www.buyya.com/java/Chapter1.pdf, Accessed 04/11/2014
Joseph
	              
	              Hi Anthony
I'm sure there are others, but my immediate thoughts point towards errors in that this potentially creates some opportunities for inadvertantly updating (editing, deleting, amending) the code, and thus impacting on the successful running of that code thereafter.&nbsp;
I would expect there to be procedures in place to mitigate against these types of mistakes.
In the Service world, this would be done in a controlled and predictable manner, typically through change control, in which you would follow specific steps (as agreed by peers, and depending on the nature of the change, perhaps also authorised by a manager, change advisory board, programme change control) and typically include a simulated test of some kind with backout plan as part of the process.&nbsp;
Best Wishes, Craig

	              
	              Hello Terry and Craig,
Aspect-oriented programming looks interesting and your example shows that there is modularity at many levers and in various appearances. And I agree that all the credit should go to OOP.&nbsp;
My idea was that both the RESTful API approach as OOP (or OO) work with modular independent entities that only communicate via fixed interfaces and predefined messages. OO is modular inside a programme while RESTfull APIs relate to the inter-programms domain. I wondered if the API approach was invented historically seen after OOP or if it was a simmultanious evolution?&nbsp;
Bram

	              
	              Hi Ricardo
Please may I clarify - I thought the likes of Java and C#, and Objective C, which are Object-Oriented programming languages, are used extensively for mobile applications across android and iOS respectively?
Best Wishes, Craig

	              
	              Hi Bram,

Yes your right. I think I mentioned in the report that I made the assumption that the GPS in question for argument sake in this report was a none updating version like the old or early models.
Your right though modern day GPS systems give traffic updates and can adjust for both shortest distance to destination and shortest time or even avoiding motorways to help with this.
I not 100% but i'm pretty sure even these new systems are not that quick at doing the updates though, so you may get 10-15 minute delays for traffic updates etc so depending on the time at which you approach the incident may still mean you get stuck in them. Also from what i've heard from others they tend to only report on certain accidents and delays. Not road works or other lesser problems.

ta
Chris

	              
	              Hi Joseph,
Thanks for the references. They provide interesting empirical investigation on code reuse.
But this is somehow different than my observation. On the homepage of most open source software that is hosted on github, there's a link "Fork Me" (don't read aloud). I see that's the real openness. So you can choose to use it, change it to make it your own or contribute back.
Fork-reuse is mostly considered the anti-reuse. Why open source developers still allow people to fork?
br, Terry

Fork Me:&nbsp;https://github.com/blog/273-github-ribbons
	              
	              Yeah I recommend firebug also .. v handy


	              
	              Hi Augusto,
Hey, you are right. The Template Method Pattern is a structured programming way! But it does use polymorphism.
But I think it doesn't really matter if it's a black cat or a white cat, as long as it ... sorry, I have no idea what cats do nowadays.
br, Terry
	              
	              Hi

That's a tough one.

The only thing I can think of why it could possibly be an issue or disadvantage is from a support / BAU situation.
EG you could get lazy, because of the ease of updates to the system you may drop into bad practice, not testing updates properly, not worrying about whether the change you have made effects any other processes or systems because HEY its easy to go back in and continue to tinker about with the code.
Maybe it lulls you into a false sense of security about the program itself. Because it's easy to update it's also easy to be updated by none approved personnel, hackers and the types.

ta
Chris

	              
	              Hi Babatunde,
Thanks for introducing Firebug. No, I haven't tried it before. I use the similar functions in Chrome browser. But so far I haven't been very happy with it. Let me give Firebug a try.
br, Terry
	              
	              Hi Anthony,
Obfuscation is a way of making programs indecipherable, in the past I have used these technique in some occasions but it was not a very good one as the resulting code could be decompiled easily. I suspect that was due because at the beginning .NET was compiled in a line by line literal binary translation (Beaucamps, Filiol 2008).
I want to specify that I have mainly used these for scripting or small programs to automate infrastructure services. Since then I have moved more towards security communications (SSL, Certificate Signing) rather than securing the code. In my case, securing the code is secondary as it is not commercial one, while the primary goal is to secure the communication because the program can transfer confidential data, such a password. In fact, in the past 10 years I have discovered that not putting critical information in the code (i.e.: do not hardcode a password in your code!), and leaving the code on a properly secured and restricted server is good enough to protect it internally to the company.
Best Regards,
Augusto

	              
	              COMPILER VS INTERPRETED CODE
Introduction
&nbsp; The early computers needed to have more memory space and hence this desire was what led to the design of the first computer.
Navigation systems-
&nbsp;This are found mainly in cars and use for driving. Some of the advantages of this types of navigation system are:

Integration: The navigation system has an appealing look to the user. For example buying a car and seeing a large built navigation screen on the dashboard of the car appeal to most people.
It mostly has a resale value, when installed in any car the resale value of that car will be relatively stable because of the navigation system in it.
It is theft resistant because it cannot be easily stolen.
There is warranty coverage, so if it has a fault it can be quickly changed.

However, it also has some demerits which are as follows:

Most navigation systems cost a fortune hence can be avoided. They cost more than a thousand dollar and some have attachments to them.
Map updates on navigation systems could be very pricey, hence making use of it could be expensive. 
User limitations are found in some navigation system. For example: Some navigation may not allow the user to input data on it while driving.

GPS Systems-
This is known as global positioning systems and is used for navigation outdoors. Some of the merits of this system are:

It is used for navigation and finding location by providing the bearing for that location.
It has other many functions asides from navigation which are barometer, electronic compass, time &amp; date etc.
It can hold a map data and routine.

Moreover, here are some of the demerits of the GPS-

The device relies mainly on satellites for its information or data.
Interference from hills, rocks etc can cut off communication which may affect the user.
GPS cannot be reliable because they still have human errors in them and this could happen in the form of bad batteries.

Compiler-
This can be defined as a code that interprets the high level language into machine language. The computer scans the program after then it interprets it into machine code, and then the computer processor executes it. After this stage that particular task will be done. See the below diagram:
&nbsp;
 
&nbsp;
Interpreter-
&nbsp;This performs the same task as the compiler but in a different way or format. The interpreter after getting the language code, it translates it to an intermediate code before converting into the machine code and then for it to perform the task. See diagram below:
&nbsp;
 
&nbsp;
Difference between Compiler and Interpreter:-
1.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; The interpreter takes a particular statement, converting it and then executes it. After this, it now takes the next statement. This is done one after another until if finishes executing the program. While for compiler, it takes the whole program, converts it to machine code then executes it.
2.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; When an interpreter is translating a program, when it get an error report it stop, the translation at that point but for compiler it generates an error report after the translation of the entire page.
3.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; For compiler, it takes a lager time to analyze the code and then translating to machine code before execution. While for interpreter it takes less time.
4.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; The average time for analyzing and the overall processing time of a code is faster for the compiler as to that of the interpreter.
Pros and Cons of compiler / Interpreter language-
Advantages of Interpreter language-

During development stage, testing and changing the application is simple and fast.
Allows the same source code to be run on any platform that has an interpreter.
The language has automatic memory management.
The platform is independent.
The language has dynamic typing.
The language has dynamic scoping.
It is a small program size language.
Reflection and reflective usage of the evaluator. Example is the first order eval function.

Disadvantages-

It is slower in running.
The source code is available to everyone (open source).
An interpreter is needed in the local machine.

Compiler language-
Advantages-

The language runs faster
The executable file is distinct from the source.

Disadvantages-

Testing the application during development requires constant recompilation.
Output always specific to a particular platform.

Conclusion-
Programming language cannot be complete without the use of either a compiler or interpreter. They are the basics elements needed to translate a program code into what the machine will understand.
References:
Difference between compiler and interpreter [Online] Available from:https://www.engineersgarage.com/contribution/difference-between-compiler-and-interpreter.(Accessed 2 November, 2014)
Pros and cons of GPS [Online] Available from:https://www.trails.com/fact_5377_pros-cons-gps-system.html. (Accessed 2 November 2014)
Pros and cons of interpreted language [online]Available from:https://www.stackoverflow.com/questions/1610539/pros-and-cons-of-interpreted-languages. (Accessed 2 November 2014)
Riodan, R. (2011) Fluent C#:Application development [online] Available from:https://www.informit.com/articles/article.aspx?P =1761784&amp;seqNum =16. (Accessed 3 November 2014)
www.edmunds.com/car-technology

	              
	              
Hi Anthony,
Obfuscation is a way of making programs indecipherable, in the past I have used these technique in some occasions but it was not a very good one as the resulting code could be decompiled easily. I suspect that was due because at the beginning .NET was compiled in a line by line literal binary translation (Beaucamps, Filiol 2008).
I want to specify that I have mainly used these for scripting or small programs to automate infrastructure services. Since then I have moved more towards security communications (SSL, Certificate Signing) rather than securing the code. In my case, securing the code is secondary as it is not commercial one, while the primary goal is to secure the communication because the program can transfer confidential data, such a password. In fact, in the past 10 years I have discovered that not putting critical information in the code (i.e.: do not hardcode a password in your code!), and leaving the code on a properly secured and restricted server is good enough to protect it internally to the company.
Best Regards,
Augusto
&nbsp;
References
Beaucamps, P. &amp; Filiol, E. (2008) 'On the possibility of practically obfuscating programs towards a unified perspective of code protection', Journal in Computer Virology 3(1), pp. 4 September 2014-1-2 doi: 10.1007/s11416-006-0029-6.
&nbsp;

	              
	              Hi Craig,
Let me try to answer your question. I could be biased, since I'm a human-programmer.
So far, there's still no silver bullet. All the efforts of achieving the goal as you mentioned didn't even get close.
Actually people were already tired about typing plain text as source code long time ago. So a lot of tools that can "generate" code by graphical modeling, or via visual interface were invented. Examples are like IBM Rational Rose and Microsoft Visual Studio.
They didn't dominate the world. Actually people talk less and less about them now. The visual editing is only a small part of Visual Studio.
If you program Java, there are many IDEs that can help you generate the scafolding code. But if you use modern language like Python, Ruby or JavaScript, the best tool you can use is the good old text editor. I use VI.
So, in the past 20 years the path is like Model Driven Architecture -&gt; Java IDE -&gt; Text Editor. It seems our software industry is going backward. Why is that?
I think it's because programming is pure intellectual work. It's both creative and artistic. From understanding the requirement, to have many views to the problem, to utilize all the knowledge and experience to create a solution, to emperically improve the solution, all these need human intellegence. The tools we had didn't free human mind but constrained human mind. A text editor frees human mind.
So, as long as the computers do not have the human intellegence, self-creating application is not possible. And when computers have that, why would they bother create it for human? They would create art to entertain themselves and create apps to serve themselves. Because that's part of the human intellegence.
Sorry for not having any references. I spent a lot of (too much) time speculating useless things like this:P
b, Terry
	              
	              Hi Bram,
I cannot find when the term API is coined. But the concept should exist way early than OO.
br, Terry
	              
	              Hi Anthony / Craig,
Craig, you are spot on! In my experience that is most common issue I have faced: because is so easy (as easy as using notepad or vi Editor), someone would go and mistakenly change things. Change control is good, but very often little things that run in VBScript, PHP, JavaScript or PowerShell are out of the scope of Change Control, or the administrators do not necessarily think it is part of change control.
I tell you a funny story: years ago someone at the helpdesk decided to change a VBScript that was built to delete computers from Active Directory. The script was meant to be run computer by computer, the idea behind was that the helpdesk would run the script every x months to delete several no longer used computers, but by using it line by line there would be some human control on it and things will not go south. So this person thought to “improve” the script by allowing a parameter to send a comma separated list of computer names. As it turns out when it came to execution the wrong list of computers was selected and converted to a long string of comma separated values, pasted as the parameter and hence executing the deletion of 200 valid computer accounts (business users’ desktops to be precise).
A simple change caused a ton of issues! I let you imagine the havoc. The funny part of this story is that nobody at the helpdesk asked if there was a backup of Active Directory (there was in fact and quite easy to restore the computer accounts) so they went ahead and fixed their own error by calling 200 people and re-binding their machines to Active Directory. That was really nuts!
Best Regards,
Augusto

	              
	              Dr. Anthony,Yesterday, I was tasked with a seemingly simple task; upon the click of the verify button for a employee, several corresponding records for that employee is expected to close. I thought it was easy, just go and incorporate a series of update statements on click of verify. But as it turned out it was not, as the associated tables that needed to be updated had triggers that needed to be enabled and disabled accordingly. I don't like to code. Today I find myself writing more code than expected and as I am staring at it, it still does not work. A half day task goes into one and a half days for a non programmer.This is one of the first disadvantages of Object-Oriented Programming (OOP), it can be harder to implement as it often requires proper coding and understanding of the code. As such the person that is required to do the programming may need to be highly skilled in that area. Below are some instances which it may be unsuited for.1. From time to time it may require a greater amount of system resources that other programming approaches. Therefore in instances when little resources needs to be involved it may be best not to use this approach.2. OOP is not supported by all programming languages, thus may languages may not be able to apply this approach. As such it is good to do a proper assessment of the type of programming language that may be better suited to design and develop your application.&nbsp;3. It may be harder to learn OOP as compared to other programming approach. Therefore consideration need to played with the knowledge base of the individuals involved in the task at hand and the time involved to complete that task. If persons involved do not know OOP or are under a heavy time constraint it my be best not to use this approach.4. OOP is more suited for large applications and not for small &nbsp;applications.5. If you need a fast low intensive application, OOP may not be suit as it may require greater resources and may perform slower than that of the requirements.References:Bansal (2002) Fundamentals of Information Technology, &nbsp;Pages 136, APH Publishing. [Online]. Available at:&nbsp;http://books.google.gy/books?id=cEvqQ_ZbtUkC&amp;pg=PA136&amp;dq=disadvantages+of+oop&amp;hl=en&amp;sa=X&amp;ei=e2lZVMq4B5fLsATerYHYAg&amp;ved=0CDcQ6AEwBQ#v=onepage&amp;q=disadvantages%20of%20oop&amp;f=false (Accessed on November 04, 2014) &nbsp; &nbsp;&nbsp;Dixit, J. B. &nbsp;(2005) Fundamentals of Computer Programming and Information Technology, Pages 411, Laxmi Publications. [Online]. Available at:&nbsp;http://books.google.gy/books?id=uI7BZgKPvO8C&amp;pg=PA411&amp;dq=disadvantages+of+oop&amp;hl=en&amp;sa=X&amp;ei=e2lZVMq4B5fLsATerYHYAg&amp;ved=0CCAQ6AEwAQ#v=onepage&amp;q=disadvantages%20of%20oop&amp;f=false (Accessed on November 04, 2014)&nbsp;
	              
	              Dear Dr. Ayoola,
When you try to optimize some inherent disadvantages from an existing method, ususally, something "evil" will appear.
Evil example 1: Template metaprogramming
TMP was discovered accidentally. The C++ template wasn't designed for meta-programming, but people found it can do a lot of stuff (in a quite strange way), because it's Turing Complete. People are so excited about this is because the template resolving happens at the compile time. So you can still program in the OOP style, having the inheritance and polymorphism. But the compiled result will have a flat class layout and no dynamic methods. The result is very compact, and optimized executable. One of the drawback is the compiling time is much longer.
It's often combine with another way to (ab)use object-oriented programming, called Generic Programming. Why I call them evil? Because they made C++ into C+++++ now. It's extreme hard to learn.
Evil example 2: Embedded C++
This goes to another extreme. The Embedded C++ is a dialect of C++ tailored for embedded use. The dynamic features and type information are removed, in the hope of getting the similar performance as C. And typically, there's no STL in embedded C++. Why I call it evil? They really should just use C.
Evil example 3: Static and Singleton
Want a smaller footprint? Use static! Or Singleton. There's only one instance in the system. Why I call it evil? It destroyed the OO concept.
Conclusion
To eliminate the inherent disadvantage of OO, probably will need to look at using other alternatives instead of seeking inside OO. Changing OO inherent stuff will lead to destroy the OO concepts and OO then will harm our design rather than help our design.
br, Terry
References:
Template metaprogramming. (2014, October 15). In&nbsp;Wikipedia, The Free Encyclopedia. Retrieved 01:50, November 5, 2014, from&nbsp;http://en.wikipedia.org/w/index.php?title=Template_metaprogramming&amp;oldid=629688737
Generic programming. (2014, October 8). In&nbsp;Wikipedia, The Free Encyclopedia. Retrieved 01:50, November 5, 2014, from&nbsp;http://en.wikipedia.org/w/index.php?title=Generic_programming&amp;oldid=628753454
Embedded C++. (2014, June 12). In&nbsp;Wikipedia, The Free Encyclopedia. Retrieved 01:51, November 5, 2014, from&nbsp;http://en.wikipedia.org/w/index.php?title=Embedded_C%2B%2B&amp;oldid=612654012
	              
	              
Hi Craig,I am no big programmer myself, as I am currently struggling in my new programming job. The information and questions asked here are fairly new, and even if some of it was thought to me before it remains tiny fragments of my imagination.&nbsp;However, to answer your question, I believe that in the context of what they are saying, they are merely referring to the flexibility of the coding structure that is often used in OOP as that of Structured programming. In that, Structured programming, surrounds itself around the developing of a algorithm and the solving of that algorithm. The entire structure therefore may not be as easily reusable as compare to that of OOP.&nbsp;I think it may also refer to the overall setup of the application at hand. In that, if it is build specifically for one thing it may be difficult to add something else or add additional features. While with the OOP approach may provide greater flexibility in that it is not that algorithm specific. To me if looking at little bits of code it may be difficult to see it, because it may all look similar. However, it there are larger amount of code involved then you get to see the true flexibility of the OOP approach to the structured approach.I do hope it is a bit clearer for you, as this is how I interpreted it to be.Tanisha.References:Harel, D. &amp; Feldman, Y.A. (2004) Algorithmics: the spirit of computing. 3rd ed. Pages 350, Pages 351, New York: Addison Wesley.Brookshear, J.G. (2011) Computer science: An overview. 11th ed. Pages 308 Boston: Pearson Education / Addison-Wesley.&nbsp;

	              
	              Hi Craig,
I agree there is a need for a end-to-end review of the entire requirements and design of the application. I gather the application has been in existence for quite some time and as such it may even need changing all together.
And like you said, it can be very frustrating having to deal with certain things. I would really like to see how a off the shelf payroll application works, as they are often some real problems with this application and they just pop-up from no where. Not to mention&nbsp;there is a file of possible errors and back end, check, updates and fixes that are involved. And information is added to that file as new problems develop. Its quite interesting actually. And very&nbsp;stressful.
Tanisha.
	              
	              Thanks for your comments Terry.
Tanisha
	              
	              Dr. AnthonyWhile the ability to easily update code may often be advantageous. There are issues or instances when it may prove to be a disadvantage. Depending on the outcome that we are trying to achieve, sometimes updating is not always the best option, as over time after constantly applying updates it may eventually become difficult to understand the code and to apply additional updates and or fixes. As such, depending on the circumstances, it may be a good idea to review the entire code, its requirements and design and then look at ways of overall improvement or if possible look at ways of implementing a replacement program where it may be necessary. As we have learned, constant updates and fixes over time can impact the original design and structure of the program.Additionally from a security perspective, if the code is accessed by unauthorized persons, this ease of updatability may prove to be a disadvantage, as these persons may be able to look at the code, interpret in and apply unauthorized updates. Thus, negatively hamper the overall functionality of the application.TanishaReferences:Harel, D. &amp; Feldman, Y.A. (2004) Algorithmics: the spirit of computing. 3rd ed. Pages 350, Pages 351, New York: Addison Wesley.Brookshear, J.G. (2011) Computer science: An overview. 11th ed. Pages 308 Boston: Pearson Education / Addison-Wesley.&nbsp;


	              
	              Tanisha,
Congrats to your promotion:-)
br, Terry
	              
	              Hi Craig,
You are right, quality is an embarrassing topic of programmers... I was told that when Nokia released their N71 model there's still 17% known bugs not fixed.
But to clarify one thing -- The "work" in "make it work" actually means "the external quality right". So it's already functioning as required, no bugs. And "right" in "make it right" means to have proper internal structure, or say internal quality.
Poor internal quality doesn't necessarily mean poor external quality. But in a longer term, poor internal quality makes it hard to keep a good external quality.
br, Terry
	              
	              Hello Class,
&nbsp;
As our usual way of rounding up the week's activity, please provide a short summary of the key 'Computer Structures' lessons learnt in week 9, from your perspective.
&nbsp;
Anthony

	              
	              Yes, good points guys. Thank you.
Tanisha - I suppose the flip-side of this is that it provides a great learning experience, in that you are being presented with many challenges which are helping you develop and hone your skills.&nbsp;
Best Wishes, Craig
	              
	              Hi Tanisha
Thanks for your response - Yes, I see that, and I appreciate your perspective.&nbsp;
Also, I'd be happy to help and share my perspectives if you want to use me for any advice as you integrate into your new role, just drop me a line at craig.thomas@online.liverpool.ac.uk.&nbsp;If you need programming specific advice, I'm sure some of our more experienced programmer class colleagues would be happy to help you.
Best Wishes, Craig

	              
	              Hi Terry
A brilliant read, thank you.
I am not really able to contribute to Anthony's question so your response has help provide some great insight with respect to not over-engineering (or abusing) existing programming capabilities, and to be open in looking at alternatives to resolve the problem.
Best Wishes, Craig
	              
	              Hi Guys,


There are many beautiful ideas that people associate with OOP. I am going to show 2 things: 

1.)&nbsp;&nbsp;&nbsp; Compared to other languages (lisps, functional languages, etc) OOP languages have no unique strengths

2.)&nbsp;&nbsp;&nbsp; Compared to other languages (lisps, functional languages, etc) OOP languages inflict a heavy burden of unneeded complexity


Those features which are potentially good (data hiding, contract enforcement, polymorphism) are not unique to OOP and, in fact, stronger versions of these things are available in non-OOP languages. Those features that are unique to OOP (dependency injection, instantiation) are awful and exist only because OOP is awful.


Best Regards, Babatunde
	              
	              Hi Augusto
Thanks for your post - I appreciate your comparisons.
I also thought your closing comments outlined another good analogy which demonstrated the need to consider your options relative to your desired outcomes which may also look to leverage other existing capabilities within the chosen option.
Best Wishes, Craig
	              
	              Hi Craig,

Good question and I share your query on this.
I don't ever see a stage where it is completely automated, after all someone has to create the interface and code behind the automation so I don't think it will ever be truely fully autonomous and self creating or anything, unless cyberdyne finally create skynet :) But for speficic niche areas of programming maybe so. Or at least project initiation phases. My thoughts are around what I'd guess you'd call Wizards but just voice activated. Like you said there may be a future where a voice activated GUI can build or package up a starting environment ready to be modified / changed.
For example:&nbsp; (voice commands)
Hi Computer: "Create NEW project named 'Hello world' using Java, implementing classes X Y Z, based on OOP"
You would then be presented with a new project area into your local development environment&nbsp; with all the base classes already inside and referenced, project named , with core OOP practices in place and ready to be modified.
Backups all preprepared and timed , source code checked into the appropriate places.
This system could have a range of base programming languages and package designs set on specific libraries etc. and software.&nbsp;
I could logically see that type of application initiation automation, try saying that 3 times fast!
But just can't see how it could ever be fully automated simply from the human aspect and abstraction.
Even things that are the same are still unique. So you will always have a need to alter / create more.

just my thoughts.
ta
Chris



	              
	              Hello Terry,&nbsp;
I might mix up the Z and Y due to English and Swiss keyboards I have to use. But that is another story.
cheers,

Bram
	              
	              Dear Colleagues,
This is a very fruitful week. I sincerely appreciate&nbsp;all the feedback from my colleagues. Again, this is a very familiar topic for me. So I've been focusing on technical writing and communicating ideas again this week. I feel that I wrote things faster than before and a drawback is the quality might become lower. I noticed that my score from Grammarly.com hasn't increased at all.
Programming is an art. Just like any other arts, to master it you need to learn a little theory and do 3 other things. These 3 other things are practice, practice and practice. And performing is not practicing. So even if your work is programming doesn't mean you've done your practice. These things are easier to explain to the other artists, I guess. You don't always perform on the stage, you spend more time practicing behind the scene.
br, Terry
	              
	              Hi Terry, As always it depends on the definition of API: &nbsp;"The concept of an API pre-dates even the advent of personal computing, let alone the Web, by a very long time! The principal of a well documented set of publicly addressable "entry points" that allow an application to interact with another system has been an essential part of software development since the earliest days of utility data processing. However, the advent of distributed systems, and then the web itself, has seen the importance and utility of these same basic concepts increase dramatically." - Martin Bartlett APIs as solution for connectng entrerprise solutions during the Service Oriented Architecture (SOA) movement, which started sometime around 2000, a portion of the SOA experiment left the enterprise and found a more fertile environment in the world of start-ups. Either way history is more complex that that one leads simply to another. I guess OO concepts also existed before OOP was recognised as a language. What counts is that a concept gets popular and translated to other domains. Greetings from Bram referencing Kin Lane http://history.apievangelist.com/
	              
	              Hello Chris,
Thank you for your response and indeed it is mentioned that a assumpution is that the GPS updates every 15 minutes or so.
But using the imaginary iff: Iff the GPS would update as quickly as your Interpreted example, would they not have the same efficiency?
Greetings from Bram
	              
	              Hi Bram,

Technically I would guess the efficiency would be the same if it was an ideal scenarios like you mention but in real-time there would be the potential for many abstractions or unknown / chaos to interfere.

EG the Sat Nav updates quickly but your on a one way street so it doesn't help. But then again this wouldn't help in the other paradigm either as it's not like your friend on the phone can magic up a new road.

So it's a difficult one to measure but i guess for this exercise it was simply a way to demonstrate the differing paradigms and how they are followed.
Interesting though!
ta
Chris

	              
	              Hi Anthony,

I've enjoyed visiting the many layers of programming paradigms this week and reviewing the differences for the HIA.
It was good to get back to my roots of OOP and Structured programming as that is really where I started my thirst for these subject many years ago now.
There have been some interesting and new discussions around the future of how programming may get too as Craig asked this week.
Overall another good subject
thanks
Chris

	              
	              Hi Bram,
Thanks for sharing. Yes, I think all these now popular concepts are based on some other previous works and there's hardly any clear roots.
Yes the concept of OO also existed for very long. But I think OO has been always about/around OOP. I don't know if you noticed that OO doesn't really reflect the human thinking, it's actually often counter-intuitive and is very unnature (if you do it right). That actually limited OO to be in the solution domain. OO way of thinking is useless in other places.
br, Terry
	              
	              Hi Anthony

I believe every view has a merit as it is from the viewpoint of the person raising the issue. So I wouldn't dismiss that thought because I'm sure in some situations OOP would be over designing and complicating things.
However my own opinion is that this is simply about what the job that is needed to be done.
If you require a solution to be designed in an OOP way because of future known and planned uses then no OOP isnt a problem but an aid to the solution and arguably the best way forward. Again this doesn't detract from some situations where this isn't the case and that code re usability just simply may not be needed so why do it.!
So I think you have asked a chicken and egg type question.
The people who claim OOP is over hyped probably have good reason to do so but also may be a factor that someone made the wrong decision at some point to implement OOP designs into a Structured environment or task to be achieved.
Similarly I've heard the opposite with Structured programmers complaining that it is too rigid and not portable/reusable enough for new projects and they end up spending a lot of time rewriting the same code again.
So yes there are merits to that line of thinking.

ta
Chris



	              
	              Hi guys, can I chip in some code optimizations techniques….



FORTRAN is used for heavy number crunching, and often decreasing the running time of a program is desirable, however, it shouldn't be set as a primary goal except on special cases.



While doing manual optimizations, it's a good idea to check the code also from a numerical point of view. At least, avoid optimizations that may lead to accuracy deterioration. 



The need for profiling


Since it's common that programs spend most of the execution time in a few small parts of the code (usually some loops), it is recommended &nbsp;to perform first "profiling", i.e. analysis of the relative execution &nbsp;time spent in different parts of the program. 


Profiling is also important when you apply optimizations, compilers and computer hardware are complicated systems, and the results of applying an optimization may be hard to guess.

Am I making sense?

Best Regards, Babatunde
	              
	              Hi Babatunde,
You are making perfect sense:-)
Let say the ability of estimating the performance of a system without actually building it is very desirable, but there's very few people can really do that.
Jeff Dean is one of them.
br, Terry
	              
	              Thanks Craig,
I will note you email and make contact with you if needed. Thanks again.
Tanisha.
	              
	              Hi Anthony,

This boils down to the term: --&gt; Refactoring
Refactoring is the process of taking an object design and rearranging it in various ways to make the design more flexible and/or reusable. There are several reasons you might want to do this, efficiency and maintainability being probably the most important. 
--&gt;Situation where this feature could also be a disadvantage

A reason not to refactor at a given moment is when a deadline is very close. Then it would take more time to do the refactoring than the deadline allows. It will be much more sensible to deliver the system as is and to delay the refactoring after the deadline. The cost of delivering a system in such a state may be very high in terms of maintenance and stability.
another, If the concerned code is neither able to compile or to run in a stable manner, it might be better to throw it away and rewrite the software from scratch. This time using refactoring to avoid the mistakes made earlier. Perhaps the system can be divided into strong encapsulated loose coupled parts (subsystems) for which the decision of rewriting or refactoring can be made independently.
 

Best Regards, Babatunde

Reference:

Refactoring your code [online] Available from: http://etutorials.org/Programming/mastering+delphi+7/Part+II+Delphi+Object-Oriented+Architectures/Chapter+11+Modeling+and+OOP+Programming+with+ModelMaker/Refactoring+Your+Code/ (Accessed on November 5, 2014) 
	              
	              Hi Terry - I knew you were going to say Jeff Dean before I even got to the end of your sentence! :-)&nbsp;
How's that for a predictive, automative and self creating algorithm! :-) &nbsp; &nbsp; or human intuition...
Best Wishes, Craig

	              
	              Hi Anthony,

I think we should care about risks with reuse because; these risks are real not made up and can make the difference between success and failure for your development effort. Here are a few risks that spring to mind:


Inflexible design/will cost too much to modify
Domain irrelevance
Inadequate documentation/training/awareness
Ability to meet deadlines/dates (schedule risk)
Increased development, testing, and maintenance costs


These are not all the risks with reuse but the major ones that I have come across.

Best regards, Babatunde.
	              
	              Hi Anthony
I must admit, that my initial discussion question answers this week were too long. But it is hard to make it short and simple when it’s an area that has so many aspects. If I was completely mastering the subject, like some of mine colleagues, then I guess it would be different.
This week was a mixture of completely new knowledge to me, but also a little bit of refreshment from courses I attended some 10-15 years ago where we studied, in general, programming languages and object oriented programming.
Here are In bullet form some of the issues I have learned about:

Principles
SP versus OOP
Compiled versus interpreted code.
The programming paradigm differences, and which programming language supports each paradigm, and how. And that there are actually some languages that support several paradigms.
Despite that the majority is for OOP there still a great deal many how would rather stick to the SP.
The choice between compiled, interpreted or a hybrid code, depends on the situation, e.g. which platform the program is supposed to run on.

&nbsp;
Best wishes
Bo W. Mogensen

	              
	              Hi Bram
Regarding the update, I would like to think that if programmed correctly, then it shouldn't matter whether its interpreted or compiled, or if not programmed correctly then you will have problems on both, sooo what to do?&nbsp;I can't help but think about and compare the updates to the daily problems we have updating our systems&nbsp;(windows, java, Explorer etc.), in controlled environments, with the tool SCCM to help us.
So perhaps we just have to wail till we all have&nbsp;some wireless connection in our car, or where ever we are, and just use web solutionse, always (will that ever happen?) updated at vendors servers.

Br. Bo
	              
	              And guys
Thank's for the flattering&nbsp;response. You made my day
	              
	              Hi all
Those were very interesting numbers that you Belinda put out.
Regarding speed, I believe that performance will always be an issue. Just as hardware and communication improves, so will the software part just it will give more possibilities, like web-services, and this will eat up the other performance improvements leaving us with the same response times perhaps a little bit cheaper.
When I worked in the bank we had a general rule of thumb that I should have my response after 3-5 seconds. If it was above, the users would complaint. I’m in a municipality now, but also here the users want fast responses, maybe not as fast as in the bank, but close.
PS: On several occasions, when we had something developed, the programmer would tell us that the system was ready and was working well. But when we tested we often experience slow responses. What the programmers usually forgot was that they were actually sitting on the mainframe having maybe 1 Gbit connection.
Br Bo
	              
	              This industry is pre-occupied with reuse. There’s this belief that if we just reused more code, everything would be better.Some even go so far as saying that the whole point of object-orientation was reuse – it wasn’t, encapsulation was the big thing. After that component-orientation was the thing that was supposed to make reuse happen. Apparently that didn’t pan out so well either because here we are now pinning our reuseful hopes on service-orientation (Dahan, 2009).
	              
	              I recently came to know about Firebug during troubleshooting one issue with HP software engineer. Yes, its a great help for developers and very useful during troubleshooting.
	              
	              Hi Numan
I'm becoming a little confused here and hoping you can help me?
Are you saying that reuse is not a good thing here - based on my learning, isn't OOP inheritance a form of reuse?
Best Wishes, Craig
	              
	              Hi Anthony,
I think that between OOP and procedural, it's more a personal choice. A solution is not better than other and most importantly, we cannot judge the value of a code only on the method used. Clearly, a code in OOP or procedural will have the same value. This is a choice you make based on several criteria: reuse, skills, design, scalability, compatibility, etc.
We cannot say that OOP is more suitable for such type of program and the procedural will be for another. It is based on the developer and the various features of the program that the choice imposes itself.
Regards,

	              
	              Hi Colleagues,
My thoughts were around the same concept. When code is easy to update (interpreted code) it allows for manipulation and can be stolen, reversed engineered and used without permission. It can also be injected with other malicious code that could be harmful or even detrimental to an unsuspecting user/reciever of the code.
Augusto, thanks for sharing that story. I use a lot of little scripts to accomplish various tasks in my regualr administrative duties but never examined the potential danger that could occur if a junior administrator thaught of exploring the scripts and modifying portions of it. I'l definately but those under a secured file and do some training on script usage.
Thanks for sharing.
Regards,
Ricardo
	              
	              Hi Belinda,
I wanted to also congratulate you ona very good post. It identified a few things i misunderstand or better yet didn't grasp properly when I first went through the material. I especially like the elephant analogy under the structure progamming concepts. This post has formed apart of my review material, thanks for sharing.

Regards,
Ricardo
	              
	              Hello Terry,
Yes, if you refer how the compiler process the function call internally yu can call it as syntactic sugar, but as per the developer point of view it is same.The whole point of using method overloading is the defining the meaningfull classes in the application architecture.Similarly, even the operator overloading is treated and processed completely different by the compiler, when you define something like;
Class A{int a;int b;
public static A operator +(A a, A b){
A c = new A();c.a = a.a+b.a;c.b = a.b+b.b;return c;
}
}

and in main function
// assign some values to objects a1 and b1 of class A.
A c1 = new A();
c1 = a1 + b1 &nbsp;
if you observe the program, even here the meaning of the '+' operator is completely changed as per the compiler,
which is not clearly visible to the user/developer :-)

References:.Net framework -&nbsp;http://en.wikipedia.org/wiki/.NET_FrameworkOperator Overloading - http://msdn.microsoft.com/en-in/library/aa288467(v=vs.71).aspx
Best regards,Kharavela
	              
	              Hi Craig,
That is a good point. I think today smart phones leverage Object-Oriented programming (OOP) languages. But I am sure that was not always the case. The old Nokia phones certainly had a lot of resource constrains so I doubt they were based off OOP languages. I think what Ricardo hints at is that OOP does not work well in systems where there exist resource constrains, mainly because objects do tend to be big. Large objects are hard to handle when you have limited RAM and almost no disk for paging.
Ricardo, the web pages are a good example. At least for HTML (declarative programming). It won't make much sense to use OOP there. However, OOP can be used on Web Applications they may seem like web sites, but the trick here is that the Web Application generates the declarative language that the browser will execute.
Best Regards,
Augusto
	              
	              I can't get more than 83% in grammarly.com, no matter what. Not sure that makes you feel better, but I guess since english is not my native language 83% is not bad. Although that makes me wonder for the final dissertation &nbsp;how much I will need to correct, gulp!
Thanks for all your contributions Terry!
	              
	              Hi Dr. Ayoola,
This week was though for me. First I got a cold and it is thought to focus having it, plus no chance to stop working to get some rest (bad side effect of working from home). Second, I know some programming, but there was a lot of material here that I never really covered. But that is also good because I got to learn a lot of things:

Programming Paradigms.
Lots of Object-oriented programming information and details from other programmers.
I used in the past Functional programming, but I did not know it was called that way nor the details of it. Was very good to study it and learn the basics.
Compiled vs Interpreted code continues to be a hot debate (have not participated in one in a while).

Best Regards,
Augusto

	              
	              Hi Augusto,
83%! OK, now I have a goal:-)
br, Terry
	              
	              Hello Anthony,
I really had to step up to do 2 DQs and an assignment. My earlier time schedule was not sufficient to deliver those 2 DQs at 24:00 at Sunday and worked a few more hours to have them both on the discussion board. Hopefully I will manage that better this week.

The subject was great and I wanted to get through the differences and history of programming languages already for a long time. Glad it finally happened.

Greetings from Bram
	              
	              Craig,
This is our unique algorithm that might never be stolen by computer. (I'm not really confident when saying that.)
I wish I can meeting Jeff Dean in person one day, that would include getting a signature on my laptop with a permenant marker and a wefie. Maybe the Google I/O conference next year...
br, Terry
	              
	              Hello Terry,
Indeed I noticed OO is quite artificial. Thank you for this thread. Looking forward to next week's discussions.
Bram
	              
	              Agreed, tnx Bo.
Greetings from Bram
	              
	              Hi Babatunde,
Thanks for your post. It triggered me to think more on this.
I won't describe reuse as good/bad thing. It's just not a very interesting goal (you just listed some reasons).
Reuse will happen/will not happen in a proper design, as a side-effect and a result. But it shouldn't be the initial goal.
br, Terry
	              
	              Hi Numan,
Thanks for sharing, very interesting and insightful idea about reuse.&nbsp;Bo Mogensen also shared the work by Udi Dahan before.
A very interesting thing here is that he (Udi Dahan) claimed that "encapsulation was the big thing". But there are also different opinions on that. Some thinks encapsulation is not a big deal. At least encapsulation is not as important as imformation hiding (see reference). And programming language like Python doesn't even have the "real" encapsulation. Python doesn't even have "real" private member field and method.
References:
http://c2.com/cgi/wiki?EncapsulationIsNotInformationHiding

	              
	              hi Anthony (Dr.),
These are some of the methods i think will be usefull.

keep to your secret at all times
the correct privileges should be used

thanks,&nbsp;
regards, martins
	              
	              Hi Craig,
Yes, OOP inheritances was a form of reuse. It's not any more since year 1994. Something big happened in the software industry that year. A book was published. And our assignment next week is about that book:-)
br, Terry
	              
	              hi Tresor,
i read through your post and i must say its really well constructed. like your conclusion how ended OOP.
welldone
martins&nbsp;
	              
	              Hi Kharavela,
As you said the operator overloading "which is not clearly visible to the user/developer". Is that a good thing or bad thing? Actually I found the explicity and implicity is very important part in a language's own design philosophy.
In Ruby, everything is so implicit, you can write Ruby code that people couldn't recognize it's Ruby.
In Python where explicit is encouraged, the BDFL won't allow too much magic like that.
br. Terry
	              
	              hi Anthony,
This week was rough for me as i had to manage my time to get in two DQ and one HIA.
overall, i learnt alot on OOP, compilers and interpreted codes, programming paradigms etc.
more knowledge gained.
Regards, martins
	              
	              Hi Augusto,
The development languages on the Nokia Symbian system are C++, Python and Java. The are all OO languages.
Symbian was actually a pretty good system. Nokia used it until 2011 before the big obvious&nbsp;trojan horse from M$ became their CEO.
br, Terry
	              
	              Dr. Anthony,
Please see below some of the methods that I believe can be used to minimize or mitigate security issues associated with interpreted code.1. Access control – enforcing stricter access on code.2. Error handling – enforcing proper error handling methods.3. Database security.4. Data protection.5. Password management and authentication.6. Session Management.7. Stricter licensing so application can not be easily acquired.I think that there may be other methods than can be explored as means of securing this type of code.Tanisha.
	              
	              


Dr. Anthony,
Please see below some of the methods that&nbsp;can be used to minimize or mitigate these issues are as follows:
1. Access control – enforcing stricter access on code.
2. Error handling – enforcing proper error handling methods
3. Database security
4. Data protection
5. Password management and authentication
6. Session Management
7. Licensing 
I think that there may be other methods than can be explored as means of securing this type of code.
Tanisha.

References:
OWASP Secure Coding Practices Quick Reference Guide [Online]. Available at: https://www.owasp.org/images/0/08/OWASP_SCP_Quick_Reference_Guide_v2.pdf (Accessed on November 04, 2014) 

	              
	              Dear Anthony,
Object-oriented languages still have to learn some engineering lessons from procedural languages. In fairness, designers of object-oriented languages did not simply "forget" to include properties such as good type systems and good modularity: the issues are intrinsically more complex than in procedural languages. Therefore, we have to work harder to produce object-oriented language designs that entail good engineering properties. We have to work even harder to produce good language designs.
BR,
KingTan Yu
Reference
Ben Brosgol(2012); Object-Oriented Programming for High-Integrity Systems:Pitfalls and How to Avoid Them; Online : http://ieee-stc.org/proceedings/2012/pdfs/2984BenjaminBrosgol.pdf
	              
	              HI all,
Not all programs can be modeled accurately by the objects model. If you just want to read in some data, do something simple to it and write it back out, you have no need to define classes and objects. However, in some OOP languages, you may have to perform this extra step. Another disadvantage is that if you force the language into the OOP concept, you lose some of the features of useful languages like the "functional languages." Another disadvantage is that one programmer's concept of what constitutes an abstract object might not match the vision of another programmer. The objects often require extensive documentation.
BR,
KingTan
Reference:
tads.org(n.d.) Object-Oriented Programming Overview; online :http://www.tads.org/t3doc/doc/techman/t3oop.htm
	              
	              Hi Anthony,
Yes, it will also be the major disadvantage that mainly realize in security issues.&nbsp; First of all is the issue of the remote user's access to the script's source code. The more the hacker knows about how a script works, the more likely he is to find bugs to exploit. With a script written in a compiled language like C, you can compile it to binary form, place it in cgi-bin/, and not worry about intruders gaining access to the source code. However, with an interpreted script, the source code is always potentially available. Even though a properly-configured server will not return the source code to an executable script, there are many scenarios in which this can be bypassed.Another reason that compiled code may be safer than interpreted code is the size and complexity issue. Big software programs, such as shell and Perl interpreters, are likely to contain bugs. Some of these bugs may be security holes. They're there, but we just don't know about them.A third consideration is that the scripting languages make it extremely easy to send data to system commands and capture their output. As explained below, the invocation of system commands from within scripts is one of the major potential security holes. In C, it's more effort to invoke a system command, so it's less likely that the programmer will do it. In particular, it's very difficult to write a shell script of any complexity that completely avoids dangerous constructions. Shell scripting languages are poor choices for anything more than trivial CGI programs.
Reference
Dominique Brezinski (n.d.); A Paranoid Perspective of an Interpreted Language; online: http://www.blackhat.com/presentations/bh-jp-05/bh-jp-05-brezinski.pdf
BR,
KingTan Yu
	              
	              Hi Dr. Anthony,
This week was one crazy whirlwind especially at work and it threw a wrench into my study plans. Nevertheless I beleive I made it through. The 2 DQ's did bold me curve ball, almost got the better of me and put me out for a duck.
I did however enjoy this week learning activities, alot of un-answered questions from week 7 and 8 were addressed in the material covered and context to some of the concepts previously learnt.
The key lessons for me was the programming paradigms, all the different languages and their evolution. I still do prefer a structureal approach to programming over OOP. Compiled and interpreted code was also very educational, learning about the differences and hybrid implementations, languages that uses both codes and how distributed applications, especially web apps make use of these codes.
Regards,
Ricardo
	              
	              Hi Martins,
thank you!
Regards,
	              
	              Hi Terry,According to the basic design principle, Yes it should be, but with reference to .Net framework, you need not bother how exactly these directives are processed!Which is good for RAD and most of the resource management would be done by framework itself!Unlike C/C++ you need not address memory in .Net, but if you want to you can define it as an unmanaged code and write your program.Regards,

	              
	              This week’s discussions compared the object-oriented paradigm with the standard structured programming approach. We considered OOP code reusability issues and at instances when the use of OOP would be recommended or inadvisable. We also looked at ways to minimize some of the inherent disadvantages of the OOP paradigm, thereby enhancing its utility.Good quality posts all round.Anthony
	              
	              The thread featured discussions that dealt with the reasons for the continued use of interpreted code, despite their slower execution speeds when compared to compiled code. The posts discussed ways to secure and protect interpreted code, using for example obfuscator schemes, and the typical problems that result from using such tools. We also discussed issues related to the ease of updating interpreted code, and when this might be disadvantageous.Nice work with the discussions.Anthony
	              
	              Hi Kharavela,
Thanks for answering my question:-)
I'm also doing a C# project right now, where I struggled a lot. Last time I program for Windows&nbsp;before this was with VC6.
br, Terry
	              
	              Hi Anthony,
I’ve found this week thoroughly enthralling and enjoyable.
It has been really interesting in researching the programming paradigms and the programming languages.
Initially it was sometimes confusing, especially when figuring out the positioning of the programming language with an associated paradigm however, as the week moved forward, I could see and figured out that there are many cross overs and changes as a result, largely, of evolution – many languages fit with many paradigms. 
I was surprised of the level of continued popularity of ‘C’ as a programming language, both in its own right, and also as the driving influence for other languages that have been developed. This was compelling, in that these developments have transitioned the imperative and structured style of ‘C’ into the object-oriented space and include languages such as C++, C#, Objective C, JAVA, Python etc. It was also useful to put clarity to compilation versus interpreted code, and when these methods of translation are used.
It has also been fascinating again to read the discussion threads, to see other perspectives, which has helped me with my understanding and thinking with the topics this week. I continue however to struggle in understanding the ‘actual code’, but I am becoming more aware of the principles, structures and functional attributes of the programming paradigms and languages.
I needed to put in a few more hours than normal, not just for the double DQ but also to help me in grasping the subject. 
It has prompted some good thinking.&nbsp;It was a good week.
Best Wishes, Craig

	              
	              Thanks Augusto
Yes, I agree.
...Crikey... it seems such a long time ago when the old nokia's were leading in the mobile phone race.
Best Wishes, Craig
	              
	              ..and perhaps also some 'smalltalk' with Kent ?... :-)
Best Wishes, Craig
	              
	              Hi Terry, Hi Chris
Thanks for your input.
I think your on to it Chris, to create a level of capability that whilst may not be exposed to business users, it could be a significant capability for developers and programmers, in creating the baseline to move forward with. And, as you say, this is created automatically but also creates it in the most efficient and effective ways - perhaps even makoing suggesting back to the developer or programmer on 'better' techniques?
Thanks Guys
Best Wishes, Craig
	              
	              Thanks for the summary/comments, Terry. 
I like the distinction you make between performing and practicing with programming - it is very apt.Regards,Anthony
	              
	              Thanks for the comments, Chris.
Glad to hear that you found the topics useful - it will be interesting to see precisely the direction in which programming goes over the next decade.Regards,Anthony

	              
	              Thanks for the summary and comments, Bo.
Don't worry unduly about writing short and simple DQs - I think it is generally beneficial to make them interesting and full of information. &nbsp;Most instructors are not overly strict about applying a word-count limit.Regards,Anthony

	              
	              Ah ok, thanks Terry
Is that the book written by the big four?
Just so I am on the same page as you (you know, this subject is brand new to me), is this because inheritance has somewhat been superseded by something called 'object composition'?
It appears, from my initial research on this book, that there is a preference for, what they refer as, 'black box' reuse, which uses references dynamically at run-time. This is instead of 'white box' reuse, which is traditional inheritance and are often visible to subclasses aswell as, again as they suggest, it breaking encapsulation?
If my interpretation is correct, is inheritance therefore a secondary consideration in most cases for OOP?
Thanks for your help
Best Wishes, Craig

	              
	              Craig, You are amazing:-) You've really got the most important things. big four --&gt; Gang of Four (not to be confused with another GoF in the contemporary history of China) Regarding the inheritance vs. composition. Yes, but I won't use the word "superseded" but "preferred". It's because it's not an on-off principle. Inheritance is often used to present a "is-a" relationship, and composiiton is used as "has-a". Can you say "a child is a parent"? What you mentioned as 'black box' is called 'information hiding'. And you've made very good points. Some says encapsulation is a special type of information hiding. Some says encapsulation is not information hiding, because it didn't "hide" anything but just grounded them somewhere. The information hiding doesn't break encapsulation, it breaks the dependencies. Breaking dependencies is very important in software design. It's llike the power socket in th wall should have no idea what you are going to plug in it.   In "most cases", as you said, I would agree that inheritance is a secondary consideration in design. However understanding inheritance and knowing it's limit is still crucial in OOP. It seems that we cannot stop the framework makers from making superclasses for others. There's an interesting read by Michael Feature: http://michaelfeathers.typepad.com/michael_feathers_blog/2013/01/the-framework-superclass-anti-pattern.html br, Terry
	              
	              Thanks Terry
Some of this is becoming more clear.&nbsp;
Sorry about the big four / gang of four error - I'm used to referring to the big four consultancies PWC, EY, Deloitte &amp; KPMG.&nbsp;
I'll take a look at your reference.
Best Wishes, Craig
	              
	              Thanks for the summary and comments, Augusto.I hope the cold is now gone - it is tough as it is to cover all the material, and make all the required submissions, without added health distractions.Regards,Anthony
	              
	              Thanks for the comments, Bram.True - the added DQ workload does require more careful time management..Regards,Anthony
	              
	              Hi Anthony

This week was a mixture of both ild and new knowledge, I have learned more about structural and object oriented programing, I learnt about compiled versus interpreted code. I learnt various programming paradigm differences, and which programming language supports each paradigm, and how and so on.

It was an interesting week.

Regards, Babatunde.
	              
	              Thanks for the comments, Martins.Yes, the extra DQ workload necessitates more careful time management.Regards,Anthony
	              
	              This Discussion is a continuation of the Discussion you started in Week 7, writing an algorithm in pseudo code.
To complete this Discussion:
Post: Update your pseudo-code for the algorithm you posted in Week 7 to address the comments and comparisons from your colleagues or the instructor. Post your updated algorithm to the Week 8 Discussion Board on Monday (Day 5) of this week.
Respond: Respond to your colleagues' revised algorithms with additional feedback by Wednesday (Day 7). You may provide feedback to any of your colleagues, even those who chose a different algorithm option than you chose.
For all Discussions (unless stated otherwise):

Create a single document with your initial post.
Post the text of your document to the Discussion Board for this Week, and upload the document using the submission link for this Discussion.
By Wednesday, make 3–5 substantial follow-up responses to your colleagues. These can include responses to your colleagues' initial posts, as well as responses to colleagues who responded to your own initial post. Your total Discussion Board participation must occur on at least 3 individual days during each week. Follow-up responses should be significant contributions to the Discussion. Do not submit your follow-up responses to Turnitin.
In general, online discussion is best when you:


Ask insightful questions.
Extend the discussion into new but relevant areas.
Model or promote critical reflection.
Support your arguments with citations and references from the assigned Learning Resources and other literature, using Harvard Liverpool Referencing Style.



Ensure that you spread your discussion posts across at least three separate days of each week. This will help maximise the value of your discussion with colleagues and serve to meet the learning objectives for each activity.
Click on the Reply button below to reveal the textbox for entering your message. Then click on the Submit button to post your message.
	              
	              How do you get from your home to your office, or the nearest store? If you live in a city, the answer may be complicated, and it might depend on whether you are walking, driving, riding a bicycle or taking public transportation. Even for one mode of transportation, there might be several possible ways, some of which are faster or more reliable than others. Deciding how to make this journey is analogous to deciding how to design an algorithm.
A complicated set of steps define how a computer must perform a task, in order to achieve the desired result. As with the journey just described, there may be many ways to reach the destination: many different algorithms can be developed to solve the same problem. However, not all algorithms are equal. For this Discussion, you will explain what makes one algorithm better than the other, as if you were explaining it to a non-technical friend.
To prepare for this Discussion

Review your weekly Learning Resources with a focus on writing algorithms.
Research the properties of an algorithm.
Reflect on what determines an algorithm’s quality.
Research examples of strong and poor algorithms to support your arguments.

To complete this Discussion Post: Create an initial post in which you analyse what makes one algorithm better than another. Address the following:

Analyse the properties of an algorithm.
Analyse the ambiguity involved in algorithm specifications.
Using your analyses, explain how you determine an algorithm’s qualities.
Compare examples of strong and poor algorithms to support your arguments.
Fully state and justify any choices, assumptions or claims that you make using the suggested resources for this week and/or your own research.

Respond: Respond to your colleagues. Address the following:

Debate your colleague’s position on what makes a high quality algorithm and what does not make a high quality algorithm.
Be sure to support any claims you make.

For all Discussions (unless stated otherwise):

Create a single document with your initial post. Your document should be 350-500 words, though you will be marked based on the quality of your writing, not on the number of words.
By Sunday, post the text of your document to the Discussion Board for this Week, and upload the document using the Turnitin submission link for this Discussion.
By Wednesday, make 3–5 substantial follow-up responses to your colleagues. These can include responses to your colleagues’ initial posts, as well as responses to colleagues who responded to your own initial post. Your total Discussion Board participation must occur on at least 3 individual days during each week. Follow-up responses should be significant contributions to the Discussion. Do not submit your follow-up responses to Turnitin.
In general, online discussion is best when you:


Ask insightful questions.
Extend the discussion into new but relevant areas.
Model or promote critical reflection.
Support your arguments with citations and references from the assigned Learning Resources and other literature, using Harvard Liverpool Referencing Style.



Ensure that you spread your discussion posts across at least three separate days of each week. This will help maximise the value of your discussion with colleagues and serve to meet the learning objectives for each activity.
Click on the Reply button below to reveal the textbox for entering your message. Then click on the Submit button to post your message.
	              
	              
Re-posting of original week 7 pseudocode

Foreword to the re-post: I have removed to "overhead" and just posted the relevant code. Also I have added line numbers for referencing.
I know my code below could have been simpler, but it’s hard to stop when my primary interest is the final user program. Hence I’ve tried to make it user friendly, i.e. making a procedure that asks the user to write the initial number (i.e. 37540) and which number to divide by (i.e. 3). Then the algorithm calculates and finds the integer results from the calculation (e.g. 37540 / 3), and presents the integers on the screen.
The code could also be more complex/longer, e.g. the sub_procedure Generate(Init.List) should be improved, but that will obviously be done in the final algorithm.
3. Procedure Find.Integers
a. sub_procedur UserInputs(init_no; init_div)
1. define init_no = 0
2. define init_div = 0
3. init_no = integer value from &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; -&nbsp;- save the input into init_no
4. &nbsp; &nbsp; &nbsp; echo “Write the initial integer number”&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- - the user writes 37540
5. &nbsp; &nbsp; &nbsp;&nbsp;If (init_no is not an integer
6. &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;then echo “Your input is not an integer, try again”
7. &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; init_div = integer value from
8. &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; (echo “Write integer number to divide by”)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- - the user writes 3
9. &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;&nbsp;If (init_div not an integer
10. &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; then echo “Your input is not an integer, try again”
11. &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; else echo “Thank you. Calculation in progress”
12. &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ) end if
13. &nbsp; &nbsp; &nbsp;) end if
14. goto sub_procedure Generate(Init.List)
b. sub_procedure Generate(Init.List)
1. define Init.List()
2. Init.List = (find number combinations from init_no)
3. &nbsp; &nbsp; &nbsp; start with init_no(1st)number&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- - returns (3)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
4. &nbsp; &nbsp; &nbsp; then init_no(1st&nbsp;and 2nd) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; - - returns (37)
5. &nbsp; &nbsp; &nbsp; etc &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; - - returns (375; 3754; 37540)
6. &nbsp; &nbsp; &nbsp; start with init_no(2nd) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; - - returns (7)
7. &nbsp; &nbsp; &nbsp; then init_no(2nd&nbsp;and 3rd)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- - returns (75; 754)
8. &nbsp; &nbsp; &nbsp; etc &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; - - returns (5740; 5; 54; 540; 4; 40; 0)
9. &nbsp;echo “The initial combinations are: “&amp;init.list &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;- - shows on screen all the numbers
10. goto sub_procure Calculate.Integers
c. sub_procedure Calculate.Integers
1. define result = 0 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;-&nbsp;- temporary container for calculated results
2. define v_no = 0 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; - -&nbsp;temporary container for count of values
3. define Integer.List() &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; -&nbsp;- Integers will be stored here
4. v_no = Count(values in Init.List) +1 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;- - returns 16, i.e. 15 + 1
5. if (v_no &lt; 1
6. &nbsp; &nbsp; &nbsp; then goto sub_procedure Finished.List &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;- - The procedure prints an error description
7. &nbsp; &nbsp; &nbsp; else
8. &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;while v_no =&gt;1 do the following steps
9. &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; v_no = v_no – 1 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;- - first number is 15 in first iteration, then 14 etc.
10. &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; result = Value(v_no) / init_div &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;- - returns the 15th&nbsp;number, i.e. 0, then 40 etc.
11. &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; If (result is an Integer &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;- - Integer is a boolean, returns True if integer
12. &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;then add result to procedure Generate(Int.List) &nbsp; &nbsp; &nbsp;&nbsp;- - the procedure adds and sorts values numerically
13. &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;else goto sub_procedure Calculate.Integers&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- - tries next number
14. &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ) end if
15. &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;end while
16. ) end if
d. sub_procedure Finished.List
1. echo “The final integers are: “&amp;Integer.List &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; - - returns (0; 3; 75; 54; 375; 540)
2. End procedure Find.Integers
4. Users perspective
Seen from the users perspective, i.e. the GUI, the final result on screen will show:
&nbsp; &nbsp; &nbsp; Write the initial numer:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;37540
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Write the integer number to divide by: &nbsp; &nbsp; &nbsp; &nbsp;3
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; The initial combinations are: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 3; 37; 375; 3754; 37540; 7; 75; 754; 7540; 5; 54; 540; 4; 40; 0
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; The final integers are: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;0; 3; 75; 54; 375; 540
&nbsp;
Best wishes
Bo W. Mogensen

	              
	              


Your algorithm should:

Search a string of at least five numbers (for example, 37540)
Identify all of the substrings that form numbers that are divisible by 3.
For example, applying the algorithm on the string 37540 should produce the following substrings (not necessarily in this order): 0; 3; 75; 54; 375; 540.







My solution to the problem:








def substrings_divisible_by_3(numberstring):
    result = set()
    for begin in range(0, len(numberstring)):
        for end in range(begin+1, len(numberstring)+1):
            subnumber = numberstring[begin:end]
            if int(subnumber) % 3 == 0:
                result.add(subnumber)
    return result









Below are the tests I used to "drive" out and verify the above solution.








# 1-number string that is not divisible by 3 should return empty
assert substrings_divisible_by_3("1") == set()
# 1-number string that is divisible by 3 should return the number
assert substrings_divisible_by_3("3") == {"3"}
# 2-number string with 1 number divisible by 3 should return that number
assert substrings_divisible_by_3("62") == {"6"}
# 2-number string with both number divisible by 3 should return 3 numbers
assert substrings_divisible_by_3("69") == {"6", "9","69"}
# test the example
assert substrings_divisible_by_3("37540") == {"0", "3", "75", "54", "375", "540"}






	              
	              


I also have solution to option 2.
Option 2: Word Search








Make a list of five words, 3-6 letters in length.
Create a string of approximately 30 letters containing some of the five words.
Your algorithm should identify all of the substrings of the longer string that match any of the five words you generated, and the number of times each one appears
For example: If you chose the words structure, such, system, blue, red, and your algorithm operates on the string jkdistructuredstrusyssystemoon, your algorithm should report that the string contains the words structure, red and system each one time, and the words such and blue zero times.









def count_words(text, words):
    result = {}
    for word in words:
        result[word] = text.count(word)
    return result








Below are the tests:








# should return empty dict when counting no word
assert count_words("a string of apporximately 30 letters.", []) == {}
# should count zero time when the string contains no word in the list
assert count_words("a string of apporximately 30 letters.", ["precise", "1234"]) == {"precise":0, "1234":0}
# should count 1 when the string contains 1 word in the list once
assert count_words("a string of apporximately 30 letters.", ["of"]) == {"of":1}
# acceptance
assert count_words("jkdistructuredstrusyssystemoon", ["structure", "such", "system", "blue", "red"]
                   ) == {"structure":1, "such":0, "system":1, "blue":0, "red":1}













	              
	              


Option 3: Consensus Algorithm






Ten people need to decide which one flavour of ice they will order as a group. There are 3 types of ice cream from which to choose.

Design an algorithm which can survey and re-survey each person, with the goal of reaching consensus on one kind of ice cream. The algorithm can present answers to each person in the group until a consensus is reached.
This task is open-ended. You may make assumptions as needed. Explain these assumptions in your initial post.
Determine whether there are situations in which your algorithm may never result in an answer, and account for this when writing your pseudo-code.







My Assumptions:






My assumptions are shown below in the form of test cases.








# if not survey answers are given to the algorithm, it should return all the options
assert consensus_icecream_flavor(None) == {"A","B","C"}

# if all the survey answers are the same, it should return consensus result
assert consensus_icecream_flavor(["A","A","A","A","A","A","A","A","A","A"]) == {"A"}

# if different survey answers are given to the algorith,
# it should return the options without the least favority one
assert consensus_icecream_flavor(["A","A","A","A","B","B","B","B","C","C"]) == {"A","B"}

# it should return all the options if there are more than one least favority flavor
assert consensus_icecream_flavor(["A","A","A","A","A","A","B","B","C","C"]) == {"A","B", "C"}









The solution derived from the above assumptions:








def consensus_icecream_flavor(answers):
    if answers is None:
        return {"A", "B", "C"}
    
    # get the votes of each flavor
    votes = {}
    for flavor in answers:
        if flavor not in votes:
            votes[flavor] = 0
        votes[flavor] += 1
    
    # reach consensus
    if len(votes) == 1:
        return {answers[0]}
    
    # get the least favorite flavor
    least_favorite = min(votes.values())
    
    # remove the least favorite flavor from the result
    result = {flavor for flavor in votes if votes[flavor] != least_favorite}
    
    # more than one least favorite
    if len(result) + 1 != len(votes):
        result = {flavor for flavor in votes}
    
    return result









Below is the consensus algorithm in action:








def icecream_survey():
    answers = None
    while True:
        options = consensus_icecream_flavor(answers)
        if len(options) == 1:
            return options[0]  # reach consensus
        answers = []
        for index in range(10):
            answer = ask_to_choose_from_options(index, options)
            answers.append(answer)









The sub-routine&nbsp;ask_to_choose_from_options(index, options)&nbsp;will ask the user of&nbsp;index&nbsp;to choose one option from the&nbsp;options, and return that option.






Existing problems of this algorithm






This algorithm will never end with an answer if two flavors constantly get the same amount of votes. It's the situation described as the last piece in the assumptions above.




	              
	              1. Introduction

An algorithm is a piece of code that handles/solves a specific problem through calculations. A problem can be solved in many different ways, thus there are often many different algorithms to choose between to solve the problem. An algorithm is like an object that can be used in several places within the same program or by several different programs (McQuain).

2. Abstract

In this document I will try to explain in plain English the different algorithm properties, their ambiguity and qualities. Since I have “no” programming experience, my focus will be on short and (hopefully) clear and precise descriptions showing my understandings.

3. Two essential types of algorithms

“There are two essential kinds of algorithms which though are equivalent in their computing power”. (Lecture notes, p. 3):





a) Iterative


These go (loop) through all the steps in the algorithm continuously until the preconditions are reached, e.g. the steps are repeated 5 times:
repeat while i &lt;= 5
..
i = i +1 &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //5 = 4 +1
&nbsp;
The phases of the repetitive control are (Brookshear, p. 207):

Initialize: Set the variables with an initial value to be used later.
Test: Decide if the repetition should continue or stop.
Modify: Change the variables set in step 1.

&nbsp;
The iterative algorithms are normally much easier to understand, sometimes even intuitive (Lecture Notes, p. 5) 
&nbsp;




b) Recursive
&nbsp;


The main difference from the iterative is that here the repetitions are subtasks (Brookshear, p. 214). Recursion has three phases (Lecture Notes, p. 5):

Decomposing, i.e. breaking the problem into smaller problems until you reach the so-called “base case”.
Solving, i.e. solving all the base cases.
Assembling, i.e. assemble the solutions from the smaller solving’s.





4. Algorithm properties

An algorithm must consist of these properties:





a) Finiteness: 


The algorithm must always terminate after a finite number of steps.*




b) Definiteness:
&nbsp;


Each step must be precisely defined; the actions to be carried out 
must be rigorously and unambiguously specified for each case. *




c) Input:
&nbsp;


An algorithm has zero or more inputs, taken from a specified set of 
objects.*




d) Output:
&nbsp;


An algorithm has one or more outputs, which have a specified 
relation to the inputs.*




e) Effectiveness:
&nbsp;


All operations to be performed must be sufficiently basic that they 
can be done exactly and in finite length.*




f) Correctness


The algorithm must produce correct results. (Brookshear, chap. 10)




* A to e are direct words from McQuain.





Ad a) Finiteness

An algorithm must always have a finite amount of steps and it must stop at some point. Otherwise it will run forever. An example can be ‘i = i + 1’ without any boundary thus this stem can run forever.

Ad b) Definiteness

The steps in the algorithm must be clearly defined, i.e. it must be unambiguous and deterministic. If it’s not clear, then there will be confusion. The textbook has the example “Deterministic Versus Nondeterministic” (Brookshear, p. 535) where a person is to go to the next intersection and there turn either right or left. If the code is Nondeterministic, then the person won’t know which way to go, and the person just has to choose on his own, i.e. the outcome is not clear.

Ad e and f) Effectiveness and Correctness 

I have chosen to describe correctness and effectiveness together. As I see it, these are more or less two sides of the same coin, i.e. if an algorithm is correct then usually it’s also effective, and if it’s effective then it is also correct, otherwise incorrect.

I would say that an algorithm should obviously produce correct results every time. But this is only possible if the input is limited to specific inputs. Otherwise, if the inputs can vary, then there can be exponentially many variations, and we will probably not be able to test them all, hence we can’t be sure that the algorithm will always produce correct results. To reach this, you have to keep checking and re-checking and use empirical methods.

Effectiveness is also called performance, and is usually calculated in:


time
space


There are a lot of variables which can influence time and space. An example can be the complexity of the calculation, e.g. if there is a lot of data to sort then depending on the data and choice of sort algorithm (e.g. Bubble sort, Selection sort, etc) it will either, or both, influence:


Time, because e.g. the Bubble sort could risk a lot of looping. (Eck, David)
Space, because if there are a lot of data which have to be stored temporarily, then the data amount will be both the input data plus the temporary data. (Eck, David)


5. Algorithm qualities

Everything in life, e.g. any programming language, has its qualities, strengths and weaknesses. I think that the qualities depend on the given situation. If for example you have a lot of computing power but limited space, and you have a sorting problem to solve/handle, then as you know, any problem can be solved by several ways/algorithms, and you just have to choose one that has the qualities that you need for the given situation, in this case something that demands less space, e.g. Bubble sort (Eck, 1997). There is always a compromise to be accepted.

6. Strong and poor algorithms

The effect of strong and poor algorithms can be enormous. A basic example could be illustrated with the control mechanisms in the algorithm. The algorithm is:


Good, if there is finiteness then it’s good since the algorithm will stop at a controlled point.
Bad, if there is no finiteness, since it then will run forever or until it crashes.


7. Conclusion

It is extremely important, even crucial, that the algorithms are constructed as well as possible and tested as much as possible. And in the case of having situations where an algorithm has to be written even though there are “flaws”, then one must ensure proper preconditions and proper error handling. I could ask: If we have an investment system and it has flaws, even known one’s, and this is the reason for great losses, then who will be reliable for potential losses, the company, the programmer?

Also it is important to continually improve the algorithms and find solutions for those situations that still have no solution like the “travelling salesman problem” (Brookshear, p. 533). 

8. References

Brookshear, J. Glenn, (2012). Computer Science An Overview. 11th edition. Boston Massachusetts: Pearson. Addison Wesley.

Eck, David. (1997). “The xSortLab Applet”. [Online] Available from: http://math.hws.edu/TMCM/java/xSortLab/index.html#info. (Accessed 25-10-2014).

McQuain. (2011-12). Virginia Tech. “Algorithms”. [Online] Available from: http://courses.cs.vt.edu/cs2104/Fall12/notes/T16_Algorithms.pdf. (Accessed 25-10-2014)

UoL. (2014). “Computer Structures — Lecture Notes Week 7: Algorithms”. &nbsp;[Online]. Available from: https://elearning.uol.ohecampus.com/bbcswebdav/institution/UKL1/201480_SEPTEMBER/MS_CKIT/CKIT_501/readings/UKL1_CKIT_501_Week07_LectureNotes.pdf. (Accessed 25-10-2014)

&nbsp;

Best wishes

Bo W. Mogensen
	              
	              Updated pseudo-code
Requirements - Option 1:
A. Search a string of at least five numbers (for example, 37540) B. Identify all of the substrings that form numbers that are divisible by 3. C. For example, applying the algorithm on the string 37540 should produce the fol low ing substrings (not necessarily in this order): 0; 3; 75; 54; 375; 540.
Revised pseudo-code
astring ←37540
bstring&nbsp;←0i←0
j←0 &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; for (i = 0 to i&lt;=length of astring) &nbsp; &nbsp; [&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; for (j = 0; j&lt;=length of astring;)&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; [&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;bstring ←astring.substring(i, j);&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;number ← Convert to Interger(bstring);&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;if (number % 3 == 0)&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Print the value of bstring; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; j←j+1; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;] i←i+1; &nbsp; &nbsp; ] &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;There are initialization states e.g. i←0 there are also test stages &nbsp;if (number % 3 == 0)&nbsp;and there is the termination stage as the program will loop until &nbsp;i&lt;=length of astring.1) Set astring to 375402) Set bstring to 03) Set I to 04) Set J to 05) for (i=0 to i&lt;=5 (length of astring)) for (j =0 to j&lt;=5 (length of astring))&nbsp; &nbsp; &nbsp; &nbsp;[&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; bstring ←astring.substring(i, j);&nbsp;(This rearranges the values in astring )&nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; number ← Convert to Interger(bstring);&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;if (number % 3 == 0)&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Print the value of bstring; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; j←j+1; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;] &nbsp; i←i+1;&nbsp; ] &nbsp; &nbsp;&nbsp;At the first instance i is set to 0 and it goes through the for loop and executes the inner for loop. For j &nbsp;less than or equal to 5 it loops the contents this loop and will continue to do so incrementing j by one at the end of each loop until j reaches 5. Then it will increment the value of i by one and start looking process all over again until i reaches 5. At each loop of the initial forloop the inner for loop is looped at a maximum of 5 times. For the inner loopj is initated to 0 and loops until it reaches 5.The statement bstring ← astring.substring(i, j) rearranges the values in the initial string using positions 0-5, thus positions 0, 1, 2,3 and 4 represents 3,7,5,4,0 respectively. The first position i indicates the position that will be taken into consideration while the second position j indicates the value that will not be taken into consideration. Thus (0,3) would be 375, hence all from position 3 onwards will not be taken into consideration, hence numbers 4 and 0 are not represented.References:Brookshear, J.G. (2011) Computer science: An overview. 11th ed. Pages 192 and Pages 195 &nbsp;Boston: Pearson Education / Addison-Wesley.&nbsp;Computer Structures — Lecture Notes, Week 7: Algorithms [Online]. Available at:&nbsp;https://elearning.uol.ohecampus.com/bbcswebdav/institution/UKL1/201480_SEPTEMBER/MS_CKIT/CKIT_501/readings/UKL1_CKIT_501_Week07_LectureNotes.pdf (Accessed on October 25, 2014)&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;
	              
	              Hello Class,Both programming languages and algorithms continually evolve. Some algorithms are more easily implemented with specific programming languages, or with newer versions of the same language.Can you remember any instances in the past when certain algorithms were easier or more difficult to implement with a particular language? What were the algorithm(s) and programming language(s) involved?Anthony
	              
	                 Algorithm Quality in A Nutshell Terry Yin October 26, 2014 How do you put an elephant into a refrigerator? • step 1: Open the refrigerator;  • step 2: put in the elephant;  • step 3: close the door. We just created an algorithm for putting an elephant into a refrigerator. But, somehow, it feels not very practical. How do we determine the quality of an algorithm? 1 Does it work Correctness is for sure an important part of the algorithm quality. How can we verify if (the representation of) an algorithm is correct? Accounting is called the “language of business”, and is known to be precise. How do they ensure the correctness of their bookkeeping? Accountants usually use an approach called double-entry bookkeeping. For instance, recording earnings of $100 would require making two entries: a debit entry of $100 to an account called “Cash” and a credit entry to an account called “Income” Wikipedia (2014a). In the end, everything should be balanced. If not, they immediately know there’s a problem. We should have this kind of redundancy for the algorithm as well. Besides the representation of the algorithm, which is the code, we should also have a representation for our intent by the algorithm. Traditional, the representation of the intent of the algorithm is often in the form of a requirement specification or test plan. But the problem of static documentation is they often “lie”. It’s hard to check if they are documenting the actual thing. And they tend to become outdated very soon. Therefore, we need a representation of the algorithm’s intent that doesn’t lie. That could be unit testing, which is some computer program checking the correctness of another computer program Wikipedia (2014e).        Below is an example of the algorithm of determine&nbsp;leap year:           def  leap_year ( year ):      if  year  %  4  !=  0 :  return  False      if  year  %  100  !=  0 :  return  True      if  year  %  400  !=  0 :  return  False      return  True          Below are some unit test to check the intent of the algorithm of leap year:           assert  not  leap_year ( 1999 )  assert  leap_year ( 2008 )  assert  leap_year ( 2000 )  assert  not  leap_year ( 2100 )               2 Internal quality of an algorithm “An algorithm is an ordered set of unambiguous, executable steps that define a terminating process” (Brook- shear 2011, p. 189). The representation of an algorithm also needs some abstraction to avoid too much detail. A generally good abstraction might not necessarily be a part of a good algorithm. Abstraction helps people understand ideas. Sometimes, it doesn’t even have to be right. For example, the arrival flight sign in most airport looks like this (The symbol next to the arrow on the left):     Figure 1: Signboard for arrival flight. September 2014, Suvarnabhumi Airport, Bangkok by Terry Yin This is a good abstraction. Everybody will understand what does it mean immediately. But the fact is no airplane lands like that. It looks more like crashing. It’s unimaginable if anybody implements landing according to that abstraction. To make the representation more precise, computer science established the basic building blocks called primitive (Brookshear 2011, p. 190). But too much detail will also prevent people from understanding the algorithm. Luckily, any programming language has some ability of abstraction, e.g. extra the details into a sub-routine. So a good algorithm should be detailed appropriately. Or following the Single Level of Abstraction Principle. Probably we should try to avoid using pseudo-code to represent an algorithm, when it’s possible to use a real programming language. The algorithm is part of the software design. As Jack Reeves pointed out:  “After reviewing the software development lifecycle as I understood it, I concluded that the only software documentation that actually seems to satisfy the criteria of an engineering design is the source code listings.” Reeves (1992)  3 Efficiency of an algorithm Our algorithm might be able to provide the functionality we need at a small scale. But will it work efficient enough when the input scales up? The efficiency is often also a part of our intent for an algorithm, which is often called non-functional requirement Wikipedia (2014c). There are many aspects in non-functional. Here we will only look at the time complexity and the space complexity.     &nbsp;       3.1 How long will it run before getting the result? An obvious quality of the algorithm is how fast it is. That is determined by the time complexity of the algorithm. Computer has been doubling its speed every 18 months, Wikipedia (2014b). But that’s still often not enough to make an algorithm does what it couldn’t within a reasonable amount of time. Because the time complexity of the algorithm could increase very fast as the amount it’s dealing scales up. The time complexity of the algorithm is often identified by the big-theta notation like Θ(n2) (Brookshear 2011, p.226), or big-O notation like O(n) Wikipedia (2014d). Say, you come to a typical American family, it’s important to remember everybody’s name. You will need to call those people by name in the future. So the complexity is O(n). For a family with n people, the time you spend on remembering the names will be proportional to n. Say, you come to a typical Chinese family, it’s important to know the relationship between each two of them. Because, well, one day you will need to say “oh, you are Zhangsan’s 3rd cousin and son of Zhangsan’s mother’s 2nd sister.” This time, the complexity is O(n**2). As you can see, the time you spend on remembering the relationship of a Chinese family increase much faster than just remembering the names as the number of family member increases. But, if you study the Chinese family deep enough, you will find that there are tricks to help you remember the relationships between each family members. You don’t have to remember each relationship, you only need to remember the family tree. Now the complexity of remembering the relationship is O(n) again. And when you need to recall the relationship between any two family members, the complexity is O(lg n). The interesting finding here is, O(lg n) increases slower than O(n). So that means getting to know a large Chinese family is actually easier than getting to know a large American family. 3.2 How much space will it take Space complexity is often another concern of algorithm, because computers have limited memory (and other resources). Time complexity and space complexity are sometime tradeable. For example, in the above Chinese family example, we use extra space in our memory to remember the relation names in a full family tree. So that we can reduce the complexity of searching the relationship between any two members from O(n**2) to O(lg n). 4 Conclusion As Kent Beck pointed out, there are three steps in designing a piece of software c2.com (2014): • Make it work • Make it right • Make it fast “Make it work” means to make it work according to the functional requirement. “Make it right” means to have the proper internal structure. “Make it fast” means to make it work according the non-functional requirement. And the order is very important. Trying to make it fast before it has the right structure will only lead to something that doesn’t work and not fast, and of course, have a messy internal structure. Regarding the elephant story: I didn’t make the story. But I heard it too long ago, and I don’t know who’s the author of it. There’s a second part (and more) of the story. How do you put a giraffe into a freezer?        References Brookshear, J. G. (2011), Computer science: an overview, Paul Muljadi. c2.com (2014), ‘Make it work make it right make it fast’. [Online; accessed 26-October-2014]. URL: http: // en. wikipedia. org/ w/ index. php? title= The_ Elements_ of_ Programming_ Style&amp;oldid= 614175936 Reeves, J. W. (1992), ‘What is software design’, C++ Journal 2(2), 14–12. Wikipedia (2014a), ‘Double-entry bookkeeping system — wikipedia, the free encyclopedia’. [Online; accessed 26-October-2014].   URL: http: // en. wikipedia. org/ w/ index. php? title= Double-entry_ bookkeeping_ system&amp;oldid= 629384284 Wikipedia (2014b), ‘Moore’s law — wikipedia, the free encyclopedia’. [Online; accessed 20-September-2014]. URL: http: // en. wikipedia. org/ w/ index. php? title= Moore% 27s_ law&amp;oldid= 626293387 Wikipedia (2014c), ‘Non-functional requirement — wikipedia, the free encyclopedia’. [Online; accessed 26-October-2014].   URL: http: // en. wikipedia. org/ w/ index. php? title= Non-functional_ requirement&amp;oldid= 627025279 Wikipedia (2014d), ‘Time complexity — wikipedia, the free encyclopedia’. [Online; accessed 26-October- 2014]. URL: http: // en. wikipedia. org/ w/ index. php? title= Time_ complexity&amp;oldid= 630268634 Wikipedia (2014e), ‘Unit testing — wikipedia, the free encyclopedia’. [Online; accessed 26-October-2014]. URL: http: // en. wikipedia. org/ w/ index. php? title= Unit_ testing&amp;oldid= 624378949     &nbsp;    
	              
	              Hi all,
One example is the tail recursion.
Many algorithms are described in a recursive way. It's hard to directly translate them into the code of a programming language, because most programming language has a limited call stack. But if the recursion only happens at the end of the algorithm, the implementation of the programming language doesn't need to preserve the current call stack. Thus the recursion can be optimized in action. If the language support tail call optimization, the code can be more descriptive by using the recursion and without sacrificing the performance.
But not many languages supporting tail call optimization. Some language explicitly annouced that they won't support it, like Python. Some languages have it in their latest version, e.g. Java 8.
Another one is the "lambda function" (or Anonymous function).
Lambda function is essential for functional programming, or functional style. In the past, only new dynamic languages have it. Simulating the same idea using Java or C/C++ has been awkward. Now new version of C++ and Java both support the lambda expression.
C++ used to be my favority programming language. I even named my son "plus" (in Chinese). I think "plus" is good, but "plus plus" is over doing it. The newest version, C++11, looks more like c# (4 pluses) to me. I haven't seen anybody starting new C++ project for quite a while...

References:
Tail call. (2014, July 2). In&nbsp;Wikipedia, The Free Encyclopedia. Retrieved 15:11, October 26, 2014, from&nbsp;http://en.wikipedia.org/w/index.php?title=Tail_call&amp;oldid=615236294
Anonymous function. (2014, October 22). In&nbsp;Wikipedia, The Free Encyclopedia. Retrieved 15:13, October 26, 2014, from&nbsp;http://en.wikipedia.org/w/index.php?title=Anonymous_function&amp;oldid=630634334
	              
	              Algorithm

1.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; What is an algorithm?

In computer science and mathematics, an algorithm is a step by step process to achieve a desired result. Algorithms are used for automated reasoning, data processing and mathematical calculations. Algorithms have a set of inputs and based on their logic, they generate a specific set of outputs.



Figure 1: Algorithm inputs and outputs

2.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Qualities of Algorithms

An algorithm must possesses the following qualities

1.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Finiteness

An algorithm must terminate after a finites number of steps.

2.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Effectiveness 

All steps must be basic and are done in finite length of time.

3.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Definiteness (Unambiguous)

Purpose of each step is clear and unambiguous.

4.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Inputs

An algorithm can have zero or any number of inputs taken from user or any other object.

5.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Outputs

An algorithm must have one or more specific outputs that follow the inputs.



3.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Algorithm Analysis

The most straight forward reason for analyzing an algorithm is to study


Its characteristics
Evaluate its appropriateness for particular applications
Perform its comparison with other algorithms for same application


Moreover, we can understand better and suggest improvements in an algorithm by analyzing it. During analysis algorithms should be simpler, shorter and well designed.

3.1&nbsp; Computational complexity

Computational complexity refers to classifying algorithms according to their computational problems and efficiency based on their inherent difficulty. These classifications are not used for comparing an algorithm in terms of its performance in practical applications. They only focus on worst case performance order of growth. 

&nbsp;

3.2&nbsp; Running time of algorithm (Runtime Analysis)

Running time of an algorithm can be calculated using following formula:

&nbsp;

Running time = Time to execute basic operation * Number of time operation needs to be executed.

Basic operations are more time consuming and normally found in the inner loop.

3.3&nbsp; Space complexity

Space complexity refers to how much memory is required to run the algorithm. For example if our algorithm add three elements x, y, z and each occupy one word space then total space required will be 3 words.

3.4&nbsp; Time Complexity

Time complexity refers to how much computer time is required to completely run the algorithm and it is usually calculated as frequency count.

3.5&nbsp; Input Size

Input size is very important in algorithms because time required to completely execute the algorithm depends on the size of input. Sometimes exact input size need to be identified to implement the algorithm and in some cases input size can be predicted.

4.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Algorithm Example of strong and poor algorithm.

The following algorithm calculate the sum of first n integers.

Algorithm 1:

Definition sumOfN(n):

&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;Sum = 0

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; for x in range(1,n+1):

&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;Sum = theSum + x

return Sum

Let’s have a look on another algorithm Algorithm 2. Which looks very strange in first look but if we analyze deeply, we will find it is also producing the same output as previous algorithm. This is an example of poor algorithm with unnecessary initialization statement and elements which are defined in very ambiguous way.

Algorithm 2:

Define hoo(gom):

&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;mike = 0

&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;for billo in range(1,gom+1):

&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;barney = billo

&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;mike = mike + barney

&nbsp;return mike

&nbsp;

5.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Conclusion.

There are many possible ways/algorithms to achieve same task but the best algorithm is considered to be the one which is clearly understandable, consuming less resources with low complexity and fast execution time.

References:

A.A.Puntambekar (2009), Analysis of Algorithm and Design, 1st Edition, September 2014, Pg. 1-18

http://www.dcs.gla.ac.uk/~pat/52233/slides/Analysis1x1.pdf [Online], (Accessed: 25 October 2014).

http://en.wikipedia.org/wiki/Algorithm [Online], (Accessed: 25 October 2014).

http://interactivepython.org/runestone/static/pythonds/AlgorithmAnalysis/analysis.html [Online], (Accessed: 25 October 2014).

	              
	              Week 7 Discussion Question - Designing an algorithm, part 1


Re-Post of ORIGINAL Week 7 Pseudo-codes

Babatunde KOLAWOLE – 26 October 2014

&nbsp;

In this discussion I’d choose OPTION 3: Consensus Algorithm.

Pseudo-code 

Pseudo-code describes a computer programming algorithm that uses the structural conventions of programming languages, but leave out detailed subroutines.&nbsp; (Wikipedia)
 

Assuming flavors of ICE CREAM are A, B &amp; C

Declare A as integer,

Declare B as integer,

Declare C as integer,

Declare HIGHEST as string,

Declare HIGHESTVALUE as integer

DIALOGUE:

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; A = 0

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; B = 0

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; C = 0

For each person

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Print “choose between A, B &amp; C”

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; If A is chosen, set A = A + 1

If B is chosen, set B = B + 1

If C is chosen, set C = C + 1

Next

If A &gt; B then

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; HIGHEST = “A”

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; HIGHESTVALUE = A

Else

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; HIGHEST = “B”

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; HIGHESTVALUE = B

End if

If C &gt; HIGHESTVALUE then

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; HIGHEST = “C”

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; HIGHESTVALUE = C

End if

If HIGHESTVALUE &lt; 10 then

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; For each person

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Print “Most People Chose “&amp; HIGHEST

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Next

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; GOTO DIALOGUE

Else

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Print “Everybody wants “&amp; HIGHEST

End if

Conclusion

For consensus algorithm, there must be an agreement: in which every correct process must agree on an equal value. Weak validity: in which if every correct process receives the same input value; they must all give same output of that value. Strong validity: in which on each correct process, its output must be the input of some correct process. And then Termination: in which all processes must decide on an output value.

Reference:

Laureate Online Education. (2014). Computer Structures – Lecture Notes Week 7: Algorithms: [online]. Available from: https://elearning.uol.ohecampus.com/bbcswebdav/institution/UKL1/201480_SEPTEMBER/MS_CKIT/CKIT_501/readings/UKL1_CKIT_501_Week07_LectureNotes.pdf (Accessed on: October 17, 2014)

J.G. (2012) Computer science: An overview. 11th ed. Chapter 5, pp. 188-222. Boston: Pearson Education / Addison-Wesley. (Accessed on: October 17, 2014)

Guy M.H (1999). “Pseudo-code” [online] Available from: http://www.bfoit.org/itp/jargon/jargon_p.html#pseudocode (Accessed on: October 18, 2014)

Thomas H. C, Charles E. Leiserson, Ronald L. Rivest and Clifford S. (2009); Introduction to Algorithms (3rd Edition); Available from: http://ldc.usb.ve/~xiomara/ci2525/ALG_3rd.pdf (Accessed on: October 18, 2014)&nbsp; 
Wikipedia free encyclopedia Consensus (computer science) [online] Available from: http://en.wikipedia.org/wiki/Consensus_%28computer_science%29 (Accessed on: October 18, 2014) 
	              
	              Hi Colleagues,

One such example is determination of log of a number to base 10. Vb6 returns log values in base e (natural number). To get the value in base 10, one would have to write a conversion function. In vb.net there is log10 () function that converts to log base 10 or the log (Double, newBase) function which lets you choose a base (newBase).

&nbsp;

References:

Virtual Basic 6 (VB6) [online] Available from: http://www.vb6.us/ (Accessed on: October 26, 2014)

Visual Basic .NET [online] Available from: http://en.wikipedia.org/wiki/Visual_Basic_.NET (Accessed on: October 26, 2014)


Best Regards, Babatunde
	              
	              1.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Analyse the properties of an algorithm.
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; An algorithm is a detailed process of how to solve a well-defined problem.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Properties of an algorithm include:
•&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Algorithms must be well-ordered- Since an algorithm details a step by step process of how to solve a problem, the steps must be correctly ordered, else the wrong instruction is performed or uncertainty ensues about which order goes first or next.
•&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Algorithms should be unambiguous, precise and simple – Each operation or step in an algorithm must be precise and lucid to the point that it does not require simplification.
•&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Algorithms should have operations than can be effectively computed- Each step/operation in an algorithm must be executable i.e. it must be possible to do
•&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Algorithms should provide solutions-since an algorithm is a detailed process consisting of a set of steps on how to solve a problem, an algorithm’s result must therefore provide a correct solution to a problem.
•&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Algorithms should have a finite number of steps-an algorithm must have an end point, a termination point.
•&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Algorithms should have zero or more pre-specified inputs and at least one output where the output statements follow the initial input statements as well as the process instructions.
&nbsp;
2.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Analyse the ambiguity involved in algorithm specifications.
&nbsp;
An algorithm is considered valid if it is unambiguous. An algorithm’s instructions must be interpreted in one unique way. To fully circumvent ambiguity in an algorithm; it is worth assuming that the giver of instructions is not necessarily the executor of the instructions, therefore the instructions must be so clear that when followed the executor requires no creative skills and can execute the given instructions without any intervention or help.
&nbsp;
&nbsp;
3.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Using your analyses, explain how you determine an algorithm’s qualities
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Based on the above analysis, I would determine an algorithm’s qualities by:
•&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; The ease of which an algorithm’s instructions can be understood; by others and on a number of levels
•&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; The simplicity of an algorithm- if &nbsp;it is clear, concise and easy to follow
•&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; If it provides &nbsp;a correct solution for a clearly defined problem,
•&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; The length of an algorithm is also paramount- the lesser the time required to execute and algorithm the better
&nbsp;
4.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Compare examples of strong and poor algorithms to support your arguments.
&nbsp;

&nbsp;Good &nbsp;algorithm

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Below is an algorithm to find the max from a given list of numbers:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; The given inputs are:

List of positive numbers: L which must have at least one number

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; The outputs are:

The largest &nbsp;number: n 

&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Algorithm:
&nbsp;

Set max to 0
For each number x on the L, compare it to max,if x &nbsp;is larger than max, &nbsp;set max to x
The largest number on the list is now max

&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Why this algorithm is good and satisfies the desirable algorithm qualities:

It is unambiguous
Each step consists of possible, executable operations 
It has finiteness, the algorithm will terminate once &nbsp;all the elements of L have been processed
The algorithm does produce a correct result

&nbsp;
&nbsp;

Poor algorithm example

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Below is an algorithm to make a peanut butter jelly sandwich:

Take two slices of bread
Put peanut butter on one side of one slice
Put jelly on one side of other slice

&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp; Why this algorithm is poor and does not satisfy the desirable algorithm qualities:

It is ambiguous as it does not state that the two slices should be put together so that the spreads are on the inside
The algorithm does not produce the correct desirable result

&nbsp;
&nbsp;
&nbsp;
References 
&nbsp;
http://courses.cs.vt.edu/csonline/Algorithms/Lessons/DefinitionOfAlgorithm/ [accessed online-October 24, 2014]
http://algorithmsandproblems.blogspot.com/2013/04/definition-of-algorithm.html[accessed online-October 25, 2014]
http://www.teach-ict.com/gcse_computing/ocr/216_programming/algorithms/miniweb/pg4.htm [accessed online-October 25, 2014]
http://people.cs.nctu.edu.tw/~cjtsai/courses/ics/classnotes/ics12_05_Algorithms.pdf[accessed online-October 26, 2014]
http://www.programiz.com/article/algorithm-programming [accessed online-October 26, 2014]
&nbsp;
&nbsp;
&nbsp;

	              
	              Hi Terry,
I like your elephant algorithm. it simplifies things for me. I honestly struggeled a bit with algorithms this past week.&nbsp;
I also like the "•&nbsp;Make it work&nbsp;•&nbsp;Make it right&nbsp;•&nbsp;Make it fast" rule you sated. That again simplifies software development or pretty much any task for that matter.
Best regards,
Belinda&nbsp;
	              
	              This is a repost from Bram's solution to problem statement option 2: Word Search

Make a list of five words, 3-6 letters in length.

-&gt; The list of 5 words: the, cat, loves, his, mat

Create a string of approximately 30 letters containing some of the five words.

-&gt; String of 30 letters:&nbsp; thecatisonthematoftheneighbour

Your algorithm should identify all of the substrings of the longer string that match any of the five words you generated, and the number of times each one appears

&nbsp;
On the DQ of week 7 I saw that king Tan Yu did the same exersize. Therefore I compared my version with his one suggesting some improvements that could help improve both our solutions. 
Besides having chosen the same problem statement (finding words in a string of letters) we also looked in the same direction for a solution (The Knuth-Morris-Prath algorithm). In the end we found some different references and therefore diverted a bit in our proposed solution. 
&nbsp;
In King Tan Yu's proposal I found the COMPUTE-PERFⅨ-FUNCTION (P) an interesting function as it looks if the pattern consists of returning patterns itself. If this is the case this information can be used to skip some positions in the investigated text string. I think this variant is also known as the Boyer-Moore string search algorithm. In my Text-string example with my chosen set of words this would not have made a difference, but still this a great optimalisation when word search inputs do have repeating patterns as the Knuth-Morris-Prath (KMP) algorithm check every single letter one by one.
&nbsp;
I wondered how King Tan Yu's version would apply different sets of words that needed to be checked. I changed the KMP solution of Simha (Simha, 2014) in order to deploy it in my overall procedure I was set to create for the 5 words and the string. One change I had to make is line 18 to return the textCursor position in order to let the procedure not stop when one word was found. Instead it starts the matching exercise again for the first letter after the match. This is also part of your solution by line 12. q ← ∏[q]. 
&nbsp;
The main procedure countWords consists of a recursive 'if-then-else' operation selecting the words one by one, start the KMP-solution to check how much they occur, until all 5 are checked. The KMP-algorithm gives back when the text is not found, the procedure counts how many times occurrences are found. This is still a weak point of my programming as one gets too many negative messages. The reporting has to be done in the procedure and I have to add that if the KMP-algorithm is not repeated a word was not found. 
&nbsp;
Both Knuth-Morris-Prath as Boyer-Moore optimised string search algorithm are ideal for single pattern searches. For our multiple word assignment we could improve our algorithm by using&nbsp; hashing for shifting substring search as worked out in the Rabin-Karp algorithm. In this method the hash value of the words are determined and look for these pattern values in the total word string. The hash values have to be created only ones and can be applied to the whole string instead of looking at every letter in a string every time (or skip if the search word has repeated pattern). 
&nbsp;
But this is something I will try for tomorrows exercise.
&nbsp;
List (1 , the , 3, S), (2, cat, 3), (3, loves, 5), (4, &nbsp;his, 3), (5, mat, 3)
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Procedure: countWords
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Input&nbsp;'list of pattern arrays and their length', N = 1, S = 0)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;// algorithm as written in section B is started with input of first of 5 words (N = 1)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // and the length of the word W, &nbsp;while the score (S) and textCursor position &nbsp;is 0
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;// as the counting begins.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1. &nbsp;&nbsp;if&nbsp;N &lt; 5 &nbsp;do&nbsp;Start Algorithm (&nbsp;KnuthMorrisPrattPatternSearch)&nbsp;and
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;Input N (pattern array, 'text array')&nbsp;and&nbsp;Input (textCursor = 0)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // algorithm is started again when a pattern is found but now
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // with updated textCursor position and Score which is also displayed
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;return = 1&nbsp;do&nbsp;Start&nbsp;Algorithm (KnuthMorrisPrattPatternSearch)&nbsp;and
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Input&nbsp;N(pattern Array, text array)&nbsp;and&nbsp;Input (textCursor )&nbsp;and&nbsp; S = S + 1,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Display&nbsp;['text array' appeared T times in 'text array']
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;// if there was no hit the next word needs to be selected and score set to 0
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;else&nbsp;N = N + 1&nbsp;and&nbsp;S = 0
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;endif
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5.&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;else
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;6.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Display&nbsp;[Ready, but does the cat love the mat of the neighbour?]
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 7.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;endif
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;8.&nbsp;&nbsp;&nbsp; endif
Algorithm: KnuthMorrisPrattPatternSearch ('text array', 'pattern array')
Input: 'pattern array' of length W, 'text array' of length T, textCursor position C
&nbsp;&nbsp;&nbsp;&nbsp; // Compute the nextIndex function, stored in an array:
1.&nbsp;&nbsp; nextIndex = computeNextIndex (T)
&nbsp;&nbsp;&nbsp;&nbsp; // Start the pattern search.
2.&nbsp;&nbsp; textCursor = C
3.&nbsp;&nbsp; patternCursor = 0
4.&nbsp;&nbsp;&nbsp;while&nbsp;textCursor &lt; T&nbsp;and&nbsp;patternCursor &lt; W
5.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;text[textCursor] = pattern[patternCursor]
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // Advance cursors as long as characters match.
6.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; textCursor = textCursor + 1
7.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; patternCursor = patternCursor + 1
8.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;else
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // Mismatch.
9.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;patternCursor = 0
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // If the mismatch occurred at first position,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // advance the pattern one space.
10.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; textCursor = textCursor + 1
11.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;else
&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; // Otherwise, use the nextIndex function. The textCursor
&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;// already points to the next place to compare in the
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // text.
12.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; patternCursor = nextIndex[patternCursor]
13.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;endif
14.&nbsp;&nbsp;&nbsp;&nbsp;endif
15.&nbsp;&nbsp;endwhile
16.&nbsp;&nbsp;if&nbsp;patternCursor = W
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // If there was a match, return 1 and the position of the
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // textCursor to be able to check rest of 'text array'.
17.&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;1
18.&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;textCursor
19.&nbsp;&nbsp;else
20.&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;-1&nbsp;and&nbsp;display[the 'pattern array' was not found in the 'text array']
&nbsp;&nbsp;
Output: 1 and textCursor if the inputted word is found, -1 if not found.
&nbsp;
&nbsp;
References 
Simha (2014) Course Materials; Module 5, Pattern search [Online]. The George Washington University. Available from http://www.seas.gwu.edu/~simhaweb/alg/lectures/module5/module5.html (Accessed on 22 October 2014).
Wikipedia. (2014). Boyer-Moore string search algorthm [Online]. Wikipedia Foundation Inc.. Available from: http://en.wikipedia.org/wiki/Boyer%E2%80%93Moore_string_search_algorithm. (Last accessed 26 October 2014).
Wikipedia. (2014). Rabin-Karp algorithm [Online]. Wikipedia Foundation Inc.. Available: http://en.wikipedia.org/wiki/Rabin%E2%80%93Karp_algorithm. (Last accessed 26 October 2014).

	              
	              Week 8 DQ - Algorithm qualities

Babatunde KOLAWOLE – 26 October 2014


Definition:

An algorithm is a finite set of discrete statements or step-by-step process in some particular sequence to accomplish a predefined particular task.


Properties:

An algorithm must have the following properties:


Input(s): An algorithm must have one (1) or more pre-specified input(s).


Output(s): An algorithm must have one (1) or more output(s).


Definiteness: Each steps of an algorithm must define clearly (i.e. without any confusion or Contradiction or Ambiguity).


Finiteness: Each steps of an algorithm must be terminated after a finite (tolerable) amount of time.


Effectiveness: Any calculations performed / Decision taken by a computer or any electronic machine with respect to an algorithm should be computable by the human beings with pencil and paper although time may be longer.

Correctness: An algorithm should always produce correct result with respect to its domain ( of the inputs ).


There are two advanced control structures and they are, Procedure and Recursion.


Procedure:

It may happen that a sequence frequently occurs either in the same algorithm repeatedly in different parts of the algorithm. In such cases, writing repeatedly of the same sequence is a wasteful activity. Procedure is a mechanism that provides a method of checking this wastage.

Under this mechanism, the sequence of instructions expected to be repeatedly used in one or more algorithms, is written only once and outside and independent of the algorithms of which the sequence could have been a part otherwise.

Syntax:

Procedure &lt;Name&gt; (&lt;parameter–list&gt;) [:&lt; type&gt;]

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;declarations&gt;

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;sequence of instructions expected to occur&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; repeatedly&gt; 

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; end;

For Instance:

Procedure sum – square (a, b : integer) integer;

{denoted the inputs a and b integers and the output is also an integer}

S: integer;

{to store the required number}

begin

S &lt;-a2 + b2

Return (S)

end;

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Program diagonal – Length

[The program finds lengths of diagonals of the sides of right angled triangles whose lengths are given as integers. The program terminates when the length of any side is not positive integer]

L1, L2; integer; {given side lengths}

D: real;

{To store diagonal length}

read (L1, L2)

while (L1&gt; 0 and L2&gt; 0) do

begin

D&lt;-square–root (sum – square (L1, L2))

Write (‘for sides of given length’, L1, L2, ‘the required diagonal length is’ D);

read (L1, L2)

end

&nbsp;

Recursion:

A procedure, which can call itself, is said to be recursive procedure/algorithm.

Ex:

factorial (1) = 1

factorial (n) = n * factorial (n – 1)


Procedure SUM (n: integer: integer)

s: integer:

If n = 0 then, return (0)

Begin&nbsp;s &lt; - n + SUM (n – 1)

end;

end;


An algorithm:



Is said to be ‘Good’, if there is finiteness, this simply means that it will stop at a controlled point. 
Is said to be ‘Bad’, if there is no finiteness, this simply means that it will run without stopping at any controlled point.



Conclusion

Usually a given problem can be explained or answered using various ideas or actions however it is not advisable to settle for the first that comes to mind. More often than not, some various ideas result in much more efficient solutions than others.

&nbsp;

&nbsp;

References:

Prof. S K. Mondal (2013) Algorithm - Definition, Properties, Iteration &amp; Recursion [online] Available from: http://algorithmsandproblems.blogspot.fr/2013/04/definition-of-algorithm.html (Accessed on: October 25, 2014)


Programming with Java - an Algorithmic Introduction Simple algorithm design (2009) [online] Available from: http://www.scifac.ru.ac.za/javabook/ch02.htm (Accessed on: October 25, 2014)

Algorithms (online) Available from: http://courses.cs.vt.edu/~csonline/Algorithms/Lessons/index.html (Accessed on: October 25, 2014)

Paul E. Dunne Algorithm Design Paradigms [online] Available from&nbsp;: http://faculty.simpson.edu/lydia.sinapova/www/cmsc250/LN250_Weiss/L28-Design.htm (Accessed on: October 25, 2014)
	              
	              Introduction
There is no universally accepted definition of the word "algorithm". An algorithm can be define as a set of instructions or steps for solving a define problem. 
Algorithms are implemented by a program or simulated. They are often composite by steps that iterate (repeat) or require some logic and/or comparison decisions. 
The algorithms are critical to information processing by computers because one program is actually an algorithm that gives the machine the steps and instructions to accomplish, and in which order to accomplish these given steps, such as solving en equation. So an algorithm is a sequence of operations that can be performed by a machine. Among the authors who support this thesis include Gurevich:
"The informal argument Turing for his thesis justifies a stronger thesis: every algorithm can be simulated by a Turing machine."
And Savane
"An algorithm is a process defined by a Turing machine."
Algorithms Expression
The algorithms can be transcribed in various kinds of notations: 

Transcription natural language tends to be verbose and ambiguous; it is rarely used for complex algorithms. 
Pseudo-code and flowcharts are designed to avoid these ambiguities and are independent of any implementation. 
Programming languages ​​are primarily used to transcribe a form that can be processed by computer, but can also be used to describe algorithms.

Algorithm Analysis
The term "algorithm analysis" was launched by Knuth. Most of those who implement algorithms want to know what resources, like time, storage space will be required for its execution. Methods have been developed for the analysis algorithms in order to obtain the quantized response.   The analysis and study of algorithms is a discipline of computer science, and is often done without the use of abstract language or hardware. But pseudo code is quite portable, simple and abstract for such analysis.
&nbsp;
Characteristics of an algorithm
The algorithm is a way for the programmer to present its approach to the problem to others. Indeed, an algorithm is outlined in a well-defined a series of operations to meet the language problem. An algorithm must be: 

Readable: the algorithm must be understandable by non-computer 
High level: the algorithm must be translated into any programming language, it is therefore not to use technical terms related to a particular program or a particular operating system 
Precise and unambiguous: each element of the algorithm should not be confusing, so it is important to remove any ambiguity 
Concise: an algorithm should not exceed one page. If this is the case, we must decompose the problem into sub-problems 
Structured: an algorithm must be composed of different parts easily identifiable

STAGES OF RESOLUTION ALGORITHM
In general, an analysis of a problem in order to solve can be reduced to a query string which makes: 

What is the result to get? (What is - it?) 
What operations are used to develop this result? (How to get this result?) 
What is needed to achieve these operations data? (What did he do I need?) 

Whatever the type of problem that will be in front of the analysis will: 

Clearly define the problem to identify (s) result (s) (Output) to get. 
Identify treatments to get the result in the problem statement. 
Identifying information (inputs) needed to achieve the proposed processing. 
Expand the algorithm step by step through games of tests to see if it produces the required result.

Thus the algorithm can be presented by the following classic pattern
&nbsp;
Input (Data) ----------------------&gt; Processing (Data) ------------------------&gt; Output (Results)
&nbsp;
Complexity of an algorithm
How to evaluate the performance of an algorithm? Different algorithms have different costs in terms of execution time (number of operations performed by the algorithm) Memory size (size needed to store different data structures for execution). 
 These two concepts are called the time complexity and space algorithm. 
Algorithmic complexity is used to measure the performance of an algorithm and compare it with other algorithms achieving the same functionality. Algorithmic complexity is a fundamental concept for every computer, it determines if an algorithm “A” is better than another algorithm “B” and if it is optimal or should not be used.
Qualities of an algorithm:
Any program provided to a computer is only the translation into a programming language of an algorithm to solve a given problem. a good program, must start with a good algorithm. it must, among others, the following characteristics: 
Be clear, easily understood by all who read it (structure and documentation) present the greatest possible generality to answer as many cases as possible be easy even for those who do not use written and through messages on the screen that will indicate what information to provide and in what form they should be introduced as well as the shares of the different expected by the user be designed to limit the number of operations to be performed and the amount of memory. 
One of the best ways to make a clear and understandable algorithm is to use a structured using only a small number of independent structures of the programming language programming. 
A development of a good algorithm technique is called top-down approach (top down). It is considered a problem as a whole, to clarify the data and results and then to decompose the problem into several simpler sub-problems to be dealt with separately and possibly broken themselves more finely.
Some characteristics of a good algorithm:

Finiteness: "An algorithm must always terminate after a finite number of steps."
Definition says: "Each step of an algorithm must be precisely defined; the actions to transpose must be specified rigorously and without ambiguity for each case." 
Entries "... amounts that are given before an algorithm begins these entries are taken in a specified set of objects...” 
Outputs: "... amounts that have a specified relationship with the entries." 
Yield: "... all operations that the algorithm must do, must be sufficiently basic to be made ​​in principle in a finite by a man using paper and pencil term." 
Maintainable (easy to understand, code, debug) 
Fast

&nbsp;
Compare examples of strong and poor algorithms
We will analyze two algorithms of search from a list and then compare the effectiveness of the two algorithms.   We will consider a list L of n numbers. Given a number x, we want to know if x is in the list (and where) or not.
1.&nbsp;&nbsp;&nbsp;&nbsp; The sequential search
To find an item in a list, the first idea is to compare x with all elements of L, from the first.   As soon as it has found an item of L equal to x, we stop.   If we reach the end of the list without finding x is that it is not there!   This method is called sequential search.   The following algorithm, written in pseudo-code, reflects this approach:
Begin 
&nbsp;&nbsp;&nbsp; READ L
&nbsp;&nbsp;&nbsp; READ x
&nbsp;&nbsp;&nbsp; i = 0
&nbsp;&nbsp;&nbsp; found = false
&nbsp;&nbsp;&nbsp; WHILE found = false AND i &lt; length (L) DO
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; If x = L[i] Then
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; found = true
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; End if
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; i = i + 1
&nbsp;&nbsp;&nbsp; End While
&nbsp;&nbsp;&nbsp; If found = true Then
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Write "x is found at rank", i-1
&nbsp;&nbsp;&nbsp; Else
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Write "x is not in the list"
&nbsp;&nbsp;&nbsp; End if
END
&nbsp;
&nbsp;
&nbsp;
2.&nbsp;&nbsp;&nbsp;&nbsp; Binary search
Binary search is a different search method. It is based on the idea of ​​dichotomy, already encountered in mathematics; find an approximate value of the solution to an equation.  This method applies to a list whose elements are first sorted in ascending order, which is assumed in the following.   The principle is as follows:   

X is compared with the core c of the list (the element row integer quotient of n by 2); 
If x &lt; c, we start by replacing L by its lower half; if x&gt; c, we start by replacing L by its upper half. If x = c, x was found! 
We continue to get reduced to a list element if this element is equal to x, then x is in the list and it was his rank, otherwise x is not in the list. 

The following algorithm, written in pseudo-code, reflects this approach:
Begin 
&nbsp;&nbsp;&nbsp; READ L
&nbsp;&nbsp;&nbsp; READ x
&nbsp;&nbsp;&nbsp; begin = 0
&nbsp;&nbsp;&nbsp; end = length (L)-1
&nbsp;&nbsp;&nbsp; middle = E((begin + end) / 2) # integer part
&nbsp;
&nbsp;&nbsp;&nbsp; WHILE begin &lt; end AND x != L[middle] DO
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; If x &lt; L[middle] THEN
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; end = middle – 1
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ELSE
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; begin = middle + 1
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; End if
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; middle = E((begin + end) / 2)
&nbsp;
&nbsp;&nbsp;&nbsp; End While
&nbsp;
&nbsp;&nbsp;&nbsp; If x = L[middle] THEN
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Write "x is found at rank ", middle
&nbsp;
&nbsp;&nbsp;&nbsp; ELSE
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Write "x is not in the list"
&nbsp;
&nbsp;&nbsp;&nbsp; End if
END
3. Comparison of the two algorithms   A simple way to compare the efficiency of these two algorithms is to study what happens in the worst case: how much iteration are done in each algorithm? 
The worst case for sequential search: 
For this algorithm, the worst case is when the desired number is not in the list. We then have a number of iterations equal to the number n of elements in the list. For a list of size 100, for example, it will take 100 iterations at worst. 
The worst case for the binary search:  
For this algorithm, the worst case is when the condition x = L [middle] is never realized.   The algorithm terminates then when you get to a list with 1 item.
We can see that the program has six towers loop to know that the number sought is not in the table, against 16 laps loop for sequential search. 
 The problem with this algorithm is that the table must be sorted before.
Advantage of binary search: High speed compared to the sequential search   

Sequential algorithm: Number of iterations of the order of the size of the table   
Dichotomous algorithm: Division by two (approximately) the size of the search space at each iteration   
In the order of log2 (table size) iterations research although individually each iteration is longer because it requires more treatment beyond a certain size table dichotomous algorithm becomes more efficient.

&nbsp;
References:
Lecture Notes, (2014). &nbsp;Week 6: Computer networks [Online]. Available from: https://elearning.uol.ohecampus.com/bbcswebdav/institution/UKL1/201480_SEPTEMBER/MS_CKIT/CKIT_501/readings/UKL1_CKIT_501_Week08_LectureNotes.pdf. (Accessed: 26 October 2014)
&nbsp;
Wikipedia Inc. (2014) [Online]: Wikipedia Inc. Available from: http://www.google.co.uk/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;frm=1&amp;source=web&amp;cd=1&amp;ved=0CCYQFjAA&amp;url=http%3A%2F%2Fen.wikipedia.org%2Fwiki%2FComparison_sort&amp;ei=XFBNVIHVNIrC7Aag0oC4Cw&amp;usg=AFQjCNHQ8zMMKuzNIx4C82wQZip2X7YGxw (Accessed: 26 October 2014)
Dailyfreecode (2014). &nbsp;Algorithms sequential [Online]: Dailyfreecode. Available from: http://www.dailyfreecode.com/code/algorithms-sequential-search-binary-2471.aspx (Accessed: 26 October 2014)
Stonybrook (2014). &nbsp;Algorithms sequential [Online]: Stonybrook. Available from: http://www3.cs.stonybrook.edu/~skiena/214/lectures/lect19/lect19.html (Accessed: 26 October 2014)
&nbsp;
&nbsp;
&nbsp;

	              
	              Introduction During last week, I talked about the basic principle of word search, naive string matching algorithm, and introduced the improving string matching, KMP algorithm. However, indeed these methods are rarely used to find the word during the reality. Just as I introduced the KMP algorithm that it is unnecessary to compare the word and the string character by character. Certain comparisons are able to be omitted so as to ameliorate the efficiency of the string matching, this is a pretty good advancement of the naive string matching. Through the example of KMP algorithm, we can found that the more comparisons are omitted and move the word faster, the more speed up the matching, thus the more efficient of searching. While Boyer-Moore Algorithm is able to maximally decrease the comparisons and rapidly move the word, it is the most appropriate approach for actual word searching. Boyer-Moore Algorithm Boyer-Moore is a string matching algorithm which ameliorates the function of word searching into a string. It is invented by Robert S. Boyer and J Strother Moore during 1977 and it comprises certain particular properties which think over the best performance string matching algorithm during general applications, such as the commands substitutions and text editors. This is because it works highest speed when the word is comparatively long and the string is moderately sized. Boyer-Moore algorithm compares the characters of the word from right to left starting with the last character of the word (rightmost). In the inspection of a probable position of word W opposite String S, a mismatching of String character S[i] = ch with the relevant Word character W[j] is processed that supposing ch is not included anywhere during the W, then moves the Word W totally past S[i], else move the W until an existence of character ch in W aligns with S[i]. Here adopting the given string and word in week 7 to present the algorithm that the algorithm begins contrasting the word from the initial position (leftmost) of string and shifts it to the right hand side as following: Supposing the string is “jkdistructuredstrusyssystemoon” and the word is “system”.     j   k   d   i   s   t   r   u   c   t   u   r   e   d   s   t   r   u   s   y   s   s   y   s   t   e   m   o   o   n     s   y   s   t   e   m  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;    First, the string aligns the word at the leftmost, and compares the last character of the word (rightmost). This is a quite smart idea, as if the last character is mismatch, so long as once comparison, it is surely that the former six characters (overall) is not the searching result. During the first comparison, the t mismatches with m, then the t is called “Bad character”, but t is contained in word “system”, thus move 2 position, and aligns the both “t”.     j   k   d   i   s   t   r   u   c   t   u   r   e   d   s   t   r   u   s   y   s   s   y   s   t   e   m   o   o   n    &nbsp; &nbsp;  s   y   s   t   e   m  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;    Compare the last character again; the “u” also mismatches “m”, and “u” is not existed in the word, this means the word can be moved directly to the position after “u”.     j   k   d   i   s   t   r   u   c   t   u   r   e   d   s   t   r   u   s   y   s   s   y   s   t   e   m   o   o   n    &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  s   y   s   t   e   m  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;    This method is known as “Bad character heuristics” that the rule of Bad character is: The move position = the position of bad character – the last occurrence of the character in the word; and if the Bad character does not appear in the word, then last occurrence of the character is -1.     j   k   d   i   s   t   r   u   c   t   u   r   e   d   s   t   r   u   s   y   s   s   y   s   t   e   m   o   o   n    &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  s   y   s   t   e   m  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;    &nbsp;     j   k   d   i   s   t   r   u   c   t   u   r   e   d   s   t   r   u   s   y   s   s   y   s   t   e   m   o   o   n    &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  s   y   s   t   e   m  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;    &nbsp;     j   k   d   i   s   t   r   u   c   t   u   r   e   d   s   t   r   u   s   y   s   s   y   s   t   e   m   o   o   n    &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  s   y   s   t   e   m  &nbsp; &nbsp; &nbsp; &nbsp;    &nbsp;     j   k   d   i   s   t   r   u   c   t   u   r   e   d   s   t   r   u   s   y   s   s   y   s   t   e   m   o   o   n    &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  s   y   s   t   e   m  &nbsp; &nbsp; &nbsp;    If the last character is matching, then compare the former position character, at this moment, the last 2 characters ate matching, it is called "good suffix", and the rule of good suffix heuristic is: Move position = the position of good suffix - the last occurrence of the character in the word Pseudocode of Boyer-Moore Algorithm  Last(ch) is the index of the last occurrence of the character in W, if ch is in W, else last(ch) = -1 match(j) is the least l &gt;= 1 thus satisfy one of the followings:    W[j+1-l...m-l-1] is a suffix of W [j+1...m-1] and W[j] &lt;&gt; W(j-l], with l&lt;= j W[0...m-l-1] is a suffix of W[j+1...m-1], with j&lt;l&lt;m s = m&nbsp;   Algorithm Boyer-Moore Match(S, W): 1.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Input: S (String, length of S = m characters) and W (word, length of W= m characters)      Output: Starting index of first appearance of S matching W, or an expression that W does not exist in S i &lt;-- m – 1 j &lt;-- m – 1 Loop if W[j] = S[i] then if j = 0 then return i { matching} else {go to search next character} i &lt;-- i – 1 j &lt;-- j – 1 else { W[j] &lt;&gt; S[i] sift the word} i &lt;-- i + m - j - 1 {reset i to where it started during the most-recent check} i &lt;-- i + max (j - last (S[i]), match (j)) {move W relative to S}   ** even if j-last(S[i]) is negative, it will still carry out &nbsp;a positive move, as match (j) is always at least 1. j &lt;-- m-1 until i &gt; n – 1 return "The word is not found in the string"      Conclusion The worst case of the running time of Boyer-Moore algorithm is quadratic that the calculation of the last function needs O(m+|∑|) running time and real searching needs A(man) running time. This means the running time of Boyer-Moore algorithm is O(nm + |∑|); when n=m, the running time is same as the naive string matching&nbsp; algorithm. &nbsp; References &nbsp; Anany V. Levitin (2011);Introduction to the Design and Analysis of Algorithms (3rd Edition); Available on :(ftp://doc.nit.ac.ir/cee/jazayeri/.../Algorithms/Books/Design%20&amp;%20Analysis%20of%20Algorithm.pdf) (Accessed on: 26-Oct-2014) &nbsp; &nbsp; Christian Lovis, MD and Robert H. Baud, PHD (2000); Fast Exact String Pattern-matching Algorithms Adapted to the Characteristics of the Medical Language; Available on :( http://www.ncbi.nlm.nih.gov/pmc/articles/PMC61442/) (Accessed on: 26-Oct-2014) &nbsp; &nbsp; Guy M.Haas (1999); Pseudocode; Available on: (http://www.bfoit.org/itp/Pseudocode.html) (Accessed on: 26-Oct-2014) &nbsp; &nbsp; H.W. Lang&nbsp;&nbsp; FH Flensburg (2008); Boyer-Moore algorithm; Available on: (http://www.iti.fh-flensburg.de/lang/algorithmen/pattern/bmen.htm) (Accessed on: 26-Oct-2014) &nbsp; &nbsp; Rami H. Mansi and Jehad Q. Odeh (2009); On Improving the Naïve String Matching Algorithm; Available on: (http://www.medwelljournals.com/fulltext/?doi=ajit.2009.14.23) (Accessed on: 26-Oct-2014) &nbsp; &nbsp; Robert S. Boyer &amp; J Strother Moore (1977); A Fast String Searching Algorithm; Available on: (https://www.cs.utexas.edu/~moore/publications/fstrpos.pdf) (Accessed on: 26-Oct-2014) &nbsp; &nbsp; S. S. Sheik,&nbsp; Sumit K. Aggarwal,&nbsp; Anindya Poddar, N. Balakrishnan,&nbsp; and K. Sekar (2003); A FAST Pattern Matching Algorithm; Available on: (http://www.math.utah.edu/~palais/dnamath/patternmatching.pdf) (Accessed on: 26-Oct-2014) &nbsp; &nbsp; Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest and Clifford Stein (2009); Introduction to Algorithms (3rd Edition); Available on: (http://ldc.usb.ve/~xiomara/ci2525/ALG_3rd.pdf) (Accessed on: 26-Oct-2014) &nbsp; 
	              
	              What makes one algorithm better than another?   &nbsp;   &nbsp;   An algorithm can be classified as a set of steps or a defined process / methodology which solves a problem.   &nbsp;   Laureate Education (2014), describes the analysis of algorithms as “A good one can sometimes be run a thousand or million times faster than a poor algorithm. In computer science, the design and analysis of algorithms is therefore of utmost importance. In many cases, algorithms are designed using abstract concepts, but then they are checked and rechecked using empirical methods, such as converting them to code and then running them on computers. “   Laureate Education (2014), also describes that there are two types of algorithm, namely iterative and recursive. Iterative being an algorithm which goes through a list of defined linear instructions performing them one by one until a result or solution is finally found. Recursive being a loop or repetitive process which performs as a sub task to the main algorithm until it finds or solves the problem. However others have suggested more broken down types such as what Sagar, V (2009) explains:   &nbsp;   Types of algorithms:    Simple recursive algorithms. - Searching an element in a list Backtracking algorithms - Depth-first recursive search in a tree Divide and conquer algorithms. - Quick sort and merge sort Dynamic programming algorithms. - Generation of Fibonacci series Greedy algorithms - Counting currency Branch and bound algorithms. - Travelling salesman (visiting each city once and minimize the total distance travelled) Brute force algorithms. - Finding the best path for a travelling salesman Randomized algorithms. - Using a random number to choose a pivot in quick sort.    &nbsp;   Essentially the above different types can still be divided into the basic two categories of recursive and iterative but it does show the complexity of the changing and diverse uses of an algorithm to solve a problem.   &nbsp;   &nbsp;   Algorithms can also have many properties but the basic of which can be explained very easily.    There is some sort of input. A finite number of actions or steps. Runs within a finite time. An output or result is produced.    &nbsp;   However this is not all the story, algorithms can have other properties on top of the ones mentioned such as described by Sagar, V (2009):   &nbsp;   Properties of an algorithm:   - Should be written in simple English   - Should be unambiguous, precise and lucid   - Should provide the correct solutions   - The output statements should follow input, process instructions   - The initial statements should be of input statements   - Every statement should be definitive   &nbsp;   Granted my own opinion of the first point is that it’s obvious that it doesn’t have to be in English per say but more whatever your clear native language is that suites you to produce the algorithm. Numbers and mathematics is the one globally recognised natural language in this case and point.   &nbsp;   Ambiguity can be a problem when creating algorithms and how you handle this ambiguity is a key quality. &nbsp;Algorithms which don’t determine a definitive result may still be valid, however none ambiguous algorithms are more deterministic.   &nbsp;   Eliminating any known or found ambiguity can be a key to producing a good algorithm.   &nbsp;   &nbsp;   How I determine the quality of an algorithm is using these following points:    Clear and easy to follow As&nbsp; simple as possible Efficiency Productive Speed    &nbsp;   These points to me show the algorithms qualities and thus can be measured. For example being clear and easy to follow, this may sound obvious but one of the key skills in creating any algorithm or pseudo code is make it clear to read and easy to follow for anyone to pick up for the first time and understand what the algorithm does. Secondly as simple as possible, now this might sound like cutting corners or to dumb down the code but it’s not. It is more aimed at making sure you are going about the set tasks in the correct orderly way so that instead of having 3 steps you could have 2 for example. Efficiency can be hard as sometimes the obvious way to do an algorithm is the efficient way as well, so you can spend a long time trying to make it more efficient when there is no need. However this isn’t always the case and so it is always best to go through your algorithm and make sure it is as efficient as possible. An example of this is in this week assignment where instead of saving all of the possible integer combinations from the string “37540” these combinations are   3, 37, 375, 3754, 37540, 7, 75, 754, 7540, 5, 54, 540, 4, 40, 0, but because we also want to get which of those numbers are divisible by “3” then it is efficient not to save off those initial numbers first and then go through them to test and resave the result. The most efficient way is to test them immediately after getting the list one by one and then only saving the results that is needed.   &nbsp;   This can also lead into the productivity quality where sometimes an algorithm doesn’t just need to produce an output but also be flexible and robust enough to change inputs for example and be prepared to handle any known negatives such as returning no results at all or handling loops more productively.&nbsp; Lastly speed; this is the essence of an algorithm in the sense that it must be the timeliest way to do the algorithm. This isn’t a race but more making sure your algorithm is running as fast as it can by cutting out as much I/O database access read/writing and general operation etc as possible.   &nbsp;   For example a bad designed algorithm to do some looped computation where you loop until X = Y but what if X never equals Y? Will it continually loop without an built in ejection counter for example loop until X = Y or Z = 1million, for each loop Z is incremented by 1 therefore if X never equals Y by the time Z equals 1 million loops then it ejects.   &nbsp;   Another example could be simply around efficiency and simplicity, where a long drawn out algorithm is overly engineered to do more or check for more than it needs to efficiently to the computation and produce an output. In some situations you know that the only input will be within a certain range or ranges so why would you build in a list of numerous outside of the range checks just to be sure. This would slow down the algorithm and include unnecessary lines of code for what will be an already known negative result as it would never meet those conditions.   &nbsp;   &nbsp;   &nbsp;   Conclusion   In essence the quality of an algorithm is based on the function it is trying to accomplish and then the best way to accurately and succinctly produce said output. Algorithms can come in many shapes and forms, some very simple and some complex, either way the essence of producing good algorithms are the same.   &nbsp;   &nbsp;     References:   Brookshear, G. (2011) ‘Computer science: An overview’. 11th ed. Boston: Addison Wesley/Pearson.    Chapter 5, ‘Algorithms’   Section 5.5, ‘Efficiency and Correctness’      Laureate Education, (2014) Analysis of Algorithms: Week 8 Lecture Notes [Video, Online], (accessed: 25/10/14)     Sagar, V (2009) ‘Algorithm, properties of an algorithm, types of algorithms‘ &nbsp;[http://careerride.com/Data-structure-algorithm-and-its-types.aspx] Accessed: 25/10/2014.&nbsp; The Scratch project at MIT (n.d.) Scratch: Imagine, program, share [Online]. Available from: http://scratch.mit.edu (Accessed: 25/10/2014).   UNF.edu, (2014) ‘Pseudocode Examples’ [http://www.unf.edu/~broggio/cop2221/2221pseu.htm] Accessed: 25/10/2014. Wikipedia, (2014) ‘Pseudocode’ [http://en.wikipedia.org/wiki/Pseudocode] Accessed: 25/10/2014.
	              
	              RE-posting of original wk7 DQ - for comments  Designing an Algorithm.    &nbsp;    We were given 3 optional scenarios to choose 1 from and I decided to look at this one.    • Option 1: Numbers. Your algorithm should:    &nbsp;&nbsp;&nbsp; Search a string of at least five numbers (for example, 37540)  &nbsp;&nbsp;&nbsp; Identify all of the substrings that form numbers that are divisible by 3.  &nbsp;&nbsp;&nbsp; For example, applying the algorithm on the string 37540 should produce the following substrings (not necessarily in this order): 0; 3; 75; 54; 375; 540.     &nbsp;  To create the pseudo code algorithm I first broke down the individual aspects to resolve in order.    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Identify a set string of at least five numbers (example given, “37540”)  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Enter the divisible number (example given, “3”)  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Strip out the entire possible sub strings within that string.  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Check to see if each of the sub string values are divisible by 3  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Report the remaining sub strings which are divisible by 3     Part 1    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; We are given the initial example as “37540” for the string value and a divisible number of “3”. However I will view it from the aspect that the user would be prompted to enter these details so they could be any value.    The user would be prompted to enter an integer value of at least five numbers and the divisible number to test against.    Procedure EnterData(entVal,divVal)(    Define aVal as int;    Define aCount as int;    Define aDiv as int;    Int aVal = 0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // value of the string for example 37540 initially set to 0    Int aCount = 0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // the string length count initially set to 0    Int aDiv = 0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // the string set initially to 0    &nbsp;  If entVal.length &gt; 4 then    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; set&nbsp; aVal = entVal;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // set the entered value to aVal    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; set aCount = entVal.length+1;&nbsp;&nbsp; // set the string length count to aCount+1    Else    Echo “sorry not enough numbers, you must enter 5 or more”;    End;    Endif    &nbsp;    If divVal &gt; 0 then&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //make sure a value is entered as the division    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; set aDiv = divVal;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // set the entered division value to aDiv&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;     Else    Echo “sorry you need to enter a division value”;    End;    Endif    )    Part 2    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; To strip out the possible sub strings we loop through each string up to the string count and then test to see if divisible by the divVal that was entered.    Procedure stripString(aVal,aCount,divVal)(    Define divisions[] as array;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // set an array to hold each of the substrings    Define tempStr as string&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // temporary string    Define X = int;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;     Define Y = int;    Set X = 0;    Set Y = 0;    While&nbsp; (X &lt; aCount)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //&nbsp; loop through each iteration from x to aCount    Do (    Loop until Y = aCount(&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // internal loop for internal Y position    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Set tempStr = aVal.SubString(X,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Y)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // set the temp string to start X and pos Y&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; If (tempStr is divisible by divVal )&nbsp; // check to see if value is divisible    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Then    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Set divisions[]&nbsp;&nbsp;&nbsp; = tempStr;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // save to array    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Else    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;   End if  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Y = Y + 1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // increase Y by 1 each loop    )      X = X +1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // increase X by 1 each loop    )    &nbsp;    Echo divisions[]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // show the final string    )    The above procedure stripString() simply takes the start and end position of the substring command in turn and then tests the substring to see if it divisible before storing it in the array. This is advantageous to do it this way as it is quicker and less I/O needed instead of saving all of the possible substrings first then checking each substring and then either saving again or displaying the values. If it is done in one go, only the result needed is saved.    This would loop through as follows if the entered value was “37540”    3, 37, 375, 3754, 37540, 7, 75, 754, 7540, 5, 54, 540, 4, 40, 0      And the divisible numbers saved to the array if the divisible number was “3” would be in order:    3, 375, 75, 54, 540, 0    &nbsp;    Conclusion    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; This whole algorithm can be more simplified in conclusion as seen below.       I prefer to show pseudo code as a flow diagram where possible as I like to visually see the data/process flow. I have used this technique often in the past when building algorithms of my own.    &nbsp;    &nbsp;  References:    Brookshear, G. (2011) ‘Computer science: An overview’. 11th ed. Boston: Addison Wesley/Pearson.    &nbsp;&nbsp;&nbsp; Chapter 5, ‘Algorithms,’ apart from the discussion on 'Efficiency and Correctness'  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Section 5.1, ‘The Concept of an Algorithm’  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Section 5.2, ‘Algorithm Representation’  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Section 5.3, ‘Algorithm Discovery’  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Section 5.4, ‘Iterative Structures’  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Section 5.5, ‘Recursive Structures’     Calpoly.edu (2014) ‘Pseudocode Standard’ [http://users.csc.calpoly.edu/~jdalbey/SWE/pdl_std.html] Accessed: 18/10/2014.    Laureate Education, (2014) Algorithms: Week 7 Lecture Notes [Video, Online], (accessed: 19/10/14)    The Scratch project at MIT (n.d.) Scratch: Imagine, program, share [Online]. Available from: http://scratch.mit.edu (Accessed: 18/10/2014).    UNF.edu, (2014) ‘Pseudocode Examples’ [http://www.unf.edu/~broggio/cop2221/2221pseu.htm] Accessed: 18/10/2014.    Wikipedia, (2014) ‘Pseudocode’ [http://en.wikipedia.org/wiki/Pseudocode] Accessed: 18/10/2014.
	              
	              DESIGNING AN ALGORITHM(UPDATED)
I choose Option 1
String numbers =”37540”
Let&nbsp; N = Length (numbers)
While (i &lt; N)
Begin
P[ i ] - -
Let&nbsp;&nbsp; J =(i mod 2) *P[ i ]
Substring = swap (numbers [ j ], numbers [ i ]
If (substring mod 3 = = 0) {print substring}
Let i = 1
While&nbsp; ( P [i] = 0 )
Begin
Let&nbsp; P [i] = 1
i + +
End while
End while
End

	              
	              Designing an algorithm&nbsp;

Choice

Option 1: Numbers. Your algorithm should: Search a string of at least five numbers (for example, 37540). Identify all of the substrings that form numbers that are divisible by 3.

For example, applying the algorithm on the string 37540 should produce the following substrings (not necessarily in this order): 0; 3; 75; 54; 375; 540.

Method

What I need to achieve, in a generic form, is: For all substrings of a Y numeric string of any length, if the substring is divisible by X, show the substring.

There are two problems to resolve: first is finding all the substrings of a string, second is finding whenever a string is divisible by X. For the second problem it exists already a well-known arithmetic operation that determines if a number is divisible by another: Modulo operation, in which the result of the operation is the remainder after one integer is divided by another (Daintith, Wright 2008b). Modulo operation can be found in any scientific pocket calculator, normally abbreviated “Mod”. Mod works as follows: 7 mod 5 = 2. Meaning that there is a remainder of 2 in 7÷5. When there is no remainder like in 75 mod 3 = 0, then we can deduct that 75 is divisible by 3. In fact, we can generalize this as: if Y mod X = 0, then Y is divisible by X.

The other problem to resolve is finding all the substrings of a string. For the sake of fully understanding the problem, I must first point out that a substring is a string inside S that occurs within S. For example a substring of 37540 is 37540 (754), while 37540 (354) is not a substring of 37540 (nor is 453) (Wikipedia. 2014).

I will take 37540 as an example to find its substrings. First, I need to create an index (NIndex) and position this index at the beginning of the string: NIndex ← 0. Having NIndex at the first integer of the string, the next logical substring is the NIndex+1 value appended to NIndex value (fist integer and second integer), and so on until I reach the end of the string. This way, for 37540, the first substring is 3, the second is 37 (7 appended to 3) and the last is the string itself (37540). Figure 1 is graphical representation of this logic.




Figure 1. Finding substrings in a string with an index.

As seen in Figure 1, this will only give me one set of substrings. How can I find the substring 54? If I look closely, all I need to do is move the index rightwards and repeat the whole process again of appending the NIndex values. Therefore, to find 54 (as shown in Figure 2), I do NIndex ← NIndex+2 (that will put my index at the number 5 in the 37540 string and then find the next logical substring: NIndex+1 value (4) appended to NIndex value (5), which will be 54. Thus, if I keep changing index for every time I reach the end of the string while the NIndex is less than the length of the string, then I will have all possible Substrings.



Figure 2. Moving the index to find other substrings.
Let me put this in pseudo code, so it is easier to understand:





NIndex ← 0 while (NIndex ≤ stringOfNumbers length)  &nbsp;&nbsp;&nbsp; do ( &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; get NIndex value &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; append NIndex value to existing NIndex value &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; display NIndex value &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; )
&nbsp;





This will be my logic backbone to create the final algorithm. I will need to execute this logic every time the index is changed, so I need a procedure with parameters (such as the current NIndex) that I can use several times. I also need to properly define “NIndex value”, which is an arranged collection of the integers in our string (in our example: 3,7,5,4,0). Such collections are called Arrays (Daintith, Wright 2008a). Hence I will call the collection of ““NIndex values” MyArray[], where MyArray[0] is the NIndex0 value (in our example: 3) and MyArray[4] is the NIndex4 value and last integer (in our example: 0).

Note that I am starting the NIndex at 0, while the String has 5 elements, which one would normally count as 1 through 5, but because 0 through 4 contains five elements and NIndex0 is the first element, then NIndex4 is the fifth and last element of the string 37540.

Let me, then, write a more complete pseudo code with a procedure, when called this procedure will display the substrings, if called with different index positions, it will eventually return all the substrings:





procedure FindDivisiblesSubstrings (NIndex, MaxLength, MyArray, DivisibleBy) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //set a IndexNext variable and add 1 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; IndexNext ← NIndex + 1 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; If (IndexNext &gt; MaxLength) Then &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //this means we reached (at the last iteration) the end of the string. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (Exit the procedure) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; end if &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //create a temporary string where to append the substrings. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; TempString ← MyArray (NIndex) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //maxLength-1 is important as I will be passing the length as 5, but the index start as 0. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; while (a ← IndexNext ≤ MaxLength-1)  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; do ( &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; append MyArray (a) to TempString &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; display TempString &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; ) end procedure //for easier reading, adding end procedure
&nbsp;





And now I can implement the Modulo operation, this procedure will then display only the substrings that are divisible by the DivisibleBy value:





procedure FindDivisiblesSubstrings (NIndex, MaxLength, MyArray, DivisibleBy) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //set a IndexNext variable and add 1 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; IndexNext ← NIndex + 1 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; If (IndexNext &gt; MaxLength) Then &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //this means we reached (at the last iteration) the end of the string. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (Exit the procedure) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; end if &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //create a temporary string where to append the substrings. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; TempString ← MyArray (NIndex) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //maxLength-1 is important as I will be passing the length as 5, but the index start as 0. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; while (a ← IndexNext ≤ MaxLength-1)  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; do ( &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; append MyArray (a) to TempString &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; If (TemporaryString Mod DivisibleBy = 0) Then (display TemporaryString) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; ) end procedure //for easier reading, adding end procedure





Result
At this point I am ready to write the complete pseudo code that will call this procedure to produce the list of substrings that are divisible by the Divisor variable. With this generic code I merely need to change the value of StringOfNumbers and the Divisor to find the answer the goal: “For all substrings of a Y numeric string of any length, if the substring is divisible by X, show the substring”.

The final algorithm is (with the example of 37540 and divisor 3):





procedure Main StringOfNumbers ← 37540 MyCounter ← 0 Divisor ← 3
//go through each integer in the StringOfNumbers and save each one as part of the array MyIntegerArray in the position MyCounter. Because the counter starts at 0 and ends with the last integer inside StringOfNumbers, the counter maximum value is always the length of the string. for each MyInteger in StringOfNumbers do ( &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; MyIntegerArray[MyCounter] ← MyInteger &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;MyCounter ← MyCounter + 1 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ) end For
//Set the Index to 0. MyIndex ← 0 repeat (
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //Execute the FindDivisiblesSubstrings procedure and move the index by 1 increment.&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;FindDivisiblesSubstrings(MyIndex, Length of StringOfNumbers, MyIntegerArray[],Divisor)&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;MyIndex&nbsp; ← MyIndex&nbsp; + 1)until (MyIndex = Length of StringOfNumbers)end procedure
procedure FindDivisiblesSubstrings (NIndex, MaxLength, MyArray[], DivisibleBy)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //set a IndexNext variable and add 1 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; IndexNext ← NIndex + 1 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; If (IndexNext &gt; MaxLength) Then &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //this means we reached (at the last iteration) the end of the string. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (Exit the procedure) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; end if &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //create a temporary string where to append the substrings. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; TempString ← MyArray[NIndex] &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //maxLength-1 is important as I will be passing the length as 5, but the index start as 0. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; while (a ← IndexNext ≤ MaxLength-1)  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; do ( &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; append MyArray[a] to TempString
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //evaluate divisibility by DivisibleBy, If divisible display the number &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; If (TempString Mod DivisibleBy = 0) Then (display TempString) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; ) end procedure





&nbsp;

Testing

To test the algorithm, since I know a little of VB.NET I created a quick console program. It is a little more complex because if the VB.NET syntax and to allow for user input and better console display, but it is basically a direct implementation of my final algorithm. Testing proved the algorithm to be correct, delivering results for 37540 but also for any other number of strings.

For example it delivered these correct results:

***** RESULTS *****

List of substrings in 7852140 divisible by 3: 0, 21, 5214, 52140, 78, 785214, 7852140, 852

*******************

I have attached (to the discussion forum) the VB.NET executable and source code for anyone that want to run the little program and have fun for it.

Attention: there is no error control in the program so using strings and divisors other than positive numbers may result in a crash of the program.

Attention: the attached program is limited (on purpose) to accept strings of Integers from 0 to 4,294,967,295 (32-bit integer or Interger32).

Further Considerations

While this algorithm solves the requested problem, there must be more elegant ways to resolve it. The major disadvantage of this algorithm is that when the string contains a big amount of integers (i.e. one billion integers) it will be very slow to process, because it goes and search every single occurrence of the substrings. Given enough time it will be fun to find an algorithm that performs better.

&nbsp;

References

Daintith, J. &amp; Wright, E. (2008a) 'Array' in Dictionary of Computing, 6th edn, Oxford University Press, Oxford Reference, .

Daintith, J. &amp; Wright, E. (2008b) 'Modulo Operation' in Dictionary of Computing, 6th edn, Oxford University Press, Oxford Reference, .

Wikipedia. (2014) Substring. Available at: http://en.wikipedia.org/wiki/Substring (Accessed: 18 October 2014).

&nbsp;
FindDivisibleSubstrings.zip&nbsp;
	              
	              Algorithm qualities
For an algorithm to function, it must have at least one input, and it must produce and output. It has to have instructions ordered so that we can follow a specific procedure. For example in 3 – 2 we need to first take the number 3 and then subtract the number 2, without a specific order we could end up doing 2 – 3. An algorithm also needs to be repeatable to be useful, inventing a onetime algorithm to make pizza will mean that only some lucky ones will eat a pizza and nobody else will ever again. Algorithms also need to be able to be described in different ways, consider there are many languages in the world and many ways to represent things. One need to be able to translate a particular algorithm into a specific language of choice, be it a computer programming language, a human communication language or a specific set of figures such a workflow.
Bookshear (2011) defines an algorithm as “an ordered set of unambiguous, executable steps that defines a terminating process” (Bookshear 2011). For the computing world the unambiguous steps are crucial as this means that at each step the information contained in that step is the only thing required to continue to the next step. An algorithm also must be correct: it must solve the given problem (Cormen et al. 2009). Of note is that there are algorithms that may not end: &nbsp;an algorithm to calculate the square root of 2, being an irrational number, will never finish execution.
I say that an algorithm should also be efficient: a problem might have multiple ways to solve it. An efficient algorithm should be the most efficient one of those solutions. This is not as easy to solve as it seems: efficiency depends on our knowledge/understanding of the problem to solve and some problems still need to be resolved. Consider, for example, the multiplication of two integers: whatever the method used multiplying 2 by 3 will always be executed faster than multiplying 62’341 by 2’376’124. So what is the fastest algorithm for multiplication of two n-digit numbers? This problem is still an unsolved one in computer science (Robinson 2005).
A strong algorithm is efficient, while a poor one is less efficient. To better understand the efficiency of algorithms, I looked at one that we use all the time: an elevator. This example is relevant to computing because most of the elevators that we find in today’s buildings are electronic, hence are managed by software. As you can imagine this can be solved in many ways. Let’s take a look at how it was solved at my own, very old, apartment building (I tested it with the help of friends):




procedure Elevator ()
if (elevator is called or destination chosen) then
&nbsp;&nbsp;&nbsp; while (elevator is moving
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; until destination is reached)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; do (move towards destination);
else
&nbsp;&nbsp;&nbsp; (wait)
end if
Call Elevator()
end procedure




&nbsp;
Consider the following facts: my building has seven floors, the elevator takes 23.85 seconds to go from the ground floor (0) to the seventh floor. In the worst case scenario, someone at the ground floor would want to take the elevator to go to the seventh floor when the elevator is at the first floor, and its destination is the seventh floor. In this case the elevator will need 20.86 seconds to reach its destination, then another 23.85 seconds to descend to the ground floor to pick up the new traveler, who will need to wait 44.71 seconds to take the elevator. This assumes that nobody will press the call button before the person in the ground floor. Perhaps almost 45 seconds is not a lot of time (let me assure you, it can be annoying); but let’s consider setting up this same elevator in a building with 30 floors. Not accounting for physics or dynamics, in this case the time from the bottom to the top will be 92.41 seconds, and the worst case scenario will have a person waiting for 3.02 minutes. That is an unbearable waiting time: this is a poor algorithm. Figure 1 shows the comparison of the 2 worst case scenarios:



Figure 1. Elevator “do not stop” worst case scenarios.
One could argue that this algorithm is efficient enough to use up to a certain number of floors, or that it is suitable for private buildings. But would this be efficient in a commercial building such a department store where people are using the elevator all day? For a department store a better algorithm, albeit not necessarily the most efficient one, would be something like this:&nbsp;




procedure ElevatorImproved ()
if (elevator is called or destination chosen) then
&nbsp;&nbsp;&nbsp;&nbsp; (move towards destination direction) &nbsp;&nbsp;&nbsp;&nbsp; while (travel in the same direction
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; until remaining requests in same direction are 0)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; do (stop at each request, subtract one from the requests in that direction);
else
&nbsp;&nbsp;&nbsp; (wait)
end if
Call ElevatorImproved ()
end procedure




Let’s remember the goal of algorithms is to find the best algorithm for a given problem. So there must be a most efficient elevator algorithm. The scope of this post is not to necessarily cover the most efficient one, but this example demonstrates how important and complex algorithm efficiency is.
References
Bookshear, J. G. (2011) '5 Algorithms' in Computer Science: An Overview, 11th edn, Addison-Wesley, pp. 187-232.
Cormen, H. T., Leiserson, E.C., Rivest, L.R. and Stein, C. (2009) '1. The Role of Algorithms in Computing' in Introduction to Algorithms, 3rd edn, The MIT Press, pp. 4-14.
Robinson, S. 2005, "Toward an optimal algorithm for matrix multiplication", SIAM news, 38(9), pp. 1-3.
&nbsp;
&nbsp;

	              
	              Hi all,    Algorithm Definition  &nbsp;&nbsp;&nbsp;&nbsp; A logical sequence of steps for solving a problem. The steps (each should be logical) should lead to a an eventual solution.    What is the criteria for a Good Algorithm?    The Algorithm needs to work be correct   The algorithm needs to execute in a reasonable amount of time   The Algorithm should be relatively simple   The Algorithm should take into account all input and cases   The Algorithm should have appropriate level of Abstraction ie being able to view the algorithm as a series of high level aggregate steps with details in lover level steps   The Algorithm should be precise    Two algorithms solving the same problem can have completely different methods, to identify the most efficient algorithm we need to find out which one is written the ‘best’.&nbsp; (BLUECC)    How much time it will take to execute the Algorithm   What is the storage/Space requirement to execute the Algorithm   The development of the Algorithm (to develop, maintain/support the Algorithm)    People do utilize neat and clever tricks to improve the space and time requirements but it could increase the human supportability of the Algorithm.  One of the important factor in Algorithm are their execution time, we are going to focus on methods to determining their speed. How does one know if an &nbsp;Algorithm is better than another. Two methods:  Method 1 – Develop and program and run the Algorithm  &nbsp;&nbsp; Develop a program to test the speed execution of an Algorithm, however there are constraints with this approach by the overhead ,&nbsp; different coding might affect the Algorithm speed, speed of a computer or test data not being able to test for all the possible permutations&nbsp;  Method 2 – Use a Mathematical analysis  &nbsp;&nbsp; Mathematically measure the running time of the Algorithm by using “time units”, some typical time units that would be considered are    Comparisons   Assignments   Input/output operations   Numerical operations    The more accurate measurement is method 2 using mathematical analysis in particular the Assignments and/or comparisons units will be used  Worst Case, Average Case, Best Case&nbsp;are way to determine Algorithmic efficiencies (SPATIAL)    Some algorithms depend only on size of data set   Other algorithms depend on nature of the data      Consider: Search for an element in a list   Best case search when item at beginning ie the minimum &nbsp;amount of time that an algorithm require to solve a problem of size n   Worst case when item at end ie the maximum amount of time that an algorithm require to solve a problem of size n   average case somewhere between ie the average amount of time that am algorithm require to solve a problem of size n         Reference  BLUECC URL http://cs.bluecc.edu/java/CS260/Notes/Efficiency.htm   SPATIAL URL http://www.spatial.cs.umn.edu/Courses/Spring12/1902/notes/Chapter04-EffeciencyOfAlgorithms.pdf   http://www.slideshare.net/fullscreen/jrcampos/03-algorithm-properties/1 &nbsp; Robin 
	              
	              DQ week 8 : Analyses of algorithms
Bram Tullemans
An algorithm is a finite sequence of logical or mathematical instructions for a solution of well defined problem. For an algorithm to work it has to terminate after a finite number of steps (finiteness), &nbsp;ambiguity has to be absent (definiteness), the information to begin needs to be specified (input), the results also (output) and the resolution of any problem instance has to be guaranteed (effectiveness).
Ambiguity arises in algorithms when these factors are not well described. For example if a process is not terminated well, and the algorithm does not tell us what to do at such moment, an ambiguous situation arises that can lead to problems. An example of ambiguity is this calculation x ← 2 + 8 * 7. As it is unclear if first 2+8 has to be calculated and the result multiplied by 7 or 7 has to be multiplied by 8 before 2 is added is unclear.
Run time evaluation is the main quality evaluation of an algorithm: The algorithm is improved on a particular input if the amount of primitive operations or 'steps' are reduced. &nbsp;An improvement is often relative to the application and can be different for alternative tasks.
For example string search optimalisation both the Knuth-Morris-Prath as Boyer-Moore algorithm are ideal for single pattern searches. The Boyer-Moore algorithm is better for search patters that have a repetition in itself as it searches for these repeating patterns and skips those searched in the main string while the Knuth-Morris-Prath algorithm checks every element of that string. Both are optimal for single word searches but for multiple word assignments the algorithm can be improved by using&nbsp; hashing for shifting substring search as worked out in the Rabin-Karp algorithm. In this method the hash value of the words are determined and look for these pattern values in the total word string. The hash values have to be created only ones and can be applied to the whole string instead of looking at every letter in a string every time (or skip if the search word has repeated pattern). This is more 'work' in the first run but improves the overall run time when different patterns have to be searched.
A verification evaluation is a checking procedure to find out if the algorithm creates the right outputs by following them step by step. The analysing of insertion sort looks at the input as the input size and form depends on the problem being studied. The verification evaluation is also a completeness check to see if all output is generated. Note that there are different way to do this for example using an average input or by doing a probabilistic analysis (randomised input). The rate of growth in a run time evaluation is interesting not the fact that there is an impact on the runtime. 
In evaluating run times there one has to assume a model of implementation to analyse different candidates of algorithms. For example by envisaging a Random Access Machine that executes instruction one by one and no concurrent processes occur.
With a lower run time the code is more efficient and besides the improving impact this has on the hardware the algorithm has also 'softer advantages' as being more aesthetic preferable&nbsp; (not to be underestimated) and the transparency of the code is improved.
Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest and Clifford Stein (2009). &nbsp;Introduction to Algorithms [OnlineI MIT press, 3rd Edition. Available from: http://ldc.usb.ve/~xiomara/ci2525/ALG_3rd.pdf &nbsp;(Accessed on: 19-Oct-2014). p23 - 29
Brookshear, G. (2012), Computer Science: An Overview, Pierson Education Inc. publishing as Addison-Wesley, Boston, United States, Version 11. ISBN 13: 978-0-13-256903-3. pp 222-238.
&nbsp;
Simha (2014) Course Materials; Module 5, Pattern search [Online]. The George Washington University. Available from http://www.seas.gwu.edu/~simhaweb/alg/lectures/module5/module5.html (Accessed on 22 October 2014).
Wikipedia. (2014). Boyer-Moore string search algorthm [Online]. Wikipedia Foundation Inc.. Available from: http://en.wikipedia.org/wiki/Boyer%E2%80%93Moore_string_search_algorithm. (Last accessed 26 October 2014).
Wikipedia. (2014). Rabin-Karp algorithm [Online]. Wikipedia Foundation Inc.. Available: http://en.wikipedia.org/wiki/Rabin%E2%80%93Karp_algorithm. (Last accessed 26 October 2014).
&nbsp;
&nbsp;

	              
	                  I recall FORTRAN 77, being a great language for doing mathematical and scientific calculations. Fortran short for (FORmula TRANslation) was a programming language designed specifically for scientists aand engineers. In the recent years, i have heard&nbsp;&nbsp;MatLab to be a good language for preforming statistical/scientific calculations., Matlab is&nbsp;designed for extensive numerical linear algebra and vectorized operations   Robin     Reference http://www.quantstart.com/articles/Best-Programming-Language-for-Algorithmic-Trading-Systems
	              
	              Week 8 Discussion Question – Algorithm Qualities
Craig Thomas – 26 October 2014
A Simple Explanation:
According to Brookshear (2012, pg. 10) “computer science is a science of algorithms”. In other words, the operation of computers relies on the successful processing of millions of algorithmic instructions.&nbsp; 
Harel and Feldman (2004, pg. 5) assert that “algorithmics is more than a branch of computer science…it is the core of computer science”
It is therefore reasonable to suggest that algorithms play a key and central role in computer science, performing many of these functions at incredibly fast speeds. 
The way the algorithm is constructed determines the efficiency and effectiveness of the process itself, so the algorithm properties are critical in the performance of the algorithm, and as such the resulting performance of the computer. This difference in time could range from milliseconds through to minutes, and whilst not necessarily noticeable by the human eye, this could be the difference between a good or strong algorithm, and a poor or weak algorithm.
As computers have evolved and become faster over time, you could assume that this becomes an inconsequence. However, you would be incorrect in that assumption. The simple reason is that in parallel with the evolution of the computer, the use and need of the computer has become even more extensive, in performing the functions that quite simply, are impossible for humans to perform manually at the speeds required. 
There are many processes that are running on a computer, some run in the background to support the user interface, or to manage file store, and some, which are directly handling the processing of applications and user instructions. All of these have some bearing on the processing within the computer CPU, so a poor or weak algorithm could have noticeable impacts on the overall performance. Equally, a good or strong algorithm can provide significant advantages.
Algorithm Characteristics
According to Cormen et al (2009) “an algorithm is any well-defined computational procedure that takes some value, or set of values, as input and produces some value, or set of values as output”. Thus, a sequence of computations that transform the input into the output.
Through many years of extensive thinking and research in computer science and the development of Algorithms, there are many forms of algorithm characterisations – there are too many to outline in this discussion paper, however typically there are two kinds of algorithm constructs, these are iterative and recursive. Iterative algorithms iterate through the instructions performing it repeatedly, until it reaches a solution. A recursive treats each task as a sub task of the previous task, and the process is invoked repeatedly to resolve the sub tasks. It is understood that these two are equivalent in computing power (Laureate, 2014).
Algorithm Efficiency and Qualities
There is a need for the algorithm to be efficient – as I have outlined in my simple explanation, computer power is not an unlimited resource therefore inefficient algorithms will have a negative bearing on the overall cost of the architecture. Thus, there is a need to maximise the efficiency of all algorithms based on the architecture being employed. 
Also, computer architecture plays a key role. As an example, in RAM architected technology, it is suggested by Cormen et al (2009) that the computer executes the process sequentially, with no concurrency employed. These are also commonly referred as serial algorithms, and typically run on single processor machines.
There are also algorithms, which are designed for multiprocessor computers, and provide instructions, which execute concurrently.
Whichever platform, there is a suggestion, that the instructions still need to be precisely defined. In most of the reading I have covered, it suggests that there needs to be a conclusion, or a finite ending to the algorithm.
It is also appropriate to use instructions that are commonly defined within a computer system such as addition, subtraction, multiplication etc, as well as data storage and controls, such as branch routines and subroutines. 
The use of these will introduce efficiencies, as the computer already has these predefined and each instruction takes a finite time for execution (Cormen et al, 2009, pg. 24). It is important however to ensure you minimise the number of calls where necessary to other routines or other parts of the computer architecture, such as CPU or memory, and perhaps use information that has already been called or is in use already.&nbsp; 
In addition to these examples for efficiency objectives, precision is also of great importance. The algorithm needs to be specific in its instruction, with no ambiguous statements or assumptions made. 
The algorithm should follow a particular flow, in that it identifies and understands the problem, works through and carries out actions to resolve the problem, and presents an answer to the problem. Input to output.
To maintain efficiency, algorithms could also look to adopt a recursive structure, in that they repeat or loop to handle identical, or similarly identical instructions. 
Rather than those instructions being written multiple times, the algorithm is able to apply a count, and execute an instruction in accordance with a count parameter, or loop until another condition is reached. However, if this condition can be pre determined, it may be more appropriate to specify this condition, rather than wait for it to occur. See my example later on, which help explain my thinking here.
According to Cormen et al (2009), recursive algorithms typically follow a divide and conquer approach, breaking down the problem into sub-problems and the re-execute the process to deal with each of those sub-problems. 
The last element I would look to highlight is error handling. An algorithm needs to know what to do in the event of an error, so an efficient and effective way of dealing with this is necessary, otherwise the algorithm could introduce delays or ultimately continually loop if not handled appropriately.
In terms of examples, let me just show the difference between a perceived good strong algorithm, and a perceived poor weak one. If we take boiling pasta, and lets just assume it takes 6 minutes for nice al dente fresh egg pasta.
So, a strong an efficient algorithm could look like:

Fill Pan with Water, add salt
Bring Water to Boil
Place Pasta in Boiling Water
Simmer for 6 minutes
Drain Pasta
Eat

It is specific, precise and carries out the instructions whilst also freeing up resources whilst it is simmering for 6 minutes. This has a pre-determined condition of 6 minutes.
An inefficient algorithm could be 

Fill Pan with Water, add salt
Bring Water to Boil
Place Pasta in Boiling Water
Simmer 
Check after 1 minute and if not cooked, continue to simmer
Repeat 5 until al dente, then go to 7
Drain Pasta
Eat

This introduces a nice loop however because it is not specific, it requires checking every minute until it is al dente. This requires system resources to perform the checking every minute, as it does not have a pre determined condition set, and so could be deemed inefficient.
Best Wishes, Craig
References
Brookshear, J. G. (2012). Computer Science An Overview. 11th ed. Boston Massachusetts: Pearson. Addison Wesley
Laureate Online Education. (2014). Computer Structures – Lecture Notes Week 8: Algorithms [Online]. Available from: https://elearning.uol.ohecampus.com/bbcswebdav/institution/UKL1/201480_SEPTEMBER/MS_CKIT/CKIT_501/readings/UKL1_CKIT_501_Week07_LectureNotes.pdf (Accessed 25 October 2014)
Laureate Online Education. (2014). Computer Structures – Lecture Notes Week 8: Analysis of algorithms [Online]. Available from: https://elearning.uol.ohecampus.com/bbcswebdav/institution/UKL1/201480_SEPTEMBER/MS_CKIT/CKIT_501/readings/UKL1_CKIT_501_Week08_LectureNotes.pdf (Accessed 24 October 2014)
Cormen, T. H., Leiserson, C. E., Rivest, R. L., Stein, C. (2009). Introduction to Algorithms. 3rd ed. Massachusetts Institute of Technology: The MIT Press. Cambridge. Massachusetts. London, England
Harel, D. and Feldman, Y. (2004). Algorithmics – The Spirit of Computing. 3rd ed. Pearson Education Limited. Biddles Ltd. Guildford and King’s Lynn. Great Britain
	              
	              Hi Belinda,
But did you figure out the giraffe algorithm? :-)
The statement "Make it work, make it right, make it fast" is attributed to Kent Beck, a friend of mine (on Facebook). He also coined the "4 rules of simple design", very insightful.
br, Terry
http://c2.com/cgi/wiki?MakeItWorkMakeItRightMakeItFast

	              
	              Hi Martins,
What is i and P? I think you've used them before gave them any meaning.
br, Terry
	              
	              Re-posting of original week 7 pseudocode
The&nbsp; algorithim below searches a string of at least five numbers &nbsp;and identifies all of the substrings that form numbers that are divisible by 3. Lines that begin with the apostrophe are comments
&nbsp;
‘ Start by declaring all the required variables to be used in the algorithm
&nbsp;
‘Holds the input number
Dim MyStr As String &nbsp;&nbsp;&nbsp;
&nbsp;
‘Stores the numbers to be evaluated for divisibility by 3
Dim MyNum As Long
&nbsp;
‘Stores the final output
Dim MyResult As String
&nbsp;
‘ Variables to be used for the loop
Dim i As Integer
Dim j As Integer
&nbsp;
&nbsp; ‘Accept a 5 digit number
&nbsp;&nbsp;&nbsp; MyStr = Inputbox(‘Enter&nbsp; a number with at least 5 digits’)
&nbsp;
&nbsp;&nbsp;&nbsp; ‘ Check whether value entered is numeric
&nbsp;&nbsp; If&nbsp; not&nbsp; isnumeric(MyStr) then
&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;Msgbox “Invalid Number”
&nbsp;&nbsp;&nbsp;&nbsp; Exit sub
Else
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ‘ Check whether value entered has at least 5 digits
If len(trim(mystr)) &lt;5 then
&nbsp;&nbsp; MsgBox “Invalid number”
End if
&nbsp; End If
&nbsp;&nbsp;&nbsp;
&nbsp; ‘ Outer loop allows for iteration of each digit in the number
&nbsp;&nbsp;&nbsp; For i = 1 To Len(MyStr)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ‘ Inner loop allows for iteration of segments of numbers within the value entered
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; For j = i To Len(MyStr)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ‘ Extract number and compare whether divisible by 3
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; MyNum = Mid(MyStr, i, j)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; If MyNum Mod 3 = 0 Then
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ‘ For concatenation of the string result, stores the first number
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; If Len(Trim(MyResult)) = 0 Then
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; MyResult = Str(MyNum)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Else
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ‘ Add subsequent numbers obtained to result string and ignore any duplicate numbers
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; If InStr(1, MyResult, Str(MyNum)) = 0 Then
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; MyResult = MyResult + ", " + Str(MyNum)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; End If
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; End If
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; End If
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Next j
&nbsp;&nbsp;&nbsp; Next i
&nbsp;&nbsp;&nbsp;
&nbsp;‘Display result
&nbsp;&nbsp;&nbsp; MsgBox MyResult
&nbsp;
References
Brookshear, J. G. Computer Science: An Overview, 11th Edition. Pearson Learning Solutions.&nbsp; 
Thayer, R. Visual Basic 6 Unleashed– September 11, 1998, SAMS
&nbsp;
&nbsp;

	              
	              Algorithm Qualities
&nbsp;
Introduction
&nbsp;
What Is An Algorithm?
&nbsp;
An algorithm is a well defined series of steps or instructions which commands the computer to solve problems. With the help of algorithms, the computer performs its operations in a systematic manner (Kharavela, Oct 2014).
&nbsp;
An efficient algorithm should possess the following characteristics (Campos, 2013) :


Simple - it should relate to the problem and resolve the problem unambiguously


Complete - it should refer and account all the scenarios


Correct - it should produce the desired output / results


Abstraction - it should have appropriate levels of abstraction


Precise - it should be accurate, most importantly!


&nbsp;
Classification
It is very important to know how much of a particular resource time or storage is theoretically required for a given algorithm. These requirement defines algorithms performance efficiency.
Algorithms can be implemented based on various criteria (Wikipedia, Data structure - Algorithms) :
&nbsp;


Recursion or iteration


A recursive algorithm which makes a self reference till a given condition is met. These iterative methods use programming constructs like looping or stack type of processing mechanism.
&nbsp;


Logical


Algorithm is comprised of logic and control. The control is applied on mathematical solution or logic which resolve the problem.
&nbsp;


Serial, parallel or distributed


Algorithms usually written in such a way that it will process the instructions one at a time, these are referred as serial algorithms. The parallel algorithms performs multiple tasks at the same time with the help of computer architecture and to provide an efficient and better results. The distributed algorithms utilize the systems connected over a network and share the process and obtain the results asynchronously to provide the final output.
&nbsp;


Deterministic or nondeterministic


These algorithms resolve the problems accurately. The deterministic algorithms provide the exact results taking the accurate decisions. Nondeterministic algorithms will provide the accurate results if not exact, by making the appropriate guesses.
&nbsp;


Exact or approximate


By using deterministic or random strategy, approximate algorithms reaches close to the true solution. This is considered to be a practical algorithm for extremely hard problems.
&nbsp;


Quantum algorithm


Quantum algorithm works based on the principles of quantum computation and used for some essential feature of quantum computation such as quantum superposition.
&nbsp;
Conclusion
&nbsp;
Algorithms are essential to process or resolve any problems which occurs in a &nbsp;computer system or in our day today activities. These algorithms help in achieving the resolution by breaking the problems into multiple steps and rules. Algorithms can be expressed in many variations such as simple english, pseudocode and/or flow-charts. It is depending on the person and the type of a problem for which we need to describe an algorithm and there is no fixed set of rules to define an algorithm to obtain a desired result.
&nbsp;
References
&nbsp;
Kharavela, Oct 2014 - https://api.turnitin.com/dv?o=467904638&amp;u=1030977172&amp;s=&amp;student_user=1&amp;lang=en_us&amp;session-id=3eb5ffedab5c002479548a82fc2eda2d
&nbsp;
Campos, 2013 - http://www.slideshare.net/jrcampos/03-algorithm-properties
&nbsp;
Wikipedia, Data structure &nbsp;- http://careerride.com/Data-structure-algorithm-and-its-types.aspx
&nbsp;
Best regards,Kharavela

	              
	              
Week 8 Initial Post – Algorithm

I. Introduction
Algorithms are all around us, to perform any task a series of steps are involved, these steps can be considered an algorithm. Without even knowing it we use forms of algorithms to do everything that we do today. If we were to prepare a simple meal, the recipe for cooking that meal is actually an algorithm. That being said, an algorithm is basically a series of instructions for performing a given task or for solving a problem. The 
This paper aims to examine the properties of an algorithm, the ambiguity involved in algorithm specifications and algorithm qualities; examining a few good and bad algorithms.

II. Properties of an algorithms
There are billions of algorithms in the world today, some very simple and some very complex. However each algorithm must have a series of basic properties that makes them an algorithm. These properties are listed below.
 A. Time bounded Algorithms are expected to have a beginning and an end, thus they cannot be set to run for an infinite amount of time. This is similar to the actions that we perform on a daily basis, if you were to bake a cake would the cake be in the oven for an infinite period of time, if so the gas will eventually finish and the cake will burn out. Thus not leading to the optimal output that we would desire. The cake has to go in the oven for a fixed time and after that the oven is turned off and the cake is considered finished baking.
 B. Clear levels of detail or unambiguous
 Algorithms are expected to have clear levels of details . Here as with the analogy of making a cake, to obtain the desired output depending on how much cake we are making we would need to add a specific amount of eggs, sugar, butter and all other ingredients. As simple as mixing the cake a series of steps may say to sir it for 30 times or stir until it is smooth with no lumps. It is the same with algorithms, they are expected to be detailed in order to achieve the desired outcome. Instructions are also expected to be unambiguous and understandable for its intended users or audience. 
 C. Ordered
 The series of steps in the algorithms are expected to be ordered. Note some algorithms are multi thread and may perform steps simultaneously while others based on defined conditions may skip certain steps and go to the others. 
 D. Possible or Executable
 Instructions involved in an algorithm must be possible or executable. For example an algorithm that instructs to list all the integers would not be possible since there are an infinite number of integers. They are also expected to be correct, that is being able to solve the problem it was developed to solve,

III. Ambiguity involved in algorithm specifications
Because of its abstract nature, algorithms are usually built suited for a specific condition and or hardware requirements. It is important to note that there is a difference between the actual algorithm and what the algorithm may represent. Moreover different algorithms can be written to represent the same problem, which may lead to issues of ambiguity. Take for instance A= (2+3) * 5 can be interpreted as A equals two plus three and the result multiply by five  or A equals five multiply by five

Issues of ambiguity usually arise because there may not be enough details in the algorithms or the details given may have varying interpretations depending on who may be reading it. Take for instance if a programmer is to read a specific set of instruction he may understand what it is doing but if a normal person is to do this they may not. An even simpler example is that of “bake the cake until it is brown”, there are different shades of brown, thus some persons may bake it browner than others. For this reason primitives were developed, “Computer science approaches these problems by establishing a well-defined set of building blocks from which algorithm representations can be constructed. Such a
building block is called a primitive.”, Brookshear, (2011).

IV. Algorithm qualities
A good algorithm is expected to meet all its specifications and or requirements. Thus being able to correctly solve a problem or provide a solution for what it was developed for and being  that unambiguous. However, while a algorithm may do this the time frame or the level of efficiency in which it can meet its requirements is the ultimately determines its quality. They are recursive type algorithms, which involve the repeating of instructions as subsets of other instructions and they are iterative structures repeat themselves in a looping manner. While repeating instruction algorithms must have the following:  A. Initialize: there must be some form of initialization or establishment of initial states,
 B. Test: there must be some form of test condition, which may compare the present state to that of a terminating state, and would terminate if the state is equal, and
 C. Modify: it must have a modivying condition that allows the changing of states and eventually the movement towards a terminating state.
 
 Example of a bad algorithm is:

While
traffic light is showing walk
Cross the road.


 Example of a better algorithm is:
 While 
traffic light is showing walk
look right
look left
If no vehicles are approaching you
 Cross the road.
 Else
 Wait 
 Exit
 


References:

Computer Structures – Lecture Notes, Week 8: Analysis of algorithms [Online]. Available at: 
https://elearning.uol.ohecampus.com/bbcswebdav/institution/UKL1/201480_SEPTEMBER/MS_CKIT/CKIT_501/readings/UKL1_CKIT_501_Week08_LectureNotes.pdf (Accessed on October 5, 2014)

Brookshear, J.G. (2011)&nbsp;Computer science: An overview.&nbsp;11th ed. pages 192, pages 188-230, Boston: Pearson Education / Addison-Wesley. pages 114)

Harel, D. &amp; Feldman, Y.A. (2004)&nbsp;Algorithmics: the spirit of computing.&nbsp;3rd ed. Pages 3-17, New York: Addison Wesley.


	              
	              Algorithm qualities
&nbsp;
Introduction:
An algorithm is an ordered set of unambiguous, executable steps that defines a terminating process (Brookshear 188).
Properties of algorithms
An algorithm must possess the following properties: (Knuth,D)

Finiteness: In essence an algorithm takes a form that has a beginning and an end. An algorithm that runs infinitely would end up exhausting the computers resources such as memory
Definiteness: The steps in an algorithm must be precisely defined and the actions to be carried out unambiguous
Input: An algorithm requires inputs from various sources that form the basis of operations being performed
Output: An algorithm generates outputs consistent with the inputs that are given to it. The axiom Garbage In Garbage Out as used in computer terms refers to the fact that computers, since they operate by logical processes, will unquestioningly process unintended, even nonsensical, input data ("garbage in") and produce undesired, often nonsensical, output ("garbage out"). (Wikipedia)
Effectiveness: Algorithm analysis often involves best case, worst case and average case scenarios. For an algorithm to be effective, consideration is given on its use of resources including time and storage.
All operations to be performed must be sufficiently basic that they can be done exactly and in finite length.

Ambiguity involved in algorithm specifications
An algorithm may be expressed in a number of ways, depending on the level of detail required. The more high level an algorithm is, the more likely the higher the level of ambiguity. The initial problem definition is important in setting the scope, function and use of an algorithm. If a problem definition is vague, an algorithm may not meet the desired objectives. This is one reason why emphasis is often placed on the requirements definition phase of any project so that the purpose and needs are well defined. A formal proof of a program’s correctness is based on the specifications under which the program was designed. Some of the ways in which an algorithm can be expressed include:

natural language: usually verbose and ambiguous
flow charts: avoid most (if not all) issues of ambiguity; difficult to modify w/o specialized tools; largely standardized
pseudo-code: also avoids most issues of ambiguity; vaguely resembles common elements of programming languages; no particular agreement on syntax
programming language: tend to require expressing low-level details that are not necessary for a high-level understanding

Determining an algorithms qualities
An algorithm’s qualities may be determined by how well it utilizes resources, its correctness and efficiency. Even though an algorithms performance and output may be compromised by flaws in computer hardware, any operation being performed on an algorithm including arithmetic computations, string comparisons or testing of logical operands must take place within a finite time and produce accurate results.&nbsp; Good algorithms distinguish between various techniques such as recursive or iterative structures, appropriate sorting and searching techniques to optimize the performance, compactness and simplicity of its operations. For example the graph below shows performance comparisons run on different sorting techniques to establish any significant time difference differences that are observable.


&nbsp;Figure 1 : Time Comparison of Quick Sort, Insertion Sort and Bubble Sort 
(Source: http://vinayakgarg.wordpress.com/2011/10/25/time-comparison-of-quick-sort-insertion-sort-and-bubble-sort/ )
Two of the popular search algorithms are linear search and binary search. While linear/sequential search scans each array element sequentially, a binary search starts with the middle of a sorted list, and determines whether the value in that position is greater than or less than the value you're looking for. A binary search is an example of a good algorithm that can be used to search from a sorted list. Once it is sorted, the best case performance is one comparison (the searched-for value is the mid-point value in the original array).&nbsp; The worst-case, where the value is not in the array, is log2N, where N is the size of the array. &nbsp;
(http://www.csit.parkland.edu/~mbrandyberry/CS1Java/Lessons/Lesson27/BinarySearchEfficiency.htm)
References

Brookshear, J. G. Computer Science: An Overview, 11th Edition. Pearson Learning Solutions. 

D. Knuth, ‘The Art of Computer Programming’, 1st &nbsp;edition (March 3, 2011), Addison-Wesley Professional; http://courses.cs.vt.edu/cs2104/Fall12/notes/T16_Algorithms.pdf

Wikipedia.&nbsp; Algorithmic efficiency. Wikipedia Foundation Inc.. Available from: http://en.wikipedia.org/wiki/Algorithmic_efficiency (Accessed 26 October 2014).
Garg, V. ‘Time Comparison of Quick Sort, Insertion Sort and Bubble Sort’ Word press, Available at http://vinayakgarg.wordpress.com/2011/10/25/time-comparison-of-quick-sort-insertion-sort-and-bubble-sort/ , Accessed 26/10/2014

&nbsp;
&nbsp;

	              
	              In order to analyse the properties of an algorithm, it can be broken down and the relative importance of the components explored. The following properties are essential to algorithm design.

Input &amp; output: An algorithm must have specific inputs and produce some output. At the beginning of the algorithm it must be clearly stated what type of input the will be expected. Similarly at the end of the algorithm one or more output/result must be presented in a particular format as the algorithm describes.
Defined: Each step of the an algorithm must define clearly without any confusion, ambiguity or contradiction (http://algorithmsandproblems.blogspot.com, 2013).
Finite: Each step of an algorithm must be executed precisely and in a timely manner, terminating completely. "A terminating process, which means that the execution of an algorithm must lead to an end" (Brookshear, 2011, p.189).
Structured: The steps in an algorithm must be ordered and well establish for their execution. this does not imply a sequence consisting of first step followed by second and so on (Brookshear, 2011, p.188).
Effective: All task carried out by the algorithm should execute achieving its objective and returning the decision taken by the operation; whether computational tasks or other types.

An algorithm is a finite set of instructions, that when followed with the properties outlined above will result the desired task being accomplished. These properties directly interrelate and provide the mechanism by which an algorithm operates.
&nbsp;
The ambiguity involved in an algorithm's specification would result in instructions that are not clear, unstructured and will not produce an effective, executable algorithm.
"The steps in an algorithm be unambiguous. This means that during execution of an algorithm, the information in the state of the process must be sufficient to determine uniquely and completely the actions required by each step. In other words, the execution of each step in an algorithm does not require creative skills. Rather, it requires only the ability to follow directions". 
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (Brookshear, 2011, p.189)
&nbsp;
&nbsp;
An algorithm's quality is determined by the structure that is used to present the information. It must be ordered and well establish, with clear objectives to be achieved precisely. The properties outlined above, when followed, will ensure a good quality algorithm.
To compare examples of a strong algorithm as appose to a poor algorithm.&nbsp; The iterative algorithm can be considered poor primarily due to the repetitive design which does not make it ideal. "This algorithm iterates through a list of instructions, perform them repeatedly until it reaches the solution"(Lecture Notes, 2013). The recursive algorithm each step of the repetition is considered as a subtask of the previous step and the procedure is invoked repeatedly to solve the subtask.
&nbsp;
In concluding it is clear that the properties of an algorithm will determine if a strong or poor algorithm is constructions.
&nbsp;
&nbsp;
References:
Brookshear, J. G. Computer Science: An Overview XML Vital Source ebook for Laureate Education, 11th Edition. Pearson Learning Solutions.
www.algorithmsandproblems.blogspot.com (2013) Algorithms[Online]. Available from http://algorithmsandproblems.blogspot.com/2013/04/definition-of-algorithm.html (Accessed: 26 October 2014).
Computer Structures - Lecture Notes (2014) Analysis of Algorithms[Online]. Available from https://elearning.uol.ohecampus.com/bbcswebdav/institution/UKL1/201480_SEPTEMBER/MS_CKIT/CKIT_501/readings/UKL1_CKIT_501_Week08_LectureNotes.pdf (Accessed: 26 October 2014).
&nbsp;
&nbsp;
&nbsp;

	              
	              
Re-post: Initial submission from Week 7 - Designing an algorithm, part 1.
Option 2: Generate pseudo-code for word search algorithm
&nbsp;

Make a list of five words, 3-6 letters in length.
Create a string of approximately 30 letters containing some of the five words.
Your algorithm should identify all of the substrings of the longer string that match any of the five words you generated, and the number of times each one appears
For example: If you chose the words&nbsp;structure,&nbsp;such, system, blue, red,&nbsp;and your algorithm operates on the string&nbsp;jkdistructuredstrusyssystemoon,&nbsp;your algorithm should report that the string contains the words&nbsp;structure,&nbsp;red&nbsp;and&nbsp;system&nbsp;each one time, and the words&nbsp;such&nbsp;and&nbsp;blue&nbsp;zero times.

&nbsp;
Start
fword ← “structure”
sword ← “such”
tword ← “system”
foword ← “blue”
fiword ← “red”
List&nbsp; ← fword, sword, tword, foword, fiword&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
String ← “jkdistructuredstrusysstemoon”
count ← 0;
procedure Search (String, List)
If (List empty)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;then
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (Declare search a failure)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; else
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (Select the first entry in list to be TestEntry;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; while (String ≠ TestEntry and there remain entries to be considered)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; do (Select the next entry in List as TestEntry);
If (fword = String)
&nbsp;then count=fword ← 1
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; else count = fword ← 0
&nbsp;
If (sword = String)
&nbsp;then count=sword ← 1
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; else count = sword ← 0
&nbsp;
If (tword = String)
&nbsp;then count=tword ← 1
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; else count = tword ← 0
&nbsp;
If (foword = String)
&nbsp;then count=foword ← 1
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; else count = foword ← 0
&nbsp;
If (fiword = String)
&nbsp;then count=fiword ← 1
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; else count = fiword ← 0
)end if
&nbsp;
Print the values stored in count.
&nbsp;
End
&nbsp;
&nbsp;
References:
&nbsp;
Brookshear, J. G. Computer Science: An Overview XML Vital Source ebook for Laureate Education, 11th Edition. Pearson Learning Solutions. 
	              
	              Hi class,
Are we submitting this part to TurnItIn too?
Is it even possible?
Best regards,
Belinda&nbsp;
	              
	              Week 7 DQ (continued) - Designing an algorithm, part 2 Last week I chose option 1 where I had to design an algorithm using pseudo code to basically search a string of five numbers, identifying and displaying substrings of numbers that form numbers that are divisible by 3. &nbsp;My initial algorithm, while somewhat effective, I must admit was a bit ambiguous and left room for improvement, particularly with regards to the variables I set out to use. There was no clear declaration of those variables and the numbering of the steps was a bit confusing. &nbsp;Based on what I learnt this week about the qualities of a good algorithm. I have rewritten my pseudo-code, with the goal to be unambiguous, convey finiteness by having a start and stop statement, clearly indicate the variables to be used and precisely initialize those variables. &nbsp; Start Use variables test, len, k, i Initialize String test = “37545”, len = length(test), k=1 For (i=0 to len-1, i++) While (k&lt;=len)  Set String sub = substring of test (Start at i, Stop at k) Use variable int num Convert sub to num If num mod 3 == 0 then   Print this string   If k == 5   k = i+1 Break while loop   Else   k ++   End-While  Stop &nbsp; References: UOL Week 7 and 8 lecture notes [accessed 25 October 2014] [Online]http://algorithmsandproblems.blogspot.com/2013/04/definition-of-algorithm.html accessed 26 October 2014 [Online] http://computer.howstuffworks.com/question717.htm accessed 26 October 2014 [Online] http://cc.ee.ntu.edu.tw/~ywchang/Courses/PD/EDA_Chapter4.pdf accessed 27 October 2014 &nbsp; &nbsp; 
	              
	              Week 7 DQ (continued) - Designing an algorithm, part 2
Re-posting of my original week 7 DQ
Option1: Numbers.
This pseudo-code is reviewed to address to comment done by Numan Arshad, we have reviewed the initial value of variable x before the boucle.
Thank you very much Numan for your comment, it was very helpful.
&nbsp;
// Declaration of variables
X int;
n int;
i int;
T[n] array;
&nbsp;
// enter a number of min 5 digits
&nbsp;
echo "Please entre an integer number with min 5 digits :"
input number Read (X); // ex. 37540
if length(X) ← 5
&nbsp; echo "Please enter a number with min 5 digits"
else
&nbsp; echo "OK! It’s a correct number"
end if
&nbsp;
// We will now insert each digit of the nmuber provided into a array
&nbsp;
n ← length(X);
T[n] ← X;&nbsp;&nbsp;&nbsp; // insert into T[n] as below
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // T[1] ← X1 (first digit)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //&nbsp; T[2] ← X2 (Snd digit)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //&nbsp;&nbsp; ...
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //&nbsp;&nbsp; T[n] ← Xn (last digit); n is the length of the number input
&nbsp;
Begin
// first we will check each digit or each entry of the array and see which one is divisible by 3 the display it
&nbsp; i ← 1;
&nbsp; for i = T[1] to T[n]
&nbsp;&nbsp;&nbsp; if T[i] mod 3 := 0
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; echo T[i]
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; echo ";"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; --// 3;0;
&nbsp;&nbsp;&nbsp; end if
&nbsp;&nbsp;&nbsp; i ← i+1;
&nbsp;&nbsp; end for
&nbsp;&nbsp; 
// Now we will check combination of 2 substrings: 37; 75; 54 and 40
// will use concatenation function (cat)
&nbsp;&nbsp; i ← 1;
&nbsp;&nbsp; for i = T[1] to T[n]
&nbsp;&nbsp;&nbsp;&nbsp; if cat(T[i]&amp;T[i+1]) mod 3 := 0
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; echo cat(T[i]&amp;T[i+1])
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; echo ";"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; --// ;75;54
&nbsp;&nbsp;&nbsp;&nbsp; end if
&nbsp;&nbsp;&nbsp;&nbsp; i ← i+1;
&nbsp;&nbsp; end for
// Now we will check combination of 3 substrings: 375; 754 and 540
&nbsp;&nbsp;&nbsp;i ← 1;
&nbsp;&nbsp; for i = T[1] to T[n]
&nbsp;&nbsp;&nbsp;&nbsp; if cat(T[i]&amp;T[i+1]&amp;T[i+2]) mod 3 := 0
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; echo cat(T[i]&amp;T[i+1]&amp;T[i+2])
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; echo ";"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; --// ;375;540
&nbsp;&nbsp;&nbsp;&nbsp; end if
&nbsp;&nbsp;&nbsp;&nbsp; i ← i+1;
&nbsp;&nbsp; end for
&nbsp;&nbsp; .
&nbsp;&nbsp; .
&nbsp;&nbsp; .&nbsp;&nbsp;&nbsp;&nbsp; // will do it for 4,5,...,n (n is the last digit of the number input
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; and the length of that number
&nbsp;&nbsp; .
&nbsp;&nbsp; .
&nbsp;&nbsp; .
&nbsp;&nbsp; 
&nbsp;&nbsp; i ← 1;
&nbsp;&nbsp; for i = T[1] to T[n]
&nbsp;&nbsp;&nbsp;&nbsp; if cat(T[i]&amp;T[i+1]&amp;T[i+2]&amp;...&amp;T[n]) mod 3 := 0
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; echo cat(T[i]&amp;T[i+1]&amp;T[i+2]&amp;...&amp;T[n])
&nbsp;&nbsp;&nbsp;&nbsp; end if
&nbsp;&nbsp;&nbsp;&nbsp; i ← i+1;
&nbsp;&nbsp; end for
&nbsp;&nbsp; 
End;
Reference:
Lecture Notes, (2014). &nbsp;Week 6: Computer networks [Online]. Available from: https://elearning.uol.ohecampus.com/bbcswebdav/institution/UKL1/201480_SEPTEMBER/MS_CKIT/CKIT_501/readings/UKL1_CKIT_501_Week08_LectureNotes.pdf. (Accessed: 22 October 2014)
&nbsp;

	              
	              Hi Bram,
YAGNI!
I would call this over-engineering:-) The requirement explicity says the string length is about 30. I don't see the necessity of using KMP or BM algorithm.
It seems that "keep it simple" is the hardest design principle, isn't it?
br, Terry
	              
	              Hi Tresor,
It's great that you mentioned the algorithm complexity. One comlexity measurement I used often is the Cyclomatic Complexity (McCabe, 1976). Cyclomatic complexity&nbsp;measures the number of linearly independent paths through a program's source code. The more independent paths in the algorithm has, the more test cases are needed to prove the correctness of the algorithm.
"A number of studies have investigated cyclomatic complexity's correlation to the number of defects contained in a function or method.", (Wikipedia).
One good thing about the cyclomatic complexity is it's very easy to calculate. All you need is just to count how many conditions (if, for, while, etc) in the algorithm. And it's easy to create computer software to do the counting automatically.
br, Terry
References:
McCabe, T. J. (1976). A complexity measure.&nbsp;Software Engineering, IEEE Transactions on, (4), 308-320.
Cyclomatic complexity. (2014, October 13). In&nbsp;Wikipedia, The Free Encyclopedia. Retrieved 10:14, October 27, 2014, from&nbsp;http://en.wikipedia.org/w/index.php?title=Cyclomatic_complexity&amp;oldid=629419830
	              
	              Hi Augusto,
In your first pseudo-code&nbsp;




NIndex ← 0while&nbsp;(NIndex ≤ stringOfNumbers length)&nbsp;




≤ perhaps should be &lt;.
br, Terry
	              
	              Hi Anthony

Handling Arbitrary-precision arithmetic within an algorithm can be an issue with certain programming languages.
For example if a solution requires very large bignum arithmetic where exact results with very large numbers are needed rather than rounded ended numbers to an Nth digit.&nbsp; For example calculating Pi or cryptography. Speed is not the issue with these but more the precision needed.&nbsp;
There are natural languages designed for this type of arithmetic such as:

Maxima
Maple
Sage
SymPy

And some which can use Arbitrary-precision arithmetic support libraries but are not built in as standard such as:

Java
Perl
Python
REXX

But knowing what you require of the algorithm first as you mention in the question may lead to you using a specific language for the task as some wont handle the operation to the correct precision needed or will need library updates to allow it to work in the first place.
This is particualrly evident when you mention changes or updates to languages over time. With the given examples the native bignum languages are designed for this function which will always be able to handle it. But what if the Perl or Python updates then rule the bignum library extension unavailable for use?

I've seen it at a lesser extent for when I worked on a project in my early days. A company I worked for had a VB6 application for a few years which provided some functions for users with the addition of an aftermarket library. However when we upgraded to VB.NET this was rendered useless. Unfortunatley I can't remember the specifics now as it has been over 14 years since this happened but I remember a massive issue with clients complaining at the lost functionality in the software.
References
Wikipedia (2014), 'Arbitrary-precision arithmetic'&nbsp; [http://en.wikipedia.org/wiki/Arbitrary-precision_arithmetic] accessed 27/10/2014
	              
	              Hi&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; UPDATED

I havent had any comment as of yet for my work this week however I've managed to simplify some of my code myself as follows:
1) Improvements by declaring variables and setting initially values in one line instead of individually. This improves simplicity and elegance to the code and reduces number of lines.

*all changes highlighted in red*


Procedure EnterData(entVal,divVal)(

&nbsp;

Define aVal,aCount,aDiv as int;

Int aVal,aCount,aDiv = 0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // Set all values to 0

&nbsp;

If entVal.length &gt; 4 then

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; set&nbsp; aVal = entVal;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // set the entered value to aVal

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; set aCount = entVal.length+1;&nbsp;&nbsp; // set the string length count to aCount+1

Else

Echo “sorry not enough numbers, you must enter 5 or more”;

End;

Endif

&nbsp;

If divVal &gt; 0 then&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //make sure a value is entered as the division

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; set aDiv = divVal;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // set the entered division value to aDiv&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;

Else

Echo “sorry you need to enter a division value”;

End;

Endif

)

Procedure stripString(aVal,aCount,divVal)(

Define divisions[] as array;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // set an array to hold each of the substrings

Define tempStr as string&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // temporary string

Define X,Y = int;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 

Set X,Y = 0;

&nbsp;

While&nbsp; (X &lt; aCount)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //&nbsp; loop through each iteration from x to aCount

&nbsp;

Do (

Loop until Y = aCount(&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // internal loop for internal Y position

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Set tempStr = aVal.SubString(X,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Y)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // set the temp string to start X and pos Y&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; If (tempStr is divisible by divVal )&nbsp; // check to see if value is divisible

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Then

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Set divisions[]&nbsp;&nbsp;&nbsp; = tempStr;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // save to array

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Else

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;

End if

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Y = Y + 1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // increase Y by 1 each loop

)

&nbsp;

X = X +1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // increase X by 1 each loop

)

&nbsp;

Echo divisions[] &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // show the final string

)

**************************************************************************

Please feel free to help me with any suggestions you might see that I have not yet.

Thanks
Chris
	              
	              Week 7 Discussion Question - Designing an algorithm, part 2


Revised/Enhanced Version of Week 7 Pseudo-code


Babatunde KOLAWOLE – 27 October 2014



Still on my OPTION 3: Consensus Algorithm, I have updated my Pseudo-code, highlighted in yellow.

Assuming flavors of ICE CREAM are A, B &amp; C

Declare A as integer,

Declare B as integer,

Declare C as integer,

Declare HIGHEST as string,

Declare HIGHESTVALUE as integer


DIALOGUE:

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; A = 0

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; B = 0

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; C = 0

For each person

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Print “choose between A, B &amp; C”

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; If A is chosen, set A = A + 1&nbsp;&nbsp; 

If B is chosen, set B = B + 1

If C is chosen, set C = C + 1

Next

If A &gt; B then

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; HIGHEST = “A”

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; HIGHESTVALUE = A

Else

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; HIGHEST = “B”

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; HIGHESTVALUE = B

End if

If C &gt; HIGHESTVALUE then

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; HIGHEST = “C”

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; HIGHESTVALUE = C

End if

If HIGHESTVALUE = A and HIGHESTVALUE = B or HIGHESTVALUE = A and HIGHESTVALUE = C or HIGHESTVALUE = B and HIGHESTVALUE = C then

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Print “We have two favorites, maybe you should consider different flavor of ice-cream”

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; GOTO DIALOGUE

End if

If HIGHESTVALUE &lt; 10 then

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; For each person

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Print “Most People Chose “&amp; HIGHEST

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Next

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; GOTO DIALOGUE

Else

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Print “Everybody wants “&amp; HIGHEST

End if

Reference:

J.G. (2012) Computer science: An overview. 11th ed. Chapter 5, pp. 188-222. Boston: Pearson Education / Addison-Wesley. (Accessed on: October 17, 2014)


Guy M.H (1999). “Pseudo-code” [online] Available from: http://www.bfoit.org/itp/jargon/jargon_p.html#pseudocode (Accessed on: October 18, 2014)


Thomas H. C, Charles E. Leiserson, Ronald L. Rivest and Clifford S. (2009); Introduction to Algorithms (3rd Edition); Available from: http://ldc.usb.ve/~xiomara/ci2525/ALG_3rd.pdf (Accessed on: October 18, 2014)
	              
	              Hi Belinda,

we are not submitting this part to Turnitin as there's no Turnitin link provided for it.

Regards, Babatunde
	              
	              Hi Augusto,
Must an algorithm have input and output? That depends on how do you define "input" and "output".
There are algorithms that don't need any input, but only depends on its internal state. For instance, the counter algorithm doesn't take any input, it return 1 first, next 2, then 3, etc.
There are algorithms that don't produce any output, but only have some side-effect. That is to change some internal or external state. If you count side-effect also as output, there is (useful) algorithms that doesn't even have side-effect. e.g.
while True:&nbsp; &nbsp; x = 1234567889 * 987654321
What does it do? Well, the room is too cold and you can use that algorithm to generate heat from the busy CPU:-)
br, Terry

References:
Side effect (computer science). (2014, August 7). In&nbsp;Wikipedia, The Free Encyclopedia. Retrieved 13:24, October 27, 2014, from&nbsp;http://en.wikipedia.org/w/index.php?title=Side_effect_(computer_science)&amp;oldid=620195088
	              
	              Hi Augusto,
And the elevator example is really good. Thanks!
The efficient algorithm for an elevator is a non-trivial problem. The solution depends a on the specific situation of using. And there are more than one aspects when evaluating the efficiency, e.g. the overall efficiency and an individual's observation. There is no one solution that fits all.
br, Terry
	              
	              Hi Everyone,Some argue that achieving better programmatic and resource use efficiency requires programmers to spend time working out ways to make their algorithms, and implementations of these, more efficient.&nbsp;However, programmer time is expensive, so there is a tendency towards expediency and putting out non-optimal program code because no-one is willing to spend the extra time (and thus money) on optimization, as long as the current code works, albeit sub-optimally.Do you agree with these sentiments, or do you have a different view of the issue?Anthony
	              
	              Hi Craig,
YAGNI.&nbsp;
There is no need to maximise the efficiency of all algorithms.
Pareto analysis is a&nbsp;very useful problem-solve tool in many domains, which is commonly known as the "80/20 rule". Like "20% of the population owns 80% of the fortune", or "80% of the revenue is from 20% of the business". When we talk about computer software time efficiency, it's often "1/99". So 1% of the algorithm takes 99% of the time. Therefore, it doesn't make much sense to improve the efficiency of 99% of the algorithm.
But where is that 1% of the algorithm? It's a myth. Probably only people like Jeff Dean (I hope you still remember the&nbsp;Chuck Norris-like story of him) would know it without actually running the software. Nowadays, there are too many abstraction layers behind an implementation of an algorithm, it's very hard to predict how the computer actually works with your algorithm because there could be optimization on any layer. So, to find that 1% of the algorithm, we need to collect facts by running it. This is called profiling.
There are situations that to build the actually system is too expensive. So we need to know the performance somewhat up-front. To get closer to Jeff Dean's level, we at least need to remember these numbers:
https://gist.github.com/jboner/2841832(OMG, I just noticed that the credit of this table goes to Jeff Dean again.)
I wish formal computer science education also teach people how to make things simple. &nbsp;New graduates tend to over engineer things. The education must have something to do with it:P
br, Terry

References:
You aren't gonna need it. (2014, September 3). In&nbsp;Wikipedia, The Free Encyclopedia. Retrieved 13:47, October 27, 2014, from&nbsp;http://en.wikipedia.org/w/index.php?title=You_aren%27t_gonna_need_it&amp;oldid=623992276
Pareto analysis. (2014, October 11). In&nbsp;Wikipedia, The Free Encyclopedia. Retrieved 13:49, October 27, 2014, from&nbsp;http://en.wikipedia.org/w/index.php?title=Pareto_analysis&amp;oldid=629136041
Profiling (information science). (2014, September 14). In&nbsp;Wikipedia, The Free Encyclopedia. Retrieved 14:07, October 27, 2014, from&nbsp;http://en.wikipedia.org/w/index.php?title=Profiling_(information_science)&amp;oldid=625538573
	              
	              Dear Dr. Ayoola,
I agree with you that there is a tendency towards expediency and sub-optimization in programming, even among some people with great reputation.
One example is&nbsp;Martin Fowler from ThoughtWorks. In his article Sacrificial Architecture 7 days ago:
http://martinfowler.com/bliki/SacrificialArchitecture.html
He said:

"But often the best code you can write now is code you'll discard in a couple of years time."

I disagree with him. (It seems I'm not choosing my battle wisely:)
I haven’t seen anybody who was smart enough to determine the life span of the software he designed at the moment of creating it.
Computer software is like a city, it evolves all the time based on the existing structure. I like what&nbsp;Lewis Mumford explained for his admiration in (the evolving of) medieval cities:

"Organic planning does not begin with a preconceived goal; it moves from need to need, from opportunity to opportunity, in a series of adaptations that themselves become increasingly coherent and purposeful, so that they generate a complex final design, hardly less unified than a pre-formed geometric pattern.”&nbsp;Mumford, L. (1961)

What Munford said is also true for building software. The existing code will be changed and extended to adopt the new need and opportunity. But if the software doesn't have the necessary quality, the cost of change will increase. So change in the code would become harder and harder. Another analogy is that the entropy of the code would by nature goes up and become more chatoic, unless we put extra force to decrease it. These extra forces to make the cost of change stable includes refactoring and unit testing.
Any organic system will eventually die, but as it lives, it should live a healthy life. I think this is even more important when we don't know how long will it live.
(The idea of "cost of change" came from the book Clean Code.)
br, Terry
 References:
Mumford, L. (1961). The City in history. Its origins, its transformation, and its prospects.
Martin, R. C. (2008).&nbsp;Clean code: a handbook of agile software craftsmanship. Pearson Education.
	              
	              Dear Dr. Ayoola,
I might have answered the wrong question in my previous reply, if the question is about optimization rather than making the proper design.
For algorithm optimization, I already shared my opinion in the post that I replied Craig.
I would like to share more.
"The First Rule of Program Optimization: Don't do it. The Second Rule of Program Optimization (for experts only!): Don't do it yet." — Michael A. Jackson
"We should forget about small efficiencies, say about 97% of the time: premature optimization is the root of all evil" -- Donald Knuth
Blindly optimze the code is not just waste of time and money, it also messes up the code. I worked in telecommunication and performance critical area, I know how harmful if a programmer focus on optimization all the time.
And it's not fair to call non-optimal sub-optimal. Non-optimal is ok most of the time. Sub-optimal is alway bad.
As I shared with Craig:&nbsp;
I wish formal computer science education also teach people how to make things simple. &nbsp;New graduates tend to over engineer things. The education must have something to do with it:P
br, Terry

	              
	              Hi Terry,
Thank you a lot because I've never hear about&nbsp;Cyclomatic Complexity before. I've also read the article on Wikipedia and it's seems quite simple to use.
it's based on simple and basic principles which consist to count the number of iterations (different paths) into a program or sub-program.
Regards,
Tresor

	              
	              Hi Babatunde
I don’t know if you noticed, but I actually posted these comments last week in week 7, but this is apparently the correct place to do it, and since you have also done some changes then obviously I have alsoJ
I love Ice cream, so I should have chosen that optionJ. Instead I chose the number optionL. So because I love Ice cream, I have chosen to comment on your code and I’ll check for correctness. As I’ve mentioned several times, I’m no programmer hence I will only give “simple” strategy feedback, and you will just have to figure out code improvement yourselfJ.
Highestvalue always &lt; 10
I would expect that most times the value will be &lt; 10 and then it will always go to Dialogue. And Dialogue sets all counters to 0; hence everyone has to choose again. I don’t think it is enough to inform every person about the highest score even though there is a possibility that they will chose differently next time since they know they have to choose just one flavour. &nbsp;I would therefor recommend that you decide on a different strategy here, e.g. always let the majority win:

Voting 2-3-5: C is the winner.
Voting 4-4-2: Let C chose again and hope for a winner.
Voting 5-5-0: then you probably have to do a loop until someone votes differently and you have a winner.
Voting 3-3-4: C is the winner. Or if you want a minimum of 50% majority, then let A and B vote again informing of the voting.

Other issues&nbsp;
If this was to be a finished program, then there should have been (and I'm sure you would have)&nbsp;some error handling e.g. if a person returns 1 instead of A. And what about asking the person if he/she is sure, e.g. in case of typing error.
Best wishes
Bo
	              
	              I am a firm believer that developer need to write the as optimum code as they have time alloted. However in reality, projects have deadlines and developers get time constraints and their work quality might get impacted. Especially when you have SOA (Service Orientated Architecture) were code is shared amongst many other areas in&nbsp;a corporation. If a developers develops poor quality code, some other developer will need to re-factor the code to be able&nbsp;to be utilized in a SOA architecture. So in the ideal case, write good efficient code in the first place!&nbsp;

robin&nbsp;
	              
	              Updated Pseudo-Code: Option 1: Numbers.
Start:
Declare Input &lt;== User Input String of at least five numbers
If (Input _size&lt;5)
&nbsp;&nbsp; Print (“Input size must have at least five digits”)
&nbsp;else
{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Declare String Array[]
Declare int Index &lt;== 0
Declare int subIndex
Declare String Temp
&nbsp;
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp; //converting input string to &nbsp;array
&nbsp;&nbsp;&nbsp; Array[] &lt;== Input string
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Loop for traversing Array[]
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; {
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Temp= Array[Index]
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;If (Integer(Array[Index]) % 3 == 0)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Print Array[Index]
&nbsp;&nbsp; 
&nbsp;&nbsp;&nbsp; //checking combination of digit with other digits in the string
&nbsp;&nbsp;&nbsp; subIndex = Index+1
&nbsp;&nbsp;&nbsp; Loop While (subIndex&lt;=Array_size)
&nbsp;&nbsp;&nbsp; {
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;//concatenate two strings and check their Mod
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Temp=Temp + Array [subIndex])
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; If (Integer(Temp) %3==0)
&nbsp;{
&nbsp;&nbsp;&nbsp; Print Temp
&nbsp;}
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; subIndex++
&nbsp;&nbsp;&nbsp;&nbsp; }
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Index++
}
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }

	              
	              
	              
	              Ah! Good catch!
	              
	              REPOST
Here is my attempt for Option 2

Algorithm that&nbsp;identifies&nbsp;substrings (RandomLetters)&nbsp;of the longer string that match any of the five words (subStringtomatch)&nbsp;you generated, and the number of times each one appears


Input Length_random&nbsp; RandomLetters = "jkdistructuredstrusyssystemoon"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;\* RandomLetter with lenght of Length_randominput Length_tomatch substringtomatch = "structure", "such", "system", "blue", "red"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \* Substringtomatch with Lengthttomatch
Counter set to zero&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \*Counter set to zero&nbsp;&nbsp;&nbsp;&nbsp; for i = 0 to (Length_random&nbsp; – Length_tomatch)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \*increment to start looping each character in the random letter string&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for j = 0 to (Length_tomatch)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\*increment the starts of the string to match&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if&nbsp; substringtomatch[ j ] = RandomLetters[ i +j ] then&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \*we have matched one character&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Counter +1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if&nbsp; Counter =&nbsp;length_tomatch then&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \* We matched the sub string pattern &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Print&nbsp; "Following word appear in the Random string" substringtomatch&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; next j&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; else&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; counter set to zero&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; next i

	              
	              Hi Terry,
A little added to this….

"Premature optimization is the root of all evil" has long been the rallying cry by software engineers to avoid any thought of application performance until the very end of the software development cycle (at which point the optimization phase is typically ignored for economic/time-to-market reasons). However, Hoare was not saying, "Concern about application performance during the early stages of an application's development is evil." He specifically said premature optimization; and optimization meant something considerably different.

In the days when he made that statement, "optimization" often consisted of activities such as counting cycles and instructions in assembly language code. This is not the type of coding you want to do during initial program design, when the code base is rather fluid.

Best Regards, Babatunde
	              
	              Hello Anthony,
I remember working with Prolog and Pacal. Prolog general purpose logic programming language with its roots in artificial intelligence and computational linguistics. It is made especially for the direct use of first-order logic deploying terms of relations represented as facts and rules. Unlike Pascal Prolog is a declarative language as the program runs query over these relations. Pascal is a procedural programming logic. These different origins makes it understandable why it is more difficult to use first order logic relation in Pascal and vice versa programming procedures in Prolog.
Greetings from Bram
	              
	              Updated Psuedo Code
Option 1: Numbers
&nbsp;The&nbsp; algorithim below searches a string of at least five numbers &nbsp;and identifies all of the substrings that form numbers that are divisible by 3. Lines that begin with the apostrophe are comments. I had thought to try and optimize the code through use of an array to hold some variables but in the final analysis felt the original representation would work just as well
&nbsp;
‘ Declare required variables to be used in the algorithm
&nbsp;
‘Holds the input number
Dim MyStr As String &nbsp;&nbsp;&nbsp;
&nbsp;
‘Stores the numbers to be evaluated for divisibility by 3
Dim MyNum As Long
&nbsp;
‘Stores the final output
Dim MyResult As String
&nbsp;
‘ Variables to be used for the loop
Dim i As Integer
Dim j As Integer
&nbsp;
&nbsp; ‘Accept a 5 digit number
&nbsp;&nbsp;&nbsp; MyStr = Inputbox(‘Enter&nbsp; a number with at least 5 digits’)
&nbsp;
&nbsp;&nbsp;&nbsp; ‘ Check whether value entered is numeric
&nbsp;&nbsp; If&nbsp; not&nbsp; isnumeric(MyStr) then
&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;Msgbox “Invalid Number”
&nbsp;&nbsp;&nbsp;&nbsp; Exit sub
Else
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ‘ Check whether value entered has at least 5 digits
If len(trim(mystr)) &lt;5 then
&nbsp;&nbsp; MsgBox “Invalid number”
End if
&nbsp; End If
&nbsp;&nbsp;&nbsp;
&nbsp; ‘ Outer loop allows for iteration of each digit in the number
&nbsp;&nbsp;&nbsp; For i = 1 To Len(MyStr)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ‘ Inner loop allows for iteration of segments of numbers within the value entered
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; For j = i To Len(MyStr)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ‘ Extract number and compare whether divisible by 3
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; MyNum = Mid(MyStr, i, j)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; If MyNum Mod 3 = 0 Then
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ‘ For concatenation of the string result, stores the first number
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; If Len(Trim(MyResult)) = 0 Then
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; MyResult = Str(MyNum)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Else
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ‘ Add subsequent numbers obtained to result string and ignore any duplicate numbers
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; If InStr(1, MyResult, Str(MyNum)) = 0 Then
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; MyResult = MyResult + ", " + Str(MyNum)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; End If
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; End If
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; End If
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Next j
&nbsp;&nbsp;&nbsp; Next i
&nbsp;&nbsp;&nbsp;
&nbsp;‘Display result
&nbsp;&nbsp;&nbsp; MsgBox MyResult
&nbsp;
References
Brookshear, J. G. Computer Science: An Overview, 11th Edition. Pearson Learning Solutions.&nbsp; 
Thayer, R. Visual Basic 6 Unleashed– September 11, 1998, SAMS
&nbsp;
&nbsp;

	              
	               
Improving the algorithm
 
 I have not received feedback on how to improve it. However there certainly certain aspects that can be improved. First is to ask for a StringOfNumbers and a Divisor, and have some checks so that these inputs do not break the algorithm.
 
My additions/corrections are in blue:
     



 
procedure Main StringOfNumbers ← (Ask for String)
 
MyCounter ← 0 Divisor ← (Ask for Divisor)
 
&nbsp;
 
//check the Divisor, division by zero is bad.
 
if (Divisor = 0) Then &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Error(“Attempted division by zero. Please execute again with a valid divisor.”)
 
end if
 
//check the StringOfNumbers values for some basic cases.
 
Select StringOfNumbers value; execute the appropriated case. &nbsp;&nbsp;&nbsp; Case (StringOfNumbers is Empty) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Error(“No string of numbers provided. Please execute again with a valid string.”)
 
&nbsp;&nbsp;&nbsp; Case (StringOfNumbers contains letters)
 
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Error(“String is not valid. Please execute again with a valid string of numbers.”)
 
&nbsp;&nbsp;&nbsp; Case Else &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (Continue execution)
 
&nbsp;
 
//go through each integer in the StringOfNumbers and save each one as part of the array MyIntegerArray in the position MyCounter. Because the counter starts at 0 and ends with the last integer inside StringOfNumbers, the counter maximum value is always the length of the string. for each MyInteger in StringOfNumbers do ( &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; MyIntegerArray[MyCounter] ← MyInteger &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;MyCounter ← MyCounter + 1 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ) end For
 
//Set the Index to 0. MyIndex ← 0 repeat (
 
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //Execute the FindDivisiblesSubstrings procedure and move the index by 1 increment.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; FindDivisiblesSubstrings(MyIndex, Length of StringOfNumbers, MyIntegerArray[],Divisor) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; MyIndex&nbsp; ← MyIndex&nbsp; + 1 ) until (MyIndex = Length of StringOfNumbers) end procedure  
 
procedure FindDivisiblesSubstrings (NIndex, MaxLength, MyArray[], DivisibleBy) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //set a IndexNext variable and add 1 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; IndexNext ← NIndex + 1 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; If (IndexNext &gt; MaxLength) Then &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //this means we reached (at the last iteration) the end of the string. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (Exit the procedure) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; end if &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //create a temporary string where to append the substrings. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; TempString ← MyArray[NIndex] &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //maxLength-1 is important as I will be passing the length as 5, but the index start as 0. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; while (a ← IndexNext &lt; MaxLength-1)  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; do ( &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; append MyArray[a] to TempString
 
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //evaluate divisibility by DivisibleBy, If divisible display the number &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; If (TempString Mod DivisibleBy = 0) Then (display TempString) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; ) end procedure  
 
Procedure Error(ErrorMessage) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Display ErrorMessage &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Stop Execution end procedure
 



 
&nbsp;
 
&nbsp;
 
	              
	               
Hi Terry,
 
Thanks for pointing out the input and output. I guess what I should have said is that an algorithm must have an initial state, this might be an input, but as you point out it does not necessarily needs to be an input. The “life” algorithm does not need an input (unless you are religious), it simply needs a defined initial state to start.
 
For the output part, it is a little difficult for me to get. But I suppose that the algorithm to drive a car does not necessarily produce a specific output (and coincidentally does not need an input). The output is the actual car being driven, whenever this is a real output or not it is probably a philosophical discussion (Abstract objects) (Gideon, Zalta 2014).Hence for me is more difficult to grasp the meaning of output. But I would agree that in the computing context then yes an algorithm does not necessarily produce a tangible output (whenever it is a side effect or not). 
 
Best Regards,
 
Augusto.
 
References
 
Gideon, R. &amp; Zalta, N.E. 2014, "Abstract Objects", The Stanford Encyclopedia of Philosophy, Fall 2014. Available at: http://plato.stanford.edu/archives/fall2014/entries/abstract-objects/
 
&nbsp;
 
	              
	              Reposting of Week 7 Discussion Question – Designing an Algorithm (Part 1)
Craig Thomas – 27 October 2014
Chosen: Option 1 – Numbers.
Please note, the Pseudo-Code Algorithm is striked - see next post for revised Pseudo-Code Algorithm.
Your Algorithm should:

Search a string of at least five numbers (for example, 37540)
Identify all of the substrings that form numbers that are divisible by 3.
For example, applying the algorithm on the string 37540 should produce the following substrings&nbsp;(not necessarily in this order): 0; 3; 75; 54; 375; 540.

I need to figure out how to be able to search a string, identify a substring from that, extract the substring, and search the original string again to the end without picking up substrings already identified. This includes being able to identify and not exceed the length of the string, and pick up valid substrings. 
SECTION A: My Initial Algorithm Part 1
Craig’s attempt at Pseudo-Code, please don’t laugh! 
BEGIN
Section 1
//Setting counter and creating a list to store integers
1.&nbsp; SET “COUNTER” to “0” &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //counter is set to 0
2.&nbsp; CREATE LIST called “INTEGER LIST”&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //creating list of integers
Section 2
//Generating list of 5 integers with repeat process/or loop
3.&nbsp; Ask User to INPUT a ‘single digit integer’ &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //user inputs the number(s) 3 [loop, user then inputs 7,5,4,0]
&nbsp;&nbsp;&nbsp;&nbsp; 3.1&nbsp; SET “COUNTER” to “COUNTER” +1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //counter is now set to 1 [loop then sets to 2,3,4,5]
&nbsp;&nbsp;&nbsp;&nbsp; 3.2&nbsp; ADD ‘single digit integer’ to “INTEGER LIST”&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //adds 3 to integer list [loop then adds 7,5,4,0]
&nbsp;&nbsp;&nbsp;&nbsp; 3.3&nbsp; IF COUNTER &lt;5 THEN repeat step 3&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //checks, and repeats if less than 5, else moves to step 4
&nbsp;&nbsp; ELSE go to step 4&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //counter is 5
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //integer list contains 37540
Section 3
//Creating a final integer substring list that are divisible by 3
4. &nbsp;&nbsp;&nbsp;&nbsp; SET “COUNTER” to “0”&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //counter is reset to 0
5. &nbsp;&nbsp;&nbsp;&nbsp; CREATE LIST called “FINAL INTEGER SUBSTRING LIST”&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //creates list for final substring integers
Section 4
//Finding first set of integer substrings (37540)
6.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; IF COUNTER &lt;5 THEN go to step 6.1 ELSE go to step 12&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //counter check to ensure less than 5 
6.1.&nbsp;&nbsp; SET “COUNTER” to “COUNTER” +1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //counter is set to 1
6.2.&nbsp;&nbsp; GET FIRST VALUE in INTEGER LIST and assign as ‘n’&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //gets 3, assigns as ‘n’
6.3.&nbsp;&nbsp; IF ‘n’ mod 3 = 0 THEN add ‘n’ to FINAL INTEGER SS LIST &nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //3 is divisible by 3, so adds 3 to ss list
6.4.&nbsp;&nbsp; GET SECOND VALUE in INTEGER LIST and assign as ‘n’&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //gets 7, assigns as ‘n’ [assume ‘n’ is superseded by next value]
6.5.&nbsp;&nbsp; IF ‘n’ mod 3 = 0 THEN add ‘n’ to FINAL INTEGER SS LIST&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //7 is not divisible by 3, so doesn’t include
6.6.&nbsp;&nbsp; GET THIRD VALUE in INTEGER LIST and assign as ‘n’&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //gets 5, assigns as ‘n’ [assume ‘n’ is superseded by next value]
6.7.&nbsp;&nbsp; IF ‘n’ mod 3 = 0 THEN add ‘n’ to FINAL INTEGERS SS LIST &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //5 is not divisible by 3, so doesn’t include
6.8.&nbsp;&nbsp; GET FOURTH VALUE in INTEGER LIST and assign a ‘n’&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //gets 4, assigns as ‘n’ [assume ‘n’ is superseded by next value]
6.9.&nbsp;&nbsp; IF ‘n’ mod 3 = 0 THEN add ‘n’ to FINAL INTEGERS SS LIST &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //4 is not divisible by 3, so doesn’t include
6.10. GET FIFTH VALUE in INTEGER LIST and assign as ‘n’&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //gets 0, assigns as ‘n’ [assume ‘n’ is superseded by next value]
6.11. IF ‘n’ mod 3 = 0 THEN add ‘n’ to FINAL INTEGERS SS LIST &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //0 is not divisible by 3, but this included as answer is 0 (zero)
6.12. READ FINAL INTEGERS SS LIST and remove any single value&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //0 removed, as this is not divisible by 3
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; of 0 (zero)&nbsp; &nbsp; &nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //counter is 1
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //final integer ss list contains: 3
Section 5
//Finding second set of integer substrings (37540)
7.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; IF COUNTER &lt;5 THEN go to step 7.1 ELSE go to 12&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //checking that the counter is less than 5&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
7.1.&nbsp;&nbsp; SET “COUNTER” to “COUNTER” +1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //counter now set to 2
7.2.&nbsp;&nbsp; GET FIRST VALUE and SECOND VALUE in INTEGER LIST&nbsp; //gets 3 and 7, assigns as ‘n’ [n=’37’]
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; and assign as ‘n’
7.3.&nbsp;&nbsp; IF ‘n’ mod 3 = 0 THEN add ‘n’ to FINAL INTEGER SS LIST&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //37 is not divisible by 3, so doesn’t include it
7.4.&nbsp;&nbsp; GET FIRST, SECOND and THIRD VALUE in INTEGER LIST //gets 3, 7 and 5, assigns as ‘n’ [assume ‘n’ values are superseded] 
&nbsp;&nbsp;&nbsp;&nbsp; and assign as ‘n’&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
7.5.&nbsp;&nbsp; IF ‘n’ mod 3 = 0 THEN add ‘n’ to FINAL INTEGER SS LIST&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //375 is divisible by 3, so includes
7.6.&nbsp;&nbsp; GET FIRST, SECOND, THIRD and FOURTH VALUE in INTEGER&nbsp;&nbsp;&nbsp; //gets 3,7,5 and 4 assigns as ‘n [assume ‘n’ value is superseded]
&nbsp;&nbsp;&nbsp;&nbsp; LIST and assign as ‘n’&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
7.7.&nbsp;&nbsp; IF ‘n’ mod 3 = 0 THEN add ‘n’ to FINAL INTEGER SS LIST&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //3754 is not divisible by 3, so doesn’t include
7.8.&nbsp;&nbsp; GET FIRST, SECOND, THIRD, FOURTH and FIFTH VALUE&nbsp; //gets 3,7,5,4,0 and 4, assigns as ‘n’ [assume ‘n’ is superseded]
&nbsp;&nbsp;&nbsp;&nbsp; in INTEGER LIST and assign as ‘n’&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
7.9.&nbsp;&nbsp; IF ‘n’ mod 3 = 0 THEN add ‘n’ to FINAL INTEGER SS LIST&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //37540 not divisible by 3, so doesn’t include
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //counter is 2
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //final integer ss list contains: 3; 375
Section 6
//Finding third set of integer substrings (37540)
8.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; IF COUNTER &lt;5 THEN go to step 8.1 ELSE go to 12&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //checking that the counter is less than 5
8.1.&nbsp;&nbsp; SET “COUNTER” to “COUNTER” +1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //counter is now set to 3
8.2.&nbsp;&nbsp; GET SECOND and THIRD VALUE in INTEGER LIST and &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //gets 7 and 5, assigns as ‘n’
&nbsp;&nbsp;&nbsp;&nbsp; assign as ‘n’
8.3.&nbsp;&nbsp; IF ‘n’ mod 3 = 0 THEN add ‘n’ to FINAL INTEGER SS LIST&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //75 is divisible by 3, so includes
8.4.&nbsp;&nbsp; GET SECOND, THIRD and FOURTH VALUE in INTEGER LIST&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //gets 7, 5 and 4, assigns as ‘n’ [assume ‘n’ is superseded]
&nbsp;&nbsp;&nbsp;&nbsp; and assign as ‘n’&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
8.5.&nbsp;&nbsp; IF ‘n’ mod 3 = 0 THEN add ‘n’ to FINAL INTEGER SS LIST&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //754 is not divisible by 3, doesn’t include
8.6.&nbsp;&nbsp; GET SECOND, THIRD, FOURTH and FIFTH VALUE in &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //gets 7,5, 4 and 0 assigns as ‘n’ [assume ‘n’ is superseded]
&nbsp;&nbsp;&nbsp;&nbsp; INTEGER LIST and assign as ‘n’&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
8.7.&nbsp;&nbsp; IF ‘n’ mod 3 = 0 THEN add ‘n’ to FINAL INTEGER SS LIST&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //7540 is not divisible by 3, doesn’t include
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //counter is 3
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //final integer ss list contains: 3; 375; 75
Section 7
//Finding fourth set of integer substrings (37540)
9.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; IF COUNTER &lt;5 THEN go to step 9.1 ELSE go to 12&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //checking that the counter is less than 5
9.1.&nbsp;&nbsp; SET “COUNTER” to “COUNTER” +1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //counter is 4
9.2.&nbsp;&nbsp; GET THIRD and FOURTH VALUE in INTEGER LIST &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //gets 5 and 4, assigns as ‘n’ [assumes ‘n’ is superseded] 
&nbsp;&nbsp;&nbsp;&nbsp; and assign as ‘n’
9.3.&nbsp;&nbsp; IF ‘n’ mod 3 = 0 THEN add ‘n’ to FINAL INTEGER SS LIST&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //54 is divisible by 3, so includes
9.4.&nbsp;&nbsp; GET THIRD, FOURTH and FIFTH VALUE in INTEGER LIST&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //gets 5, 4 and 0, assigns as ‘n’ [assumes ‘n’ is superseded] 
&nbsp;&nbsp;&nbsp;&nbsp; and assign as ‘n’&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
9.5.&nbsp;&nbsp; IF ‘n’ mod 3 = 0 THEN add ‘n’ to FINAL INTEGER SS LIST&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //540 is divisible by 3, so includes
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //counter is 4
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //final integer ss list contains 3; 375; 75; 54; 540
Section 8
//Finding fifth set of integer substrings (37540)
10.&nbsp;&nbsp;&nbsp; IF COUNTER &lt;5 THEN go to step 8.1 ELSE go to 12&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //checking that the counter is less than 5
10.1. SET “COUNTER” to “COUNTER” +1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //counter is now 5
10.2. GET FOURTH and FIFTH VALUE in INTEGER LIST&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //gets 4 and 0, assigns as ‘n’ [assumes ‘n’ is superseded]
&nbsp;&nbsp;&nbsp;&nbsp; and assign as ‘n’
10.3. IF ‘n’ mod 3 = 0 THEN add ‘n’ to FINAL INTEGER SS LIST&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //40 is not divisible by 3, so not included
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //counter is now 5
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //final integer ss list contains 3; 375; 75; 54; 540
Section 9
//Show final list of Integers divisible by 3
11.&nbsp;&nbsp;&nbsp; SHOW FINAL INTEGER SS LIST &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //shows: 3; 375; 75; 54; 540
&nbsp;
12.&nbsp;&nbsp;&nbsp; END
END

	              
	              Hi Terry
Like Belinda, I really like your elephant example. And I guess, since algorithms are supposed to be unambiguous, that you use the same algorithm. And the family example is also good, especially the one about the Chinese family reminds me of the Faroes where I’m fromJ.

Otherwise, I have these comments:

Ad 1)

I think your idea of following the bookkeeping principle is very interesting – though I’m thinking that perhaps it can be even more bookkeeping like than “just” having the requirement specifications and test plan, which by the way I agree are mostly too static. But regarding the test process, I think (focusing on Quality Assurance) that this is a branch (I can’t find the correct word) of its own, and it’s very important for the QA’s to be open minded to other tests than just the one’s described in the static document.

And by the way, a cousin of min has worked with IT audit. And we have discussed, and agreed upon, that too many programs don’t make sure that the errors don’t happen at all. That we have too many follow up activities afterwards to check if the programs did their job correct. A short comparison could from work today: We have this software deployment tool called SCCM from Microsoft. When we install new computers, the technician is prompted to choose a few things, e.g. which Office version (standard or pro) to install. This step is actually not needed because all machines are separated into two main OU, and depending on which, the must have either standard or pro. So instead of letting the technician make an error here, it should just lookup which OU it belongs to, and then it’s all good.

Ad 2)

Regarding your last paragraph about avoiding pseudo coding, I guess (correct me if I’m wrong), that you are saying that because you are an intelligent programmer with a high level of abstraction – and I can respect that. But I have on several occasions experienced that good programmers got stuck and not being able to solve the task. Perhaps it was because the requirements specifications were not good enough? But I do know, that putting the programmer together with a non-programmer, and talk about the case and rewriting parts of the code to pseudo code (also flow charts have been excellent) made the programmer find the way.

Ad 4)

“Make it work * Make it right * Make it fast”

This is a very good “rule of thumb”. I haven’t seen it written like that before. But I use it other areas in my coordination/project management work. Especially when we are implementing a new system, or even upgrading, and we run into unforeseen problems, we sometimes have to prioritize in the exact order that you write, and it cannot be done in e.g. the reverse order.

&nbsp;

Best wishes

Bo

	              
	              ALGORITHM QUALITIES
Introduction
An algorithm is a code that has a series of steps for solving a problem. These problems could be data processing, automated reasoning or calculations.
Abstract
In this paper, I will share more light on the common elements of algorithm, the properties of algorithm, its various types, common ways it could be expressed etc. The explanations will be short and precise&nbsp;to the best of my knowledge.
Properties of Algorithms
There are various properties that an algorithm must have for it&nbsp;to be&nbsp;valid. These properties include:

Input
Output
Definiteness
Finiteness
Effectiveness
Correctness
Written in simple English

Input: All algorithms must have inputs which should have specified&nbsp;goals. These&nbsp;goals&nbsp;could be single digit or more.
Output: Algorithms must have outputs. When a code&nbsp;is written, it&nbsp;is meant&nbsp;to produce a result. This result or output could be single or multiple outputs and this must&nbsp;have a relation&nbsp;or&nbsp;must be&nbsp;as a result of the&nbsp;input.
Definiteness: The processes of an algorithm&nbsp;must be&nbsp;specific for it to run or work properly. It&nbsp;must be&nbsp;self-explanatory-meaning it&nbsp;should be&nbsp;easy to understand.
Finiteness: There&nbsp;should be&nbsp;an end to an algorithm’s processes or steps. This is very essential as it shows the process of the code has ended.
Effectiveness: This is the degree to which the algorithm is successful in producing the desired result.
Correctness: All the&nbsp;input&nbsp;data&nbsp;must be&nbsp;correct&nbsp;for the algorithm&nbsp;to work&nbsp;right.
Writing in English: All data inputs of an algorithm&nbsp;must be&nbsp;written in English.
&nbsp;&nbsp;&nbsp;&nbsp; However, an algorithm&nbsp;is classified&nbsp;into different types which are as follows:

Recursive algorithm: These are algorithms that&nbsp;are solved&nbsp;with the concept of recursion into a simpler&nbsp;way. Here, the algorithm&nbsp;is broken&nbsp;down from the main problem into a simpler form of understanding.
Iterative algorithm: This type of algorithm involves looping. Algorithms&nbsp;are solved&nbsp;using loops to make the problem simpler.

Forms of algorithms are linear programming, graphs, random etc
&nbsp;&nbsp;&nbsp;&nbsp; Furthermore, algorithms&nbsp;can be&nbsp;expressed in different ways which are flowcharts, natural language, pseudo-codes, programming language etc.
&nbsp;&nbsp;&nbsp;&nbsp; Moreover, these are some basic elements of an algorithm:&nbsp; they acquire data (input), algorithm has to do with computation, it involves&nbsp;selection, iteration and gives a report result (output).
Conclusion
Algorithms form the basis of solving problems which are computer generated. When constructing algorithms, more emphasis&nbsp;should be&nbsp;on testing the written algorithm to check for flaws.
References
Data structure – algorithm, properties of an algorithm, types (n.d) Available&nbsp;from:https://www.careerride.com/data-structure-algorithm-and-its-types.aspx. (Accessed 26 october 2014)
Definition of algorithm (n.d) Available&nbsp;from:https://www.algorithmsandproblems.blogspot.com/2013/04/definition-of-algorithm.html. (Accessed 26 october 2014)
Knuth, D. (n.d) Algorithms [Online] Available&nbsp;from:https://www.courses.cs.vt.edu/cs2104/fall12/notes/T16_Algorithm.pdf. (Accessed 26 october 2014)
Properties of Algorithm (n.d) Available&nbsp;from:https://www.thefreedictionary.com/properties+of+algorithm. (Accessed 26 october 2014)
	              
	              Revised Algorithm: Week 7 Discussion Question – Designing an Algorithm (Part 2)
Craig Thomas – 27 October 2014
Task: To revise pseudo-code for WK7 algorithm based on feedback / further learnings: 
Chosen: Option 1 – Numbers.
Your Algorithm should:

Search a string of at least five numbers (for example, 37540)
Identify all of the substrings that form numbers that are divisible by 3.
For example, applying the algorithm on the string 37540 should produce the following substrings&nbsp;(not necessarily in this order): 0; 3; 75; 54; 375; 540.

I need to figure out how to be able to search a string, identify a substring from that, extract the substring, and search the original string again to the end without picking up substrings already identified. This includes being able to identify and not exceed the length of the string, and pick up valid substrings. 
SECTION B: My Revised Algorithm Part 2
I have tried to maintain a simple logical approach to the pseudo-code and will attempt to apply a more efficient, elegant and effective loop to identify the substrings. I have taken advice and guidance from colleagues, as well as reviewing their submissions, which has provided some more insight to the efficiency in pseudo-code algorithms. Here we go… 
BEGIN
String() ß 3,7,5,4,0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //creating string with 37540
X ß 0&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//setting counter x and index to zero
FinalString() &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//creating final string (currently empty)

Display String (1,2,3,4,5) ß X&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;//displays 37540
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; If X mod 3 = 0&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;//if x [37540] is divisible by 3 (working assumption is if zero this is true)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Then include X in FinalString&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;//37540 is not divisible by 3&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
Display FinalString&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;//FinalString includes: currently empty

X ß 1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//counter x and index becomes 1
While (X &lt;5)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;//count is 1, and is less than or equal to 5. 
Do&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Display String(X)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//displays 3… and then 7, 5, 4, and 0 whilst X less than or equal to 5
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; If X mod 3 = 0 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;//if x [3] is divisible by 3 (working assumption is if zero this is true)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Then include X in FinalString&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //puts number to FinalString if divisible by 3 (assume ‘Else Continues’)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; X ß X + 1&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //counter becomes 2… and so until not less than or equal to 5&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
Display FinalString&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;//FinalString includes: 3
&nbsp;
X ß 1&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;//counter x becomes 1
Y ß X + 1&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//counter y becomes 2
While (X &lt; 5)&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;//count is 1, and is less than 5.
Do&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Display String (XY)&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//displays 37 …and then 75, 54, 40 whilst y less than 5
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; If XY mod 3 = 0&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;//if xy [37, 75, 54, 40] is divisible by 3 (same working assumption)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Then include XY in FinalString&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;//puts number to FinalString if divisible by 3 (assume ‘Else Continues’)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; X ß X + 1&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;//counter becomes 2… and so on whilst less than 5
Display FinalString&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;//FinalString includes: 3; 75; 54
&nbsp;
X ß 1&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;//counter x becomes 1
Y ß X + 1&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//counter y becomes 2
Z ß Y + 1&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//counter z becomes 3
While (X &lt; 4)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//count is 1, and is less than 4.
Do&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Display String (XYZ)&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //displays 375 …and then 754, 540 whilst X less than 4
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; If XYZ mod 3 = 0&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;//if xyz [375, 754, 540] is divisible by 3 (same working assumption)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Then include XYZ in FinalString&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;//puts number to FinalString if divisible by 3 (assume ‘Else Continues’)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; X ß X + 1&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;//counter becomes 2… and so on whilst less than 4
Display FinalString&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//FinalString includes: 3; 75; 54; 375
&nbsp;
X ß 1&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//counter x becomes 1
Y ß X + 1&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//counter y becomes 2
Z ß Y + 1&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//counter z becomes 3
A ß Z + 1&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//counter a becomes 4
While (X &lt; 3)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//count is 1, and is less than 3.
Do&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Display String (XYZA)&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;//displays 3754 …and then 7540 whilst X less than 3
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; If XYZA mod 3 = 0&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//if xyza [3754, 7540] is divisible by 3 (same working assumption)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Then include XYZA in FinalString&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;//puts number to FinalString if divisible by 3 (assume ‘Else Continues’)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; X ß X + 1&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//counter becomes 2… and so on whilst less than 3
Display FinalString&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//FinalString includes: 3; 75; 54; 375; 540
END miserably L
So, I have made a very small step closer to making this pseudo-code algorithm a little more efficient and a little more elegant, but I acknowledge I have not yet managed to fully figure out how to perform the loop more effectively by grabbing all of the necessary numbers in the string in the first set of loops, and thus making the pseudo-code algorithm both fully efficient and fully elegant.
Best Wishes, Craig&nbsp;
References
Laureate Online Education. (2014). Computer Structures – Lecture Notes Week 7: Algorithms [Online]. Available from: https://elearning.uol.ohecampus.com/bbcswebdav/institution/UKL1/201480_SEPTEMBER/MS_CKIT/CKIT_501/readings/UKL1_CKIT_501_Week07_LectureNotes.pdf (Accessed 16 October 2014)
Brookshear, J. G. (2012). Computer Science An Overview. 11th ed. Boston Massachusetts: Pearson. Addison Wesley
Meyer, H. B. (n.d.) Eratosthenes’ sieve [Online]. Available from: http://www.hbmeyer.de/eratosiv.htm (Accessed 17 October 2014)
	              
	              Hi Anthony
I am not able to give any response to this, I have little/no programming experience and havent really paid too much attention to its evolution over the years. I've tended to focus on outcomes from the computer, and not necessarilly inputs/computer language technologies to drive the outcomes.
That said, this is exactly why I wanted to do the Masters. This last couple of weeks have been incredibly difficult for me. I am learning, but not to the extent of being able to confidently write or interpret code. This is a huge learning curve for me.
Best Wishes, Craig.
	              
	              hi Belinda and Terry,
Very insightful statement! It works for me as this week was very demanding. The statement "Make it work, make it right, make it fast" will help anyone struggling with algorithm once you are determined to get it right. This was my case this past week.
Regards, Martins&nbsp;
	              
	              Extended Algorithm
&nbsp;
I would like to update my Numbers algorithm, as I have made a typo in the looping;
I have highlighted the defective statement for your reference, which was making the programme repeat unnecessarily.

Numbers :


Start


Declare variable sNum


Declare counter variable i =0


Declare counter variable j =0


Accept string with at least 5 digit in length as an input and store in sNum


If length of sNum is less than 5 digits or sNum contains any alphabets then


Display Error Message 


goto Step 5



Loop until (j &lt; Length of sNum)


Let i = 0


Loop until (i &lt; (Length of sNum)-1)


Loop until(i &lt;(Length of sNum) - j


Declare iTemp to store the individually extracted digits from sNum


Let iTemp = MID(sNum,i,(j+1))


If the remainder of iTemp divided by 3 equals 0, then print the value of iTemp


Increment the value of counter i by 1


Go back to Step 7.2



Increment the value of counter j by 1


Go back to Step 7



Stop


&nbsp;
Trace :
&nbsp;
Tracing the above algorithm with “198657”
&nbsp;

      



iNum


Loop index j value


Loop index i value


iTemp


Print Output




198657


0


0


1


-




198657


0


1


9


9




198657


0


2


8


-




198657


0


3


6


6




198657


0


4


5


-




198657


0


5


7


-




198657


1


0


19


-




198657


1


1


98


-




198657


1


2


86


-




198657


1


3


65


-




198657


1


4


57


57




198657


2


0


198


198




198657


2


1


986


-




198657


2


2


865


-




198657


2


3


657


657




198657


3


0


1986


1986




198657


3


1


9865


-




198657


3


2


8657


-




198657


4


0


19865


-




198657


4


1


98657


-




198657


5


0


198657


198657





  
Best regards, Kharavela
	              
	              Thank you Terry,
A Yawn can be a constructive reply.
I think I understand what you are saying but knowing Anthony he probably wants us to deploy our code next week on a string of millions of words:) Therefore the KMP algorithm can be used and improved with the BR-substring analyses and even RK-algorithm to optimise the search in large databases. As far as I know the Rabin-Karp-algorithm is still at the indexing basis of database queries.
But I understand that for 30 letter strings this is overdoing it an in that sense not optimised code. In the end it will take more run times for a complex code indexing all possible words to integers to find those in the main string.
So following your advice I will simplify my solution in order to improve it in pseude code as follows:
findWordsinstring(words,string)
1. find words(1 ... 5) in string(string)
2. if yes return 1 for every found word
3. if no return 0 for every word that was not found
string (thecatisonthematoftheneighbour)
words (the, cat, loves, his, mat)
Result ((the, 3)(cat, 1),(loves, 0), (his,0),(mat,1)
It might be not very descriptive with regard to sub problems but it is short and simple.
Greetings from Bram
	              
	              Hi Terry
Ha... YAGN!
It's me that's yawning at all your clever programming solutions - "you show off"..!! :-)
It's interesting that you say we dont need to maximise the efficiency of all Algorithms, yet in all the reading I've done so far (I agree, it is not thesis level research...but) it all seems to point towards efficiency?
I get your argument and the 80/20 rule, I agree and use that myself in many cases, but that is in estimating and thinking broadly, not necessarily in solutioning.&nbsp;I am currently being led by my reading and not by my experiences (as I have no programming experience) and so far, it suggest efficiency is important. Equally, in most if not all of the other classroom colleague posts that I have read so far, effciency is referenced as a core attribute to the qualities of an algorithm. You even reference it yourself.
I believe Computer Science is currently in a position of luxury. I would argue that 25 years ago, efficiency was crucial and was the start and end points of most computer programs - in that if they were not efficient, the system would simply grind to a halt. I recall being a datacentre computer operator, working through the nightshift running various batch routines. If the routines had not completed by certain times, they were cancelled, as the priority for the computer itself was making sure we hit 'online day' by 6am for users. These cancellations were generally due to inefficient programs or inefficient handling of errors.&nbsp;
It is also worth highlighting that 'marginal gains' can have a huge impact. Whilst you suggest that it may not be the case in Computer Science and it is only really noticeable by the likes of Jeff Dean,&nbsp;I suspect there are mission critical real time systems that expect 100% effciency. Also,&nbsp;if we didn't adopt any form of efficiency, then the costs on computer architecture would surely rise, the user experience would be degraded, and perhaps all programmers would just become apathetic. Who then would we turn to, to ensure that at least some efficiency is provided?
I guess I understand that nowadays, efficiency may not be the start and end of the computer program because of the computing power available, however surely we should look to maximise algorithm efficiency wherever possible? Otherwise, isn't that just poor practice, complacency or even lazyness?
I appreciate your insight Terry - My humble apologies for any overengineering on my part :p
Best Wishes, Craig
	              
	              Hi Anthony
I've just responded to an interesting point made by Terry on a similar subject.&nbsp;
My view here is that any self respecting programmer who believes that they are at the top of their game should just know all the ways to build their programs in the most efficient way and adopt best practice.
Whilst I agree that programmer time is expensive, non optimal code shouldn't be the starting point, surely? Equally, bad code could turnout to be needing a rewrite, and then the cost rises.
Aren't there certain standards of coding, in relation to how a program works, and not just what a program does?&nbsp;
I guess a good analogy could be a 'michelin star' chef, who can build a wonderful dish and yet myself, with the same ingredients could pull something together that tastes ok, but isn't anywhere close to being in the same league as the 'michelin star' chef. Would you expect the 'michelin star' chef to just throw something together that serves a purpose, or would you expect the chef to create the best that can be created from the ingredients available? This comes down to the professional capability of the chef, just as I'd expect a professional programmer who is at the top of their game to equally create the best that can be created.
Perhaps this is where you would have different levels of programmer, ranging from junior positions who are not focused on quality, just expediency. And then, more senior programmers who apply their skills in developing much better code?
Best Wishes, Craig
	              
	              All,
That , Make it work make it right make it fast quote is very much like many others i've heard in the past..

in essence it's all still: JFDI

I'm sure many of you should know what JFDI means!

Isn't it all just common sense though? To do any job you're not going to make it not work make it wrong and make it slow... although I've know a few so called programmers potentially in that category :)

Chris


	              
	              I could not help myself and have to post this:
RabinKarpStringMatcher (T,P,d,q)
1. n = T.length
2. m= P.length
//sets h to highest order digit to be substracted for shifting through S)
3. h= d(m-1) mod q 
4. p=0
5. t0 = 0
// compute hash p for P and t for substring of T while shifting place
6. for i = 1 to m
7.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; P= (dp + P[i]) mod q
8. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;t0= (dt0 + T[i] mod q
// look at possible shifts s
9. for s = 0 to n-m
// compute hash p for P and t for substring of T while shifting place
10.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if&nbsp; p== ts
// compare hashes
11. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp; P[1..m] == T [s + 1 .. s + m] 
// if match use brute force comparison
13&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if s &lt; n - m
14&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; t s+1 = (d(ts - T [s + 1}h) + T[s + m + 1) mod q
The advantage is that the processing done in the loop 6 to 8 executes m times. But being hashes they only have the be run once. Compared to the Knuth-Morris-Prath algorithm the loop has to run pass all words (or in King's DQ all words minus the doubles) which means a m(n − m + 1) times, because it is possible that every hash code matches and therefore every brute force comparison has to be done. 
In a very unlikely worst case scenario this can also happen for the Rabin-Karp model as it could be possible that every hash matches and therefore all letters have to be compared one by one. 
Now when looking for multiple words in large databases this improves the run time of the process compared to both the KMP and BR-algorithm.
Simha (2014) Course Materials; Module 5, Pattern search [Online]. The George Washington University. Available from http://www.seas.gwu.edu/~simhaweb/alg/lectures/module5/module5.html (Accessed on 22 October 2014).
Wikipedia. (2014). Boyer-Moore string search algorthm [Online]. Wikipedia Foundation Inc.. Available from: http://en.wikipedia.org/wiki/Boyer%E2%80%93Moore_string_search_algorithm. (Accessed 26 October 2014).
Wikipedia. (2014). Rabin-Karp algorithm [Online]. Wikipedia Foundation Inc.. Available: http://en.wikipedia.org/wiki/Rabin%E2%80%93Karp_algorithm. (Accessed 26 October 2014).
Alvares (2014). CS 383Algorithms: String matching: the Rabin-Karp algorithm [Online]. Boston College. Available from http://www.cs.bc.edu/~alvarez/Algorithms/Notes/stringMatching.html. (Accessed on 27 October 2014)

	              
	              Hi
As Joseph and Chris have remarked on in the week 7 part 1 discussion, there are several areas where my algorithm should have been better. I have tried, but must admit, that I have yet to succeed in making an entirely new improved version.&nbsp;E.g. I recognise that the parts B and C should have been just one code part.&nbsp;Also I seem to have lost track of the variables, so these I would have to make clearer.
The only part I have revised at present time is part A. Part B it have mentioned below, but just in plain English since I don’t have time enough to finish it (hopefully I will continue tomorrow). Also I guess that I expect to merge part C and 4.
A. Initial user dialogue
// Prompt the user for input and validate immediately if the entered values are integers.
Procedure UserInputs(init_no; init_div) def init_no as integer&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; // Users initial value def init_div as integer&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;// Users divisor
set init_no = 0
set init_div = 0
&nbsp;
set init_no = echo “Write the initial integer number”&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; // saves user input, e.g. 37540
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if init_no not integer
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; then set init_no = echo “You must enter an integer, try again”&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;//new dialogue if not integer
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; else 
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; set init_div = echo “Write integer divisor”&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;// saves users divisor input, e.g. 3
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if (init_div not integer
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; then set init_div = echo “You must enter an integer, try again”&nbsp; &nbsp;//new dialogue if not integer
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; else echo “Thank you. Calculation in progress”))
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ) end if
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ) end if
Next
&nbsp;
B. Identify substrings dividable by divisor
// Identify all the different substrings of the initial number, and check if they are dividable by the divisor, and if yes then save these substrings in an array.
&lt; here should be the code which I hopefully will write later &gt;
C. Show the user the initial input and the findings
// Show to the user all the numbers.
echo “Your initial number was: “ &amp; init_no&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;// it will return 37540
echo “Your divisor was: “ &amp; init_div&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;// it will return 3
echo “The substrings we found are: “ &amp; Init.List&nbsp; &nbsp; &nbsp;// it will show the numbers&nbsp;3; 37; 375; 3754; 37540; 7; 75; 754; 7540; 5; 54; 540; 4; 40; 0
Echo “The final integers are: “ &amp; Integer.List&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; // It will show the numbers&nbsp;0; 3; 75; 54; 375; 540

Best regards
Bo W. Mogensen
	              
	              Kharavela,
I like this. It is simple and easy to follow, much like what an algorithm is supposed to be. I like the fact that it is numbered is an added plus. The algorithm is also similar to mine. Good job.
Tanisha
	              
	              Hi Martins,

While I understand what I and J are as I looked at the same problem, I also agree with Terry in that to make it read better then i migt have been good to include declarations and some notes with the code to explain it.

thanks
Chris

	              
	              Hi Augusto,
Your car example is a good one. Side-effect is typically what a user needed from a computer program rather than the output of the algorithm.
When you send an SMS, it doesn't matter to a user if the algorithms output 1 or 2. The user cares about whether the message has been sent. And that is a side-effect of the program.
br, Terry
	              
	              Hi Bo,
I like the points you listed in your reply. Thanks!
Regarding testing (or Quality Assurance), I agree it's a totally different profession than programming, which requires a different skill-set and mind-set. But testing should be "looking for things we don't yet know". That could be some missing requirement or something we couldn't possibly know up-front, for instance, whether a computer game is fun to play or not. But for the things we already know and agreed, programmers shouldn't rely on the others to check them. They should check these things by themselves.&nbsp;And these checks should be automated. I think you and your cousin's concern is very valid.&nbsp;
Regarding the pseudo-code, I meant to say if you can represent the algorithm in either pseudo-code and real code, why not just use real code? And real code has the adantage of being able to be verified directly. All programming languages (including assembly) has some ability of abstraction. So it's possible to write code that is both runnable and easy to be understood by human. I always avoid pseudo-code is because I want to focus on both detail and abstraction, and not become an Architecture&nbsp;Astronauts (see the 2nd article in the reference, very interesting to read).
BTW, I don't often draw flowchart for algorithm, but when I do, I draw it on the whiteboard, never on a computer. I draw the flowchart to have a conversation with the other programmers. I don't need flowchart to represent an algorithm on computer, the code itself represent it. I need to make sure the flowchart will be erased quickly after the conversation, because it will become a lie very soon.
br, Terry
References:
James Bach, TESTING AND CHECKING REFINED,&nbsp;http://www.satisfice.com/blog/archives/856
http://www.joelonsoftware.com/articles/fog0000000018.html
	              
	              Hi Bram,
Yeah. But probably Antony whould ask us to deal with string of a million words with a lots of repeatations, which require a different optimal solution:-) Probably he's tired of the string exercise. We will never know.
So the best decision we can make at this moment is to keep it simple. Because simple things are easier to change in the future.
br, Terry
	              
	              Hi Craig,
The "80/20 rule" doesn't seem to convince you:-)
The fast/slow of an algorithm isn't good/bad, it's just required/not-required.
Yes, good engineers are often lazy (who said that?). They know how to maximize the Return Of Investment.
br, Terry
	              
	              Hi Chris,
You have a good point that the "make it work make it right make it fast" is "common sense".
To me, a lot of (or most) really useful knowledge in computer science is innate knowledge. You don't really need a degree to be able to understand them:-)
br, Terry
	              
	              Hi Babatunde,
Thanks for the reply:-)
Yes, there are important performance related decisions that we have to make before most of the other things. It's also a necessary skill for a programmer to be able to predict the scale of the performance of a program without actually building it.
But as Knuth said, it's not the "97%". Most optimization need to be done based on a "mature" implementation and based on profiling. This is becaming more true today. Because there are more and more layers of abstraction behind a computer program. It's nearly impossible for a programmer to guess how does the CPU actually run his solution.
I worked with several telecommunication products, where people generally believe everything need to be optimized to extreme. I've seen project struggle with the performance at the end of their "development cycle" as you said. And the reason is exactly because they have been optimizing all the time.&nbsp;There's so much sub-optimization and the code base is so messed up with all the optimal hacks. I suspect that the overall system performance might even be better if they don't optimize at all.
br, Terry

	              
	              Hi Terry
Do you feel that "lazyness (often)" translates to "ROI (maximised)" ?
In my mind, it is often that lazyness actually translates into a negative return on investment, with retrospective activity needed to subsequently 'fix' the ineffective or inefficient work, thus resulting in additional cost, time, effort, performance, experience etc.
As you say, make it work, make it right, make it fast - all of these contribute to efficiency and effectiveness, but complacency and lazyness will have a detrimental impact on these.
Best Wishes, Craig
	              
	              Hello Dr. Anthony,The varying levels of complexity that may be involved in developing a software, can make a situation very challenging and wearying for programmers. Much time is often spent trying to coming up with appropriate solutions to a problem and further implementing the solution. Other issues may involve the time that may be allocated to programming the applications, as such, programmers may often be placed in a situation where they have to meet certain deadlines, thus leaving little time for additional work. Thus, there is no wonder as to why a situation as described in your post, may occur.Even the testing that may be involved in software development are sometimes deemed inadequate, as errors are sometimes discovered as a result of a untested scenario or a unpredictable outcome. &nbsp;It is however a good idea to explore other software development tools that aims at shortcomings such as those mentioned above. Shark gives programmers an opportunity represent their program in forms such as pseudo-code and gives them “a means of including assertions such as preconditions,&nbsp;postconditions, and loop invariants within the program.”, Brookshear, (2011). Thus allowing more proof of accuracy and correctness.Perhaps more research need to be looked into developing applications such as this one. Thus allowing ease of testing and better assertiveness. Therefore less time will be spent by the programmers trying to come up with the appropriate testing scenarios for their algorithms. Thereby aiding in producing more efficient algorithms.References:Brookshear, J.G. (2011) Computer science: An overview. 11th ed. Pages 230 and Pages 223-230 &nbsp;Boston: Pearson Education / Addison-Wesley.&nbsp;Computer Structures — Lecture Notes, Week 7: Algorithms [Online]. Available at:&nbsp;https://elearning.uol.ohecampus.com/bbcswebdav/institution/UKL1/201480_SEPTEMBER/MS_CKIT/CKIT_501/readings/UKL1_CKIT_501_Week07_LectureNotes.pdf (Accessed on October 25, 2014) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
	              
	              
Option 1 Analysis
For testing and verification purposes it was decided that the values presented in the requirement will be used in the analysis and development of the pseudo-code. Thus it was apparent that the following had to be done


Store the string, variable created as string astring


Rearrange the values in the string, producing all possible sub values.


Store the sub-values in a new string, variable created as string bstring


Create another variable to perform the computation on the string, variable of type integer num


Check all sub values to see if it is divisible by 3, that is sub-value mod 3 produces an output of zero.


If sub-value mod 3 produces an output of 0, print sub-value.


From looking at what is required, we can see that there will be several rearranging of values and computation will have to be performed on each rearranged value. Because there would be one specific computation that needs to be performed and one type of rearrangement method involved, it is suggested that we use an iterative structure in developing the algorithm be used. This structure include for and while statements. Here a series of statements are executed in a looping manner. Other conditional statements will also need to be included.
Based on the requirements it was decided that a series of variables needed to be created.
These were as follows:


Variable to store the initial string to be searched, which would be of type string; astring


Variable to capture substrings created, which would be of type string; bstring


Two variable for counting purposes, which would be of type integer; i and k.


Variable to compute the of type Integer, num. 



3. Deliverables
 A. Search a string of at least five numbers (for example, 37540)
 B. Identify all of the substrings that form numbers that are divisible by 3.
 C. For example, applying the algorithm on the string 37540 should produce the fol low ing substrings&nbsp;(not necessarily in this order): 0; 3; 75; 54; 375; 540.

Thus the below pseudo-code is suggested. 
astring ←37540
i←0
j←0 
&nbsp; &nbsp; &nbsp; for (i = 0 to i&lt;=length of astring)
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; [
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;for (j = 0; j&lt;=length of astring;) 
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; [&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; bstring ←astring.substring(i, j);
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; number ← Convert to Interger(bstring);
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; if (number % 3 == 0) 
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Print the value of bstring
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;j←j+1 
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ]
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; i←i+1 
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;] 
 
There are initialization states e.g. i←0 there are also test stages if (number % 3 == 0) and there is the termination stage as the program will loop until i&lt;=length of astring.

1) Set astring to 37540
2) Set bstring to 0
3) Set I to 0
4) Set J to 0
5) for (i=0 to i&lt;=5 (length of astring)) [
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; A) for (j =0 to j&lt;=5 (length of astring)) [
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 1. bstring ←astring.substring(i, j); (This rearranges the values in astring )
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 2. number ← Convert to Interger(bstring);
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 3. if (number % 3 == 0) 
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;a. Print the value of bstring
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 4. j←j+1 ]
6) i←i+1 ] 
At the first instance i is set to 0 and it goes through the for loop and executes the inner for loop. For j  less than or equal to 5 it loops the contents this loop and will continue to do so incrementing j by one at the end of each loop until j reaches 5. Then it will increment the value of i by one and start looking process all over again until i reaches 5. At each loop of the initial for statement the inner for statement is looped at a maximum of 5 times. 
The statement bstring ← astring.substring(i, j) rearranges the values in the initial string using positions 0-5, thus positions 0, 1, 2,3 and 4 represents 3,7,5,4,0 respectively. The first position i indicates the position that will be taken into consideration while the second position j indicates the value that will not be taken into consideration. Thus (0,3) would be 375, hence all from position 3 onwards will not be taken into consideration, hence numbers 4 and 0 are not represented.

References:
Brookshear, J.G. (2011)&nbsp;Computer science: An overview.&nbsp;11th ed. Pages 192 and Pages 195 Boston: Pearson Education / Addison-Wesley. 

Computer Structures — Lecture Notes, Week 7: Algorithms [Online]. Available at: 
https://elearning.uol.ohecampus.com/bbcswebdav/institution/UKL1/201480_SEPTEMBER/MS_CKIT/CKIT_501/readings/UKL1_CKIT_501_Week07_LectureNotes.pdf (Accessed on October 25, 2014)
  



	              
	              Updated pseudo-code with minor adjustments
Option 2: Generate pseudo-code for word search algorithm

Make a list of five words, 3-6 letters in length.
Create a string of approximately 30 letters containing some of the five words.
Your algorithm should identify all of the substrings of the longer string that match any of the five words you generated, and the number of times each one appears
For example: If you chose the words&nbsp;structure,&nbsp;such, system, blue, red,&nbsp;and your algorithm operates on the string&nbsp;jkdistructuredstrusyssystemoon,&nbsp;your algorithm should report that the string contains the words&nbsp;structure,&nbsp;red&nbsp;and&nbsp;system&nbsp;each one time, and the words&nbsp;such&nbsp;and&nbsp;blue&nbsp;zero times.

&nbsp;
Start
fword ← “structure”
sword ← “such”
tword ← “system”
foword ← “blue”
fiword ← “red”
List&nbsp;← fword, sword, tword, foword, fiword&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
String ← “jkdistructuredstrusysstemoon”
fcount ← 0; scount ← 0; tcount ← 0; focount ← 0; ficount ← 0
procedure Search (String, List)
If (List empty)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;then
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (Declare search a failure)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; else
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (Select the first entry in list to be TestEntry)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; while (String ≠ TestEntry and there remain entries to be considered)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; do (Select the next entry in List as TestEntry);
If (fword = String)
&nbsp;then fcount=fword ← 1
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; else fcount = fword ← 0

&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;If (sword = String)
&nbsp;then scount=sword ← 1
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; else scount = sword ← 0

&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;If (tword = String)
&nbsp;then tcount=tword ← 1
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; else tcount = tword ← 0

&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;If (foword = String)
&nbsp;then focount=foword ← 1
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; else focount = foword ← 0

&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;If (fiword = String)
&nbsp;then ficount=fiword ← 1
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; else ficount = fiword ← 0
)end if
&nbsp;
Print the values Structure='fcount'; Such='scount'; System='tcount'; Blue='focount'; Red='ficount';
&nbsp;
End
&nbsp;
&nbsp;References:
&nbsp;Brookshear, J. G. Computer Science: An Overview XML Vital Source ebook for Laureate Education, 11th Edition. Pearson Learning Solutions.

	              
	              Hi Craig,
Bill Gates said:

“I choose a lazy person to do a hard job. Because a lazy person will find an easy way to do it.”

So in that sense lazyness maximises the ROI. But if you take lazyness as irresponsibility, then it's a totally different story.
Leonardo da Vinci said:

"Simplicity is the ultimate sophistication."

Keeping things simple is very hard.
Let me ask you this question: In our algorithm creation exercise option 2, we are asked to write an algorithm to count how many times some words appear in a string of about 30 charactors. Which algorithm is better, the simple and stupid solution or the highly optimzed solution with AMP algorithm?
br, Terry
	              
	              Hi Tanisha,
A couple of comments:
In you code &nbsp;for (i=0 to i&lt;=length of astring) the "&lt;=" need to be "&lt;". You count for 0, so the last index in the string is n-1. This problem is so common that it even has its own name, called Off-by-one error.
In you inner loop, j should not start from 0 but probably from i.
One minor issue is the name astring and bstring could be something more meaningful. You might want to take a look at the sidebar "Naming Items in Programs" on page 197 of our text book (Computer Sicence Overiew)
Hopefully, you find my comments useful:-)
br, Terry

References:
Off-by-one error. (2014, October 10). In&nbsp;Wikipedia, The Free Encyclopedia. Retrieved 06:20, October 28, 2014, from&nbsp;http://en.wikipedia.org/w/index.php?title=Off-by-one_error&amp;oldid=629068638
	              
	              Hi Ricardo,
One of the useful algorithm design principle is DRY, which means Don't Repeat Yourself:-)
br, Terry
Don't repeat yourself. (2014, September 23). In&nbsp;Wikipedia, The Free Encyclopedia. Retrieved 06:43, October 28, 2014, from&nbsp;http://en.wikipedia.org/w/index.php?title=Don%27t_repeat_yourself&amp;oldid=626734680
	              
	              Hi Craig,

Nice post…..when designing software at a system level, performance issues should always be considered from the beginning. A good software developer will do this automatically, having developed a feel for where performance issues will cause problems. But an inexperienced developer will not bother; misguidedly believing that a bit of fine tuning at a later stage will fix any problems.

Best Wishes, Babatunde.
	              
	              
	              
	              Hi Terry,
Thanks for your reply as well,
I've worked on systems where the architects adhered to "Hoare's Dictum". All goes well until realistic large-scale testing is performed and it becomes painfully obvious that the system is never going to scale upwards. Unfortunately by then it can be very difficult to fix the problem without a large amount of re-design and re-coding.
If Performance issues were considered from the beginning, there shouldn’t be any problem or what do you think?
Best Wishes, Babatunde
	              
	              Hi Babatunde, As I said, I agree there are some (very few though) design decisions about performance you have to make up-front before most of the other work. So it's not black and white. Would you agree that the process of making software system (or designing any new system) is the process of learning? As we develop the system, our knowledge about the system keep increasing. We start to realize things we don't know and we got feedback from the implementation of our idea. So our knowledge keep increasing as we making the system. Let's say we are not completely stupid at the very beginning (we at least have some idea about the product we are going to make), then our knowledge keep increasing as we make the product. Until the end, we know everything about how to make the product -- because we've made it. Our learning curve will be like this:   My question is: when is a better moment to make a design decision about performance, at point A or at point B? br, Terry
	              
	              Hi Bo,

Thanks for your reply.

I noticed your comment in last week 7 posts that’s why I have a revised version here.
The extra line of code ensures that if there is a case of two highest votes, there would be a message to that effect and the process would repeat.

Best Wishes, Babatunde
	              
	              Hi Colleagues,
Nice posts...
How about as a final thought, make your process repeatable. A disciplined team overcomes new obstacles and builds success by creating a predictable environment and controlling the amount of uncertainty in a project. When you fail, learn from your mistakes – don’t hide from them. Establish patterns and reuse proven techniques. Build and maintain the teams and practices you need to win consistently.
At the end you could modify the statement "Make it work, make it right, make it fast" then add “Make it Repeatable”
Best Wishes, Babatunde
	              
	              Hi Everyone,Do you have any suggestions for the types of metrics that could be used to assess and compare algorithms, quantitatively or qualitatively?Anthony
	              
	              Hi Babatunde,
I think repeatable is a good thing, and the team thing you shared is also good. I just could not link them with designing an algorithm. "it" here doesn't mean the practice but the deliverable. In our context, it's the algorithm we are designing.
I guess it will make a good topic for week 10 or 11...
br, Terry
	              
	              Hi Anthony,

Since in computer programming, optimization is the practice which generally consists to reduce the execution time of a function, the space occupied by the data and program into the memory, or energy consumption.
This step usually come at the end, when we finished writing code and successfully testing it and ensure that the program works correctly, i.e. produces the expected result. Because do this before is generally considered as a waste of time.
In this regard Donald Knuth said, "We should forget about small local optimizations, say, 97% of the time: premature optimization is the root of all evils.”
One thing is true for the program to work and perform the functions for which it was created, generally the trend is to stop by, especially if you have already reached or exceeded the limits of time allowed to the project.
We promptly implement the non-optimal program by saying that we will definitely improve with future releases.
Regards,
Tresor
	              
	              Hi Terry,
DRY principle, I like it&nbsp;J
 
 
Regards,
Tresor
 


	              
	              Hi Tanisha
Can you just help me understand please, by specifically pointing out exactly where your algorithm pseudo-code find the substring of the string; just for example, finding 54 within the string 37540?
This is a massive learning curve for me, so really keen to leverage your knowledge.
Best Wishes, Craig
	              
	              Hi Terry&nbsp;
Yeah, it’s an interesting quote from Bill Gates – he is an amazing and fantastic pioneer of recent times, however it doesn’t convince me that lazyness translates into maximising ROI.
In my experience (granted nothing like Bills), when you have people who are disinclined to work (lazyness), this has major broader implications. As you have alluded, irresponsibility is another question - I believe that lazyness is a form of irresponsibility and leads to complacency, and complacency leads to sub-standards. Sub-standards, in my experience, generally lead to a negative ROI.
In my opinion, lazyness does not translate into efficiency. Efficiency translates into efficiency. So, I am not convinced about the accuracy of the quote.
However, I most definitely subscribe to Leonardo’s quote, and for me, if you are able to simplify it, you are the master of your subject.
I suspect that is perhaps why you may have been yawning on my algorithm quality initial response J, because it wasn’t simple or sophisticated – the fact is, I am not a master of this subject and I guess this is where the over engineering comes from, because of my lack of understanding. This is exactly why I am doing this programme.
In answer to your question, I would be looking for a simplified and optimised solution. I wouldn’t want a stupid solution.
Can you give me your perspective on AMP algorithm please? It appears to be a reconstruction optimisation function from sparsity undersampling tradeoffs? I am struiggling to comprehend this, are you able to provide a simple explanation in layman's terms?
Best Wishes, Craig
	              
	              Hi Craig,

do you want me to explain how the substring works ? or is it specifically in tanisha's code that you want her to explain.?

I did the same option you see

Chris

	              
	              Hi Terry

You raise an interesting question at the end there:
"When is it a better moment to make design decisions about performance.?" 

For me the answer is BOTH
Let me explain.
If you know of performance improving or performance limiting options or choices at the start then they should be decided upon then, at the start and not ignored. However you shouldn't linger too long on it at the start at the potential hindrance of the project overall. I have literally known of projects that have spent months to years discussing the best way to do something only for it to then be out of date by the time they go to implement it.

The main aim is to deliver what is needed for the solution first. Then if you have the time and ideally during system testing and UAT testing you would address any areas of improvement ongoing from there. you'd then go through rigourous performance testing as well and penetration testing if needed.
Obviously you may then say well it's better at the start because if you follow some kind of waterfall approach to designing a system then everyone knows if you get to the end and then need to make a large structural change you then have to go back to the start and the cost / time would be exponentially larger.
But I find in todays market and in my experience nobody works the waterfall approach and more and more are using agile methods to development.
Obviously this is all down to personal opinion of course and as such there probably isn't a complete right or wrong answer but more areas of known grays due to experience.
I should add that it also really does depend on what the job or project is. Some situation may need massive up front decision making to be done with even continual checking of best practice during the development lifecycle where others may be small projects or fixes where performance may not be the overall issue but rather getting it done will be.

Ta
Chris




	              
	              I also agree with Tanisha here

Good job Kharavela, it is easy to follow and reads well and has not gone into an over engineering of the problem.
I was trying to do the same with my report this week but I just can't help myself and I always write more due to a fear of not explaining things enough.

ta
Chris

	              
	              Hi Terry,

Concerning your solution to Option 1, its structure is rather too advanced to be a pseudo-code.

Even though there's no specific way of writing pseudocode, your solution has very close resemblance to an already established programming language.

I am interested in knowing what language you used.


Best Regards, Babatunde
	              
	              Hi once again Terry,


I also noticed something about your code. It runs the risk of encountering an ArrayIndexOutOfBoundException

First, the outter loop runs from 0 upto the length of the string… That means, during the last iteration for that loop an index that is +1 greater than the maximum index in the string array would be encountered i.e. arrays are indexed from 0,1,...n-1

Where n is the number of elements.

A possible correction would mean that the outter loop would be:

For begin in range (0, len(numberstring)-1):

Same goes for the inner loop, It would encounter the same problem.

A correct form would've been

For end in range (begin+1, len(numberstring)-1):

And lastly, I think your approach appears to be bruteforce since it doesn't provide a way of checking if a particular number sequence had been tested in a prior iteration of the loop


These are my observations….I hope you would clear me out as i am not good at programming.

Best Wishes, Babatunde
	              
	              
	              
	              Hi Chris,
Please do go on with your explanation as it may be in different view as what Tanisha might explain.
we are all learning, Thanks
br, Babatunde
	              
	              Hi Tresor
I think you have, perhaps unwittingly, touched on a really important point, namely the conflicting priorities between two discrete IT functions.
Anthony raises a good question, and it is really interesting to read some of the responses so far, particularly referencing the point at which consideration should be given for the performance of the algorithm or program.
If we take the business world, traditionally we have development and operations as separate functions within IT. 
Although development and operations are essentially two sides of the same coin, the integration between both functions is, surprisingly, very poor in many businesses. Both functions have differing priorities and operate under different constraints – as examples, development functions are challenged to get the code out, and operational functions are challenged with keeping it running. Equally, the line-of-business functions are also ‘somewhat removed’ from the process.
I typically come from an operations environment, and can confirm that we spend an enormous amount of effort and time looking at how we can optimise operations, or business processes, or user experience, and unfortunately, in more cases than I believe should be acceptable, this is because we have sub standard and sub optimal programs. 
Yet there are some suggestions that we should perhaps forget about optimisation at a program level?
I realise there are different scales here, from small to incredibly large algorithms or programs, however surely we should be looking to adopt development practices that comply with certain standards? 
Typically, the feeling has been that the development teams just throw stuff over the fence and operations can worry about it later. We have seen this evolve over time, and for example we have seen testing being a joined-up exercise, but it hasn’t resolved it completely. 
Over recent years, we have seen the emergence of the DevOps concept, in which software development and service operations are much more integrated and consideration for the whole organisation is given. This promotes strong collaborations and working relationships, of which the intention is to deliver and deploy software much quicker, but equally with lower failure rates, better quality with improved performance and reliability. 
This is achieved in essence by ‘gluing’ the respective priorities together and through strong teamwork, there is consideration for priorities equally. 
To your closing point of improving non-optimal programs in future releases, I believe DevOps will further emphasise the need for optimisation to be continually considered throughout the development lifecycle and not just at the end, thus reducing the impact and implication of any degradation on service operations and the business as a whole, thus maximising return on investment.
Best Wishes, Craig
	              
	              Hi Chris,
Ah, thank you. Both please?
This is the bit I have been struggling with and whilst Augusto has tried to explain the recursive loops, which I believe I now understand when searching for 1 integer (and the next, and the next) within a string, I am struggling to see how it works when trying to get multiple integers within a string i.e. 54 from 37540, or 754 from 37540, or 7540...etc
I can see it is in relation to assigning a max length but just cant figure out how it works. I guess this is were I would prefer a "taught" component within the programme, rather than it being a "research" or "read" one.
Thanks for any help
Best Wishes, Craig

	              
	              Hi collegues,


Interesting question and nice posts,


For me, having access to direct feedback and the ability to understand the performance of our designs in detail allows us to test our assumptions, and there really is no good excuse for still poor-performance. Importantly, getting performance feedback during early design stages not only helps programmers/designers to make any specific program/design perform better, but it also helps us to develop a better intuition for asking the right metrics and making the right decisions. This approach revolutionizes the way we design for performance.


Performance based design means that we can understand the software system we design, it means that we can understand the relative impact of different strategies, and even more than that, how these strategies affect each other.


Best Regards, Babatunde
	              
	              Hello Ramin,
Appreciate your code. It is very effective for finding a word in a string. How would this look when you have to find 5 words? Is it a simple recursive loop or is more needed?
Another question would be how it will find multiple instances of words when you have a string with a repetition of words in it. Following your example there is not repeating occurrence and therefore it is perhaps not needed, but still. 
Greetings from Bram

	              
	              Hello Terry,
I appreciate your comment and I know I can overcomplicate the things for the exmaple used. But following my string and search words I have to check for double occurences and shift through the wole string. I do not see how I can do it more simple than I did in my first solution using at least the KMP-algorithm.
Greetings from Bram
	              
	              Hello Ramin,
Thank you for this summary. It is realy helpfull and I will try BLUEC and SPATIAL the nex time.
Greetings from Bram
	              
	              Hi Terry,
Thanks for note. As for now this is the only way I understand to comfortably represent the logical steps of the option 2 algorithm in pseudo-code. I am still trying to get option 2 represented using recursive structures but I'm not quite there as yet.&nbsp;
I've been reading the threads, trying to trace and follow some of the responses, especially the ones that you have submitted which I'm still not able to trace successfully. So I know I still have a lot more to cover. I realise that some of the colleagues here use different forms of representation for there pseudo-code, I'm assuming base on specific programming languages of which I have no idea.&nbsp;
I'll try and get the recursive version of my pseudo-code done and posted which will be much more condense and then you can give me some suggestions and possible explain what I doing wrong.
Thanks again for the feedback.

Regards,
Ricardo
	              
	              
 
Hi Terry,
 
Agreed! You normally build the elevator with the building so engineers account for the proper mobility of people, including, but not limited to an elevator. But I think that the main purpose of algorithm research is to find the most generic, effective/efficient algorithm overall. It may seem utopic, or irrelevant for an elevator, but there are more serious problems out there that deserve that type of research. I choose the elevator example because of it being non trivial and at the same time illustrate the many possibilities that there could be for it.
 
On the other side when I think about algorithms needed to move the Mars rovers, I think it is imperative to make the most efficient ones, because those that seem efficient enough or non-trivial on earth may not be valid on any other planet. We want them to be used on any planet so that the research is cost effective over the years. For example: if my drone stops or get stuck, I just walk to it and move/fix it. But going to Mars to move a rover is slightly more complicated. Just slightly. :-)
 
Best Regards,
 
Augusto
 

	              
	              Hi craig and Terry,
wonderful posts from both of you and really insightful but i would go with craig on this. Laziness cannot translate into ROI rather laziness breeds complacency.
Bill Gates may have had an interesting quote but that, to me is his own ideology. some other guy may be using a hardworking guy to achieve an average result. Having to use a lazy person in a job role would breed substandards and in turn complacency. Like craig mentioned, laziness cannot translate into efficiency, efficiency translates into efficiency.
thanks
martins
	              
	              Hi&nbsp;
QA:&nbsp;I agree. But form my many testing experiences I have often&nbsp;(many times)&nbsp;experienced that the programmers did not test the "know" properly. Sometimes because the requirements weren't described good enough, sometimes they mis-interpreted, and other times just forgot or didn't see the errors. E.g. they know that a certain field should contain an integer thus they only test integers and not letters&nbsp;(which some users will type by some reason). So only together, back and forth, will the best results be reached.
Pseudo code:&nbsp;Regarding the pseudo code versus real code I do understand you. If you would have to every time to do both pseudo and real code, the I think it would get boring and time consuming. It's just that sometimes the people working together need a common ground to stand on, and real code can be a totally strange language (Esperanto) too&nbsp;them. But I have also see code which even I can understand the meaning/principle of.
Flowcharts: Yes, these are best on a white board. But sometimes they are almost static, and then it can be good to look at again. And also for a new programmer who replaces the former can have great use of this - or at least that's what I have been told.
BTW, your second link was great "fun" to read, and I do understand his point.
Bo
	              
	              Hi Craig, ok cool. I'll try to explain as best and clear as I can It's basically a loop within a loop first you calculate the string total length so for this example "37540" has a string length of 5 you set 2 base integers for the loops example X and Y .. X = 0 and Y = 1 X is the first loop and what is classed as the string starting position Y is the count from that point into the existing string How you get the substring is based on the starting point and then the number of integers to count too or in essence a start and end.   SO example a start of 0 and end of 1 would return "3" a start of 0 and end of 2 would return "37" a start of 0 and end of 3 would return "375" It is shown as a command procedure like this&nbsp;&nbsp;&nbsp; --- subString(X,Y,value) where value is "37540" so if substring(2,2,"37540") would return 54 as is starts after integer 2 and then ends after 2 more look at this image I produced for my report   You can see from that the position of the starting loop via the black line and the X value. This is the equivalent outer loop.&nbsp; So if X is 4 then the substring starts at position 4 after the 4th integer and only loops once because the total is 5 and returns "0" After each loop you return or save the integer or in this case check to see if it is divisible by 3 first then save if it is or continue if it is not The Y value then always starts again as 1 and it always loops until X + Y is the string length total. So the code would look something like this:   Value = 37540 X = 0 Y = 1 Loop until X = 5&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; //outer loop &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; loop until X + Y = 5&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp; // inner loop until start and end position adds up to 5&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; subString (X, Y, Value)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // get the substring value from start position and end position of the value &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Y = Y + 1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // increment the end position &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // loop to substring again X = X + 1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // increment the start position Y = 1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // reset the end positon back to 1 return&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // return to loop &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; So this is just the loops to get the substring values and in operation it would look like this: X = 0, Y = 1, substring = 3  X = 0, Y = 2, substring = 37  X = 0, Y = 3, substring = 375  X = 0, Y = 4, substring = 3754  X = 0, Y = 5, substring = 37540  X = 1, Y = 1, substring = 7  X = 1, Y = 2, substring = 75  X = 1, Y = 3, substring = 754  X = 1, Y = 4, substring = 7540  X = 2, Y = 1, substring = 5  X = 2, Y = 2, substring = 54  X = 2, Y = 3, substring = 540  X = 3, Y = 1, substring = 4  X = 3, Y = 2, substring = 40  X = 4, Y = 1, substring = 0     Obviously for this assignment you'd then add in the calculation to see if it is divisible of 3 etc but for what you asked about how it gets the substrings and the loop within the loop thats how it acts.   I hope this helps to explain it a bit better.   Something to note though is that different programming languages do it slightly differently to the above , some use absolute positions in the string rather than counting an example is like this: http://www.w3schools.com/jsref/jsref_substring.asp but in essence they all work by taking a start and end position. The thing about this assignment is because it was pseudocode it's assumed in reading Tanisha's for example that you knew how substrings work so I completely agree with you as it's not a taught part of the course to a none programmer or just someone who hasnt come across them before in coding it would be difficult to get your head around.   I hope i've helped in some way   ta Chris   
	              
	               
Hi Terry and Craig,
 
Interesting discussion! I agree and disagree with both of you.
 
I think that we need to separate what science aim to achieve and what a commercial venue expects to achieve. Computer Science aims at having all algorithms to be high efficient/effective: minimize resource consumption/usage. Science, overall, aims to discover the best most precise knowledge, computer science is no different.
 
So an 80/20 value could be high efficient for a particular system(s), but not for others. Take windows for example: have you ever bought a 100% efficient/effective, bug free, copy of Windows. Ever? That is the 80/20 rule that I think Terry is referring to. Windows does not need to be a 100% efficient set of algorithms to be sold. A commercial company aims at making money, if they can make money out of a product working at 10%, they will do it.
 
Say there is an algorithm to fly a spacecraft from one planet to another. Computer science (or just science), cannot be happy with only 80/20. That algorithm may simply make the spacecraft stop somewhere in between the planets because it runs out of fuel, and it is not like you can stop somewhere to fuel up. Our cars may run at 80/20 efficacy, but something in the vastness of space cannot afford this. I guess one of the reasons space exploration cost so much is because the research done aim at being highly effective.
 
For these kinds of things, algorithms must be extremely efficient. When we look at software, we are used to our little gadgets at home, and our corporate systems that are used to pay our salary. But those are systems that concern us relatively little (one day late paycheck due to performance issues is annoying but not the end of the world). However, there are a lot of algorithms out there that are unrelated to this kind of software, some are used in life supporting systems and some in scientific research related to atomic fission, others in systems that handle energy distribution in cities. Not all algorithms are equal in importance, but nevertheless that is not an excuse not to try to find the best one or the most efficient one, especially if you are in the scientific domain.
 
Best Regards,
 
Augusto
 
	              
	              sorry.. my slight code mistake... god i wish we could edit your own posts on here... mistake highlighted
value = 37540X = 0Y = 1Loop until X = 5&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //outer loop&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; loop until X + Y &gt; 5&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // inner loop until start and end position greater than 5 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; subString (X, Y, Value)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // get the substring value from start position and end position of the value&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Y = Y + 1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // increment the end position&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // loop to substring againX = X + 1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // increment the start positionY = 1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // reset the end positon back to 1return&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // return to loop
	              
	              Hi Chris,
Before your inner loop you need to reset the value of Y.
br, Terry
	              
	              Hi Terry,
thanks but this should be ok as I reset it at the end where I say Y = 1 but you could have it at the start or end I guess.
	              
	              Hi Chris,
Thanks for answering the question. I don't think I can answer my question better than you did. Thanks.
This might set the ground for answering Babatunde's original question:
"If Performance issues were considered from the beginning, there shouldn’t be any problem or what do you think?"
Yes, performance issues need to be considered all the time including the beginning. But only considering it in the beginning doesn't solve all (not even most) of the problems. Because we have the least knowledge about how to make the product at the beginning. We are more likely to make a bad decision at the beginning.
What if the code cannot be improved later? That's why I cited that we need to first make it work, than make it right, and than, if needed, make it fast. The step of "make it right" make sure the code is changeable.
br, Terry
	              
	              H Babatunde,
It's Python. I used Python mainly because the editor I use is IPython notebook. I use it to write my assignments because it can embed Python code and run it. Very convinient for writing computer science papers.
I'm really ashamed of the clarity of my solution of option 1:( You are not the only one complaining about it. I think I should have done better than this.
br, Terry
	              
	              Hi Babatunde,
The code is not clear to you because I depended too much on Python syntax. I should have had better abstraction.
No, it doesn't have the risk of&nbsp;ArrayIndexOutOfBoundException. I've tested it, as you can see the test cases following my solution. The method range(0,5) will yield 0,1,2,3,4. That's exactly why I don't use pseudo code. Using a real programming langauge avoid&nbsp;ambiguity.
Yes, it is brutefoce approach. But it doesn't have the duplicates as you suggested, the sequence is like 1, 12,123,1234,12345, and than 2,23,24,25, than 3,34,35, than 4,45 than 5. I don't know if there's a more efficient way of doing it. And I shouldn't even care as I'm a "lazy" programmer. As long as my solution looks simple and fulfill the needs, I'm happy with it.&nbsp;
br, Terry
	              
	              Did you, oh you did. The indentation in the post is messed up a bit. But given your intent there shouldn't be any ambiguty.
Good, I was wrong.
	              
	              You Welcome Bram
	              
	              Thank you so much for this Chris - I'm going to work through it tomorrow - I really appreciate your help.
Best Wishes, Craig
	              
	              Hi Chris
I couldn't resist not going through it quickly
This is really illuminating.&nbsp;There are two key parts to this that have already triggered a eureka moment, which are;
loop until x+y=5 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; This is the piece which I have struggled to figure out!!!
substring (x,y,value) &nbsp; &nbsp; &nbsp; &nbsp; This is the other key piece. I need to just re read this to see how x stays at 1 whilst y &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;moves through 2, 3, 4 etc.. and then how the loop then moves x to 2, and then moves y &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;through 3, 4, 5 etc...
Thanks Chris
Best Wishes, Craig
	              
	              Hi Craig,
Unfortunately I'm with you on this one. I only did a little programming back in high school as an introduction into computing. Throughout my career I have not been involved in any software development that would require applicable knowledge of programming techniques. I did some C# and C++ programming back in college but nothing comprehensive.&nbsp;
What I do recall from those days was that more complex structures were easier to implement in the object oriented C++ environment, easier to compile and to identify errors within the code. As for Borland C#, the language seemed to manage simple tasks better than complex ones and as such development favoured the C++ language which I gather was further developed into the now .net framework by Microsoft which provides a comprehensive and consistent model with built in standard libraries for building todays generation of applications.

Regards,
Ricardo

	              
	              Hey Terry,
I agree with Belinda, the elephant algorithm is an excellent example of simple. I also like the Kent Beck three steps for designing software,&nbsp;"•&nbsp;Make it work&nbsp;•&nbsp;Make it right&nbsp;•&nbsp;Make it fast" rule, I'm sure I can use this in various areas of my job.
Regards,
Ricardo
	              
	              Hi Bram,
Yes, I think simple is a lot harder than complex.
br, Terry
	              
	              Hello Dr. Anthony,As more and more programming languages are developed they are becoming a bit easier to use and understand as the syntax resembles that of English statements, an example being, Python, which focuses on readability.&nbsp;With the exception of the Scratch program, I have not been exposed any other programming languages that can be used to express algorithms.&nbsp;My experience of developing an algorithm was by simply looking at the problem and trying to come up with a suitable algorithm to solve it. In addition to this I recall much difficulty in try to represent the algorithm using a flow chart as opposed to simply writing the algorithm, particularly if the algorithm was long. It was also difficult to interpret these diagrams, if large, as it may have represented other algorithms. This issue has been highlighted in the text.Nevertheless, an excellent example of using a program to write an algorithm is that of using the scratch software. As you simply think through the steps involved in the algorithm and by dragging and dropping the appropriate English like commands where necessary, you can get the desired outcome.&nbsp;References:
Brookshear, J.G. (2011) Computer science: An overview. 11th ed. Pages 269 and Pages 223-230 &nbsp;Boston: Pearson Education / Addison-Wesley.&nbsp;MIT (n.d.) Getting started with Scratch [Online]. Available from: http://info.scratch.mit.edu/Support/Get_Started,&nbsp;(Accessed: 27 April 2014). &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;


	              
	              Hi Craig,
I already regret to use to word "lazy" for describing good things. I also regret to use the word "stupid" for good thing (that was from the KISS principle, Keep It Simple &amp; Stupid). I will carefully choose positive words for good things later:-)
I would suggest you to leave KMP algorithm alone and focus on essential things. Essential things are also very important part that composes the system, not just interesting and difficult parts of the system.
br, Terry
	              
	              Hi Augusto,
I also like Mars rovers and spaceship. But I feel a bit uneasy if we use them as examples for software system all the time. I feel that move our attention away from essential things.
The 20/80 rule is the same 20/80 rule in critical system.
Sorry to say it, but I don't think you have any reasoning for "but nevertheless that is not an excuse not to try to find the best one or the most efficient one, especially if you are in the scientific domain". There are just too many excuses for not doing it.
br, Terry
	              
	              Hi Babatunde,
I like that, it reminds me of our work processes here. We have employed the &nbsp;agile methodology in our work processes and we need to be predictable in the amount of work we take on every sprint as it enables the stakeholders to plan ahead and estimate product release dates. It takes a big person to acknowledge failure &nbsp;and in turn learn from it. I'll definitely be sharing your statements with my team.
Thanks.
kind regards,
Belinda
	              
	              Hi Terry and Tresor,
I've heard about Cyclomatic Compexity in software testing.
Basically it is a source code complexity measurement that is correlated to the number of coding errors. It's calculated by developing a Control Flow Graph of the code that measures the number of linearly-independent paths through a progrma module.
The theory is that if you lower the program's cyclomatic complexity , you lower the risk to modify and make it easier to undersand.
&nbsp;It can be represented using the below formula:

Cyclomatic complexity = E - N + P 
where,
  E = number of edges in the flow graph.
  N = number of nodes in the flow graph.
  P = number of nodes that have exit points

Example :
IF A = 10 THEN 
 IF B &gt; C THEN 
   A = B
 ELSE
   A = C
 ENDIF
ENDIF
Print A
Print B
Print C
FlowGraph:

 The Cyclomatic complexity is calculated using the above control flow diagram that shows seven nodes(shapes) and eight edges (lines), hence the cyclomatic complexity is 8 - 7 + 2 = 3
	              
	              Hi Class,Have you worked with any algorithms that were designed or optimized for use on parallel-processing computers with multiple processors? If so, were there any special requirements for these algorithms?&nbsp;Also, was the parallelism initiated programmatically (that is, within the program code), or was this done at operating system or processor level?Anthony
	              
	              Hello Class,As our usual way of rounding up the week's activity, please provide a short summary of the key 'Computer Structures' lessons learnt in week 8, from your perspective.Anthony
	              
	              Thats ok Terry, i'm glad you checked it because it made me look at it again to make sure I had added the reset into it.
always good to confirm each others works.
thanks
Chris

	              
	              Hi Craig,

That's great, I hope I helped your understanding a little with the explanation.
I'm sure there will be many a time in the forthcoming weeks and months where I need some clarification on other subjects that I don't understand so well.
so it's good to help.
If you do have any more questions about this specific looped substring feel free to ask

Chris

	              
	              Thanks for this Belinda
How do you tell which nodes have exit points? Which are the 2?
Best Wishes, Craig
	              
	              Hi Anthony
I have no real specific experience of algorithms or coding previously however, I have come across function point analysis. I am not too sure whether this is a valid classification of assessment for comparison of algorithms, either quantitatively or qualitatively however, it&nbsp;was one of the methods that I have come across to help estimate the size of software development, to give an estimation during the analysis and design phases of the costs and effort associated in the delivery of new solutions.
Best Wishes, Craig
	              
	              Hi Craig,
I would say Function Point is one of the measurement for algorithms in the field. Similar measurement like story point.&nbsp;They are relative estimation about the size of the effort for producing the algorithm.
br, Terry
	              
	              Dr Ayoola,
One interesting&nbsp;phenomenon I came across in parallel-processing, is people's opinion about the global state in programming.
In the beginning I learned that global state is important for parallel-processing, because the processes need to communicate with each other using the global states. The global states are often implementation in the Singleton Design Pattern.Therefore, we also need other algorithms like semaphore to ensure the mutual exclusive access to the global state.
As I learned functional programming, e.g. the MapReduce Algorithm (Dean, 2008, OMG, I cannot believe that I'm citing Jeff Dean!), I realize global state is not necessary for parallel-processing. If the function can run without any side-effect (means not depending on or changing any global state), it can be assigned to any process or processor without needing any mutual exclusion. Therefore, it makes parallel-processing much easier. Given the quite obvious trend toward parallel-processing in the computer science, I would say functional programming, or functional programming style will be the next big thing after object-oriented programming.
(The Singleton in my post is not the kind of Singleton you can find in the duty free shops. That Singleton is always good. The Design Pattern Singleton is often bad.)
br, Terry
References:
Dean, J., &amp; Ghemawat, S. (2008). MapReduce: simplified data processing on large clusters.&nbsp;Communications of the ACM,&nbsp;51(1), 107-113.
	              
	              Hi Belinda,
Thanks for explaining the Cyclomatic Complexity. This could be a good reply to one of Dr. Ayoola's new initial questions in the thread. It's one of my favorite measurement for the code.
br, Terry
	              
	              Hi Craig
"END IF"s.
br, Terry
	              
	              Hi colleagues,
If you reached the end of my week8 paper, the correct algorithm for putting a giraffe into a refrigerator is:

open the door
take out the elephant
put in the giraffe
close the door

I hope you didn't forget the elephant in the freezer. It's ancestors extincted during the ice age. I don't think it enjoyed being in the freezing cold refrigerator for the entire week 8.
Unlike the poor elephant, I did enjoy my week8. Again, it's a familar topic for me. So content-wise, it has been easy for me. But what added most of the value to me is the practice of technical writing and communicating ideas. I notice that I have a lot to improve in these two areas. I'd like to send my&nbsp;sincere thanks to colleagues who interacted with me. I did also learn a lot of new stuff during the course of the writing and communication.
Hopefully, I can come up with something for week9 to save the poor giraffe out of the freezer soon. Maybe with a duck...
br, Terry

	              
	              Hi Anthony
I have not knowingly come across this previously, however I suspect that given the IT functions I have worked in, algorithm design for parallel processing is likely to have been a key consideration with our application architecture teams.
My reading so far suggests that algorithms that are designed for use on parallel processors must be able to easily separate the problem into a number of parallel tasks, and that they do not necessarily have any dependencies between those parallel tasks. This is known as algorithm parallelisability, and for those algorithms that are easily parallelisable, they are also known, strangely, as ‘embarrassingly parallel’. The opposite of this are inherently serial problems, which cannot be parallelised at all (Wikipedia, n.d.).&nbsp;
It is suggested by Foster (2010) that Parallel Algorithm design promotes “integrative thought” with an engineering approach that is methodical, whereby cost and performance are considered in the design, and mechanisms are in place for evaluating a range of options and alternatives.&nbsp;
Best Wishes, Craig
References
Wikipedia. (n.d.). Parallel Algorithm [Online]. Available from: http://en.wikipedia.org/wiki/Parallel_algorithm (Accessed 29 October 2014)
Foster, I. (2010). Designing Parallel Algorithms: Part 1 [Online]. Available from: http://www.drdobbs.com/parallel/designing-parallel-algorithms-part-1/223100878 (Accessed 29 October 2014)

	              
	              Thanks Terry
Yes, this is very much where my roles have moved slightly into the realms of software development, albeit minimal and in relation to outcomes (management of effort and costs with third parties).
It'll be interesting to see what other responses come through regarding metrics/measurement set for algorithms.
Best Wishes, Craig
	              
	              Thanks Terry, brilliant.
I was thinking those, but wasnt sure as they both led to another 'node'.
Cheers, Craig.

	              
	              I agree, thanks Belinda - great explanation&nbsp;
	              
	              Terry,
Let me have a go, please...? I am now a master of programming pseudo code, cyclomatic complexity assessments and parallel algorithms :-)
1. Open Door
2. Take out the Giraffe with left hand Put in the Duck with right hand
3. Close Door
No, wait... what about...
1. Open door with left hand&nbsp;Take out the Giraffe with right hand
2. Put in the Duck with right hand Close Door with left hand
Parallel processing and cyclomatic complexity at its best :-)
Best Wishes, Craig
	              
	              Hi Anthony
This week has, as week 7, been very challenging for me because of my lack of programming skills. Still I find it very interesting, and I always like and try to understand other people and subjects.
It was very interesting to see have many variations there were of the algorithms, and how some focused on solving “just” the assignment while others also tried to create error handling and user interaction.
I also found it interesting to discuss the need for pseudo code. I don’t think there will ever be just one decision on this matter; it will depend on the situation, people and skills.
And of course I also learned about the qualities of algorithms and why we need them.
PS: I hope I survieve next week :-)

Best wishes
Bo W. Mogensen

	              
	              Hi Anthony,
Another incredibly tough week, for me personally.
This week has again been incredibly challenging with having no programming experience. 
I have taken my learnings from previous weeks and I have also reviewed everyone’s responses to maximise my learning. 
I have struggled with this not including a “taught” element in the module. I have had a lot of fun though, with a big learning curve!
I have been grateful to everyone who has responded to my questions, with special thanks to Augusto and to Chris, who have used up some of their valuable time to help me with my programming.
I can see that programming could be a rewarding profession and a lot of fun in a working environment...&nbsp;if you know what you’re doing.
Best Wishes, Craig
References
Laureate Online Education. (2014). Computer Structures – Lecture Notes Week 8: Analysis of Algorithms [Online]. Available from: https://elearning.uol.ohecampus.com/bbcswebdav/institution/UKL1/201480_SEPTEMBER/MS_CKIT/CKIT_501/readings/UKL1_CKIT_501_Week08_LectureNotes.pdf (Accessed 23 October 2014)
Brookshear, J. G. (2012). Computer Science An Overview. 11th ed. Boston Massachusetts: Pearson. Addison Wesley

	              
	              Hi Chris
Ive been through your explanation a couple of times to make sure I get it, which I do. Thank you so much.
I think I would prefer the absolute string positions, even with your counting example, however once you understand which one you are adopting/using, it is fairly straight forward. &nbsp;I have now got it, thank you so much.
Can I just check, the returns - you have two returns, both are indistinguishable from each other, so my assumption is that the first return is dealing with the loop it is within, and the second return is in the outer loop and deals with that loop?

I guess these these sort of programming techniques are straight forward if you have had the basic training and I guess they stay with you as you develop and as you hone your skills.
I think that if you have a level of programming skillset already, then any new logic is probably more straight forward to learn and pick up moving forward. The trouble was that my starting point was zero, and with no real practice previously, it was difficult to just figure it out without some help.
Thanks again Chris, you have been really helpful
Best Wishes, Craig&nbsp;
	              
	              Hi class,
The past 2 weeks were very difficult for me. This week started off terrible, I struggled with algorithms and &nbsp;projecting them in pseudocode.
After going through the learning resources, the discussions and google, I think I'm &nbsp;finally getting it.
Also after I learnt about the qualities of an algorithm, I was able to then identify bad algorithms right off the bat which has given me some confidence in my understanding of &nbsp;this topic.
Thank you colleagues for your posts, @Terry thank you for your elephant algorithm that was an aha moment for me. That is really is that simple.

Best regards,
Belinda
	              
	              Hahaha Craig.
Good one!

	              
	              Hi Terry
Thanks, will do.
Best Wishes, Craig
	              
	              Hi Numan
Firstly, you made a fine and clear post. And I’m sure you have also thought about these questions:
Do you agree, that the quality ‘Effectiveness’ should also embrace ‘Correctness’? I mean, if the algorithm is not correct, then to me it is obviously not effective.
Also in par. 2.2 you only mention “length of time”. I agree, but I would also think that space is an issue here, i.e. it takes a lot of space (you reference this in your par. 3.3) then it’s not effective – agree?
Best wishes
Bo

	              
	              Dear Treson,
Your are most welcome ! I am glad that I could help.
You can further improve your algorithm by decreasing the number of loops. I notices you are using n+1 number of loops for a string of n numbers which is increasing the complexity of your algo.
Best Wishes,,,Numan Arshad.
	              
	              Who said, human cant do parallel processing :)
Best Regards,,,
Numan
	              
	              Hi Anthony and collegues
It’s my experience that it’s only natural to spend time on optimizing at the start. Some have the rule of thumb that says 80% design and 20% programming (yes I know this is very squarely put). There are several issues in these decisions, e.g.:

What’s the business case
How much time to we have
If it works, don’t fix it
Limitations in resources (memory, disk, bandwidth)
Etc.

I have several times, both as customer and deliverer, decided to release systems knowing it’s not optimal, but it’s more important for the business to get the functionality rather than wait for “perfection”.
Of course a programmer and colleagues (e.g. DBA’s) should do as good a job at the start as possible. But like someone (I think it was Terry) mentioned, it’s very much up to skills, and also knowledge (see graph that Terry showed further up in this discussion) about programs behaviour at the start and later.

Best wishes
Bo

	              
	              Hi Anthony
Following a little help from my friends, I have produced a third revision to my algorithm. Here we go…&nbsp;
BEGIN&nbsp;
String() =&nbsp;37540&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//creating string with 37540
X =&nbsp;0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;//setting counter X to 0
Y =&nbsp;1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;//setting counter Y to 1
L =&nbsp;5&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;//setting string length L to 5
T =&nbsp;3&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;//divisible by 3
FinalString()&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;//creating final string (currently empty)
IF X &lt; L&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;//first loop, outer loops. If X is less than or equal to 5
&nbsp;&nbsp;&nbsp;&nbsp; IF X + Y &lt; L&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;//second loop, if X+Y&lt;5
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;THEN Find Substring (X, Y, String)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;//finds sub string value from the start (X) and end (Y) position of string
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; IF Substring mod T = 0&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//if sub string is divisible by 3
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; THEN Include Substring in FinalString&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;//includes sub string in final string
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; END IF&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //ends substring mod IF
&nbsp;&nbsp;&nbsp;&nbsp; END IF&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //ends X+Y&lt;L IF
&nbsp;&nbsp;&nbsp;&nbsp; Y = Y + 1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //increment end (Y) by 1 each time, keeping start as X 
&nbsp;&nbsp;&nbsp;&nbsp; Return&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //loop returns to inner loop sub string (loops until X + Y &gt; L)
X = X + 1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //increments start position (X) by 1
Y = 1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;//resets end (Y) back to 1
Return&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //loop returns to outer loop
END IF&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //ends X&lt;L IF
Display FinalString&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;//displays 3; 75; 54; 375; 540
END much happier J

There may still be some small issues on the IF, THEN, END IF's, but this is significantly different from my first attempt.
Best Wishes, Craig

	              
	              Hi Terry,
I think that came out wrong. What I mean by trying to find the best and most efficient one is that, individually, we should always try to give our best. We do not need to find the most efficient, universally valid algorithm. I do not believe that it is too costly to find the best one; it is more likely that individually our best effort might be will be flawed. We are humans, and we have limits. Even Einstein didn’t find all the algorithms or the most efficient ones for the universe!
My point is that, if as an individual you do not try to give the best of you always, then you simply will produce sub-products/services/algorithms. Whatever you produce as an individual will depend on your knowledge. For example, we can see it in these discussions: you are clearly a programmer, and your algorithms are way better than those of us that are not programmers. Some of us simply do not have that knowledge and experience to produce the most efficient one compared to yours. Imagine me doing your programmer job! The result would be bad. Sadly, that happens too often.
But you mention it yourself too: it is a matter of perception. Some efficiency may not be perceived as bad (or good) depending who is looking.
But I don’t think you should feel uneasy about some “extreme” examples. Things that may happen because of atomic or space research could have an extreme impact in everyone life and computing science too, hence impacting any software.
Take a very simple example: what’s the fastest multiplication algorithm? There are many, and it is still an unsolved computing problem (Robinson 2005). If nobody would have tried to do better (and keep trying) and everyone would be happy with the status quo, then we would still be using the very first multiplication algorithm, and that has quite some repercussions in today life: our iPhones or Androids would be much, much slower to compute things. Things that may happen because of atomic or space research could have an extreme impact in everyone life and in computing science too, hence impacting any kind of software.
References
Robinson, S. 2005, "Toward an optimal algorithm for matrix multiplication", SIAM news, 38(9), pp. 1-3.
&nbsp;
	              
	              Hi Kharavela
I agree, this looks very good and understandable. Like Chris I also tend to over-write. I tried to make it simpler, but&nbsp;(honestly) because of lack of programming skills, I just couldn't figure it out.
Br. Bo
	              
	              &nbsp;
Introduction
What makes for a GOOD algorithm?

It must work. 
It must complete its task in a finite (reasonable) amount of time. 

Two or more algorithms that solve the same problem can be very different and still satisfy these two criteria. Therefore, the next step is to determine which algorithm is "best." There are generally two criteria used to determine whether one algorithm is "better" than another.

Space requirements (i.e. how much memory is needed to complete the task). 
Time requirements (i.e. how much time will it take to complete the task). 

A third criteria that should be considered is the cost of human time. That is, the time to develop and maintain the program. A clever coding trick that may improve the space and/or time requirements can often be offset by the lost of program readability that results in the increased human cost of maintaining the program. Therefore, this and the previous courses have put a great emphasis on good programming style and conventions.
&nbsp;
Algorithm qualities
&nbsp;
&nbsp;
Every algorithm should have the following five properties: 
&nbsp;

&nbsp;Finiteness: terminates after a finite number of steps 
Definiteness: rigorously and unambiguously specified 
Input: valid inputs are clearly specified 
Output: can be proved to produce the correct output given a valid input 
Effectiveness: steps are sufficiently simple and basic. 

&nbsp;
Factors - Quality of good Algorithm 

Time - To execute a program, the computer system takes some amount of time. The lesser is the time required, the better is the algorithm.
Memory - To execute a program, computer system takes some amount of memory storage. The lesser is the memory required, the better is the algorithm.
Accuracy - Multiple algorithms may provide suitable or correct solutions to a given problem, some of these may provide more accurate results than others, such algorithms may be suitable.
Sequence - The procedure of an algorithm must form in a sequence and some of the instruction of an algorithm may be repeated in number of times of until a particular condition is met.
Generality - The designed algorithm must solve isolated problem and more often algorithms are designed to handle a range of input data to meet this criteria, so the algorithms must be generalized.

&nbsp;
Ambiguity
Ambiguity is inherent to natural language. It is the concept of words having more than one meaning. However, in formal texts and situations, ambiguity often causes misconceptions and errors. In formal situations, such as in RS, ambiguity is redundant and should be avoided as much as possible. There are four types of ambiguity as following:
&nbsp;

Lexical ambiguity - When the ambiguity lies in a single word, it was called lexical ambiguity; lexical ambiguous word has more than one meaning. Hereby, ‘meaning’ refers to the definitions a lemma has in a dictionary. The words that cause lexical ambiguity are called polysemy and homonyms. Words are polysemy when they can convey multiple related meanings. All meanings have the same etymological background and are related to each other.
Referential ambiguity - Referential ambiguity takes place when a word can refer to more than one person or object. Mostly, words such as ‘these’ or ‘that’ are involved. Words that refer to another point in a text are called anaphors. Anaphors can cause ambiguity because it is not always clear to what person(s) or object(s) the anaphors refer in a text. Next to anaphors, there is a second type of expression in which a reference is made.
Scope ambiguity - When other constituents in its structural context determine the meaning of a constituent in a sentence, we say that the constituent is in the scope of the constituents that determine its reference.
Structural ambiguity - Sometimes, a whole sentence can have more than one meaning, while none of the words in the sentence are ambiguous. The ambiguity in that sentence then lies in the structure or the syntax of the sentence.

&nbsp;
Algorithm Specification
To avoid the Ambiguity in the Algorithm, the simple way is to make use of the Algorithm Specification. Algorithm Specification Language is usually used to prevent the occurrence of Ambiguity. A pragmatic approach to algorithm specification and verification is presented. The language AL provides a level of abstraction between a mathematical specification notation and a programming language, supporting compact but expressive algorithm description. Proofs of correctness about algorithms written in AL can be done via an embedding of the semantics of the language in a proof system; implementations of algorithms can be done through translation to standard programming languages. The proofs of correctness are more tractable than direct verification of programming language code; descriptions in AL are more easily related to executable programs than standard mathematical specifications. AL provides an independent, portable description which can be related to different proof systems and different programming languages. Several interfaces have been explored and tools for fully automatic translation of AL specifications into the HOL logic and Standard ML executable code have been implemented. A substantial case study uses AL as the common specification language from which both the formal proofs of correctness and executable code have been produced.
&nbsp;
The language, AL, which has been developed and used, is very simple: it just consists of type and function declarations. So it doesn’t actually do anything but maps out how the computation can be achieved. Data types describe the problem space and solution space and the functions describe the algorithm for computing the required solutions. The complexity of needing an execution model in the semantics is avoided and since one can reason from the declaration of a function about the relationship between any possible result and the corresponding function parameters, one loses very little. The language looks simpler than mathematical notations since it does not have any quantifiers or other mathematical symbols. It is simpler than most programming languages due to the small number of constructs. AL corresponds to a subset of a functional language like Standard ML (SML). The important design features of AL are:

a small number of primitives
expressive primitives
a declarative model
a simple function definition mechanism
lack of IO primitives

&nbsp;
Conclusion
The different algorithms that people study are as varied as the problems that they solve. However, chances are good that the problem you are trying to solve is similar to another problem in some respects. By developing a good understanding of a large range of algorithms, you will be able to choose the right one for a problem and apply it properly. 
References
Chun-Jen Tsai (2012); Algorithms; Available on: (http://people.cs.nctu.edu.tw/~cjtsai/courses/ics/classnotes/ics12_05_Algorithms.pdf) (Accessed on: 29-Oct-2014)
Jon Kleinberg (2005); Algorithm design; Available on: (http://www.icsd.aegean.gr/kaporisa/index_files/Algorithm_Design.pdf) (Accessed on: 29-Oct-2014)
Steven S. Skiena (2008); The Algorithm Design Manual; Second Edition; Available on :( http://sist.sysu.edu.cn/~isslxm/DSA/textbook/Skiena.-.TheAlgorithmDesignManual.pdf) (Accessed on: 29-Oct-2014)
&nbsp;
&nbsp;
&nbsp;
	              
	              Hi Dr Ayoola,
As you hinted, the main problem with this tendency is economics. From my personal experience in all these years, the problem is not with a bad or non-efficient code but with bad programmers and companies’ unwillingness to spend the right money. Way too often companies simply hire mediocre programmers to do coding and fixing the code they produce becomes quickly very expensive.
Let me give you a perfect example:
I am an infrastructure engineer, give me a datacenter and I will virtualize your infrastructure in a breeze. However, years ago, I was the subject matter expert for Active Directory (AD), and there was a need to integrate that system with the HR system so that accounts would be controlled by HR policies.
Back then you needed to hire someone to custom code this kind of integration. At that time, the company did not want to spend money on this. Because I did some basic coding in Visual Basic management thought I could do the integration. I did it in four weeks, even if I was not a programmer, so my code and algorithms were not the most efficient ones. After one year of company growth, the sync between AD and the HR system went from 30 minutes to around 48 hours! Those extra hours were hurting other systems and creating performance issues in some key servers.
For sixth months, we struggled with this integration, and I kept alerting management that we needed to bring a programmer onboard. When we finally brought a programmer on board, he quickly understood the requirements and took two weeks to code the integration from scratch. The 48 hours sync went down to 5 minutes… that was even less than my initial 30 minutes! Those two weeks of professional coding cost less than my four weeks, all relative maintenance and troubleshooting work. I am a mediocre programmer; I am not shy to say it, but that is not my job, never was.
But I also think that companies do invest in trying to make the best program code by hiring the right programmers. Where I currently work, few years ago, I met a mathematician whose work was to figure out a better, more efficient algorithm to store geometric arcs in a file. This algorithm was to be used in AutoCAD products. The reason for this need was that saving lots of them (apparently they are used a lot) with the current algorithm would produce hundreds of gigabytes of data. Those are the kind of professionals that companies must hire to do software, they may not be cheap, but they will always produce high-quality code and automatically improve it over time.
The problem, skill set aside, is not about expediting software delivery, but mostly that not all software is created equal. Software that a company sell has a relatively high standard of quality. But that software that is developed internally or used as a second-class software in a company is the one that suffer the problem of being non-optimal because nobody wants to spend money there.
Best Regards,
Augusto

	              
	              LOL! you guys made me cry while laughting!
	              
	              Hi Terry
I wish (or perhaps one day hope) I could read code
I have a question for you - How quick was you able to knock up these algorithms for each of the options and, do you just know how to do this or did you need to refer to anything? Cheers
Best Wishes, Craig
	              
	              Hi Dr. Ayoola,
This week was particularly challenging with the discussions. There are a lot of good programmers in this class and they were certainly leading the pack, I learned a lot by reading their discussions. Terry’s contributions were terrific across the board(s) (pun intended).
What have I learned?

Algorithms can be as simple as they could be hard!
Algorithm efficiency is a hot topic and very relative.
If I had a zoo, I know now how to put a giraffe into a refrigerator. That is a good start!

Best Regards,
Augusto

	              
	              ..and now I am crying with laughter!! Well funny...
I concur Augusto, the perspectives are definitely relative and it has been really interesting and educational in reading the responses from colleagues who are clearly super bright. I take my hat off.
Best Wishes,&nbsp;Craig
	              
	              Hi Anthony.
I found this week's topic on Algorithm quite interesting as it took me way back into the basic concepts and building blocks for programming. The assignments also gave me a good understanding of the approaches to searching and sorting with the most interesting one for me being the&nbsp;Boyer-Moore algorithm and basic discovery of simple formula for the arithmetic series calculation.
What I have also learnt is the importance of looking up and referencing various algorithm techniques and the fact that a piece of code can always be improved on
Joseph
	              
	              Hi Terry.
As you have alluded to, your code is pure programming language and I also like you, sometimes suffer that problem of wanting to go straight into the coding without writing out the overall approach in psuedo code. I think the opportunity for us in this assignment was to be able to step back a bit from the core programming and generate psuedo code that would be most understandable to most people and even non-programmers.
What I am wondering though is that if you were explaining this concept to other programmers working under you, would you not abstract your instructions a little and use psuedo code; especially if there would be no fixed choice of the language to use?
Joseph
	              
	              Dear All, &nbsp; A simple parallel algorithm for the general pairwise interactions problem might create N tasks. Task i is given the datum Xi and is responsible for computing the interactions {I (Xi, Xj) | I =/=j}. One might think that as each task needs a datum from every other task, N (N-1) channels would be needed to perform the necessary communications. However, a more economical structure is possible that uses only N channels. These channels are used to connect the N tasks in a unidirectional ring. Hence, each task has one inport and one outport. Each task first initializes both a buffer (with the value of its local datum) and an accumulator that will maintain the result of the computation. It then repeatedly   sends the value contained in its buffer on its outport,  receives a datum on its inport into its buffer,  computes the interaction between this datum and its local datum, and  Uses the computed interaction to update its local accumulator.   This send-receive-compute cycle is repeated N-1 times, causing the N data to flow around the ring. Every task sees every datum and is able to compute all N-1 interactions involving its datum. The algorithm involves N-1 communications per task. It turns out that if interactions are symmetric, we can halve both the number of interactions computed and the number of communications by refining the communication structure. Assume for simplicity that N is odd. An additional N communication channels are created, linking each task to the task offset [N/2] around the ring. Each time an interaction I (Xi, Xj) is computed between a local datum Xi and an incoming datum Xj, this value is accumulated not only in the accumulator for Xi but also in another accumulator that is circulated with Xj. After [N/2] steps, the accumulators associated with the circulated values are returned to their home task using the new channels and combined with the local accumulators. Hence, each symmetric interaction is computed only once: either as I (Xi, Xj) on the node that holds Xi or as I (Xi, Xj) on the node that holds Xj.    References Guy E. Blelloch and Bruce M. Maggs (2009); Parallel Algorithms; Available on: (https://www.cs.cmu.edu/~guyb/papers/BM04.pdf) (Accessed on: 29-Oct-2014) Ananth Grama, Anshul Gupta, George Karypis, and Vipin Kumar (2003); Principles of Parallel Algorithm Design; Available on: (http://www-users.cs.umn.edu/~karypis/parbook/Lectures/AG/chap3_slides.pdf) (Accessed on: 29-Oct-2014) KingTan Yu
	              
	              Hi Terry.
Your algorithm is interesting, though I don't quite get how the following line extracts the required result
result[word] = text.count(word)The algorithm is meant to return both the occurrence of the string and the number of times it appears. How does thisactually get to fit in the array declared? Or is it the understanding that text.count(word) returns both the text and number of times it occurs?Joseph
	              
	              hehe

For some reason I kept thinking about footprints in the butter..

Or was this a joke I think I heard a long time ago, something about how do you know thre is an elephant in your fridge?
Check the butter for footprints.

something like that.
have you allowed for the butter abstraction Terry.?

	              
	              &nbsp;
Dear All,
Not every problem requires the most efficient solution available. For our purposes, the term efficient is concerned with the time and/or space needed to perform the task. When either time or space is abundant and cheap, it may not be worth it to pay a programmer to spend a day or so working to make a program faster.
However, here are some cases where efficiency matters:

When resources are limited, a change in algorithms could create great savings and allow limited machines (like cell phones, embedded systems, and sensor networks) to be stretched to the frontier of possibility.


When the data is large a more efficient solution can mean the difference between a task finishing in two days versus two weeks. Examples include physics, genetics, web searches, massive online stores, and network traffic analysis.


Real time applications: the term "real time applications" actually refers to computations that give time guarantees, versus meaning "fast." However, the quality can be increased further by choosing the appropriate algorithm.


Computationally expensive jobs, like fluid dynamics, partial differential equations, VLSI design, and cryptanalysis can sometimes only be considered when the solution is found efficiently enough.


When a subroutine is common and frequently used, time spent on a more efficient implementation can result in benefits for every application that uses the subroutine. Examples include sorting, searching, pseudorandom number generation, kernel operations (not to be confused with the operating system kernel), database queries, and graphics.

In short, it's important to save time when you do not have any time to spare.
When is efficiency unimportant? Examples of these cases include prototypes that are used only a few times, cases where the input is small, when simplicity and ease of maintenance is more important, when the area concerned is not the bottle neck, or when there's another process or area in the code that would benefit far more from efficient design and attention to the algorithm(s).
Reference
David M. Mount (2003); Design and Analysis of Computer Algorithms; Available on: (http://www.cs.umd.edu/~mount/451/Lects/451lects.pdf) (Accessed on: 29-Oct-2014)
BR,
KingTan Yu
	              
	              Hi Craig,
I'm glad I could help
Your correct the first return is the inner loop and the last return is the outer loop. Ideally for pseudocode you could name them 'returnInner' and 'returnOuter' for more clarity.
And yes in practice the absolute subString position is better especially I find with character strings such as sentences and words.&nbsp; Small number substrings such as this I thought it easier to explain and follow if I used the counter method but both are valid and worth knowing of in case you come across them again.
And your right. I started my career doing this kind of thing many moons ago and it has stuck with me. I do far more team management now so rarely dip my toes into programming unfortunately but once you know it it does stick with you.
Thanks
Chris


	              
	              hi Terryand Tresor,
Tresor-Thanks for the insightful write up i have not heard of the word cyclomatic complexity before. i had to read it up and am grateful to Terry I did.
&nbsp;Regards
martins

	              
	              Hi Anthony,

I have to admit personally I don't believe I have in my career to date. Certainly not that I know of in any specific developments.
Looking into it I noticed a trend in mathematical computation and I enjoyed reading about Grid Computing for parallel bioinspired algorithms by N. Melab∗, S. Cahon, E-G. Talbi (2005).
Funny enough they show some good pseudocode in the article as well. They discuss that it was looked at to solve real world optimization problems and data mining.
"Grid computing has recently been re-vealed as a powerful way to deal with time-intensive problems,&nbsp;The obtained results are convincing, both in terms of flexibility and easiness at implementation, and in terms of efficiency, quality and robustness of the provided solutions at run time "
They talk about the paallelism being controlled via the operating system and using master-slave modelled partioning to avoid over heavy CPU exposure.
If I get more time I want to read more about 'ParadisEO' as a framework.
references
N. Melab∗, S. Cahon, E-G. Talbi (2005) 'Grid Computing for parallel bioinspired algorithms' [http://sci2s.ugr.es/docencia/algoritmica/Melab-Grid.pdf] (accessed: 29/10/2014)
	              
	              Hi Craig.
Just to contribute towards your question of the use of return in a loop, the purpose of the nested loop in this case is to enable extraction of a number by referencing its coordinates subString (X, Y, Value)&nbsp;.
The program requires that you extract all occurrences of numbers contained in the given string 37540 i.e 3,37,375,3754,37540. The initial offset in this case is 3. You would then move to the next number in the string (7) and again extract the numbers arising from that ordinal position (7, 75,754,7540), the next number is 5 and you would do the same thing to get 5,54,540 and so on (4, 40). So notice that we are holding one starting position fixed, and then moving through the string to obtain the other numbers.
This is precisely what the outer loop does, it takes each individual number 3,7,5,4,0 as the reference starting position and allows you obtain through an inner loop, the occurences of numbers obtainable from that sarting position. So while the outer loop holds the starting position constant, the inner loop moves through each position extracting the individual numbers.
The first return statement applies to the inner loop &nbsp;(loop until X + Y &gt; 5)&nbsp; and what it does is to keep transfering control of the program execution to the start of the inner loop as long as X+Y &lt; 5 . &nbsp; The inner loop has to run and complete its iteration before returning control to the outer loop&nbsp;
When the condition fails to hold (X+Y&gt;5), the program terminates the inner loop and moves to the statement that follows. The next return statement&nbsp;then moves program execution to the start of the outer loop which has now been incremented (X=X+1) to pick the next number and starting point.&nbsp;
The use of 5 here represents the number of characters in 37540.
Hope this helps in addition to the earlier comments by Chris
Joseph&nbsp;



	              
	              Hi Chris
Yes, management has introduced alot of abstraction which in one sense, doesn't really help in developing skills, but this is where you build great teams, and trust their skills and judgement to handle the detail, and as managers you then focus your efforts on oversight, opportunities and outcomes. I really wish I'd looked at this type of programme 10-15 years ago!
Best Wishes, Craig
	              
	              Hi Joseph,
Here the variable result is a 'dictionary' or a 'map', it's a quite common data structure. I'm sorry for using it before we really touched the topic in this course. But I guess it's not view hard to understand, and can be very useful once you know it.
A 'dictionary' is like, well, a dictionary. It has entries and the content for each entry, and can be accessed with the squre bracket. For instance:
result["sun"] = "star in the solar system"result["moon"] ="represents my heart"print result["sun"]output: star in the solar system
That's how I managed to return both the words and their counts in one variable result.
br, Terry
	              
	              Hi Chris
Did your reading show any indication on how the operating system actually 'knows' the parallelisability of an algorithm/program and therefore can easily seperate the problem into discrete tasks across parallel processors knowing there is no interdependency between any of the tasks?
Best Wishes, Craig



	              
	              HI Chris,
How do you know there's an elephant in your fridge? Great:-)
As I'm studying and preparing the week 9 DQ. I realize this could have different solutions in different programming paradigms.
Structured Programming: You keep the record of the content in the fridge somewhere. Go and check the record (or butter!)
Object-oriented programming: You need to ask the fridge, like fridge.hasElephantInside()
Functional programming: I don't know and I don't care, you gave me the fridge.
Just analogy here. And analogy is not the real thing. But it helps us understanding the concept. And it's fun.
br, Terry
	              
	              Dear Anthony,
Another interesting week and I thouroughly enjoyed it. It was really refreshing to read all the search and sorting concepts once again.Till date my favourite sorting algorithm is Radix sort, because it is based on the concept of linked list :-).
Best regards,Kharavela
	              
	              Hi Joseph,
Yes, it does. It backs up my understanding of Chris' earlier feedback. It's always good to get multiple explanations to help crystalise my learning. Thank you.
One thing that I have been wondering, and this is linked to the discussions on efficiency (Terry is going to go mad at me here :-$).... &nbsp;given that the loop has already found the integers in the first iteration, do you know if there is a technique that 'assumes' the next integer, based on the fact that it has found and identified them already, previously?
I suppose this question is similar to playing cards, in that as an example, let's say I have a hand of 5 cards, of which I already know what they are, so without needing to look at them again, I know that if I pull a new card from the deck, instinctively I know if it is a card that precedes or succeeds those I already have. Does that make sense? The fact that the search has already gone through once, why should it really? need to do it again?
Best Wishes, Craig
	              
	              Hi Joseph,
There's no programmer working under me, only programmers working with me:-) It doesn't make sense to create a hierachy for programmers (yes, I know it's common).
I agree that you probably need to step back a bit in order to learn programming step by step. I kind of didn't realize that during the week and I skipped that step. Given another chance, I would contribute more 'step back' content to our discussion, to maximize everybody's learning.
Should programmers communicate via pseudo-code?
Not in a formal way. It's ok that as we are discussing in front the whiteboard or a piece of paper, we scratch our idea by pseudo-code or diagram. But it doesn't make sense to formally document it anywhere, because it will become code very soon. Remeber DRY princicple? You don't want to keep two different things representing the one same idea. And the real code should have the good level of abstraction as well, to provide the similar readability as pseudo-code.
You used an interesting word "instruction". I suppose that hinted that I'm handing over my ideas to the programmer "under" me and let him implement my ideas. In my opinion, if I can write it in psuedo code, why don't I just write real code by myself?
br, Terry
	              
	              Hi Augusto,
Before we continue, just what is a most efficient algorithm?
br, terry
	              
	              hello class,
Reading through the "giraffe in the fridge" story kept me laughing all through. Thanks Terry, craig for that it made me forget the challenging- week 8.
Basically, this week has been very challenging for me but i also learnt alot like
the cyclomatic complexity,
Algorithm coding, pseudo -code,
Algorithm Efficiency etc.
I have had to also learn that with programming you can do alot. Looking forward to week 9!

Best Regards
martins
	              
	              Hi Craig,
I believe in a couple weeks or so you will realize these problems are pretty trivial and essential. I do need to refer to Google from time to time, not for solutions but for some details because I keep forgetting the detail of certain programming language. Option3 is a bit tricky, but most of the time I spent on understanding the requirement and making my assumptions are reuqired by the instruction.
br, Terry
	              
	              Hi Dr. Anthony,
In assessing and comparing algorithms the following metrics can be considered. For qualitative assessment the metrics such as:&nbsp;

Computational efficiency
Space efficiency
Correctness and readability
Finiteness
Elegance

On thte quantitative side of assessment, the following metrics can be considered:

Structure
Completeness

These metrics are just a few of that which can be employed for qualitatively and quantitatively assessing and comparing algorithms.

Regards,
Ricardo
	              
	              
Hi all,
I believe some of you requested an explanation of my code. I think Chris did an excellent job with his explanation. However to add to that, I would just like to say that the substring is a Java function operates in such a way that it prints the value at the first position in the bracket onwards but excludes the value at the last position onwards. Thus assuming the the string is “37540”, we give them positions 0, 1, 2, 3, 4 respectively; 3 being at position 0, 7 at position 1, 5 at position 2 and so forth as shown in the table below. Therefore substring (0, 3) is represented as 375, which will be starting from position 0 but excluding all positions from position 3. Position 0 being number 3 and position 3 being at number 4. Further substring(2,3) would print number 5 because we are starting at position 2 and we are excluding from position 3 onwards. Substring (1,5) would be 7540 and position Substring (0,5) would be 37540. So as the loop is iterated it grabs these positions accordingly.

     



0


1


2


3


4




3


7


5


4


0





To Terry the count up to five because we need to include all positions up to position 4 and in order to do this we need to get a higher number than 4 because substring(0,4) prints 3754 excluding position 4 which is 0.
As it relates to the setting of i to 0 at the beginning of the loop, in the code, I hope this further clarifies why it was set as it did, position 0 needed to be a starting position. 
Thanks for the comment on the declaring of variables. 
References:
Horstmann, C.S. &amp; Cornell, G (2008)&nbsp;Core Java: Volume I – Fundamentals,&nbsp;8th ed. Pages 230 and Pages 68 Sun Microsystems, Inc. ISBN-13: 978-0-13-235476-9

	              
	              
Hi all,
I believe some of you requested an explanation of my code. I think Chris did an excellent job with his explanation. However to add to that, I would just like to say that the substring is a Java function operates in such a way that it prints the value at the first position in the bracket onwards but excludes the value at the last position onwards. Thus assuming the the string is “37540”, we give them positions 0, 1, 2, 3, 4 respectively; 3 being at position 0, 7 at position 1, 5 at position 2 and so forth as shown in the table below. Therefore substring (0, 3) is represented as 375, which will be starting from position 0 but excluding all positions from position 3. Position 0 being number 3 and position 3 being at number 4. Further substring(2,3) would print number 5 because we are starting at position 2 and we are excluding from position 3 onwards. Substring (1,5) would be 7540 and position Substring (0,5) would be 37540. So as the loop is iterated it grabs these positions accordingly.

     



0


1


2


3


4




3


7


5


4


0





To Terry the count up to five because we need to include all positions up to position 4 and in order to do this we need to get a higher number than 4 because substring(0,4) prints 3754 excluding position 4 which is 0.
As it relates to the setting of i to 0 at the beginning of the loop, in the code, I hope this further clarifies why it was set as it did, position 0 needed to be a starting position. 
Thanks for the comment on the declaring of variables. 
References:
Horstmann, C.S. &amp; Cornell, G (2008)&nbsp;Core Java: Volume I – Fundamentals,&nbsp;8th ed. Pages 68 Sun Microsystems, Inc. ISBN-13: 978-0-13-235476-9

	              
	              Hi Dr. Anthony,
The closest relationship to parallel processing I can identify is within the continuous advancement in some production applications that has boost massive improvements with advancements in processor technology. One video production application I can think of, scale enormously with processor improvements and uses as much of the CPU resources that can be made available.
This type of optimized processing must be designed within the porgram code of the application, where the underling algorithm makes uses of the multiple processors to scale the performance of the application and deliver outstanding results in comparison to single CPU designed systems.

Regards,
Ricardo
	              
	              Hi Dr. Anthony,
Another challenging week that somehow got away from me and the structure I had in place for studying. Nevertheless I was able to get some work done. I believe I have a better understanding of algorithm, pseudo-code. Iterative vs recursive algorithm, iterpreting them and understanding their efficiency and usage. I think I have the concept but would need more exposure especially in writing the pseudo-code.
I did enjoy the post from my classmates and trying to trace some of the pseudocodes. learning about the different algorithms that were being used to solve problems.
Regards,
Ricardo.
	              
	              Thanks Terry and Craig:-)

	              
	              Hi Dr. Anthony,
I am sorry to say that I do not have any such experience in this regard and as such I may not be able to effectively comment.
Tanisha
	              
	              The thread featured discussions on algorithm qualities, and what aspects characterized and differentiated program algorithms. The posts discussed the importance of matching algorithms to programming languages, and the implications of striving to optimize algorithms/program code, together with tools for carrying this out.We also looked at the types of metrics that could be used for assessing or comparing algorithms, and special requirements for algorithms used in programs running on parallel systems.There were many good contributions to the discussions – well done all!Anthony
	              
	              Hello everybody who can read this,
Thanks for the giraffe conversation.
It was nice to have a two weak excersise ad you could reflect on the first week and improve you own work. Therefor thank you for your comments Terry. Your suggestion to not build a fridge for an elephant if you only have to freeze a mouse was very usefull.
Especially using Scratch was very nice. For some reason visualising programming works better for me than code or logical representations.
Greetings from Bram
	              
	              Hi, all

I may not know if I have come across this previously, but I know that parallel computing is a platform that harmonizes all essential computing resources and components together for seamless operation. It also ensures scalability and efficient performance under procedural but stepwise design. 

The ultimate aim of parallel computation is to reduce total cost of ownership and increase performance gain by saving time and cost.


Best Regards, Babatunde
	              
	              Hi Craig.
I believe what you are saying is possible so that one does not have to use an inner and outer loop. So basically have a read once and then process kind of algorithm. Actually when I was thinking about this initially, where I got stuck with that was to find a formula or means of determining the possible total&nbsp;number of linear combinations in the given value 37540. Once you get the total number of possible number combinations, then somehow extract them and test whether they are divisible by 3.
Assigned Value - 37540



&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;

Total Number Combinations




3


37


375


3754


37540


5




&nbsp;


7


75


754


7540


4




&nbsp;


&nbsp;


5


54


540


3




&nbsp;


&nbsp;


&nbsp;


4


40


2




&nbsp;


&nbsp;


&nbsp;


&nbsp;


0


1




&nbsp;


&nbsp;


&nbsp;


&nbsp;


&nbsp;


15




IF we could get this into a 2 dimensional array, then we could use a single loop based on fixed positions to extract the values and test for the required condition
dim MyArray(5,5)
dim i as integer
dim MyStr as string
dim MyResult as string
MyStr="37540"
for i= 1 to 5 ' Extract the values
myarray(i, 1)=mid(MyStr,i,1)
myarray(i, 2)=mid(MyStr,i,2)
myarray(i, 3)=mid(MyStr,i,3)
myarray(i, 4)=mid(MyStr,i,4)
myarray(i, 5)=mid(MyStr,i,5)
next i
'This way after 5 iterations, all the variables are loaded in the array
' Next determine those that are divisible by 3 in the same manner
MyResult=""
for i = 1 to 5
&nbsp; &nbsp; &nbsp; if myarray(i,1) mod 3 =0 then ' Check if divisible by 3 and add to result
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; MyResult=MyResult &amp;&nbsp; myarray(i,1)&nbsp;&nbsp;&amp;&nbsp;"," 
&nbsp; &nbsp; &nbsp;endif

&nbsp; &nbsp; &nbsp; if myarray(i,2) mod 3 =0 then ' Check if divisible by 3 and add to result
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; MyResult=MyResult &amp;&nbsp;&nbsp;myarray(i,2) &amp;&nbsp;"," 
&nbsp; &nbsp; &nbsp;endif

&nbsp; &nbsp; &nbsp; if myarray(i,3) mod 3 =0 then ' Check if divisible by 3 and add to result
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; MyResult=MyResult &amp;&nbsp;&nbsp;myarray(i,3)&nbsp; &amp; ","
&nbsp; &nbsp; &nbsp;endif

&nbsp; &nbsp; &nbsp; if myarray(i,4) mod 3 =0 then ' Check if divisible by 3 and add to result
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; MyResult=MyResult &amp;&nbsp;&nbsp;myarray(i,4)&nbsp;&nbsp;&amp;&nbsp;"," 
&nbsp; &nbsp; &nbsp;endif
&nbsp; &nbsp; &nbsp; if myarray(i,5) mod 3 =0 then ' Check if divisible by 3 and add to result
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; MyResult=MyResult &amp;&nbsp;&nbsp;myarray(i,5)&nbsp;&nbsp;&amp;&nbsp;"," 
&nbsp; &nbsp; &nbsp;endif
next i

This is where Terry could perhaps come in with another brain wave on how to this can be made more efficient but to me it's a bit cumbersome and involves many more lines of code as opposed to the use of a nested loop. As it is I did not make any use of the fact that there were 15 possible sequential number combinations. The operations in the second loop could actually be combined into the first loop




The inner loop is what helps get that, however, manually counting
	              
	              Hi Terry.
Thanks for that heads up.&nbsp;
Joseph
	              
	              Hi Everyone,

This week has been very challenging from different aspects. I personally found the Scratch program easy to use and the individual assignments pretty straight forward and enjoyable to actually use a tool to produce some operation. Time and efffort wise I havent had as much time to spend on this weeks activities as I'd like but unfortunately life and family come first. I loved interacting with fellow students and I feel we have a really interesting and diverse group of students to ask questions/ bounce ideas and even tell jokes and help each other which is really nice.
Who'd have guessed at the start that during a Masters programme on computer structures we would be debating about fridges/elephants/giraffes and butter.
I just hope we get a nice long break over christmas and don't start the next module until the new year so I don't burn out.

Overall another good week
	              
	              Excellent, I like that!

	              
	              Hi Chris,
a good week indeed.
For Web Science &amp; Big Data kindly check the link below:
http://www.support.liverpool-online.com/~/media/Files/UOLCSS/calendars/2015/WSDB_2015_Module_Calendar.pdf 
br, Babatunde
	              
	              Hi all,
Yes, a tough but very enjoyable week.
For Information Technology please see:
http://www.support.liverpool-online.com/~/media/Files/UOLCSS/calendars/2015/INTC_2015_Module_Calendar.pdf
Best Wishes, Craig
	              
	              Hi
Thanks for that.



	              
	              Hi Anthony.
On this one, I have not worked on any particular algorithms designed for use on parallel-processing computers with multiple processors. What I do know is that database systems such as SQL Server are designed with parallel-processing in mind to optimize performance and leverage on the additional processor cores on a computer in order for program instructions to be executed simultaneously and spread the load where possible.
Joseph
	              
	              Hi Craig,   Yeah or at least how I read it to be was that this Grid system using C++ abstract classes. This uses an OS level master controller which uses a grid software toolkit to do low level virtual functions. This master controller basically takes the request and comminicates to all the slave systems in the grid, then the resource management part of the system then knows what resource it needs and where it is available on the grid to be used and has built in request failure detection as well and from there it splits the request up into much smaller packets/objects and sends them off to the relevant slaves to process at the same time. They show it like this :   ParadisEO is the framework and Condor is the resource management tool. "An important issue to deal with in the gridification process  of ParadisEO is fault-tolerance. MW automatically reschedules  on Worker Nodes unfinished tasks which were running on pro-  cessors that failed. However, this cannot be applied to the mas-  ter process that launches and controls tasks on Worker Nodes.  Nevertheless, a couple of primitives are provided by MW to  fold up or unfold the whole application enabling the user to  save/restart the state in/from a file stream. Note that only the  Master Node is concerned by these checkpointing operations" The failover as described above is really quite good because you can process say 100 tasks to the nodes and then if 5 fail the other 95 wait while still completed and the system resends the other 5 failed processes to other nodes if it was a cpu failure for example.   I'm not sure exactly how the ParadisEO framework splits the request as such other then to say it takes the request and uses &nbsp;optimization solvers embedding single and multi-objective meta-heuristics (evolutionary algorithms and local searches). Whatever that means! What I took that to mean is that there are super smart algorithms that detect and split the request up and optimise the request into as many smaller requests as it can. The article at times was difficult to follow unless your in that field or work but interesting all the same. ta Chris
	              
	              Hi Anthony,

I found this to be interesting and challenging. I believe I have a better knowledge of algorithm, the difference between the algorithm and also pseudo-code, the representation of an algorithm, their efficiency, and the various structures used in that representation.

I also enjoyed posts from colleagues in class while tracing some of the pseudo codes, and I had fun while working with scratch.

A good week! Looking forward to week 9

&nbsp;

Best Regards, Babatunde&nbsp;
References

Laureate Online Education. (2014). Computer Structures – Lecture Notes Week 8: Analysis of Algorithms [Online]. Available from: https://elearning.uol.ohecampus.com/bbcswebdav/institution/UKL1/201480_SEPTEMBER/MS_CKIT/CKIT_501/readings/UKL1_CKIT_501_Week08_LectureNotes.pdf (Accessed 24 October 2014)
	              
	              Thanks for this Chris
Really interesting stuff.
I could be misinterpreting this, and perhaps completely wrong, but may be the ParadisEO framework quickly scans (I'm assuming nanoseconds) the algorithm (no execution), to determine whether there are interdependencies (inputs/outputs) with the other algorithms? and if so, it doesn't split them?
Ultimately, I agree, unless your in this field, it may be difficult to follow the detail.
My ask was to just try to understand how the operating recognises parallelisability of the algorithm when it is not hard coded, and it appears that there are toolkits and frameworks for this, that sit in between the application and the OS. The failover and error handling is a critical aspect too.
Thanks for this Chris, really interesting
Best Wishes, Craig

	              
	              Hi Tanisha
Thanks for this
The slightly confusing bit for me was that it uses the string that it starts on, but not the string that it ends on.
I think once you understand how it works, in terms of the starting point and ending point, it is straight forward. Thank you for your help.
Best Wishes, Craig
	              
	              Hi Joseph,
Thank you so much for taking the time to do this - I really appreciate your input.
I think I agree with your conclusion in that, this appears to be a more complex algorithm. However, it perhaps could still be more efficient in that it doesn't require multiple loops, &amp; takes the data that it has already found, and just continues executing the algorithm - in others words, it just executes the algorithm top to bottom without needing to do loops to go get more data?
I realise this could well be unimportant given the circumstances on the algorithm itself, in that the efficiency is not an important factor, I just wanted to know wether you could introduce efficiencies at this level, albeit the algorithm then appears less efficient and more complex - in other words, a more complex and longer algorithm but perhaps more efficient overall?
Hope that makes sense?
Best Wishes, Craig
	              
	              Thanks Terry, &nbsp;I stand corrected.
I see also that there's the perfectionist element in you. One of the challenges often faced with management is that when a good programmer moves to a different level of management responsibility due to their experience, that opportunity to write code reduces and would require them to manage (not micromanage) those working with them; basically letting go of some of the level of control over detail.
So one possible reason for you not to write the real code by yourself would be to give another the opportunity to do so based on a broad outline in psuedo code.
Joseph
	              
	              Hi Craig.
Even in this circumstance, a loop would still be necessary because it helps avoid a situation where the same piece of code would need to be repeated 5 times. In this case, since the code outline is the same and all that is changing is the position of the data item to be extracted 3 7 5 4 0, the loop helps to move across the value and extract the number based on the 1st, 2nd, 3rd, 4th and 5th position hence (i,1), (i,2), (i,3), (i,4) etc with the varying element (i) changing based on the loop iteration
Joseph
	              
	              Hi Joseph,
How come an experienced programmer becomes something he's not good at (e.g. management)?
I thought an experienced programmer would become, well, a better programmer. Isn't that people good at management becomes management?
And how do I lead real programmers by writing pseudo-code? I would rather not write code at all.
Would you call a doctor who wash his hands before doing an operation a "perfectionist" only because you haven't seen any doctor did it before?
br, Terry
	              
	              Hi Joseph,
Understood. I guess it is not as intuitive (albeit instructed) as I was hoping then. Thanks for your input
Best Wishes, Craig
	              
	              Thanks, Terry - it looks like you guys have definitely mastered the finer points of "pseudocoding" :-)!
Anthony
	              
	              This week’s activity featured follow-on discussions about&nbsp;the&nbsp;pseudocode generated last week to implement specific algorithms, and the enhancements made to these.
Anthony

	              
	              Thanks for the comments, Craig.I think you are actually doing quite well with this - possibly you have a fair amount of latent programming talent :-).Regards,Anthony
	              
	              Thanks for the comments, Belinda.Yes, it is does sometimes take a while to fully develop a programming mindset, but I can see that you are indeed getting the hang of it. &nbsp;Of course, we do have to give some credit to Terry's elephant analogy :-)!Regards,Anthony
	              
	              Hi Terry.
Interesting thread this is becoming. The question on how an experienced programmer becomes something he's not good at, can be explained in the context of&nbsp;what is &nbsp;known in management theory as the Peter Principle. This&nbsp;was&nbsp;coined by&nbsp;Canadian psychologist Laurence J. Peter stating that "in a hierarchy every employee tends to rise to their level of incompetence"
The principle holds that in a hierarchy, members are promoted so long as they work competently. Sooner or later they are promoted to a position at which they are no longer competent (their "level of incompetence"). So a programmer can be very good at programming, but not necessarily management. And what often happens in organizations due to multiple projects needing to be done, is to get the most experienced to oversee the work of others (thus promoting them to a level for which they may not be at their best)
Nothing wrong with being a perfectionist by the way. I have a workmate who is a great asset because of their ability to pay attention to the smallest detail and this comes in handy because of the '3rd eye view'.
Joseph
References
1) Laurence J. Peter, The Peter Principle: Why Things Always Go Wrong, Reprint edition (October 25, 2011) , HarperBusiness
2) Peter Principle, Available at&nbsp;http://en.wikipedia.org/wiki/Peter_Principle, Accessed 01/11/2014
3) Sutton, B. 'Work Matters - If The Peter Principle is Right, Then Organizations Should Randomly Promote People', Available at &nbsp;http://bobsutton.typepad.com/my_weblog/2010/10/ig-nobel-prize-winner-if-the-peter-principle-is-right-then-organizations-should-randomly-promote-peo.html, Accessed 01/11/2014

	              
	              Hi Joseph,
I like your reply:-)
Do you assume a programmer's next incompetent level is management?
Why do you still call me perfectionist? Is it relative to your professional working environment?
br, Terry&nbsp;
	              
	              Thanks for the comments, Augusto. &nbsp;As Craig also said, the perspectives on what has been learnt are quite often relative/subjective. &nbsp;I suspect the both programmers and non-programmers in the class all learnt something from each other, albeit from different perspectives.
Regards,Anthony
	              
	              Hi Terry.
The answer to that is yes and no as it &nbsp;really depends on an individual programmer. There are those who would flourish when reassigned to management but there are those who would suffer. I have an example of friend who worked very well as a programmer and as new projects came up for the company, the software firm he worked for hired fresh graduates as developers and he was tasked to oversee them. So he moved from mainstream programming to planning out the work, reviewing their work and attendings the many management meetings and client facing presentations. He got bored and left to open his own firm as he felt that he was not applying his skills well.
Personally, I try to do less of programming nowadays and more of management work.
There is a quote that says "In the battle between the lion and shark the terrain determines the outcome". A lions terrain is on land and if it were fighting a shark on land, it would be a no-contest. A sharks terrain is on water and if it were fighting the lion in water, its no wild guess who would win. (&nbsp;Dr Wale Akinyemi -&nbsp;www.powertalks.biz/quotes-by-dr-wale-akinyemi/ )
Allow me to withdraw my use of the word perfectionist, as it's probably coming across with a negative connotation. What I meant to say is that you have a keen eye for detail based on my observation. I've heard for instance of people with super vision, able to see colors not visible to the normal human eye.
Thanks
Joseph



	              
	              Hi Joseph,
Thanks for the new quote, I like it very much.
I also have friends that quit or threatened to quit when being de-promoted to manangement. One of them said: "I choose to do this computer work is because I hate people." From then on, I stop assuming people always want to be management.
One of the (potential) problem of making good programmers managers is when they stop programming for many years they still think they are good programmers. But this industry develops so fast, every area is not the same in a very short time. I would say probably half of the technologies software developers are going to use 10 years later haven't been invented yet. From last year or so, large software companies like Motorola started to fall apart. It was both sad and interesting to observe the middle layer management looking for jobs.
Don't get me wrong, I'm not saying management is something wrong. I don't want to limit myself with only programming as well. I also think leadership skills is needed if I want to make something.
I'm currently working on my next in-competency level -- developing a web-site using Ruby on Rails. Ruby is new to me, and the kind of business the website is doing is also new to me.
Thanks for not calling me&nbsp;perfectionist any more:-) I'm sharing something I believe and practice, and I do have real friends and real communities where these ideas are just common sense to them.
br, Terry
	              
	              Thanks for the comments, Bo. &nbsp;Yes, not unexpectedly, there was a fair amount of variation with the alogorithms/pseudocode, however all had some degree of merit.Regards,Anthony
	              
	              Thanks for the comments, Joseph. &nbsp;
True we can almost always improve program code - the trick is knowing when to stop trying to introduce further enhancements!Regards,Anthony
	              
	              Thanks for the comments, Kharavela . &nbsp;
Sorting algorithms have always generated interest - my favourite is the ultra-recursive quick-sort algorithm&nbsp;Regards,Anthony

	              
	              Thanks for the comments,&nbsp;Martins. &nbsp;
Glad to hear that you had a good week.Regards,Anthony

	              
	              Data structures make up the foundation of most major pieces of software. The ability to find, retrieve and present bits of data quickly and efficiently is paramount to the success of software on multiple fronts too. For example, not only does data need to be effectively retrievable on strictly a functional level, but on a user-facing front as well. Merely consider the last time you used a program that took too long to load or respond to an input. Users are generally not patient, so selecting and using the appropriate data structure is imperative to making, and keeping, them happy.
As you might imagine, depending on the context of the software you are creating, your choice of data structure will vary greatly. Moreover, as the software becomes more complex, these structures become even more challenging to work with. Nevertheless, most projects, regardless of scale, are going to have recognisable trends indicative of far simpler programs. As such, for this Discussion, assume you have the task of developing a data structure for a phone’s contact list. What data structure options do you have? Which option is the best?
To prepare for this Discussion:

Review your Weekly Learning Resources with a focus on data structures.
Reflect on the data in your phone’s contact list and how you would logically organise it.
Compare the different types of data structures and reflect on how your phone’s contact data could be organised.

To complete this Discussion: Post: Create an initial post in which you compare multiple data structures. Address the following:

Analyse the merits and weaknesses of common types of data structures and identify which data structure is best suited for a phone contact list.
Using your analysis, explain why your identified data structure is most suited for a phone contact list.
Fully state and justify any choices, assumptions or claims that you make using the suggested Learning Resources for this Week and/or your own research.

Respond: Respond to your colleagues. Address the following:

Recommend other data structures your colleague could use for a phone contact list and explain your reasoning using your own analyses on data structures.
Be sure to support any claims you make.

For all Discussions (unless stated otherwise):

Create a single document with your initial post. Your document should be 350-500 words, though you will be marked based on the quality of your writing, not on the number of words.
By Sunday, post the text of your document to the Discussion Board for this Week, and upload the document using the Turnitin submission link for this Discussion.
By Wednesday, make 3–5 substantial follow-up responses to your colleagues. These can include responses to your colleagues’ initial posts, as well as responses to colleagues who responded to your own initial post. Your total Discussion Board participation must occur on at least 3 individual days during each week. Follow-up responses should be significant contributions to the Discussion. Do not submit your follow-up responses to Turnitin.
In general, online discussion is best when you:


Ask insightful questions.
Extend the discussion into new but relevant areas.
Model or promote critical reflection.
Support your arguments with citations and references from the assigned Learning Resources and other literature, using Harvard Liverpool Referencing Style.



Ensure that you spread your discussion posts across at least three separate days of each week. This will help maximise the value of your discussion with colleagues and serve to meet the learning objectives for each activity.
Click on the Reply button below to reveal the textbox for entering your message. Then click on the Submit button to post your message.
	              
	              Software development is a conglomerate of many different disciplines. However, while you might be familiar with these disciplines (programmers, artists, designers, project managers, etc.), you are probably less familiar with the personality nuances of each one. For example, an artist’s thinking is more based on what is happening around them. They work with the current situation and make the best of it. Conversely, someone who excels at project management is going to be someone highly organised and typically hates doing things that are last minute and outside their plans.
As you might suspect, communication between such very different individuals is a tremendous challenge. Communication tends to be the first thing to break down in these kinds of development teams. A great deal of the time, this happens in a development cycle’s initial systems analysis phase too. Considering this is a foundational phase in development, failure to communicate here can set the rest of the project up for disaster. In this Discussion, you will analyse the communication dangers in the analysis phase and provide suggestions to remedy said dangers.
To prepare for this Discussion:

Review your Weekly Learning Resources with a focus on communication and the phases of software development.
Research common personality types and what kind of careers/people they are often associated with.
Reflect on means to alleviate communication breakdowns.
Identify past projects you have worked on to help inform your Discussion post.

To complete this Discussion: Post: Create an initial post in which you analyse the importance of communication during the analysis phase of software development. Address the following:

Explain two to three reasons communications can breakdown during the systems analysis phase of software development.
Recommend solutions for alleviating communication breakdown.
Fully state and justify any choices, assumptions or claims that you make using the suggested Learning Resources for this Week, your own research and your own experiences.

Respond: Respond to your colleagues. Address the following:

Provide additional causes for communication breakdowns and suggest additional ways to mitigate such breakdowns.
Be sure to support any claims you make.

For all Discussions (unless stated otherwise):

Create a single document with your initial post. Your document should be 350-500 words, though you will be marked based on the quality of your writing, not on the number of words.
By Sunday, post the text of your document to the Discussion Board for this Week, and upload the document using the Turnitin submission link for this Discussion.
By Wednesday, make 3–5 substantial follow-up responses to your colleagues. These can include responses to your colleagues’ initial posts, as well as responses to colleagues who responded to your own initial post. Your total Discussion Board participation must occur on at least 3 individual days during each week. Follow-up responses should be significant contributions to the Discussion. Do not submit your follow-up responses to Turnitin.
In general, online discussion is best when you:


Ask insightful questions.
Extend the discussion into new but relevant areas.
Model or promote critical reflection.
Support your arguments with citations and references from the assigned Learning Resources and other literature, using Harvard Liverpool Referencing Style.



Ensure that you spread your discussion posts across at least three separate days of each week. This will help maximise the value of your discussion with colleagues and serve to meet the learning objectives for each activity.
Click on the Reply button below to reveal the textbox for entering your message. Then click on the Submit button to post your message.
	              
	                     Mary Had A Little Lamb Terry Yin November 5, 2014    (PDF version in attachment) Every time I heard my daughter singing "Mary had a little lamb", I couldn't help but thinking "Mary should have some vegetable as well."    The "Mary had a little lamb" story is borrowed from Weinberg's book&nbsp;(Gause &amp; Weinberg 1990, p.54).       There are two essential questions in making a software system,&nbsp;WHAT&nbsp;we are making, and&nbsp;HOW&nbsp;to make it. The&nbsp;system analysis&nbsp;phase is mostly about WHAT, since that's the most urgent question. We call it&nbsp;requirement analysis. But sometimes it's also about&nbsp;HOW, since in some situation, the feasibility and alternatives of the solution are also urgent question. In that case, we might also need to do operational research like the&nbsp;feasibility study. If communication breakdowns during the system analysis, we will end up making the wrong software. So how can we avoid communication breakdown? Let's look at the reasons for communication breakdown respectively and see what we can do to improve it.       1. Ambiguity       As the "Mary had a little lamb" story told us, human language can be ambiguous. There are some approaches in modern software development to avoid it.       1.1 Specification By Example       As the "Mary had a little lamb" story told us, human language can be ambiguous. There's a simple but very powerful tool to avoid the ambiguity, that is the question "can you give me an example?" The examples we use to clarify the requirement shouldn't be the general example, but need to be very specific scenarios, for instance, "given Craig is a VIP user, when he purchase the SbE book, he should have 50% discount." And if you feel one specific example is not general enough to cover the requirement, you can always add another specific example. And more.         Figure from&nbsp;Bridging The Communication Gap&nbsp;(Adzic 2009, pdf version p.27).       This is quite different than the traditional approach to analyzing the requirement. Traditional approach like use cases try to generalize the requirement and systematically cover the requirement. SbE doesn't aim to systematically cover the requirement but focus on value and priority of each scenario.       1.2 System Metaphor       Specification by Example is used to communicate the requirement. How shall we communicate the HOW part from the system analysis? One approach is called&nbsp;system metaphor&nbsp;Beck (2000). A system metaphor is a story that even the client can understand to explain how the system works. It helps to unify the terms we use across the whole development team.       2. Handoff       Being a Chinese myself, it surprised me to know that Chinese Whisper is a communication game played around the world, especially in UK [colleagues, confirmation needed here]. We have the same game in China but we have a different name for it. (And no, it's not called British whisper.) It's a hilarious game showing us that a piece of message can be totally distorted after being retold several times. The traditional software development typically involves many moments of handoff. The business analyst passes the requirement to the architect. The architect passes the design decisions to the developer. The developer passes the software to tester. In each of these handoff, there will be information lost and mistakes.       2.1 Feature Team       One of the modern approaches to avoid handoff in software development is to build the&nbsp;feature teams&nbsp;Deemer et al. (2012). Feature team, in contrast to the traditional&nbsp;single-disciplined team&nbsp;or&nbsp;component team, is a small software development team includes all the necessary disciplines to deliver a software feature. One of the benefits of a feature team is to minimize the handoff.       2.2 Stop Writing Start Talking       The purpose of many documentation in traditional software development is to have the handoff. And that is usually not a good way of communication. Face-to-face communication is often more effective.&nbsp;Specification workshop&nbsp;is one of the collaborative ways to do system analysis&nbsp;(Adzic 2009, p.59). Instead of the requirement owner "push" the requirement to the development team, in the workshop, development team "pull" the information from the requirement owner, by asking questions like "can you give me an example ..."       3. Long Feedback Cycle       If our understanding about the requirement is wrong, the long feedback cycle will make it worse. We need shorter PDCA cycles to continuously verify our analysis,&nbsp;Wikipedia (2014).       3.1 Iterative development       The traditional waterfall approach has been proved not working and the modern software development model is always iterative,&nbsp;Larman &amp; Basili (2003). Iterative development aims to get full feedback as early as possible. The full feedback should not be from any interim product, but the real working software.       3.2 Acceptance-Test Driven Development       An even more detailed approach for system analysis is called Acceptance-Test Driven Development (Larman &amp; Vodde 2010, ATDD, p.42 59). This is highly related to Specification by Example as mentioned earlier. The requirement is explored in a collaborative way in a workshop. During the iteration, the acceptance test and the software are developed in concurrence. At the end of the iteration, the acceptance test can be used to confirm the requirement has been fulfilled.         4. Conclusion       Communication can breakdown. The breakdown of communication during system analysis is very costly. We need to avoid the ambiguity, handoff and long feedback cycle. This paper is mainly based on the work of Craig Larman, Bas Vodde and Gojko Adzic. If you are interested in this topic, I would like to recommend the 4 books in the references written by them.       References Adzic, G. (2009), Bridging the communication gap: specification by example and agile acceptance testing, Lulu. com. Beck, K. (2000), Extreme programming explained: embrace change, Addison-Wesley Professional. Deemer, P., Benefield, G., Larman, C. &amp; Vodde, B. (2012), ‘The scrum primer 2.0’. Gause, D. C. &amp; Weinberg, G. M. (1990), ‘Are your lights on’, Dorset House . Larman, C. &amp; Basili, V. R. (2003), ‘Iterative and incremental development: A brief history’, Computer 36(6), 47–56. Larman, C. &amp; Vodde, B. (2010), Practices for scaling lean &amp; agile development: large, multisite, and offshore product development with large-scale Scrum, Pearson Education. Wikipedia (2014), ‘Pdca — wikipedia, the free encyclopedia’. [Online; accessed 5-November-2014]. URL: http: // en. wikipedia. org/ w/ index. php? title= PDCA&amp;oldid= 630479736    
	                Attachment: &nbsp;Mary Had A Little Lamb.pdf (267.308 KB)
	              
	                    Data Structure For A Contact List Terry Yin November 6, 2014    (pdf version attached) A phone contact list is a collection of contacts. The common collection data types are,&nbsp;(Brookshear 2011, p.342):  List&nbsp;represents a linear collection. Items can be accessed in sequence or by index. Set&nbsp;represents a collection of unique items. There cannot be duplicated items, and there's no order in the collection. Map&nbsp;or dictionary/hash, represents a key-value collection, where items can be accessed by a unique key. Tree&nbsp;is a collection that preserve the relationship among the items in a tree structure. A tree makes it easy to access a subgroup of items. A tree also builds short paths to access any item in the tree.&nbsp;  These are the very general collection types. And when we talk about data structures, there are at least two different concerns. One concern is the abstract behaviour. The other concern is the underlying implementation details. In this paper, we mostly talk only about the abstract behaviour of the data type. This paper will use the phone contact list as an example to show the thinking process of selecting a proper data structure based on different requirement.        Data Type for A Contact¶        We need to define a basic datatype for the contact list. It's a&nbsp;user-defined data type&nbsp;(Brookshear 2011, p.368). We call it&nbsp;Contact. The requirement for the contact is:   Contacts should have some predefined fields, including first name, last name and phone number. It should have self-defined fields per contact   We will ignore the self-defined fields to keep things simple because our focus in the paper is on the collection. To define the&nbsp;Contact&nbsp;type in Python:         class Contact: &nbsp; &nbsp; def __init__(self, first_name, last_name, phone_number): &nbsp; &nbsp; &nbsp; &nbsp; self.first_name = first_name &nbsp; &nbsp; &nbsp; &nbsp; self.last_name = last_name &nbsp; &nbsp; &nbsp; &nbsp; self.phone_number = phone_number &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; # we define the __repr__ function to display the content of the contact &nbsp; &nbsp; def __repr__(self): &nbsp; &nbsp; &nbsp; &nbsp; return "&lt;contact&gt; %s %s: %s" % (self.first_name, self.last_name, self.phone_number)          Basic User Scenarios: Linear Access        Basically, a phone contact list need to:   User adds a new contact User updates a contact information User browses the contacts in alphabetic order          A basic list would be enough to fulfill the basic user scenarios because it's simple.         contact_list = list() contact_list.append(Contact("Terry", "Yin", "119")) contact_list.append(Contact("Teddy", "Bear", "000")) contact_list.append(Contact("Jeff", "Dean", "911")) print(contact_list)             [&lt;contact&gt; Terry Yin: 119, &lt;contact&gt; Teddy Bear: 000, &lt;contact&gt; Jeff Dean: 911]            Because we inserted "Terry" first and then "Jeff", the list is not sorted in alphabetic order. To browse the contacts in alphabetic order, we have two choices:  Insert the new contact in the right place when adding a contact. Sort the list before browsing.  As browsing is a much more frequent operation than adding, we may choose option 1 to make the browsing faster and easier.        More User Scenarios: Access With Keys        A typical phone contact list will also do:   User searches by name User deletes a contact by selecting the name   These behaviours can be implemented with a list as well. But maybe not efficient enough. It looks like a map is an easier solution. A map in Python is called a&nbsp;dict,&nbsp;Wikipedia (2014a).         contact_dict = dict() contact_dict["Terry"] = Contact("Terry", "Yin", "119") contact_dict["Teddy"] = Contact("Teddy", "Bear", "000") contact_dict["Jeff"] = Contact("Jeff", "Dean", "911") print(contact_dict) print(contact_dict["Terry"])             {'Teddy': &lt;contact&gt; Teddy Bear: 000, 'Jeff': &lt;contact&gt; Jeff Dean: 911, 'Terry': &lt;contact&gt; Terry Yin: 119}     &lt;contact&gt; Terry Yin: 119            One of the drawbacks of using a map (with the current requirement) is that a map doesn't keep the order information. So in order to browse the contact list in alphabetic order, we need to sort all the keys of the map first.        More User Scenarios: Duplicated Entries &amp; Search With Prefix        The contact list on the phone I'm using right now (Samsung S4) can also do:   User adds a duplicated contact   Given a contact already exists in the contact list When user adds a contact with the same name Then both contacts should exist in the contact list   Deleting a contact should not delete other contacts with the same name As the user is typing the name, the phone should list the contacts with the given prefix already typed dynamically.&nbsp;          Now a map or dict doesn't fit the need anymore, because there will be duplicated keys. A map cannot directly have duplicated keys. Considering user also need to look up in the contact list by prefix of a name, a good choice could be a&nbsp;Trie&nbsp;Wikipedia (2014b). A trie is a special type of tree. It forms a tree structure based on the prefix of the keys. It has similar time complexity as a map, but it also provides a subset of all the prefixes, and it's also ordered. A trie is also easier to be modified to allow duplicated keys.   Most programming languages doesn't support trie data structure directly. We need to implement the trie by ourselves. The trie will be like:         from collections import defaultdict  class Contacts_Trie: &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; def __init__(self): &nbsp; &nbsp; &nbsp; &nbsp; # contacts on this node &nbsp; &nbsp; &nbsp; &nbsp; self.contacts = list() &nbsp; &nbsp; &nbsp; &nbsp; # the sub notes of this nodes, it's a dict of other Contacts_Trie &nbsp; &nbsp; &nbsp; &nbsp; self.sub_notes = defaultdict(Contacts_Trie) &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; def add(self, suffix, contact): &nbsp; &nbsp; &nbsp; &nbsp; if suffix == "": &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; self.contacts.append(contact) &nbsp; &nbsp; &nbsp; &nbsp; else: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; self.sub_notes[suffix[0]].add(suffix[1:], contact)  &nbsp; &nbsp; def contacts_with_prefix(self, prefix): &nbsp; &nbsp; &nbsp; &nbsp; if prefix == "": &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; return self._get_all_contacts() &nbsp; &nbsp; &nbsp; &nbsp; else: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; return self.sub_notes[prefix[0]].contacts_with_prefix(prefix[1:]) &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; def _get_all_contacts(self): &nbsp; &nbsp; &nbsp; &nbsp; result = self.contacts &nbsp; &nbsp; &nbsp; &nbsp; for key in sorted(self.sub_notes.keys()): &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; result += self.sub_notes[key]._get_all_contacts() &nbsp; &nbsp; &nbsp; &nbsp; return result          Below is an example and test for using the trie. As you can see, we can get all the contacts who's name have prefix "Te", and duplicate names are allowed.         contacts = Contacts_Trie() contacts.add("Terry", Contact("Terry", "Yin", "119")) contacts.add("Teddy", Contact("Teddy", "Bear", "000")) contacts.add("Terry", Contact("Terry", "Again", "111")) contacts.add("Jeff", Contact("Jeff", "Dea", "911")) print contacts.contacts_with_prefix("Te")             [&lt;contact&gt; Teddy Bear: 000, &lt;contact&gt; Terry Yin: 119, &lt;contact&gt; Terry Again: 111]            As now duplicated names are allowed, we cannot use name to identity contact entry anymore. So a new field&nbsp;id&nbsp;need to be introduced to the&nbsp;Contact&nbsp;type.&nbsp;id&nbsp;is a unique number to identify a contact.        Even More User Scenarios: Mulitiple Indexes        The contact list on my phone can also do:  Incoming call shows the contact name (by matching the phone number) User searches with phone number User searches with inital         Now there are more than one way to search in the contact list. Our solution with the trie to store the contacts is not enough. One possible solution is to have&nbsp;multiple index trees. Each node in the index tree has a reference to the contact item, which probably is stored in a list.   One of the drawbacks of this solution is adding and deleting an item from the contact list will need to update many index trees. By doing so, we sacrifice the performance for adding and deleting to improve the performance for looking up.        Conclusion        This is a great mental exercise for studying data structure. But, after this long discussion, what is the best choice for the data structure of the contact list? My answer is&nbsp;I don't know. And I'm pretty sure&nbsp;my current design is wrong. Why do I know I'm wrong? It's&nbsp;because I've always been wrong. Experiences tell me, I've never created the right design just by one try for any non-trivial software. However, I have tried my best to speculate about the design of this contact list. I will only know where is wrong in my design when and after I implement my speculative ideas. And when there's requirement change, my design will be wrong again.    References Brookshear, J. G. (2011), Computer science: an overview, Paul Muljadi. Wikipedia (2014a), ‘Associative array — wikipedia, the free encyclopedia’. [Online; accessed 6-November- 2014]. URL: http: // en. wikipedia. org/ w/ index. php? title= Associative_ array&amp;oldid= 630919453 Wikipedia (2014b), ‘Trie — wikipedia, the free encyclopedia’. [Online; accessed 6-November-2014]. URL: http: // en. wikipedia. org/ w/ index. php? title= Trie&amp;oldid= 616659272       
	                Attachment: &nbsp;Data Structure For A Contact List.pdf (244.678 KB)
	              
	              1 Introduction   This week’s focus is on reasons for communications breakdown during systems analysis phase in software development. Having said that, most of the things I’m going to touch on are general.   There has and always will be projects that fail and the failing can happen in all phases, they sometimes even fail before they get to be projects. The reasons are many, and communications and information, or rather lack of it, is definitely one of the sure winners in this “race”.   I would like for the reader to also think ‘information’ when reading the word ‘communications’. This is because the word communications it often used in the form of direct communication via speech or mail between parties, but communication also includes other information like documents containing requirements specifications etc.   2 Thesis   Communications is one of the ground pillars in any project or phase, as well as life in general. Without correct, good, and timely communications, then the project is surely doomed to fail.   Whether it’s in person or via email, with a sponsor or a stakeholder, effective communication serves as the very bedrock of business.   PMI (2013)   3 Reasons for communications breakdown during the analysis phase   3.1 General list of reasons   There are so many reasons for communications to breakdown that it’s hard just to mention 2 or 3. So I’ll start with a list of a few:    The stakeholders are not active or clear. The leaders are not continuously “on”, e.g. the project manager. The specification requirements are not clear, e.g. wrong type of documentation. The scope is not agreed upon. The timeframe is to short forcing people to ignore communications. The stress level is too high making people forget about communications. Forgetting to break down into smaller solvable packages (base case). Jumping over the planning/analysis part (80/20 rule). Etc.    Below I will just deal with 2 reasons. Bear in mind that it’s not prioritized.&nbsp;   3.2 Reason 1: Software requirements specifications (SRS) not complete   “In a sense, this document is a written agreement between all parties concerned” (Brookshear, p.304).   In this document you should find enough and clear information, communicating what the stakeholders expect, how the software is supposed to be designed and work. The SRS should contain descriptions and diagrams, and could even contain pseudo code.   The SRS is normally thought of as a static document. I would argue that it should rather be a dynamic document and be updated for every iteration. Iterations can be inside a phase or in the phase transition. The reason is that teams always gather more knowledge about the subject/problem as time goes, and this knowledge should be brought to use. Of course the project must be careful to be true to the original project otherwise the project will risk the following: “changing requirements are the major causes of cost overruns and late product delivery” (Brookshear, p. 304).   Let’s get back to the reason for breakdown with focus on the SRS. Perhaps the most important part is the shareholder part.&nbsp; If the project doesn’t ensure proper shareholder anchoring, then this will influence the entire project and it will almost be doomed to fail. Also, and mentioned, the project team continuously gathers more knowledge and some of this knowledge should be communicated to the shareholders.   So for conclusion, if you want to increase probability for hindering communications breakdown, then:    Involve the shareholders at the beginning and throughout the entire project. Decide on iterative or agile methods; do not use the plain “waterfall model”. Make sure all relevant information, e.g. the SRS, is clearly communicated and understood.    3.3 Reason 2: Choosing the correct analysis methodologies   Depending on the complexity of the software, and use of e.g. imperative programming (IP) vs. object oriented programming (OOP), then the project should decide on which “modelling techniques and notational systems” (Brookshear, p. 316) to use.   Dataflow diagrams have proven to be an excellent way of “improving communications between clients and software engineers” (Brookshear, p. 317). They can help identifying procedures; show the information flow and more.   Also another tool called “data dictionary” has been used by the software engineers. This tool “is a central repository of information about data items throughout a software system” (Brookshear, p. 317).   Then there is the notational system called “Unified modelling language”, in short UML, which is actually a collection of tools. This is aimed at OOP and has grown in popularity (Brookshear, p. 318). The tools here are e.g.    Use case diagrams Class diagrams Interaction diagrams, hereunder Sequence diagrams    In conclusion, a software project should:    Decide on which diagrams to use. Ensure that the diagrams favour both client and software engineer. The diagrams should be clearly communicated to all relevant parties.&nbsp;    4 Recommended alleviating actions   I have already mentioned several tools, documents etc. to alleviate communications breakdowns, thus I will now focus on other “soft” issues which can alleviate breakdowns starting with a short list of examples:    The project type should be chosen, e.g. agile like scrum. Meeting should be held at a regular basis, preferably frequent and short versus few and long. Status reports should be designed suitable for the project, and communicated to relevant parties. Decide on “rules of engagement”, i.e. how the team culture etc. should be. (Hartman, 2009) Strive towards openness in collaboration ensuring that visibility of all relevant issues.    5 Conclusion   Communications cannot be exaggerated; it is crucial to any type of project or work where humans are involved. The graph below from a PMI white paper shows, from a positive angle, the importance of communications.          It is also my own experience, that lack of communications is the sure way to breakdown in communications and in worst case, project failure, be it:    Lack of direct communications, especially speech face to face. Lack of documentation. Lack of or wrong choosing of methodologies. Etc.&nbsp;    6 References   Brookshear, J. Glenn, (2012). Computer Science An Overview. 11th edition. Boston Massachusetts: Pearson. Addison Wesley.   Hartman, Bob. (2009). “New to Agile? Use a Rules of Engagement document.”. [Online] Available from: http://www.agileforall.com/2009/05/new-to-agile-use-a-rules-of-engagement-document-2/. (Accessed 07-11-2014).   Marraju, Bollaparagada V. (2013). ” Agile vs Iterative vs Waterfall models”. [Online] Available from: http://www.slideshare.net/MarrajuBollapRagada/agile-vs-iterativevswaterfall?qid=7f0e9a77-6da6-4f4a-884d-cff13f0edb81&amp;v=qf1&amp;b=&amp;from_search=1. (Accessed 07-11-2014).   PMI (2013). “Communication: The Message Is Clear”. [Online] Available from: http://www.pmi.org/~/media/PDF/Knowledge%20Center/Communications_whitepaper_v2.ashx. (Accessed 07-11-2014).   PMI (2013). “THE HIGH COST OF LOW PERFORMANCE: THE ESSENTIAL ROLE OF COMMUNICATIONS”. [Online] Available from: http://www.pmi.org/~/media/PDF/Business-Solutions/The-High-Cost-Low-Performance-The-Essential-Role-of-Communications.ashx. (Accessed 07-11-2014).   UoL. (2014). “Computer Structures — Lecture Notes Week 10: Software engineering and data structures”. [Online] Available from: https://elearning.uol.ohecampus.com/bbcswebdav/institution/UKL1/201480_SEPTEMBER/MS_CKIT/CKIT_501/readings/UKL1_CKIT_501_Week10_LectureNotes.pdf. (Accessed 07-11-2014).   &nbsp;   Best wishes   Bo W. Mogensen 
	              
	              Hi Bo,
Thanks for the insightful post. I enjoyed reading it. I especially like that you've mentioned that the SRS should be a "dynamic documnet".
The Rules Of Engagement by Bob Hartman looks pretty scary though. I don't think I'm going to sign such an agreement:P
br, Terry
	              
	              Communications breakdowns during the analysis phase of software development
 Software development begins with the requirements analysis phase with the goal to identify what the system needs to do and how external people will use the system. This is a crucial phase on the software Life Cycle (Bookshear 2011). In order to identify the requirements of the software, the stakeholders must communicate with the Analysts or Developers. This must be a two-way communication to ensure the requirements are transmitted, understood and validated (Al-Rawas, Easterbrook 1996a). Thus, communication breakdowns in this phase can have profound consequences in the design and implementation phases of software.
I will expose here three communication problems that can happen during the analysis phase.
Domain knowledge
The first communication problem during this phase is related to domain knowledge. Shortage of knowledge is common in most organizations (not only related to the software industry), so individuals do not have all the necessary knowledge in a particular field (Walz, Elam and Curtis 1993). Consequently, stakeholders rarely have the knowledge to understand the technicalities of software development. But most importantly, Analysts or Developers may not have a domain-specific knowledge required to understand the stakeholders. Take for an example the case of a Java developer that has been developing software for the automotive industry but that is hired to develop software for the healthcare industry. Knowledge acquisition is a very time-consuming activity (Walz, Elam and Curtis 1993), when not done properly or underestimated then communication failures are bound to happen.
Organizational Barriers
Companies are often organized in functional areas and teams, this dictates two important things: how individuals and teams communicate between each other and how individuals are selected to be part of the different phases of the software development cycle.
Participants of the analysis phase may not be able to communicate effectively between due to certain company guidelines. Policies such as “only managers can interact with external customers” can have an important impact on the analysis phase when gathering customer requirements. How companies are organized can also influence communication between the analysis phase participants and participants of other phases as they may not be the same people at all (Al-Rawas, Easterbrook 1996b).
Common sense would be to choose the best individuals (in the domain knowledge area) to participate in the analysis phase (or for any phase really). However, as Al-Rawas and Easterbrook (1996) found out this selection relies heavily on the company authorities (whenever or not an individual has an authoritative position) rather than domain knowledge (Al-Rawas, Easterbrook 1996b). Having the wrong people in the can end up in communication failures.
Personality Types
Although Jung’s theory of psychological types is quite debated in psychology. Some studies related to software engineering have been done based on Jung’s categorization. As it turns out the majority of people working in software engineering are ISTJs (Introversion, Sensing, Thinking and judging). What these results hint at is that in the software engineering profession there is a majority of introverts, “who typically have difficulty in communicating with the user” (Capretz 2003). As Capretz continues in his research, this translates in Analysts or Developers not able to properly verbalize how a task need to be performed. In turn, because the software development cycle depends on documentation important requirements or procedures might be lost along the different documents.
I believe that, although a debatable argument in terms of Jung’s psychological types, personality traits do have play at how people behave in any group. Whenever someone is an introvert, shy or the best team player, it affects how communication is driven an executing within a team. Failure to have a good mix of different personalities will hinder the group communications (internal and external) and performance.
How to alleviate communication breakdowns
In terms of domain knowledge, the best solution is training and thoroughly, critical analysis. In my experience training is always underestimated and under budgeted, organizations and teams need to recognize when training is mandatory in order to produce proper results. Also, individuals must be critical and accept what they do not know. I have seen very often people not admitting their knowledge limitations.
Organizational barriers are the most difficult to address as it requires a critical introspection of companies on how they work. Nevertheless, an effort to communicate better between different software development phases could be as simple as structure meetings with the right people and between the phases. Meetings should always happen between all stakeholders and analysts/developers without any exclusions. The analysis phase should have participants that are not only managerial, but from all the organization levels relevant to the software development cycle. For example, having managers defining a software solution requirements for the network administrators without having any network administrator in the analysis team, is simply a bad idea.
Personality traits play a role in any team. Thus, it is important that during the analysis phase a good mix of personalities are brought into the team. The key factor is that a good team work is established between all participants (stakeholders and analysts/developers). This may, very well, be over sighted, but management should consider steps into improving the formation of teams.
&nbsp;
References
Al-Rawas, A. and Easterbrook, S. (1996a) 'Communication Problems In Requirements Engineering: A Field Study', Proceedings of the First Westminster Conference on Professional Awareness in Software Engineering, London, 1-2 February. pp. 1. Available at:&nbsp;http://www.cs.toronto.edu/~sme/papers/1996/NASA-IVV-96-002.pdf.
Al-Rawas, A. and Easterbrook, S. (1996b) 'Communication Problems In Requirements Engineering: A Field Study', Proceedings of the First Westminster Conference on Professional Awareness in Software Engineering, London, 1-2 February. pp. 6-8. Available at:&nbsp;http://www.cs.toronto.edu/~sme/papers/1996/NASA-IVV-96-002.pdf;.
Bookshear, J. G. (2011) '7.2 The Software Life Cycle' in Computer Science: An Overview, 11th edn, Addison-Wesley, pp. 304.
Capretz, L.F. (2003) 'Personality types in software engineering', International Journal of Human Computer Studies 58(2), pp. 7 November 2014-207-214 doi: 10.1016/S1071-5819(02)00137-4.
Walz, D., Elam, J.J. and Curtis, B. (1993) 'Inside a software design team: Knowledge acquisition, sharing, and integration', Communications of the ACM 36(10), pp. 8 November 2014-63-77 doi: 10.1145/163430.163447.
&nbsp;

	              
	              Week 10 DQ 1- Communications

Babatunde KOLAWOLE – 8 November 2014 

Introduction


The strength of a design team (Programmers, artists, designers, project managers, etc.) most times lies in its noticeable heterogeneity, but could also be problematic because every person has different ways of thinking, and different ways of doing things; an engineer’s expertise and experience is different from that of an artist, who’s expertise is also different from a designer and a project manager.


A Programmer with different expertise and experience
An Artist with different expertise and experience
A Designer with different experience and expertise
A Project manager with different experience and expertise


Let’s say we put them all in a design team for a particular task.

Each member brings different experience to the task, knowing they all have different areas of responsibility with radically different concepts of design on their minds.

The content of cognition on design in communication between these different members is the major element that will influence any collaborative design process, and communication breakdowns often pose difficulties.


The Analysis Phase

The analysis phase of software development determine the essential quality or required activity of the software, independent of how these required activities will be completed. This analysis phase defines as well, the state of difficulty that a customer needs to be resolved. The ready for delivery result at the end of this analysis phase is a demand document. Most preferably, in the best case this document states in a clear and precise fashion what is to be built.

The analysis phase represents the “what” phase which will be summarized below






Table 1: The Analysis Phase: What does the system do?





Phase


Deliverable




Analysis


Requirements Document



&nbsp;

Domain Ontology



&nbsp;

- Things



&nbsp;

- Actions



&nbsp;

- States



&nbsp;

Typical Scenarios



&nbsp;

Atypical Scenarios











The requirement document may be communicated in words and or pictures based on mathematical logic. In a traditional manner, the document is written in English or another written language.

The requirement document does not specify the architectural or implementation details, but specifies information at the higher level of description. 

To make by combining materials and parts, a quality interview/communication with the costumer is to be made and it require a skill that takes time and practice, this can be achieved by performing in-depth requirements gathering and analysis whereby the analyst will be able to provide the design team a solid document which will make their job easier with a good work ethic.

Communication Breakdown

Having said that, a poor communications &amp; misunderstanding creates a gap between what is required by the client and what gets created which has an impact on the cost and product delivery. As described in Figure 1below;





Solutions for alleviating Communication Breakdown


To avoid the above misunderstanding and communication breakdown, some effective ways to improve communication in system development are listed summarized:



Communication style


For effective communication, one needs to know and comprehend the nature of various communication styles, as they have an effect upon each other. Listed here are four styles a manager, employee and customer can put in service when communicating with each other: the open, closed, blind and hidden styles. Shown in Figure 2;




Open style: this style tend to use the process of offering valid and well-reasoned opinions about the work of others, including both positive and negative comments, in a friendly manner rather than an oppositional one while empowering everyone through team decisions, feedback and willing to share feelings, knowledge which may lead to productive conflicts
Closed style: clients involved in this style are usually productive, hard workers who experience physical well-being or relief when working with programs and rather than interacting with or towards others people.
Blind style: in most situations, clients involved in blind style are subject-matter experts that carry out actions with little or no input from other people. They so much believe in themselves than others, people in this category are advantageous in solving program bugs and other issues since they are predominately subject matter experts. 
Hidden style: clients involved in this style of communication prefers social environment, they want to be friend with everyone, get along with them and try every means to avoid conflicts. This makes them to be generally liked by people. Hidden people oftentimes appear to be interested because they ask meaningful questions and stimulate others to share, using this means to disguise their lack of revelation.


Effective Listening


Effective listing grabs the opportunities and issues to be found and solved. Some particular errors in system design are due to poor listening, listed here are some stages of listening a system designer can make use of to improve his or her listening skills: Sensing, Interpreting, Evaluation, Responding and Memory.


Sensing: face the speaker and maintain eye contact for a basic ingredient of effective communication. Listeners ascertain to the highest degree, importance of the received information.
Interpreting: listen to the words and try to picture what the speaker is saying. Listeners make clear meaning for the message heard, seen and felt.
Evaluation: be attentive, but relaxed. Listeners evaluate and think about the message they received.
Responding: wait for the speaker to pause to ask clarifying questions then give the speaker regular feedback. System designer must transmit information and their feedback after sensing, interpreting and evaluating.
Memory: here is the last stage where the information is to be lay aside for future use and shared with others. 


Effective Structure of Questions for Interviews


A skilled system analyst should create (as an entity) questions during the demand phase when using any system methodology; listed are some different methods that allow the interviewer to resolve conflicting information:


Open-Ended Questions: allow the person who is interviewed to decide what type of information to share.
Closed Questions: control the answers of the person who is interviewed.
Hypothetical Open Questions: allow the person who is interviewed answer the question in the way he or she prefers.



Conclusion

I agree that communication breakdown can occur during analysis phase of software development, in almost every study; communication related issues are the most frequent problems on analysis, but strictly correctly identified required activity resulting from effective communication and collaboration among all stakeholders will provide the best chance of creating a system with no faults which will fully satisfy the needs of the customer.


Personally, I believe that a better understanding will increase the quality of analysis, which saves time for the design and implementation phases of the systems development life cycle.


References


UoL. (2014). “Computer Structures — Lecture Notes Week 10: Software engineering and data structures”. [Online] Available from: https://elearning.uol.ohecampus.com/bbcswebdav/institution/UKL1/201480_SEPTEMBER/MS_CKIT/CKIT_501/readings/UKL1_CKIT_501_Week10_LectureNotes.pdf. (Accessed on 6 October 2014)


Brookshear, J. Glenn, (2012). Computer Science An Overview. 11thedition. Boston Massachusetts: Pearson. Addison Wesley, p304-307


C Eckert, The Communication Bottleneck in Knitwear Design: Analysis and Computing Solutions [online] Available from: http://www.cse.dmu.ac.uk/~mstacey/pubs/bottleneck/communication-breakdown.pdf (Accessed on 6 October 2014)


Dianne S, (2012). Forbes 10 Steps To Effective Listening [online] Available from: http://www.forbes.com/sites/womensmedia/2012/11/09/10-steps-to-effective-listening/ (Accessed on 6 October 2014)

TechTarget. (1999). Software Development Lifecycle. First published 2009, from searchsoftwarequality: http://searchsoftwarequality.techtarget.com/sDefinition/0,,sid92_gci755068,00.html (Accessed on 6 October 2014)


Jeremy B, Importance of Interview and Survey Questions in Systems Analysis [online] Available from: http://www.umsl.edu/~sauterv/analysis/Fall2010Papers/Brugger/index.html (Accessed on 6 October 2014)
Brume O, (2010). Communication Methods Effective Ways to Improve Internal and External Communication in System Development [online] Available from: http://www.umsl.edu/~sauterv/analysis/Fall2010Papers/Oduaran/ (Accessed on 6 October 2014) 
	              
	              Hi Augusto,
Thanks for the very informative post.
I'm an ENFP. Probably not very suitable for an engineer:(
I'm a "P" and my wife is a "J". We are a nice team when going on a vacation (and she does all the planning). I'm adding the MBTI test for the reference of people who don't know it before:-)
br, Terry
References:
Myers-Briggs Type Indicator. (2014, November 6). In&nbsp;Wikipedia, The Free Encyclopedia. Retrieved 23:18, November 8, 2014, from&nbsp;http://en.wikipedia.org/w/index.php?title=Myers-Briggs_Type_Indicator&amp;oldid=632755324
	              
	                  Data structures are abstraction defined on a collection of data and its associated operations. Data Structures are designed for improvement of programs and algorithms. Examples are stacks, queues and heaps. Data structures can also be used for conceptual unity, e.g. name and address of person. Design of the data structures in a program efficiency is important. (SWEBOKv3)  Data structures can be linear or nonlinear from a physical and logical ordering.  Linear structures organize data items in a single dimension, where each data entry has one predecessor and one successor (with exception of first and last entry).  Nonlinear structures organize data items in two or more dimensions, where one entry can have multiple predecessors and successors. E.g. heaps, hash tables and trees  Another type of data structures is known as compound structures, compound data structures build on top of other data structures. E.g. graphs or sets  I am going to recommend using a TRIE (The word "trie" comes from Retrieval, and it is pronounced as "try") data structure for designing a phone contact list.  The contact list will have &nbsp;with 3 fields name, phone number and address.  Why use Trie (stackoverflow)  TRIE is an ordered tree data structure and utilizes strings as keys. Not like &nbsp;Binary Trees, TRIE does not store keys that are associated with the node. The key is identified based on the location of the node on the tree. Any descendants of a node will share a common prefix of the key string associated with that node, thus, trie is also know as as Prefix Tree.&nbsp; Trie would be best suited for the phone contact list due to its efficient in matching strings capabilities  Explanation of Trie (quora)  A prefix tree, or trie (often pronounced "try"), is a tree that does not hold keys, but&nbsp; holds partial keys.&nbsp; For example, if we have a prefix tree that is storing strings, then each node would contain a character of a string.&nbsp; If we have a prefix tree that is storing arrays, each node would be an element of that array.&nbsp; The elements are ordered from the root.&nbsp; E.g&nbsp; if we look at the a prefix tree with the word "helloworld" in it, then the root node would have a child "h," and the "h" node would have a child, "e," and the "e" node would have a child node "l," etc.&nbsp; The lowest node of a key would have a boolean flag on it indicating that it is the end of a node with some key.&nbsp;&nbsp; Prefix trees are good for looking up keys with a particular prefix.     Example:     &nbsp;  References  SWEBOKv3 Guide to the software engineering body of knowledge version 3.0 page 232  QUORA URL http://www.quora.com/What-is-the-difference-between-a-tree-a-prefix-tree-and-a-radix-tree  STACKOVERFLOW URL http://stackoverflow.com/questions/9107438/data-structure-for-phone-book/  &nbsp;  &nbsp;  &nbsp; 
	              
	               &nbsp;  &nbsp;  &nbsp;  &nbsp;     &nbsp;  In soft-ware development, communication means that different people are working on a common&nbsp;project agree to a common deﬁnition of what they are building, share information and&nbsp;mesh their activities. Kraut and Steerer (1995)  &nbsp;  Communication Types and Techniques (cf. Pikkarainen et al., 2008)  &nbsp;  Informal  &nbsp;    Face-to-Face discussions, different location or distributed teams   Informal discussions by use of different communication channels   Any form of ad hoc communication    &nbsp;  Formal    in a Group, steering committee meetings and milestone meetings   Status meetings reviewing project updates   Formal meetings by using different communication channels   Formal Documentation, e.g. speciﬁcation doc, reports or meeting minutes   Source Code review    &nbsp;  &nbsp;  Communications Best Practice  Create tangible expectations with defined deadlines for all stakeholders in the project, including the development team. Effective communication depends on developing clear expectations that are transparent to everyone in the project. All team members should know the major deadlines and milestones for the projects. If everyone on the project is not set &nbsp;clear deadlines and expectations, project team &nbsp;won't be able to communicate with each other regarding any of their deadline issues. (EHOW)  Re-iterate and follow-up when communication request. E.g. if a developers team member has promised a specific documentation team member to have a piece of the code updated by a certain time, the documentation team member should follow-up to make sure the code has been updated before proceeding with any documentation. This is to ensure that the documentation team member does not begin to document the code before changes have been made, otherwise the work is outdated and irrelevant.  Be clear and concise project meetings. It is important to understand what information is important to each stakeholder and spend appropriate time in meetings wisely. Stay to the point. E.g. &nbsp;The development team may not need to know every detail of the testing team's progress in a meeting. However, they will need to know what changes to make to the system based on testing results.  Document all changes to the project in writing and communicate to all team members. &nbsp;This will ensure that everyone on a project is on the same page. This practice will all reduce confusion and keeps the communication about the changes relevant to everyone.  Create an honest and open communication environment. If a team member is not able to honestly &nbsp;communicate their work progress, the project will suffer. You need to promote an open communications dialogue  &nbsp;  Keep an open door policy, promote an environment that any developer can communicate, escalate any issues early. Be the conduit for communication between all team members     All of the above communications best practices applies throughout the project life cycle including the analysis phase of the project.  Analysis phase of the project will set the ground work for the project. Analysis phase will determine the more detailed estimates, cost, framework, &nbsp;methodology and pace for the overall project.  Reasons why communications channels may not always be as effective in projects during the analysis (or discovery phase):    Client/Customer in a rush to get the projects underway and might want to skip or shorten the discovery/Analysis phase   The technical team might believe they have deep subject matter experience in the project and decide to forgo or shorten the discovery/Analysis phase   The team is not relaxed and does not feel they can communicate their real concerns about the challenges of the project.    Steps to take to ensure a successful Analysis/Discovery phase  Follow the SDLC (assuming a waterfall approach) development methodology. Ensure Analysis has set aside at least 15% of the overall time/budget of the overall project. The output of the analysis feeds the design, the output of design feeds code phase etc…  Establish a team sprite, everyone is in it to succeed.  &nbsp;  Reference  Robert E. Kraut and Lynn A. Streeter. Coordination in software development.  Commun. ACM&nbsp;, 38(3):69–81, 1995.  &nbsp;  M. Pikkarainen, J. Haikara, O. Salo, P. Abrahamsson, and J. Still. The im-pact of agile practices on communication in software development.  EmpiricalSoftw. Engg., 13(3):303–337, 2008  &nbsp;  URL http://www.ehow.com/how_6163272_achieve-effective-communication-software-project.html  &nbsp; 
	              
	              Hello Class,

Later in the Master’s Program, some of you will delve further into Agile software development techniques.&nbsp;&nbsp;Which of these methods, if any, have you been exposed to, and how in your opinion can “Agile” techniques help reduce the likelihood of miscommunications, during large software system development?
&nbsp;
Anthony

	              
	              Hi Everyone,
One of the data structures often encountered is the sparse matrix data structure. &nbsp;What are the basic characteristics of such structures?
Which commercial software applications have you come across that typically use sparse matrix data structures?&nbsp; Could any of these applications use alternative data structures, or is the sparse matrix the only viable data structure?
Anthony

	              
	              Week 10 DQ1 – Software Development: Communications
Craig Thomas – 9 November 2014
Communications is a key and critical activity with all software engineering projects, and the importance of effective communications through the entire software development lifecycle should not be underestimated.
The importance of communications is not just concerned with the project team. It is also just worth highlighting that consideration is also needed for the software communication mechanisms, with particular consideration for maximising cohesion and minimising coupling and interdependencies with the different modules or programs within the software solution itself.
This paper however will focus on teamwork communications through the requirements analysis phase. 
Software Development Lifecycle
Typically, there are four key phases in the software development lifecycle, namely: requirements analysis, design, implementation and testing. There are a number of different software development lifecycle methodologies such as waterfall, spiral, agile etc. Whilst they offer different approaches, they each have common functions for dealing with each of the four key lifecycle phases.
Requirements Analysis
The requirements analysis phase is characterised by the intensity and importance of communications activities. It is during this phase that the various stakeholders must be able to communicate their requirements to the software developers and that the software developers are able to communicate the specifications generated back to the stakeholders for validation (Al-Rawas and Easterbrook, 1996).
According to Brookshear (2012, pg. 304), the requirements analysis phase requires significant input from all key stakeholders. Typically, project definition workshops will be undertaken and as part of that process, a requirements specification is produced, which becomes the agreed position between all parties in relation to the articulation of the requirements.
This specification will include a firm objective in describing ‘what’ services that are proposed as well as identifying any key dependencies and constraints (Brookshear, 2012, pg. 304). Any ambiguity here could have varying degrees of impact and as such, it is therefore crucial that this starting point is clear and fully understood by all key stakeholders, and is carefully coordinated. It is also important to note that whilst the ‘how’ is a key component to the design phase within the lifecycle, Brookshear (2012) suggests that software engineers may also look to consider ‘how’ the ‘what’ can be delivered as part of the requirements analysis phase, as both the ‘how’ and ‘what’ are often intrinsically linked, sometimes inseparable.
There are a number of outputs or tools of the trade that can be used from the requirements analysis phase, which serve to help crystalise exactly ‘what’ will be delivered. These can include objective statements, structure charts, data flow diagrams, data dictionaries and use case diagrams. As part of the design phase focusing on the ‘how, outputs are also extended to include examples and demonstrations, blueprints, mockups or models. 
I have worked in environments where prototyping has been adopted at the beginning of the project, which help to ‘bring-to-life’ an example of the solution. This was particularly effective in demonstrating complex customer management systems, showing ‘what’ it could look like with a clear representation of the customer data through the portal. 
The requirements specification is a key area of potential pitfall in relation to communications. According to Brookshear (2012), it is ever too often that this specification “fails to provide this stability” which potentially results in some form of impact to the overall solution and as such, may not deliver what the client stakeholder requires (Brookshear, 2012, pg. 304). 
Furthermore, Curtis et al (1988) suggest that the requirements specification is also sometimes complex and extremely large, meaning that no one member of the software team has read it all which also potentially leads to misunderstandings and conflicting views. Therefore, it is critical to provide validation and agreement to the specification.
According to Capretz (2002, pg. 214) “the software field is dominated by introverts who typically have difficulty in communicating with the user”. Let’s take the software engineer / programmer. The Myers Briggs Type Indicator &nbsp;(MBTI) instrument suggests that programmers are typically introverts and INTJ type (Know Your Type, 2014), which supports Capretz assertion. This personality type becomes frustrated if not given the opportunity to make things better.
Curtis et al (1998) provide a useful diagram (Fig. 1) that demonstrates both the type of challenges faced by the programmers, as well as, when used properly, highlighting some key questions for information and dependencies to help supplement their knowledge that is critical to the performance of the project members. 
Fig. 1
&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
Knowledge transfer increases the awareness across the entire project, with the concept of a ‘collective mind’ (Fig. 2) McChesney and Gallagher (2003). 
Fig. 2
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;
In addition to the project review workshops, a number of iterative or incremental software development lifecycle methodologies also prescribe various operating practices to accommodate for this need, and are usually coordinated and managed by the project manager. As used in the agile methodology there are activities such as daily stand ups and briefing meetings for efficient and effective communications. These are best done face to face, to enhance the engagement between the project teams and the customer representatives. This is an effective method for describing the activities being performed, and for any barriers to be addressed immediately.
This also enables an iterative or incremental review of the requirements specification and if needed, creates the basis for change through the project lifecycle, and preventing the delivery of a solution that ultimately, is not fit for purpose. 
Project managers tend to be extroverts, perhaps MBTI ENTJ type, who are great leaders and managers and can plan long term. They also have skillsets to help in resolving conflict, so particularly effective at navigating through some complex and political situations.
Adappt (2011) provide a useful diagram (Fig. 3), which highlights the agile methodology and the benefits that can be realised from such an approach, with particular reference to team communications and visibility and openness with customers.
Fig. 3&nbsp;&nbsp; 
&nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp;
&nbsp;
This is in contrast to the traditional and less adaptive approach, such as the waterfall methodology, which although is well suited to contractual software developments in providing a clear distinction, breakpoint and output from each lifecycle phase, it also means that the requirements specification is generally a fixed agreement, with any changes being complex to introduce and manage.
Another approach, via the spiral methodology, whereby risk is a key aspect of the development process, status meetings are held with all key stakeholders to review the progress and to agree on a go/no go decision relative to the next steps in the phase. The critical aspect to this regular contact is to ensure that there are no misunderstandings or miscommunications in the software project deliverables. Software engineers have found that this straightforward and frequent communication with all stakeholders is mandatory (Brookshear, 2012, pg. 304) for a successful project delivery.
Conclusion
In summary, there are many reasons for how communications can breakdown with the software development lifecycle, however there are also ways to mitigate and limit this through the adoption of effective communications management practices.
In closing, I would subscribe to the same conclusions offered by McChesney and Gallagher (2003) in that, “co-ordination theory was useful for conceptualizing, abstracting and analyzing dependencies in project activity… and collective mind theory was useful for describing the use of tacit knowledge and the role of co-ordination mechanisms in support of this”
Best Wishes, Craig
References
Laureate Online Education. (2014). Computer Structures – Lecture Notes Week 10: Software engineering and data structures [Online]. Available from: https://elearning.uol.ohecampus.com/bbcswebdav/institution/UKL1/201480_SEPTEMBER/MS_CKIT/CKIT_501/readings/UKL1_CKIT_501_Week10_LectureNotes.pdf (Accessed 7 November 2014)
Brookshear, J. G. (2012). Computer Science An Overview. pp. 301-333. 11th ed. Boston Massachusetts: Pearson. Addison Wesley
Virginia Tech. (n.d.). Software Life Cycle Models [Online]. Available from: http://courses.cs.vt.edu/~csonline/SE/Lessons/LifeCycle/index.html (Accessed 6 November 2014)
Al-Rawas, A., and Easterbrook, S. (1996). Communications Problems in Requirements Engineering: A Field Study [Online]. Available from: http://www.cs.toronto.edu/~sme/papers/1996/NASA-IVV-96-002.pdf (Accessed 7 November 2014)
McChesney, I. R., and Gallagher, S. (2003). Communications and co-ordination practices in software engineering projects [Online]. Available from: http://pisis.unalmed.edu.co/~cmzapata/cursos/requisitos/communication.pdf (Accessed 7 November 2014)
Capretz, L. F. (2002). Personality Types in Software Engineering. pp. 214. [Online]. Available from: http://www.eng.uwo.ca/people/lcapretz/mbti-IJHCS-v2.pdf (Accessed 7 November 2014)
Know Your Type. (2014). Personality Testing for Groups and Individuals [Online]. Available from: http://www.knowyourtype.com/myers-briggs/16-types/intj/ (Accessed 7 November 2014)
Curtis, B., Krasner, H., and Iscoe, N. (1988). A Field Study of the Software Design Process for Large Systems. pp. 1268-1287 [Online]. Available from: http://www-public.int-evry.fr/~gibson/Teaching/CSC7003/ReadingMaterial/CurtisKrasnerIscoe88.pdf (Accessed 7 November 2014)
Adapt. (2011). Agile Thinking [Online]. Available from: http://www.adappt.co.uk/agile-thinking (Accessed 8 November 2014)
	              
	              Dear Dr. Ayoola, This is an interesting topic:-) I've never used it myself. Typically, a real-world systems it's not so coupled that everything is related to everything else. Usually, one thing is depending on one or a couple of other things. Therefore, using a full matrix to represent the coupling is a bit wasteful. And this type of matrix is called a sparse matrix. Because a sparse matrix can be optimized in storage and algorithms, it makes it possible to deal with large matrix more efficiently. It's a very popular data structure in many areas including data mining and graphic processing. In Python, the Scipy library provides most of the sparse matrix storage type and the functionalities:        from  scipy.sparse  import  *        B  =  rand(  10  ,  10  ,  0.1  )       stripe  =  hue  *  B      The above example created a 10*10 random sparse matrix. And then use it to compute with other matrix. One example of application of sparse matrix (not commercial though) is the colorizing function in the open source graphic processing software GIMP. I have no idea about other software like Photoshop, because they are not open source.       br, Terry Reference: Sparse matrix. (2014, November 8). In&nbsp;Wikipedia, The Free Encyclopedia. Retrieved 14:49, November 9, 2014, from&nbsp;http://en.wikipedia.org/w/index.php?title=Sparse_matrix&amp;oldid=632990265 http://docs.scipy.org/doc/scipy-0.14.0/reference/sparse.html https://github.com/weibel/colorize-gimp
	              
	              1 Introduction   In IT, a structure is a way of organizing things, e.g. organizing data in a way so that they are easy to store, find and manipulate/process. We use data all the time whether we think about it or not hence we also use data structures and therefore we are dependent on the engineers to choose as correct data structures as possible.&nbsp; An example is the use of our phone. But most of us don’t remember all phone numbers and other contact information’s thus we need a well-constructed contact list – and this will be the focus example in this assignment.   2 Thesis   Humans and IT can’t do without structures. Therefore engineers need to continuously improve data structures suitable and efficient for current and upcoming demands.   3 Common types of data structures   The most common types of data structures are listed below. Depending on the end-user solution and the amount of data, then some structures are more efficient than others.       Name   Description / Characteristic   Comments / Examples     Arrays   There are two types:  Homogeneous: same type of data Heterogeneous: different data types  &nbsp; Arrays are essentially tables with one or more dimensions with rows and columns. &nbsp; They are static in shape and size, i.e. a block of memory big enough for the data is reserved upfront. &nbsp; Data are not necessarily stored sequentially, thus the use of indexes identifying the rows and columns (if two dimensional). &nbsp; When implementing arrays we must take into consideration whether the data are:  Homogeneous, herein one or many dimensional Heterogeneous, herein contiguous or separate blocks    A contact list will contain different data types, thus the heterogeneous array would be better suitable for a contact list. &nbsp;     ID   Sname   Lname   Cell     1   Abc   Def   123     2   Qwe   Poi   987     &nbsp; &nbsp;     Lists   There are two types of lists:  Ordered lists, contiguous and static length Linked lists, data not contiguous.  &nbsp; Ordered lists are typically characterized with having the data in contiguous order and also the data will typically have the same length. &nbsp; Linked list data are scattered around, but kept track of with pointers, and the last cell contains ‘Null’. &nbsp; Almost any data can be visualized in a list, and typically it’s a list of names.   Linked list:        Stacks   Stacks are lists characterised by the LIFO principle, i.e. the entry going last in is the one to go first out. Linear. It’s a restricted list   Stacks are typically use in so-called backtracking situations.        Queues   Queues are also lists, but characterised by the FIFO principle, i.e. the entry that was first in is the one to go first out. Linear. It’s a restricted list, though it’s open it two ends.   This could typically be illustrated ticket sales system where the first buyer is first served. &nbsp;       Trees   Trees are characterised by the hierarchical view. Nonlinear. The simplest form is called binary tree.   The tree could be used in the contact list in several ways, e.g.:  Structuring in alphabet Structuring by company Etc.        4 Analyse merits and weaknesses of common types of data structure   Before going into the specifics of merits and weaknesses, I will ad, that the programming frameworks are often designed with templates that “fix” the data structure, i.e. the programmer in principle don’t have to think about the structure.   Also when discussing which data structure to implement, one also has to analyse whether the data are static or dynamic. In the case of a phone contact list, the data is more of a static nature.       Name   Merits and Weaknesses     Arrays   Merits:  Simple Fast  &nbsp; Weaknesses:  They are fixed in size, so if we need to add more data than originally reserved we will in worst case have to move the entire array to a new location. There must not be gaps.      Ordered lists   Merits  The simple lists are best for simple data, e.g. names, and best if the data is always organized in static sized data blocks. Data are in chronological order and easy to locate  &nbsp; Weaknesses:  When something is deleted then, in worst case, the entire list must be reorganized. If more data are added then in worst case the entire data must be moved to a new area.      Linked lists   Merits:  Flexibility Very useful with dynamic data Don’t need adjacent memory blocks Data can be scattered around, but still kept in order.  &nbsp; Weaknesses:  Complexity There are a lot of pointers to keep track of. Harder to find errors.      Stacks   Merits:  Good for backtracking. Pointers can make stack a bit more flexible  &nbsp; Weaknesses:  More restricted than lists.      Queues   Merits:  Simple  Weaknesses:  Restricted in use      Trees   Merits:  Hierarchical Sorted  Weaknesses:  Can be slow&nbsp;        5 Which is best for a phone contact list?   I would recommend the Tree data type because of its properties, e.g. that the data are always sorted.   6 Conclusion   Phones evolve all the time. It’s not enough anymore to just have a plain contact list. The list must integrate to e.g. GIS and GPS systems, mail, and other systems it such a way, that it’s fast and easy to find not just the person but also other relevant information regarding this person.   There is a huge difference between the way data are actually stored and how we normally picture them in our head or in diagrams. Computer memory is organized in a linear way, not in e.g. grids like we know it from spreadsheets.   Programmers will have to decide and make trade-offs between flexibility and complexity.   7 References   Brookshear, J. Glenn, (2012). Computer Science An Overview. 11th edition. Boston Massachusetts: Pearson. Addison Wesley.   UoL. (2014). “Computer Structures — Lecture Notes Week 10: Software engineering and data structures”. [Online] Available from: https://elearning.uol.ohecampus.com/bbcswebdav/institution/UKL1/201480_SEPTEMBER/MS_CKIT/CKIT_501/readings/UKL1_CKIT_501_Week10_LectureNotes.pdf. (Accessed 07-11-2014).   Wikipedia. (2014). “Data structure”. [Online] Available from: http://en.wikipedia.org/wiki/Data_structure. (Accessed 07-11-2014).   &nbsp;   Best wishes   Bo W. Mogensen 
	              
	              Hi Terry
Yes, the rules of engagement can be scary. I actually never write these down, but I have in some projects talked about some rules in the project group, especially if I expect some human problems/challenges. So the rules are merely an attempt&nbsp;not to forget or ignore them, but consider them and evaluate whether you want to do anything about it.

Br. Bo
	              
	              


Week 10 DQ 1 : Communications


1. A communication gap may arise between the a user group and the development team during the systems analysis phase as a result of misunderstanding or a complete lack of understanding of each other’s domain knowledge, without common ground e.g. terms and vocabulary that is clear and concise such differences in domain knowledge become problematic and could&nbsp; result in a major communication breakdown. For example an architect should understand what the building he is designing will be used for if they are going to deliver a stellar design.





Areas of knowledge


Abstract Knowledge


Concrete Experience




Users’ present work


1. Relevant Structures on users’ present work

Users and Developers need


4. Concrete experience with users’ present work

Users have, and Developers need




New System


2. Visions and Design proposals

Users and Developers need


5. Concrete experience with new system

Users need




Technological Options


3. Overview of Technological options

Developers need


6. Concrete experience with technological options

Developers have, and Users need






Solution:
Domain knowledge is absolutely essential, therefore the development team needs to be brought up to speed about the inner workings of the organisation-so that they may be able to see the problem from their users’ perspective and acquire the necessary knowledge required to better understand the problem from a business standpoint and not just from an IT perspective where the only communication between the development team and their users is through documents and tickets.&nbsp; Tools and techniques to facilitate this knowledge development on both parties involve; observations, interviewing users, developers In order to improve understanding between the development team and the users, there should be some domain knowledge exchange, which can be done using the following techniques ; doing users work, video recording, mock-ups, think-aloud experiments, use of diagrams, conceptual modelling, prototyping, data flow diagrams and drawing Entity relationship diagrams among many other techniques.



2.Inappropriate, incomplete or inaccurate documentation- the systems analysis documentation focuses on the desired system, not the current system. Thus the deliverables from this phase can be seen as the blue print from which the new system will be built. If the documentation is inappropriate, incomplete or inaccurate a wrong system could be built. Communication difficulties here stem from deficiencies in the problem definition such as incompleteness, poor assessment of the problem, a lack of misunderstanding, unclear goals and a lack of awareness of all the necessities. An incomplete requirements document can be seen as being a product of context avoidance and poor communication.

Solution:
Development teams must ensure that the documents produced from the analysis phase are using common vocabulary and notation that is unambiguous, precise, has clear beginning and end with no loose ends (completeness) as well as a high level of abstraction. It might be beneficial to additionally represent the user requirements in a&nbsp; non-textual format for instance using graphical models such as diagrams, tables, storyboards, equations and using prototypes. to facilitate communication and ensure there is no ambiguity, use of standard notation is a must. Documents and designs should be reviewed to ensure that nothing has been missed.

References:
http://www.businessjournalz.org/articlepdf/BMR_11108.pdf [Online] accessed 8th November 2014
http://www.researchgate.net/publication/254063865_DESIGN_OF_INFORMATION_SYSTEMS_SIMULATING_THE_EFFECTIVENESS_OF_KNOWLEDGE_TRANSFER_THROUGHOUT_THE_SYSTEM_ANALYSIS_PHASE [online] accessed 8th November 2014
http://pdf.aminer.org/000/367/564/knowledge_discovery_in_scheme_approximation.pdf [online] accessed 9th November 2014
http://pdf.aminer.org/000/558/112/challenges_in_requirements_engineering.pdf [online] accessed 9th November 2014


	              
	              Data Structures, a phone contact list
Before determining which data structure is most suite for a phone contact list, I need to describe the basic data structures. There are three basic data structures: Arrays, Lists and Trees (Bookshear 2011b).
Arrays (or simple List) are a group of elements of the same type (Bookshear 2011a). Arrays can be one-dimensional or multi-dimensional (Close et al. 1998). Arrays are quite effective and fast, especially at store small quantities of data, but they can become slow and complex when using multi-dimensional arrays (Carnegie Mellon University. 2012).
Lists can be divided in three groups: Lists, Stacks or Queues. Lists, or simple lists, are just a collection of data structured in rows, that is, it is the same as a one-dimensional array (Bookshear 2011b).  A stack is a list in which entries are managed at the top of the list (named Head). Like a stack of plates in which you can take out and add in plates only from the top, these lists can only be manipulated using that principle (last-in, first-out, LIFO). Stacks are particularly useful in certain scenarios, however in scenarios where data in the middle of the stacks needs to be accessed stacks become inefficient (Carnegie Mellon University. 2012, Ye 2014). A queue, as described by its name, is a list organized similarly of that of a supermarket cashier queue in which elements that get first-in, get first-out (FIFO), elements get queued at the end of the queue (Tail), while elements get served (or removed) at the start of the queue (Head). Like stacks queues are useful for certain scenarios (such as print queues), but are not suitable when we need to use often data in the middle of the queue.
A Tree is a data structure which elements are organized in a hierarchical way (Bookshear 2011b). Much alike a company employee organization with the CEO being the root node and each employee underneath being a node. As per this definition I can deduce that a tree data structure is particularly fast for indexing (it is quite fast to determine the position of an employee in a hierarchical graph), but because a tree could be quite big then storing the tree in a proper way in memory and traversing it can become cumbersome.
A phone contact list
The most simple contact list of a phone must contain a name and a phone number. But, indeed, the list can contain much more information than this, such as (but not limited to), last name, middle name, address, email, secondary phone number, twitter account and birthday. When it comes to organization, the most common way to organize the list is by alphabetical order based on the name field (whenever it is the first name of the family name, or both).
In order to understand better what kind of data structure will suit better a phone contract list, let me use Figure 1 to illustrate a simple contact list implemented in different data structures. (Note: there are more entries in the list; the figure only highlight some of them).


Figure 1. A simple contact list on different data structures.
&nbsp;
The very first thing that figure 1 display is that none of the structures, but the two-dimensional array one suffice to cover the basic contact list (name, phone number). However, all the structures fall short when we consider a modern contact list with many fields. Perhaps a multi-dimensional would solve the problem, but it can easily become cumbersome to manage. In theory, if it is only to organize the basic contact list, a two-dimensional array is enough. There are scenarios in a phone where this list is good enough. If fact, Array and Tree structures are suitable for different scenarios in the same phone. When I open the Contact application, I will see a list of alphabetically ordered contacts, when I press one contact I see the details of that contact. That list could be easily a two-dimensional array like Contact(name, whatToDoWhenPressed). Instead, if I use the search function in the Contact App, as I type something the application suggest me the names associated with those letters. This search function can easily be organized in a Tree structure similar to the one in Figure 1, where nodes are the letters of the alphabet and child nodes contain the name (or partial name) of the contact, or the object reference of the contact, similar to the “object” whatToDoWhenPressed.
As it turn, out, a Contact entry is a collection of data, which coincidentally match the idea of Object in the Object Oriented programming (OOP) paradigm. Thus, perhaps is better to present the data as an Object rather than a data structure, while the Object data will store the data using a data structure. At that point we could use these basic data structure to index and search the objects and some of their internal data. Figure 2 attempts to display a simplistic way of implementing such scenario.


Figure 2. Simplistic way of implementing a phone contact list with objects and using data structures to index the objects.
Conclusion
As simple as it sounds: implementing a phone contact list is not as easy as it seems. Simplistic speaking it can work with a two-dimensional array, but it most likely is going to be a mix between OOP objects and data inmplementation.
References
Bookshear, J. G. (2011a) '6.2 Traditional Programming Concepts' in Computer Science: An Overview, 11th edn, Addison-Wesley, pp. 252.
Bookshear, J. G. (2011b) '8.1 Basic Data Structures' in Computer Science: An Overview, 11th edn, Addison-Wesley, pp. 342-345.
Carnegie Mellon University. (2012) Unit 6A. Organizing Data: Lists. Available at: http://www.cs.cmu.edu/~tcortina/15110sp12/Unit06PtA.pdf (Accessed: 9 November 2014).
Close, B.D., Robbins, A.D., Rubin, P.H., Stallman, R. and Oostrum, P.V. (1998) Multi-dimensional Arrays. Available at: http://www.staff.science.uu.nl/~oostr102/docs/nawk/nawk_87.html (Accessed: 9 November 2014).
Ye, Y. (2014) Lists, Stacks, and Queues. Available at: http://mendel.informatics.indiana.edu/~yye/lab/teaching/spring2014-C343/lists.php (Accessed: 9 November 2014).
&nbsp;
&nbsp;
Attached Original Word Document, for easier reading.

	                Attachment: &nbsp;Week-10-Discussion2-Data Structures.docx (210.559 KB)
	              
	              Communication dangers in the analysis phase of a project.   &nbsp;   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; There are 101 possible reasons behind why issues and differences arise between the different stems or departments that make up a development or project team. Software development naturally includes many facets of disciplines from programmers, system testers, configuration and release teams, 3rd party suppliers, communications, project managers, commercial and others. The inherent deviances in the members of staffs skill sets to do these rolls can become an issue, I like to relate it to the well know but badly worded ‘Chinese Whispers’ .   The interesting thing about Chinese Whispers is the ability that some then have the chance to knowingly change what they heard into something more humorous or contentious. While this is fun and harmless in a simple game, the same can and does occur in the professional world whether to edge changes or decisions more to their favour or specifically worse.   Haughey, D (2014) describes the worrying statistic from 1995 that “only 16% of software projects were successful 53% challenged (that is cost overruns, budget overruns or content deficiencies) and 31% cancelled.”   He further added that “The average software project runs 222% late, 189% over budget and delivers only 61% of the specified functions. Evidence suggests little has changed since then.”   &nbsp;An interesting thought to start with so why does this happen?   There are many factors within this from not having enough time for the project a cause of under estimating the time needed to build a system or program a piece of software. Budget constraints are a problem by giving unrealistic project costs at the outset to start the project, inadequate testing, lack of quality assurance and most importantly a lack of or issue with communication which is what we are targeting for this assignment.   &nbsp;   What it is that makes a project succeed?   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Dyer, S (2006) explains, “More than 95 percent of team members said that good communication was the reason for their success. When I asked the teams “from your experience, what is it that makes a project fail?” more than 95 percent said that poor communication was the reason for their failures.”   &nbsp;   So what are the reasons communication breaks down?    Fear Expectations Confusion Momentum Dissatisfaction Commitment Incompetence Politics    &nbsp;   Fear can mean different things; fear of change above what the programmer knows and is comfortable with. They may suggest one angle of the solution because it’s what they know and are surer of over fear of the unknown changes which may be cheaper or better solutions. To solve this you must work on trust amongst the teams and departments, trust leads to calm relaxed and frank meetings where fear of change can be eliminated.   Expectations are the differences that one person or team thinks and another expect.        &nbsp;- Programming sucks by Peter Welsh   &nbsp;   The example by Peter Welsh has been around for quite a while but explains it visually really well about the differences each team can see a project as. To get around this you must have a strong and clearly defined project plan, functions analysis report, project deliverables, technical specifications, fully formed requirements and detailed documentation explaining each and every nuance of the work to be done with agreed signoff from all parties.   Confusion is well known; sometimes people do it on purpose because they feel it’s easier to confuse others with technical jargon:        The above being an example, there can be confusions over roles and responsibilities on the project, confusions over timescales and deadlines which is why a good project manager is worth their weight in gold. Without good project management is where I feel the key aspect of communication failures occur. Someone needs to drive all of the teams to one goal and provide the communication skills to align departments and understanding.   Momentum is more concerned about the moments within projects where a lack of team direction is evident. With a loss of momentum in a project it can breed frustration and lack of concentration which is controlled by good communication both to help resolve any delays but also to help motivate the relevant silos of work.   Dissatisfaction is a common factor and is hard to immediately resolve. It comes more from the working environment and individuals happiness in both their position in the job and in the project. With dissatisfaction comes resentment and lack of enthusiasm and thus communication suffers.   Commitment or a lack thereof can be a cause of misdiagnosed communication. EG a perceived lack of commitment. After all people are at work to earn money or at least 99% of them are. Many people treat a job simply as a job and have neither enthusiastic commitment nor a stunted laziness towards work. However others can perceive this as a lack of commitment if improperly communication or understood.   Incompetence is a simple one to view with communication issues. On projects sometimes one of the team or even a whole department’s incompetence to do their part of the tasks can be poorly communicated in the right ways. Either overly aggressive reactions cause further problems or not reporting it at all can lead to the obvious communication issues if someone doesn’t know their own job.   Politics is the final aspect I want to cover and is extremely difficult to quantify. A business direction or plan can and does change during the lifetime of a project and during the analysis phase this can lead to poor decisions being made on either knee jerk reactions to pre-empt future potential political decisions or to simply ignore them altogether. &nbsp;&nbsp;&nbsp;&nbsp;Members of staff can have ulterior motives as well. I have known for communication to be specifically sabotaged for his or her own gains. Sounds scary I know but you never really know your fellow members of staff until such things happen. &nbsp;&nbsp;   &nbsp;   &nbsp;   Conclusion   Communications can break down in many stages of the development lifecycle. We talked specifically about the analysis phase and the way gathering the right documentation from the outset and agreeing upon all important decisions early on. The role of the project manager is essential for communication to work. The statistics I have shown I believe are very true in that when a project works 95% will agree it was because of good communication throughout and when it fails 95% agree it was because of the communication problems. Good business communications plans and management practices with project managers being helped by making sure they are up to date with training and all communication techniques is key. Yes you will always have differences and problems when a programmer talks to an artist who talks to a tester who talks to the guy who boxes the software for the shelf. But within the entire project manager should be in the middle like a link between them all or a neutralising effect to balance the communications.   &nbsp;   I’ll leave you with this great Dilbert Clip:        References:   Brookshear, G. (2011) ‘Computer science: An overview’. 11th ed. Boston: Addison Wesley/Pearson.   Dyer, S (2006) ‘A failure to communicate’ [http://www.projectsatwork.com/article.cfm?ID=232485] (Accessed: 9/10/2014)   Haughey, D (2014) ‘Why software projects fail and how to make them succeed’ [http://www.projectsmart.co.uk/why-software-projects-fail.php] (Accessed: 9/10/2014)   Lorin J. May &nbsp;‘Major Causes of Software Project Failures’ [http://www.cic.unb.br/~genaina/ES/ManMonth/SoftwareProjectFailures.pdf] (Accessed: 9/10/2014) 
	              
	              Developing data structures for a mobile phone contacts list.   &nbsp;   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Data structures come in many forms; they make up the foundation of any piece of software and are essential for the storing, updating, retrieval and deleting of critical information. For this assignment we are tasked to look at the scenario of creating a phone contacts list.&nbsp;   So what different types of structures are there to use?   The Java collections framework, Wikipedia, B (2014) list the following:    List Stack Queue Double-ended queue Set Map    &nbsp;   Wikipedia, C (2014) goes further to explain the different data types being Primitive, Composite and Abstract.       Primitive   Composite   Abstract     Boolean   Array   Array     Character   Record   Container     Floating-point   Union   Map     Double   Tagged Union   Multimap     Integer   &nbsp;   List     Enumerated   &nbsp;   Set     &nbsp;   &nbsp;   Multiset     &nbsp;   &nbsp;   Priority queue     &nbsp;   &nbsp;   Deque     &nbsp;   &nbsp;   Stack     &nbsp;   &nbsp;   String     &nbsp;   &nbsp;   Tree     &nbsp;   &nbsp;   Graph       &nbsp;   While we can see that the primitive data types are not what we are looking for with this example, mobile contact lists could be within the composite or abstract types.   So first let’s lay out what we would need this contacts list to do first.   I’m using my own iPhone as an example for the images and Java for code examples.   We would need the ability to:    Create a new contact Update an existing contact Arrange/search through a contacts list Select/display a contact Delete a contact.    &nbsp;   What information does a contact hold?    Name Phone Number Email address    &nbsp;   Modern contacts lists usually can hold far more information that this such as a photo and company name etc. which can be seen in the image fig.1 below but this example we can stick to the above three which Brookshear, G (2011) describes as a ‘user-defined data type’. Essentially it is a basic building block primitive type, but ‘home-made’ which are conglomerates of these types under a single name.   In Java this could be represented something like this:   import java.io.Classes; // import all relevant classes   &nbsp;   class AddContact {   &nbsp; // This function adds a new contact   &nbsp; static Contact PromptForAddress(stdin, PrintStream stdout) throws IOException {   &nbsp;&nbsp;&nbsp; Contact.Builder contact = Contact.newBuilder();   &nbsp;   &nbsp;&nbsp;&nbsp; stdout.print("Enter a contact name: ");   &nbsp;&nbsp;&nbsp; contact.setName(stdin.readLine());   &nbsp;   &nbsp;&nbsp;&nbsp; stdout.print("Enter an email address: ");   &nbsp;&nbsp;&nbsp; String email = stdin.readLine();   &nbsp;&nbsp;&nbsp; if (email.length() &gt; 0) {   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; contact.setEmail(email);   &nbsp;&nbsp;&nbsp; }   &nbsp;   &nbsp;&nbsp;&nbsp; while (true) {   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; stdout.print("Enter a phone number: ");   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; String number = stdin.readLine();   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if (number.length() == 0) {   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; break;   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }   &nbsp;   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; contact.addPhone(phoneNumber);   &nbsp;&nbsp;&nbsp; }   &nbsp;   &nbsp;&nbsp;&nbsp; return contact.build();   &nbsp; }   &nbsp;   A simple AddContact class which prompts for contact name, email and phone number.        FIG.1 – iPhone Contacts list  You can see from the above images, my phone lists contacts in name order alphabetically and unlike my code example in Java which goes through each step to add the contact details in turn, the iPhone is form based. Which means you can add as little or as much information first in any order before the contact is saved.   After comparing the many different data types on offer to be used as a contacts list I’ve decided the best data type to be used is the ‘List (abstract data type)’. A user defined data type is good for most things but if fails to allow the creation of new data types in the fullest sense.   The reasons behind this choice as described by Wikipedia, D (2014) are:    It is possible to retrieve the element at a particular index. It is possible to traverse the list in the order of increasing index. It is possible to change the element at a particular index to a different value, without affecting any other elements. It is possible to insert an element at a particular index. The indices of higher elements at that are increased by 1. It is possible to remove an element at a particular index. The indices of higher elements at that are decreased by 1.    &nbsp;   No doubt other data types could be used for lists such as this but above features of an abstract list cover all functions needed for a mobile phone contacts list that we need. Alternatives could be arrays, or a combination of trees and maps for specific uses. Each has advantages and disadvantages so it is very difficult to specify a ‘best practice’ solution.   Now the sorting of a list doesn’t have to be predetermined, for example you could have a setup where you specifically need to enter new contacts into the right position much like writing your contacts into a paper diary in order and the list is shown in the order you enter the details. For a contacts list however you don’t need to be this rigid. When you view the list simply arrange the list in alphabetical order by name.   Something similar this:       &nbsp;&nbsp;   .  sort  (  addressBook  .  getContacts  ()),     &nbsp;   So it will not matter what order the list is stored in but the procedure to view the list of contacts will be sorted on generation. As the addressBook function calls the getContacts() procedure it will read all the contacts and sort them by name.   Similarly with the editing or selection of a contact individually from this list:   onSelect.view(addressBook.getContact()),&nbsp;   All contacts have a unique contact id when saved in the list because the one of the positive points to a List (abstract data type) is the ability to have names the same, so you could easily add two contacts with the name of John Smith and the list won’t mind. Selecting on a phone also is either by touch screen or controlled pointer of some sort so there doesn’t need to be any function or trait from the data type as long as the device knows which entry to select and view.   In Java the general use for this type of application would be a list array which shares both the structure an allocation of an array with the same abilities as a list.   &nbsp;     &nbsp;   Conclusion   In summary there are far too many data types to really go into detail with. I selected the List (abstract) data type as I felt it matched the needs for the solution the best. The largest part of the control however is down to the software, viewing in alphabetical lists, searching, selecting and deleting is all software based rather than a control of the data type. The choice of data type simply has to have the ability to store the data in a structured way. It must allow duplicate entries which the List data type does because of the unique id of each entry. It must have the ability to edit individual entries and perform to a good standard.   &nbsp;   &nbsp;   &nbsp;   References:   Apple (2014), ‘Address book framework reference for iOS’ [https://developer.apple.com/library/ios/documentation/AddressBook/Reference/AddressBook_iPhoneOS_Framework/AddressBook_iPhoneOS_Framework.pdf] (Accessed: 8/10/2014)   Brookshear, G. (2011) ‘Computer science: An overview’. 11th ed. Boston: Addison Wesley/Pearson.   Google (2014), ‘Protocol Buffers’ [https://developers.google.com/protocol-buffers/docs/javatutorial] (Accessed: 8/10/2014)   Wikipedia, A (2014) ‘Collection (abstract data type)’ [http://en.wikipedia.org/wiki/Collection_(abstract_data_type)] (Accessed: 8/10/2014)   Wikipedia, B (2014) ‘Java Collection Framework’ [http://en.wikipedia.org/wiki/Java_collections_framework] (Accessed: 8/10/2014)   Wikipedia, C (2014) ‘List of data structures’ [http://en.wikipedia.org/wiki/List_of_data_structures] (Accessed: 8/10/2014)   Wikipedia, D (2014) ‘List (abstract data type)’ [http://en.wikipedia.org/wiki/List_(abstract_data_type)] (Accessed: 8/10/2014) 
	              
	              Communications
Communications is a key function of any relationship. Due to an individual’s understanding that is often influenced by school of thought, education, environment and other factors, there’s always the propensity for people to have different approaches to problem solving, priority setting and decision making.
In the software world, the analysis phase is a fact finding and requirements gathering period fundamental in establishing the objectives of the software development exercise and obtaining sufficient information that would enable a requirements specification document to be defined.
Often times the challenge in this is that the party for whom the software development exercise is being done for, may not have a clear understanding of what they need. The need to be able to clearly communicate requirements helps define the approach to be used in formulating the solution as well as identification of the appropriate technologies that would be used. Adequate and clear communication is also important during this phase because in a contractual situation, the requirements specification that is developed as a result form a legal document that would be relied on should there be any sort of dispute on the project later on.
Software development projects require the participation and involvement of all interested parties and stakeholders for the problem at hand to ensure that user acceptance later is achieved.
Communications breakdown may happen for various reasons including


Lack of clear objectives for the project. If the goals of a project are not very clear to those participating in it, this could lead to different interpretations of priorities. This has an impact as well on the scope.
Lack of senior management support, participation and interest in the project at hand. Without senior management support, critical decisions that may need to be taken are often compromised.
Vested interests/Group Dynamics. Due to group dynamics, it is possible to be in a situation where key stakeholders and participants in a project are in opposing camps. For instance a committee headed by a finance person against a panel of ICT specialist is very likely to differ on budget issues and choice of technology to apply. An example is the Biometric voter registration kit (BVR) that was procured for my countries 2007 elections. It appears that some of the recommendations made by ICT were ignored and the resultant system failure partially blamed on machines procured that did not meet desired specifications.
Inability to decide. This is when no clear buck stop is available to listen in on various options and choose which to go for in the event of an impasse.


&nbsp;
Solutions for alleviating communications breakdown

The best form of communication is having things in writing. This minimizes misunderstanding and helps clear ambiguity. Also ensures that any agreed on item is written down for future reference
Use of appropriate questionnaire and checklists to obtain responses to desired information. When sourcing for information from respondents, having a structured questionnaire also helps to ensure that there is specificity.
Having a project governance structure in place to ensure that the project is organized in a way that has the participation of all the key stakeholders. This can be achieved through a steering committee composed of heads of departments&nbsp; and senior management, key data owners and data custodians
Incorporating key stakeholders and ensuring a structured way of decision making ( deciding how to decide)

References

Professor Roberto, M. &nbsp;‘The Art of Critical Decision Making’, Available at http://anon.eastbaymediac.m7z.net/anon.eastbaymediac.m7z.net/teachingco/CourseGuideBooks/DG5932_G2N3I9.pdf , Accessed 09/11/2014
Virginia Tech (n.d.) Software life cycle models [Online]. Available from:

http://courses.cs.vt.edu/csonline/SE/Lessons/LifeCycle/index.html, (Accessed 09/11/2014).

Virginia Tech (n.d.) The waterfall model [Online]. Available from:

http://courses.cs.vt.edu/csonline/SE/Lessons/Waterfall/index.html, (Accessed 09/11/2014)
&nbsp;

	              
	              The analysis phase of the software development consists of gathering requirements for a system that needs to be designed and implemented in the next phases. In this phase all stakeholders need to define what the requirements are and the analysts need to define the validly and the possibility to incorporate these into a software system. This phase should result in a Requirements Specification documents for the designers.
3 potential reasons of communication breakdown in system analysis phase of software development.
1- No clear demarcation between commercial and technical requirements. Stakeholder meetings consists out of people with different professional backgrounds and expectations. Clients are looking for a solution to a business problem and the technical people have to find the solution using the tools they have access to. For small projects they can operate in one team regularly reviewing together the progress but for big projects this does not work. 
A solution can be to first define the commercial requirements as clear defined propositions. During the different phases of the software development, the found solutions can be cross checked with this list.
2- Shifting or contradictory requirements guarantee a communication breakdown when there is no embedding for re-evaluation the requirements during the development process. With advancing insights during the development process requirements can also change. This is often seen as a problem or pushed, by those who have to find the solution, to an 'out of scope' category. 
A solution is to incorporate this in the development process and not write everything in stone during the analysis phase. In agile development procedures one can work with temporal technical requirements that are refined after every iteration. Also in this example it works best if the commercial requirements are defined separately. 
3- Closed, blind and hidden and open communications are ways of communicating that will not convey the right information. Closed communication encloses little information done by people who are less 'effective' in communications outside their group of direct peers and often are more introspective but have a great concentration for performing dedicated tasks. A blind communicator will tell what has happened but will not allow other opinions as he considers him or herself to be the specialist. The hidden communicator has another agenda most of the time to please everybody. Open communicators can be very blunt, everything is said and everything is on the table, but he loses his audience half way as they are irritated by his or hers style. 
A solution for these forms of communication can only be education or team building. By education one can develop communicating and listening skills. By understanding how communication takes place problems can be avoided before they happen. A good project manager could be the intermediate. 
Teams consist of people with different professional, cultural and personal backgrounds. In those teams the play different roles and are each responsible for their own separate tasks. Finding out how to test and classify people in order to map them to specific jobs is the holy grail of Human Recourses. There is the Big Five classification originated from work by Ernest Tupes and Raymond Christal in 1961 while working for the Personnel Libratory of the US Air Force System Commands and that was refined over the years with empirical data. 
There is a great overlap with the Typological theories from Carl Gustav Jung (1921) which were worked out to the Myers-Briggs Type Indicator in 1962 without any empirical study. Both systems state that people consist of contradictionary traits like inventive/curious and consistent cautious which defines ones openness to experience. One system talks about 5 of those traits the other about 4, but the idea of both is that one can identify what kind of traits a person can perform easily and which cost him or her more energy. An &nbsp;introvert person will for example be able to perform specific well defined tasks and it will cost this person more energy to find creative solutions on open problems. An optimal team consists out of people each excelling in a different trait representing all categories.
Illustration: The insight discovery colours is a Jungian scheme which is used a lot to categorise staff and create teams. (source. http://www.mrdynamics.com/insights-discovery/insights-discovery-colours)


&nbsp;
References:
Brookshear, G. (2012), Computer Science: An Overview, Pierson Education Inc. publishing as Addison-Wesley, Boston, United States, Version 11. ISBN 13: 978-0-13-256903-3. pp 300-360
&nbsp;
Oduaran, B. (2010) Effective Ways to Improve Internal and External Communication in System Development [Online]. University of Missouri. Available from: http://www.umsl.edu/~sauterv/analysis/Fall2010Papers/Oduaran. (Accessed: 09 November 2014).
Wikipedia. (2014). Big five personality traits [Online]. Wikipedia Foundation Inc.. Available from: http://en.wikipedia.org/wiki/Big_Five_personality_traits#Relationship_to_the_Myers.E2.80.93Briggs_Type_Indicator. (Last accessed 01 November 2014).
&nbsp;
&nbsp;

	              
	               I.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Introduction A data structure is a way to organize and store information for easy access or other purposes, it is an interface that consists of a set of procedures to add, deleted, access, reorganize, data. Standard Operations on Data Structures: Two types of operations research / data access and modification operations. Search, examples:  Search(S, k) : returns a x pointer to an element in S such that x.key = k, or NIL if no such element belongs to S. Minimum(S), Maximum(S) : returns a pointer to the element with the smallest (resp. &nbsp;large) key Successor(S, x), Predecessor(S, x) returns a pointer to just the largest element (resp. smaller) than x in S, Null if x is the maximum (resp. minimum).  Modification, examples:  Insert(S, x): insert the element x in S. Delete(S, x): delete the element x in S.  Implementation of a data structure Given an abstract data type, several implementations are usually possible, and the complexity of operations depends on the implementation, not on the abstract data type. •&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; The building blocks to implement a data structure depends on the implementation language •&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; A data structure can be implemented with another data structure II.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Some standard data structures  Stack data structure: collection of objects available in a LIFO policy  &nbsp;A stack is a linear list I for which the insertions and deletions are made at the beginning of I:  The first value of I is the top of the stack, Stack a value is to insert it at the beginning of I, Unstack a value, if I is not empty, it’s select the first value and remove it from I.   &nbsp;  File : collection of objects available in a FIFO policy  A File is a linear list I for which the inserts are made at the end of I and deletions are made at the beginning of I:  The first value of I is the head of the File and the last is the tail of the File, Enter a value, is to add it at the end of I, Output a value, if I is not empty, it’s select the first value and the remove it from I.    BINARY TREE  A binary tree is a finite set of values which is:  is empty, or composed of a root and values of disjoint binary trees called the left subtree and right subtree of root. (Donald E. Knuth, The Art of Computer Programming, Volume 1, Fundamental Algorithms, 3rd Edition, p. 312)    Vector: collection of ordered objects available from their rank  Dynamic set of objects occupying successive whole ranks, for consultation, substitution, insertion and deletion of elements at arbitrary ranks  Interface   Elem-At-Rank (V, r) returns the element at rank r in V. Replace-At-Rank (V, r, x) replaces the element at rank r by x and returns this object. Insert-At-Rank (V, r, x) inserts the element x to the rank r, increasing the rank of the following items. Remove-At-Rank (V, r) extracts the element located at row r and r removed, reducing the rank of the following items. Vector-Size (V) returns the size of the vector.   Applications: Dynamic table management of the elements of a menu,. . . Implementation: linked list, repeating table.   Dictionary  A dictionary is a data structure for storing words. In this course, we consider that the word is stored in a variable "word" which can know the length of the "length" type function. &nbsp;  Ring Buffer  The Ring Buffer is a better data structure to represent the contact list in a phone. Indeed in a Ring Buffer, all the elements are connected, including the first and the last element. The figure below illustrates this data structure.  This Data Structure can be simply or doubly linked. Its uniqueness is to not have the queue. The last element of the list points to the first. Each element has a following. It is a kind of list which includes an additional feature for the movement in the list, "it has no end." To make the list is endless; the pointer of the last element point to the first element of the list instead of NULL value. Here we will never get to a position where we cannot get around. On reaching the last element, it's moved automatically to the first element. In short, it is a rotation. III.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Conclusion An algorithm is nothing if it does not have appropriate data structure to store its data; it has presented here some very common data structures on which are developed solutions for complex problems (stack, file, dictionary, etc.). IV.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Reference: J. G. (2011) Computer science: An overview.&nbsp;11th ed. Boston: Pearson Education / Addison-Wesley., Section 7.1-7.5. Lecture Notes, (2014).&nbsp; Week 6: Computer networks [Online]. Available from: https://elearning.uol.ohecampus.com/bbcswebdav/institution/UKL1/201480_SEPTEMBER/MS_CKIT/CKIT_501/readings/UKL1_CKIT_501_Week10_LectureNotes.pdf. (Accessed: 08 November 2014) Wikipedia Inc. (2014) Data Structure [Online]: Wikipedia Inc. Available from: http://en.wikipedia.org/wiki/Data_structure (Accessed: 08 November 2014) &nbsp; 
	              
	              
	                Attachment: &nbsp;week10_DQ_2.doc (75.5 KB)
	              
	              Week 10 DQ2 – Software Development: Data Structures
Craig Thomas – 9 November 2014
Data structures are a way of abstracting the arrangement and organisation of data, thus allowing the user to access the data as abstract tools rather than forcing the user to think in terms of specific computer memory organisation. 
Programmers can develop logical data constructs, thus allowing algorithms to be expressed in very creative ways and enable the programmer to reference these constructs, rather than the actual physical memory itself. 
This is in contrast to the contiguous cell-by-cell arrangement typically provided by the computers main memory (Brookshear, 2012). In high-level programming, variables, otherwise known as ‘pointers’, are used to store references to other data values (Parlante, 2000) and are used to ‘point’ towards the location of those data values. In essence, these pointers are a replacement, or perhaps a virtualisation, of the traditional general-purpose registers in low-level machine language and can be used to build very flexible and complex data structures (Virginia Tech, n.d.). 
The design of this conceptualisation is a key attribute to an efficient algorithm.
This paper will compare a number of data structures, and provide highlights as well as identifying a data structure type, which would be suited to a phone contact list. 
Basic Data Structures
There are several basic data structures, such as arrays, lists, stacks, queues and trees (Virginia Tech, n.d.). 
There are two types of array, the first being the homogeneous array, which is a block of data that contains entries of the same type. The second, which is the heterogeneous array, which may include blocks of data entries that are of different types or components, for example a block of data relating to a car manufacturer, with a type of car, model type, and perhaps colour type. 
One of the merits of the array data structure is that it can mirror the contiguous cell-by-cell method of main memory, enabling the ability to access and manipulate retrieval sequence. One-dimensional array mirrors actual memory, and is a collection of memory cells, with multi-dimensional arrays being a collection of one-dimensional arrays (Virginia Tech, n.d.).
Another data structure is a list, which is a collection of entries that can be arranged sequentially. This is known as a linear structure, and items are kept in specific order, such as alphabetical or numerical. This is perhaps the simplest form of a data structure with the beginning of the list called a head, and the end of the list is called a tail (Brookshear, 2012). Whenever the list is updated, for example by adding data, this is placed in the correct sequence so that the list is always sorted. The list operates with a form of abstraction, in that the programmer does not worry about the details on how the list works, and is just concerned with what the list does. (Virginia Tech, n.d.).
This method can be useful for storing entire lists in a single block of memory with successive entries following each other in a contiguous fashion. This is particularly useful for storing static lists, but insertion and deletion can introduce time-consuming shuffling of entries. This is potentially an option for a basic phone list that is not time critical.
Keeping with the list data structure, there are two structures that are special types of lists. The first is a stack, in which entries are inserted and removed only at the head. A stack is particularly effective for data structures that require retrieval in reverse order, supporting requirements for last-in first-out, or LIFO, characteristics (Brookshear, 2012). 
The second is a queue, in which new entries are inserted at the tail, and existing entries removed at the head. A queue is effective in supporting first in first out, or FIFO, characteristics, requiring entries are removed from a queue in the order in which they were stored (Brookshear, 2012). This is particularly useful for an underlying structure supporting a buffer and temporary placement of data. As such, I would suggest this is perhaps not suited for a basic phone list.
The last data structure I intend to cover is the tree. The tree structure provides an hierarchical collection of data. This is similar to an organisation chart, in that there are branches within an organisation and not withstanding an organisation that adopts a matrix structure, the branches typically do not merge lower down the hierarchy. This is a multi-dimensional structure. The key components to clarify relative to the tree structure are that the positions within the tree are referenced as nodes, with the top position known as a root node, and the branch positions being known as terminal nodes. Essentially, in its broadest sense, ancestor nodes are treated as parents, and terminal or descendant nodes are treated as children in the parent child structure relationship (Brookshear, 2012). 
The beauty of the tree structure is that if you select any node in the tree, you also are able to link to the connected nodes within the tree. One of the common tree structures is the binary tree, whereby each parent has no more than two children. As such, in addition to the data, there are two pointers, which point to the right hand child and to the left hand child (Fig. 1) 
Fig. 1. 
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
If you select any node in the tree, you will find that the node together with the nodes below it also have the structure of a tree (Brookshear, 2012).
The tree structure would be useful to adopt for hierarchical data constructs and is beneficial for storing data that is easily searchable, as well as useful for representing sorted lists of data. In addition, you could also insert specific data values at a certain positions on the tree. (Wikipedia, n.d.). 
Where the data structure is static, perhaps for example employing a maximum number characters that are allowed within the tree data structure, this would also help ensure the tree is ‘balanced’, which if not balanced, can take much longer for searching. 
In conclusion and based on my research and reading so far, the tree data structure could potentially be suited for a basic phone contact list. However, I am left questioning how you could achieve a tree data structure whereby a parent has more than two children.
Best Wishes, Craig
References
Laureate Online Education. (2014). Computer Structures – Lecture Notes Week 10: Software engineering and data structures [Online]. Available from: https://elearning.uol.ohecampus.com/bbcswebdav/institution/UKL1/201480_SEPTEMBER/MS_CKIT/CKIT_501/readings/UKL1_CKIT_501_Week10_LectureNotes.pdf (Accessed 7 November 2014)
Brookshear, J. G. (2012). Computer Science An Overview. 11th ed. Boston Massachusetts: Pearson. Addison Wesley
Parlante, N. (2000). Pointers and Memory [Online]. Available from: http://cslibrary.stanford.edu/102/PointersAndMemory.pdf (Accessed 9 November 2014)
Virginia Tech. (n.d.). Data Structures [Online]. Available from: http://courses.cs.vt.edu/~csonline/DataStructures/Lessons/index.html (Accessed 7 November 2014)
Wikipedia. (n.d.). Tree Data Structure [Online]. Available from: http://en.wikipedia.org/wiki/Tree_(data_structure)#Common_uses (Accessed 8 November 2014)
	              
	              Hi Colleagues,
My apologies, I attach the correct diagram (subtle amendment on arrow connections)

Best Wishes, Craig.

Author: Craig Thomas Date: Sunday, November 9, 2014 5:12:44 PM EST Subject: RE: Week 10 DQ2 - Data structures

Week 10 DQ2 – Software Development: Data Structures

Craig Thomas – 9 November 2014
Data structures are a way of abstracting the arrangement and organisation of data, thus allowing the user to access the data as abstract tools rather than forcing the user to think in terms of specific computer memory organisation.
Programmers can develop logical data constructs, thus allowing algorithms to be expressed in very creative ways and enable the programmer to reference these constructs, rather than the actual physical memory itself.
This is in contrast to the contiguous cell-by-cell arrangement typically provided by the computers main memory (Brookshear, 2012). In high-level programming, variables, otherwise known as ‘pointers’, are used to store references to other data values (Parlante, 2000) and are used to ‘point’ towards the location of those data values. In essence, these pointers are a replacement, or perhaps a virtualisation, of the traditional general-purpose registers in low-level machine language and can be used to build very flexible and complex data structures (Virginia Tech, n.d.).
The design of this conceptualisation is a key attribute to an efficient algorithm.
This paper will compare a number of data structures, and provide highlights as well as identifying a data structure type, which would be suited to a phone contact list.&nbsp;
Basic Data Structures
There are several basic data structures, such as arrays, lists, stacks, queues and trees (Virginia Tech, n.d.).
There are two types of array, the first being the homogeneous array, which is a block of data that contains entries of the same type. The second, which is the heterogeneous array, which may include blocks of data entries that are of different types or components, for example a block of data relating to a car manufacturer, with a type of car, model type, and perhaps colour type.
One of the merits of the array data structure is that it can mirror the contiguous cell-by-cell method of main memory, enabling the ability to access and manipulate retrieval sequence. One-dimensional array mirrors actual memory, and is a collection of memory cells, with multi-dimensional arrays being a collection of one-dimensional arrays (Virginia Tech, n.d.).
Another data structure is a list, which is a collection of entries that can be arranged sequentially. This is known as a linear structure, and items are kept in specific order, such as alphabetical or numerical. This is perhaps the simplest form of a data structure with the beginning of the list called a head, and the end of the list is called a tail (Brookshear, 2012). Whenever the list is updated, for example by adding data, this is placed in the correct sequence so that the list is always sorted. The list operates with a form of abstraction, in that the programmer does not worry about the details on how the list works, and is just concerned with what the list does. (Virginia Tech, n.d.).
This method can be useful for storing entire lists in a single block of memory with successive entries following each other in a contiguous fashion. This is particularly useful for storing static lists, but insertion and deletion can introduce time-consuming shuffling of entries. This is potentially an option for a basic phone list that is not time critical.
Keeping with the list data structure, there are two structures that are special types of lists. The first is a stack, in which entries are inserted and removed only at the head. A stack is particularly effective for data structures that require retrieval in reverse order, supporting requirements for last-in first-out, or LIFO, characteristics (Brookshear, 2012).
The second is a queue, in which new entries are inserted at the tail, and existing entries removed at the head. A queue is effective in supporting first in first out, or FIFO, characteristics, requiring entries are removed from a queue in the order in which they were stored (Brookshear, 2012). This is particularly useful for an underlying structure supporting a buffer and temporary placement of data. As such, I would suggest this is perhaps not suited for a basic phone list.
The last data structure I intend to cover is the tree. The tree structure provides an hierarchical collection of data. This is similar to an organisation chart, in that there are branches within an organisation and not withstanding an organisation that adopts a matrix structure, the branches typically do not merge lower down the hierarchy. This is a multi-dimensional structure. The key components to clarify relative to the tree structure are that the positions within the tree are referenced as nodes, with the top position known as a root node, and the branch positions being known as terminal nodes. Essentially, in its broadest sense, ancestor nodes are treated as parents, and terminal or descendant nodes are treated as children in the parent child structure relationship (Brookshear, 2012).
The beauty of the tree structure is that if you select any node in the tree, you also are able to link to the connected nodes within the tree. One of the common tree structures is the binary tree, whereby each parent has no more than two children. As such, in addition to the data, there are two pointers, which point to the right hand child and to the left hand child (Fig. 1)
Fig. 1. 

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 

If you select any node in the tree, you will find that the node together with the nodes below it also have the structure of a tree (Brookshear, 2012).
The tree structure would be useful to adopt for hierarchical data constructs and is beneficial for storing data that is easily searchable, as well as useful for representing sorted lists of data. In addition, you could also insert specific data values at a certain positions on the tree. (Wikipedia, n.d.).
Where the data structure is static, perhaps for example employing a maximum number characters that are allowed within the tree data structure, this would also help ensure the tree is ‘balanced’, which if not balanced, can take much longer for searching.
In conclusion and based on my research and reading so far, the tree data structure could potentially be suited for a basic phone contact list. However, I am left questioning how you could achieve a tree data structure whereby a parent has more than two children.
Best Wishes, Craig
References
Laureate Online Education. (2014). Computer Structures – Lecture Notes Week 10: Software engineering and data structures [Online]. Available from: https://elearning.uol.ohecampus.com/bbcswebdav/institution/UKL1/201480_SEPTEMBER/MS_CKIT/CKIT_501/readings/UKL1_CKIT_501_Week10_LectureNotes.pdf (Accessed 7 November 2014)
Brookshear, J. G. (2012). Computer Science An Overview. 11th ed. Boston Massachusetts: Pearson. Addison Wesley
Parlante, N. (2000). Pointers and Memory [Online]. Available from: http://cslibrary.stanford.edu/102/PointersAndMemory.pdf (Accessed 9 November 2014)
Virginia Tech. (n.d.). Data Structures [Online]. Available from: http://courses.cs.vt.edu/~csonline/DataStructures/Lessons/index.html (Accessed 7 November 2014)
Wikipedia. (n.d.). Tree Data Structure [Online]. Available from: http://en.wikipedia.org/wiki/Tree_(data_structure)#Common_uses (Accessed 8 November 2014)


	              
	              Data structures
Data structure is a way of organizing the data in a system which can be used efficiently. (Wikipedia)The data structures acts as a foundation for the application and this will affect the performance of the application. 
Primitive data types : (Wikipedia, List of data structures) &nbsp;BooleanIntegerDoubleChar 
Composite and Abstract data types : ArraysStructuresLists Classes In this discussion we will be using phone contact list as an example to define and classify the data structures to develop the phone book application.
Phone book / Contact list - Requirements
To define a phone book we need to accumulate1. Person Name2. Country code &amp; Phone number3. Address details4. Search options4.1 By Name - First Name, Last Name 4.2 By Address - Zip code, area4.3 By Number4.4 By Country code
By considering the possible search criterias as an end result from the application, data structure for the application could be define as :
class Contacts{
 public string FirstName{ get;set;} public string LastName{get;set;} public string DoorNo{get;set;} public string AreaStreet{get;set;} public string State{get;set;} public string Country{get;set;} public string Zipcode{get;set;} public int CountryCode{get;set;} public int PhoneNumber{get;set;} public char Status{get;set;}
}
We can create a list of the contacts class, 
List&lt;Contacts&gt; phone_book = new List&lt;Contacts&gt;();
Using this list we can perform all the basic operations of adding new information, updating the information or even deletion of the contact information from the phone book.
The main idea behind splitting the name and address is to make the search simpler and quicker from the application execution point of view.
While adding new contacts we need to first validate the details for the duplicate entry. in our application case or in a general scenario name could be repeated or two person can have same name. Hence, we search for the existence of the number first, then the other details. When all these validations are passed successfully we will insert an entry into our phone_book list.
phone_book.add(new Contacts{“Kharavela”, “Ballal”,”#30”,”Mackie road, Filton”,”Avon”,”UK”,”BS347NA”,44,0123456789,’A’}
In my Phone book application I have a Status attribute, which is used to limit the search only with active records hence increasing the search performance. 
Similar to class, we can define array of structures and use it in our application. Major help in using either class or structures would be;
1. Easy to identify the fields2. Performance improvement over the simple or multidimensional arrays3. Easy to operate on individual element or contact
ConclusionThe data structure for this phone book application could be simple and volume of the data also may not increase, in such cases these simple structures can survive. When we build any product it would be based on the given requirement and scenarios undertaken by the architect and we cannot assure that over period of time the same design can withstand the data inflow. I could say that designing and defining the data structure would be an iterative process, and will evolve as the person gains more experience in the application and its usage pattern.
References:
Wekipedia, Data structures - http://en.wikipedia.org/wiki/Data_structure
Wikipedia, List of data structures - http://en.wikipedia.org/wiki/List_of_data_structuresBest Regards,Kharavela
	              
	              The first category is the array that consists either of the identical type of entries (homogeneous) or different types of entries. An array consist of rows and columns and a position or cell is identified in pairs of numbers representing the row and column.
The contact list can be made with a heterogeneous list with the first column the ID the second the first name, third family name, third address etc.. This is probably the most simple data structure to use for a contact list. In order to optimise storage for a dynamic list (deleting or creating new entries) the components can be stored in separate locations using variable pointers to identify this location (linking). 
The second category is a list either as an ordered (contiguous and fixed length) or linked list (not contiguous). A list has a head and a tail and is placed in chronological order. Entries can be added on top pushing the rest down. By using a stack data structure the rows are changed by Last In First Out (LIFO) order for backtracking activities. A queue data structure is open on both ends as it follows the order First In First Out (FiFo). 
Creating long list constructions one needs to optimise data for storing to avoid resorting procedures when rows have to be deleted or inserted. To resolve these problems they lists of names are stored in different areas creating a 'linked' lists were beginning and ends are identified by variable pointers.
A linked list could be an option for our contact list solution as is easier to delete and add new entries at alphabetical ordered rows. Both the stack as the queue are to limited in used for our contact list.
The third category is a tree that allows entries to be categorised in hierarchical organisations. A top node is called the root and from that point the branches are created. The last position in a branch is the terminal node defining the depth of the tree as number of horizontal rows counting from the root. 
The three is optimal for relational databases like organisational charts. In our contact list example it can be used for alphabetical structuring with the end branch consisting of a node with a name and siblings for address, tel. number etc.. When the branches are not in equal length the search of a tree can be slow. This will of course be the case in our example.
Conclusion
It is difficult to say which option works best for our exercise creating a contact list as it is possible in almost every example. Because we are not talking about a big Customer Relation Management system the impact for search and or reshuffling are minimal. As we have to choose one I think the linked list option is the best. It performs perhaps worse for entry and deletion as it has to reshuffle after such a change but this is less important than the bad performance with regard to search that we can expect in the tree option. 
Brookshear, G. (2012), Computer Science: An Overview, Pierson Education Inc. publishing as Addison-Wesley, Boston, United States, Version 11. ISBN 13: 978-0-13-256903-3. pp 342-360

	              
	              Hi Bo,
I like Michael James's checklist better:
http://scrummasterchecklist.org
To me, a checklist is a lot nicer than rules. Especially, it comes in the form of questions. MJ's checklist doesn't ask you to sign it:-)
br, Terry
	              
	              Hi Bram&nbsp;
I read your initial response on communications with interest, particularly where you reference personality type indicators, and that an optimal team consists of people excelling in a different trait representing all categories.
I believe this is a really importance aspect that is so often overlooked in many situations in business, and is not given the real focus that is needed.
There are too many occasions when teams are not operating in the most effective way and managers naturally lean towards believing this because of operating processes or systems, or inter departmental relationships, perhaps a lack of objectives and direction, or even substandard employee performance.
All of these are important however, in my experience, and in addition to the obvious and important needs of skillsets and experience, there is just not enough consideration given to the right balance of personalities in the first place. 
Thanks for your post Bram
Best Wishes, Craig.

	              
	              Hi Augusto,
I agree with your conclusion. The phone contact list problem sounds simple or even trivial. But in fact it can be quite complex.
br, Terry
	              
	              Communications
Introduction
At one or the other point in our lives, we have heard or read about the importance of effective communication. There is no doubt that communications and conversations, either formal or informal, are most certainly the best ways to ensure smooth functioning of an organization.
With reference to the software development lifecycle and from my experience, the single most important task is to get your client’s requirements are target dates correct. The most reliable approach is to clear out all ambiguities by discussing the requirements in detail with the concerned parties, over email or the phone or in person, as and when feasible. Also, we need to make sure that all important firm-related information, for example, strategies and new process implementations, is communicated, not just horizontally between supervisors and managers, but also among the rest of the employees, developers, testers etc. In other words, a certain level of ‘need-to-know’ must be established for all kinds of information.
International standard for software life cycle processes ISO/IEC 12207 defines the SDLC in following stages: (SDLC Quick guide, Overview)
Stage 1: Planning and Requirement Analysis
Stage 2: Defining Requirements
Stage 3: Designing the product architecture
Stage 4: Building or Developing the Product
Stage 5: Testing the Product
Stage 6: Deployment in the Market and Maintenance
Stage 1, is very important of all other stages as an analyst would discuss and gather the information from client who may not be well versed with computer jargon or systems, and it will become analysts responsibility to gather the right information and analyse them and pass on the information to other stages of development.
The development life cycle is a procedural process, if the information flow momentum is not maintained between the stages then development team end up building unwanted product!
&nbsp;
From my past experiences I strongly feel that following are the major points to alleviating the communication breakdown:


Effective analysis : As stated before analysis should be done properly and discussed among the team member for the better understanding of the problem in hand.


Signoff by client : Any changes or new requests should be signed off by client and then shared with team after clearing all the queries associated with the change.


Efficient Planning : Planning could be attributed to resource, infrastructure and / or project development. All these are very important elements, which contribute effective delivery of a product, need to go under thorough planning process.


Proper usage of SDLC : Project management can be done in various ways. To produce and meet the deadline by stakeholder, we need to follow any of the defined best development life cycle models. SDLC-agile, iterative or SCRUM, whichever best fits your project.


&nbsp;
ConclusionI have shared most of these from my personal experience and I believe one of the most important reasons for communication breakdowns is imprecise emails. Miscommunication in the form of poorly written emails largely amount to the failure of IT and any other projects.
The second key reason would be the miscomprehension of words. An email from a supervisor to his team members may be full of work-specific jargon but when communicating with clients, Sometimes, mistranslation and mispronunciation could account for communication breakdowns too.
ReferencesSDLC Quick guide, SDLC Overview - www.tutorialspoint.com/sdlc/sdlc_quick_guide.htm
&nbsp;
Best regards,Kharavela
	              
	              Week 10 DQ2 - Data structures
Babatunde KOLAWOLE
&nbsp;
Introduction
The use of data structures and algorithms is the nuts-and-bolts used by programmers to put in and influence data. We have several data structures and it is of great significance to know pros and cons of each, so as to understand the data structure that will be most befitting at any state of affairs. This discussion will concisely depict some common data structures, when to use them and their advantages and disadvantages.
Dynamic Array
Marked by correspondence or resemblance--&nbsp;Vector, Resizable array, Array list
The simplest data structure is the array which is built into every single programming language; the simplest dynamic array is a wrapper around a static array. Once the first array storage space runs out, a fresh bigger array is apportioned, and the elements of the first array are replicated to the fresh array. 
Items are reached like a static array, though more execution provides extra safety checks when doing this. Under normal conditions, arrays expand only from one end, this outcomes in an improved performance for operations on that end. Be that as it may, some executions increase in size at the cost of additional memory.
Arrays are pleasing to the eye mainly because of their simplicity. On the other hand, most other data structures incur additional processing overhead, arrays do not, their ability to grow and shrink in size makes them more versatile, but they retain their very fast indexing and iterations. Inserting and deleting items at the end of a dynamic array is also fast, but doing the same anywhere else means all the items after the adjustment must be copied and moved.
Advantages

Very fast access if index known
Quick inserts

Disadvantages

Slow search
Inserting items in middle of an array is slow i.e. meaning that you must create a new one, then copy items from the old to the new
Fixed size

Given these advantages and drawbacks, you can see that arrays are best used when storing data of a fixed size.
Linked List
Marked by correspondence or resemblance--&nbsp;singly linked list, doubly linked list
A linked list wraps each element in a node. For a singly linked list, each node contains a reference to the next node. A doubly linked list contains references to both the next and previous nodes. The advantages of a doubly linked list are that deletions become easier and the list can be traversed in either direction, but this comes at the cost of more memory overhead.
Linked lists a good data structure is you will frequently add or delete elements at both ends or if you will need to insert and delete elements while iterating through the list. They are, however, very poor at random element access. Iterating over the list is also a bit slower than dynamic arrays, because of the number of references that need to be traversed.
Advantages

Quick inserts
Quick deletes
Modifications made while iterating over the list are fast

Disadvantages

Slower iterations
Inefficient memory usage for each element
Random access is slow

Binary Tree
Marked by correspondence or resemblance--&nbsp;Red-black tree
A binary tree is comprised of nodes; each node in this binary tree also has two children which are referred to as the left and right child. The right child is greater while the left child less than the parent. This plain structure permits many operations to be carried out on a binary tree logarithmically. Advanced variations like the red-black trees keep the tree balanced so that the worst case performance remains logarithmic.
Almost all operations on binary trees have logarithmic performance. This is both a strength and weakness. While the operations are not the fastest, they are very consistent.
Advantages

Quick search, quick inserts, quick deletes
Elements remain balanced always

Disadvantages

Deletion algorithm is complex
Complex to implement
Slower iteration
Items must have an ordering

Hash Map
Marked by correspondence or resemblance--&nbsp;Hash table
Hash table or hash map is comprised of a set of keys, each uniquely mapping to an abstract part of something. The keys are given a complete different form or appearance by an algorithm within an integer which is then used as an index into an array. The existing components are stored in the array. Because of its limited size, conflict may take place when two keys hash to closely similar value. In such cases, collision resolve routines have to be utilized in order to reduce performance.
A hash table is best at putting in a big number of components that need to be located speedily. Find, insert, and remove all operate in constant time so even very large data sets can be operated on efficiently. This performance does have a cost. While, the operations may be constant time, that constant can be quite large. Consider hashing by name. Every character of the name must be run through some algorithm so that it produces a sufficiently random hash value. For a long name, this could take some time. For this reason, once an element has been found, a reference to it should be kept so that it does not need to be located repeatedly.
Advantages

Very fast access if key is known
Constant time find, insert, and delete

Disadvantages

Slow deletes
Slow access if key is not known
Performance degrades as table fills up
Inefficient memory usage

Conclusion
In computer science, a hash map or hash table is a data structure that uses a hash function to map identifying values, known as keys (e.g., a person's name), to their associated values (e.g., their telephone number). Thus, a hash table implements an associative array. The hash function is used to transform the key into the index (the hash) of an array element (the slot or bucket) where the corresponding value is to be sought.
References
Jeffrey M. Hunte Programming Data Structures and Algorithms [online] Available from: http://www.idevelopment.info/data/Programming/data_structures/overview/Data_Structures_Algorithms_Introduction.shtml (Accessed on 7 November 2014)
Wikipedia The Free Encyclopedia Associative array [online] Available from: http://en.wikipedia.org/wiki/Associative_array (Accessed on 7 November 2014)
Entropy Interactive Computer Science – Data Structures [online] Available from: http://entropyinteractive.com/2011/02/computer-science-data-structures/ (Accessed on 7 November 2014)
Wikipedia The Free Encyclopedia Trie [online] Available from: http://en.wikipedia.org/wiki/Trie (Accessed on 7 November 2014)
Wikipedia The Free Encyclopedia Hash table [online] Available from: http: http://en.wikipedia.org/wiki/Hash_table (Accessed on 7 November 2014)
	              
	              Hi Craig,
what you presented is a binary-tree. You can have more than 2 sub-nodes if you use a list or dictionary in a node instead of L-C/R-C for child nodes.
br, Terry
	              
	              Hi Craig,
The differences are so subtle, it took me like 2 minutes to spot them:-)
br, Terry
	              
	              Hi Belinda,
The table in the beginning of your post is a bit messed up, but I like it very much. I think you are making a very good point. The complexity of a project comes from 2 sources:

New science
Existing system

br, Terry
	              
	              Hi Craig,
Good point. Most companies treat human as resource. They even have a department for that.
br, Terry
	              
	              Introduction
&nbsp;
The requirements of system are defined during the analysis phase of software development, different of how the requirements will be achieved. This stage determines the issue that the client is attempting to settle. The final deliverable achievement of this stage is a requirement document. Preferably, this document declares in a precise and clear pattern what is to be constructed. During this stage, developers always face a lot of troublesome; one of those is the communication issues.
&nbsp;
Communication
&nbsp;
The characterizations of the analysis phase of software development is demonstrated by fierce communication activities and relates a multiple scope of persons, such as artists, designers, programmers, project managers, end users and etc, who vary on standards of knowledge, skill, status and background. The target of these activities is to realize a comprehending of the issue and one that have to be partaken between diverse persons, a mission enabled all the more strenuous by the volatility, complexity and vastness of the requirements. What is more, a heightened volume of communicative skill is demanded to overcome the semantic gap which alienated parties, such as designers and end users, necessarily facilitate. Predictably then, effective communication has been infamously hard to realize and is a relapsing issue during the extraction of requirements. The main cause for the presence of communication issues situates in the case that software development is extremely much behavioral processes, where people and organizational factors have a major undertaking on the design. Thus the effective communication is pivotal during the software development.
&nbsp;
Communication Difficulties
&nbsp;
Large-scale software development projects often incur severe breakdowns throughout the analysis phase. These issues caused the severe breakdowns are not only technological in property but also communication and coordination, and it is these inestimable elements which accustomed fashions essay to prevent owing to their unpredictability, as well as more socially-oriented ways try to solve. There have been a lot of researches which have classified the disparate kinds of issues during software development, and most supposing not all of them are as an outcome of a breakdown in communication and coordination. K. Lyytinen and R. Hirschheim (1988) defined development breakdowns into four categories: correspondence; process; interaction; and expectation. Except the first kind of breakdown, there are five probable reasons of the three rest kinds of breakdown, introduced as following in descending order of impact:
&nbsp;

Defective communication between persons. 
Lack of proper and suitable knowledge or partaken comprehending
Inaccurate, incomplete or inappropriate documentation 
Be short of a systemic procedure
Bad management of resources (including human resources)

&nbsp;
Four recommendations
&nbsp;
To minimize the breakdowns caused by the communication and coordination, there are four recommendations should be held during the software development.
&nbsp;

One of adventures is neglecting the users in design, this obstructs communication with those persons who will finally apply the software and whose rejection will fairly clearly avoid the communication of precious requirements. As well as there still appears to be a short of focal point in requirements approaches on the exhaustive make-up of owners.
In case a sufficient medley of business users and IT have been chosen then interplay should continue on a cooperative foundation. Both comprehensive parties should possess appropriate input into the layout of the system so as to cultivate a significance of proprietary and hold side by side of variations that occur during the system. Controlling the degree of liability, level of decision-making and the counterpoise between designers and users, appears a linchpin filed where little experiential study happens to notice exercise.
During the participation of communication behaviors, there seems to be a tripartite section between knowledge negotiation, user acceptance and knowledge acquisition, while intrinsic quality these activities are closely combined. Knowledge has to be acquired with responsiveness to the circumstance, which places the designers in a favorable situation to negotiate the requirements with as much knowledge to hand as probable. User acceptance is a conduct which relates the combination of propositions of both users and IT. Time have to be used at the front end of design to assist all parties recognize the situation of the prospective system, not only from a use expected but also in terms of wider organizational restrictions and social. Although not straightway productive, such behaviors finally produce the agreeable acceptance of a system.
All of the approaches adopt an assortment of skills to bring about the communication behaviors. It is able to recommend that diverse skill adoption provides a method of indicating the same information in distinct methods for effective conduct or understanding specific kinds of information, such as the fine distinctions of work behavior, while more work is demanded on skill choice, use and derived interests during such approaches.

&nbsp;
Conclusion
&nbsp;
There are a lot of traps in attempting to enable valid adoption of limited communication paths. One of the perils is that every party expounds affairs on the basis of whose own background suppositions. This is particularly troublesome with non-interplay communication, such as the documents of specification, where there is no occasion to examine that the users has comprehended the content as was anticipated.
&nbsp;
References
&nbsp;
Ajmal Iqbal, Cigdem Gencel, Shahid Abbas (2012); Communication Risks and Best Practices in Global Software Development; Available on: (http://www.bth.se/fou/cuppsats.nsf/all/daf21f1dac978492c12579560073463b/$file/BTH2011Iqbal.pdf) (Accessed on: 09-Nov-2014)
&nbsp;
Anandasivam Gopal, Tridas Mukhopadhyay, and Mayuram S. Krishnan (2002); The Role of Software Processes and Communication in Offshore Software Development; Available on: (http://www.researchgate.net/profile/M_Krishnan3/publication/220424743_The_role_of_software_processes_and_communication_in_offshore_software_development/links/02e7e5242450f878c9000000) (Accessed on: 09-Nov-2014)
&nbsp;
Bernd Bruegge &amp; Allen H. Dutoit (1997); Communication Metrics for Software Development; Available on: (http://www.ices.cmu.edu/ndim/papers/icse97.submitted.pdf) (Accessed on: 09-Nov-2014)
&nbsp;
K. Lyytinen and R. Hirschheim (1988); Information systems failures: a survey and classification of the empirical literature. Available on: (http://www.researchgate.net/profile/Kalle_Lyytinen/publication/234809690_Information_systems_failuresa_survey_and_classification_of_the_empirical_literature/links/004635303cbc838fbc000000?origin=publication_detail) (Accessed on:09-Nov-2014)
&nbsp;
Laurie William (2004); Risk Management; Available on: (http://agile.csc.ncsu.edu/SEMaterials/RiskManagement.pdf) (Accessed on: 09-Nov-2014)

	              
	              Introduction
Although computers are able to carry out literally billions of mathematical calculations every second, when an issue becomes complicated and large, Capability is still able to be a major apprehension. One of the most critical respects to how immediately an issue is able to be addressed is how the data is saved in the memory.
Simple Data Structures
Primitive variables is the simplest data structures which preserve a unitary value, thus they are of limited utilization. When numerous concerning values demand to be saved, an array is adopted. A rather more complicated idea are pointers, even if alike primitive. Pointers merely preserve a memory address which keeps certain useful sheet of data in theory rather than preserving a real value oneself.&nbsp; 
Arrays
Another quite simple data structure is Arrays that they are able to be deemed as a table of a regular length. Arrays are great due to their straightforwardness, as well as are quite suitable for circumstances where the data items amount is known or able to be defined programmatically. If the programmer demand a block of code to compute the mean of some numbers, an array is an ideal data structure to preserve the respective values, As they have no particular sequence, and the essential calculations do not demand any dedicated processing otherwise to iterate via all of the values. Another main merit of arrays is which are able to be randomly accessed by index.
Linked Lists
A linked list is one of data structures which is able to preserve a random amount of data items, and are able to readily alter size to remove or add items. At a linked list simplest, it is a pointer to a data-node. And then every data-node is constituted of data, where are perhaps a note with some data values. And the pointer is set to null at the end of the list. According to the property of its design, a linked list is good for data storing where the amount of items is perhaps unknown, or subject to change. But it is unable to offer the approach to access a random item from the list, lack of beginning at the first and crossing via each node until the programmer achieve the one who demand, thus it is quite inefficiency.
Doubly linked list
A concerning data structure with linked list is known as doubly linked list, the main discrepancy form a traditional linked list is which the root data structure saves a pointer to both the beginning and ending nodes. Thus every respective node has a connection to both the former and latter node during the list. This establishes a more nimble structure which lets shift in both paths.
Queues
A queue is one of the data structures which are the most appropriate depicted as "first in, first out". A true life instance of a queue is bank customers lining up at the bank counter. Since every customer goes in the bank, who is "enqueued" at the last; when a counter is available, who is "dequeued" at the fist of the line. Breadth-First-Search (BFS) is the good example of adopting a queue that will bring about a satisfactory solution. 
Stacks
&nbsp;In a sense, Stacks are the contrary of queues that stacks can be depicted as “last in, first out". The typical instance is the heap of books on the table. The readers are able to keep on adding books to the heap indeterminately, but who will only remove a book form the top of the heap every time, which is the last book that was placed. 
&nbsp;Trees
&nbsp;Trees are other types of data structures that consist of one or more than one data nodes. The primary node is known as the "root", and every node has more "child nodes" (also can be zero). The largest amount of "child nodes" of a unitary node, and the largest distance downward of "child nodes" are restricted in certain conditions by the accurate kind of data expressed by the tree. The most general instance of a tree is an XML document.
&nbsp;Binary Trees
&nbsp;Binary tree is the particular genre of three that is also happened to be the most efficient approach to access (read and save) a set of data which are able to be indexed by a crux value during certain way. The concept behind a binary tree is that every node has two "child nodes" at most. During the most representative performances, the crux value of the left hand side node is less than which of its parent node, and the crux value of the right hand side node is greater than which of its parent node. Therefore, the data saved in a binary tree is usually indexed by a crux value. When crossing a binary tree, it is easy to define which "child node" to cross when searching a particular crux value.
&nbsp;Priority Queues
&nbsp;During a representative BFS algorithm, a queue operates good for tracing of what forms have been accessed. As every new form is one more operating pace than the present form, adding new positions to the last of the queue is enough to guarantee that the fastest way is searched first. But the supposition here is that every action from one form to the next is a unitary pace.
Hash Tables
&nbsp;There is an unmatched data structure called Hash tables which is adopted to achieve a "dictionary" interface, where by a group of indexes that every index has a correlative value. This is like a typical dictionary, where reader is able to search a value (content) by a particular word (index). 
Conclusion 
Data structures are simply another collection of tools which should be during the tool kit of a experienced programmer. Frameworks and comprehensive libraries are accessible with great majority languages today to preempt the requirement for an ample comprehending of how to carry out each of such tools. The outcome is that programmers can fast bring about quality solutions which make use of formidable concepts. The challenge depends in aware which one to choose.
&nbsp;References
&nbsp;Chris Okasaki (1996); Purely Functional Data Structure; Available on: (http://www.cs.cmu.edu/~rwh/theses/okasaki.pdf) (Accessed on: 09-Nov-2014)
&nbsp;
Granville Barnett, and Luca Del Tongo (2008); Data Structures and Algorithms: Annotated Reference with Examples; Available on: (http://www.mta.ca/~rrosebru/oldcourse/263114/Dsa.pdf) (Accessed on: 09-Nov-2014)
&nbsp;
John Morris (1998); Data Structures and Algorithms; Available on: (https://www.cs.auckland.ac.nz/~jmor159/PLDS210/ds_ToC.html) (Accessed on: 09-Nov-2014)
&nbsp;
Kurt Mehlhorn and Peter Sanders (2007); Algorithms and Data Structures; Available on: (https://people.mpi-inf.mpg.de/~mehlhorn/ftp/Mehlhorn-Sanders-Toolbox.pdf) (Accessed on: 09-Nov-2014)
&nbsp;
N. Wirth (1985) (Oberon version: August 2004); Algorithms and Data Structure; Available on: (http://www.ethoberon.ethz.ch/WirthPubl/AD.pdf) (Accessed on: 09-Nov-2014)

	              
	              Data Structures
A data structure is a particular way of organizing data in a computer so that it can be used efficiently.&nbsp; (en.wikipedia.org/wiki/Data_structure )
Common types of data structures include arrays, graphs, trees, lists and stack. Each data structure has its own area of application.
Arrays are typically used in those instances where data representation is in the form of rows and columns. A data item is referenced by its ordinal position through the row, column intersection. A database is a good example where an array representation is applied through an array of structures. An array makes it easy to identify and locate a data item using its index, but its disadvantage lies in the fact that it may waste memory as the allocation is done whether or not a data item exists in it. An Arrays also has the disadvantage that it only allows storage of data of one type and the addition or deletion of a data value, would mean having to shift subsequent elements. Searching for a data value in an array is also slow because each position has to be compared with the search item. Arrays are only useful in those requirements that operate with a fixed number of elements that would be accessed using an index.
Linked Lists are used to hold sequential data and make reference to other data items in the list through the use of pointers. One only needs to know the first data value is stored in order to add or remove others. There are several applications of linked lists such as the doubly linked list. Merits of the linked list include the fact that adding or deleting a data value in the list does not require shifting of the subsequent elements but only changing of the pointer. Demerits include the fact that they may use more memory because of the additional requirement of a pointer against each data value and that location of a data value in the list requires that the entire list is traversed from the head.
A stack is a data structure that works on the principle of Last In First Out &nbsp;(LIFO). Stacks are useful in instances where recursive functions are required because the stack would always “know” what came before it. In a stack, addition, removal and inspection of&nbsp; data items occurs at the top of the stack.
A Queue is another type of data structure, and this one is designed to work on the principle of FIFO (First In First Out). An example of where queues are applied is in a printer. In this application, print jobs are queued in such a way that the first print job that came in is the one that is executed first. Demerits of the queue include the fact that it may take a long time to process some instruction as the computer has to wait for all other preceding tasks in the queue to be completed first.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
Phone contact List&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
The data structure that would be most suitable for a phone contact list is the SortedDictionary&lt;K,T&gt; data structure. The SortedDictionary&lt;TKey, TValue&gt; generic class is a binary search tree with O(log n) retrieval, where n is the number of elements in the dictionary (MSDN). This is a data structure that stores key-value pairs that are sorted on the keys. This makes addition, removal and searching of an element very fast. Because a phone book typically requires, addition of new contacts, removal and searching of contacts, the SortedDictionary&lt;K,T&gt; data structure would be the best application because it would allow searching and sorting by key and provide for faster addition and removal operations for phone data that may be unsorted
References

Navok, S. Kolev, V. ‘Fundamentals of computer programming with C#’, (n,d) Available at http://www.introprogramming.info/english-intro-csharp-book/read-online/chapter-19-data-structures-and-algorithm-complexity/ , Accessed (09/11/2014)
Brookshear, J. G. Computer Science: An Overview, 11th Edition. Pearson Learning Solutions.
.NET framework 4.5, SortedDictionary&lt;TKey, TValue&gt; Class, Available at http://msdn.microsoft.com/en-us/library/f7fta44c(v=vs.110).aspx , (Accessed 09/11/2014)


	              
	              Software analysisI. IntroductionIt is no secret that the software analysis phase of the software development life cycle can be of some challenge as it relates to the communication of information between stakeholders and System Engineers. This breakdown in communication often leads to system delays, drawback and may even affect the overall functionality of the software involved.&nbsp;It is often said this this communication breakdown is a result of language barriers, communication channel used and one-way communication, among others. This paper aims to look at some these causes of communication breakdown in the software analysis phase and possible means of avoiding them.II. Systems or requirements analysis The initial stage of the software development life cycle is that of the requirements analysis phase. It is at this phase that the software designer meets with the stakeholders and the initial or general requirement of the software are gathered; that is a list of what the software is expected to do is gathered at this stage. At this stage the time lines, security requirements and so forth may also be decided upon. Stakeholders may be persons with financial or legal ties to the system and its intended users.Once the list of the requirements are gathered it is placed in a Software Requirements Specification Document, which serves an agreement between the parties involved. It also serves as a guide for the software development process and may resolve “disputes that may arise later in the development process”, Brookshear, (2011).&nbsp;It is during this phase that a series of challenges as it relates to communication may arise. If the right requirements specifications are not communicated to the system engineers or programmers, the correct end product may not be developed. As such it is pertinent that the correct information is communicated to all parties involved in the software to be developed.III. Reasons communications can breakdown during the systems analysisThere are many associated reasons for a possible breakdown in communication to occur between the designers and stakeholders during the analysis phase of a software project. Some of the possible reasons for such challenges to occur are highlighted in the sections that follow. A. Language barrier &nbsp; &nbsp; &nbsp; &nbsp; It is a known fact that some the language and terminology used by software Engineers is often different from that used by the stakeholders. As such Engineers from time to time may find it difficult to express certain technological concepts for the layperson to understand. At times even though the stakeholders may still not understand what is being said they are often reluctant to express this lack of understanding. As such there is a breakdown in communication, because one party may interpret something one way and another may not understand it or may interpret it differently. This variance in interpretations may lead to contrary system requirements. B. Notation Another issue may lie in the notations and or representations used in the requirements documents. Engineers often use things such as pseudo-code, and flowcharts to represent varying aspects of the requirements. This language though usually easy to understand from a programmer's perspective may not be so easy for a layperson and more specifically the end user to understand. This may result in varying or misinterpretations.On the contrary from a programmer's perspective if lots of textual content is presented in the requirements, the programmer may find it difficult to interpret and may become overwhelmed with the large amounts of textual data, which will once again present challenges in establishing of the requirements. &nbsp; &nbsp; &nbsp; &nbsp; C. Communication channel used. &nbsp; &nbsp; &nbsp; &nbsp;The means of communication involved in the analysis phase can also present a challenge in terms of communication. Communication can be formal and informal. Formal communication is usually a planned means of communicating, such as scheduled meetings. While informal communication may be unplanned and may include casual conversations about the application involved. It is wise to avoid informal communication as often information may not be communicated to all parties involved and as such it can even lead to grievances among individuals as they may have felt that they were excluded from the planning phase.Apart from this, the language used in emails, if not clearly written can lead to misinterpretations. Also attention needs to be place on if all parties have seen and read the email correspondence sent. As this can lead to parties possibly being absent from meetings or their inability to respond in a timely manner to requests that may have been sent to them. D. One-way communication &nbsp; &nbsp; &nbsp; &nbsp; One-way communication has to do with presenting messages or information to individuals or stakeholders without being sure if the message is clear or if the message was understood. In cases when presenting facts or simple, easily understood information this communication type may not present any challenges, however in other cases it often poses challenges and may lead to misinterpretations.IV. Solutions for alleviating communication breakdown A. Avoid the use of informal discussions and if used, confirm in writing, information share to the parties involved. B. Take care when typing and sending emails, insure that the information presented is clear and precise. Additionally follow-up emails with a phone call to ensure that parties have received and read the email correspondence. Attention should also be placed in adhering to general email etiquette. &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;C. While it may be better for programmers to understand certain forms of notation, we have established that this may not always be the case for stakeholders. As such, try to establish a prober balance when writing system requirements documents and ensure that you follow up with stakeholders to ensure they understand the document. Where needed offer the relevant explanations, of parts in the document.&nbsp; &nbsp; &nbsp; &nbsp;D. Reduce the use of technical jargon when writing documents. Always ensure that your document is understand by a non-technical person.&nbsp; &nbsp; &nbsp; &nbsp;E. Place emphasis on face-to-face communication. This is a effective means of communicating as non-verbal messages that often cannot be communicated through the telephone or the sending of emails can be observed here. Hence, you are in a better position to interpret Stakeholders reactions to certain things better and they can interpret yours as well. &nbsp;Stakeholders will also be more inclined to ask questions, for matters that they are not clear about in such a forum.  &nbsp; &nbsp; &nbsp; &nbsp;F. If something is not clear to you it is important that you clarify any assumptions that you may have with the stakeholders, paraphrasing can be a good technique here.&nbsp;Adversely ensure that stakeholders understand your perspective on varying aspects of the requirements.IV. ConclusionThere can be a series of suggestions, as it relations to avoiding communication breakdown during the analysis phase of the project, most of which may require the general building on communication skills. However, it is always important that you constantly follow-up with the stakeholders involved; hold brief meetings, make use of technological channels that may be available when needed, ensure that your emails were received, read and understood. This maybe a challenge at first but it is better than implementing a project that has major flaws or that does not meet the requirements of the stakeholders, which could possibly lead a rebuilding of the software.References:Portny, S. E. (2010) Project Management For Dummies. 3Rd-edition pages 264-272, Wiley Publishing, Inc. [Online]. Available at:&nbsp;https://khmerbamboo.files.wordpress.com/2014/09/project-management-for-dummies-3rd-edition.pdf (Accessed on November 9, 2014)Brookshear, J.G. (2011) Computer science: An overview. 11th ed. pages 304, &nbsp;Boston: Pearson Education / Addison-Wesley.Rawas, A. A.&nbsp;and Easterbrook, S. (1996) Communication Problems In Requirements&nbsp;Engineering: A Field Study pages 2-10 [Online]. Available at:&nbsp;http://www.cs.toronto.edu/~sme/papers/1996/NASA-IVV-96-002.pdf (Accessed on November 9, 2014)

	              
	              project-management-for-dummies...
	              
	              

Week 10: DQ Data structures

Data structure is the way in which data is organised and stored. Data structures can firstly be classified into groups:


Linear-which includes stacks, queues and lists
Sets- which are unordered but contain unique elements
Dictionaries- these encompass key value pairs which are organised in the form of hash tables
Tree-like- this include binary trees, balanced trees and B trees
Others- this includes priority queues, bags, graphs and multi sets etc..







Data Structure


Advantages


Disadvantages




Array



Allows items to be quickly interred
it’s easier to access individual oems if the index is known




The size of the array is fixed
searching and deleting are slow





Ordered Array



Searching is faster compared to an unsorted array




inserts and deletes are slow
The size of the array is fixed





Stack



Allows for a last in first out (LIFO) approach




Accessing other items is slow





Queue



Allows for a first in first out (FIFO) approach




Accessing other items is slow





Linked list



Facilitates quick inserts and deletes




Search is low





Binary Tree



If the tree is balance it facilitates quick search, inserts and deletes




the algorithm used for deletion is complex





Red-Black tree + &nbsp;
2-3-4 Tree



Because the tree is always balanced it facilitates quick search, inserts and deletes





it is difficult to implement





Hash Table



Access is very fast if the key is known
Allows for quick inserts




Access is slow if the key is unknown
there is inefficient memory usage
deletes are slows
A hash table can have key collisions where different keys are are accidentally mapped to the same position on the has table





Heap



Allows for quick inserts, deletes as well as to the largest item




Access to other items is slow





Graph



It is the best at modelling real world situations




Some of its algorithms are complex and slow






Trie(prefix tree)




Allows for quick search
Unlike in imperfect hash tables; there are no key collisions
Tries provide sorting key entries alphabetically&nbsp;




Tries can be slow when searching
May require large amounts of memory as each character and not a whole chunk may require separate memory allocation






For a phone contact list, i think the Trie/prefix table is best suited because of the following reasons:

it is useful and highly efficient in matching strings because of their fast insert, delete and search time&nbsp;
They can provide ordering of key entries in alphabetical order which would make it easy when searching for names on one’s contact list
Unlike in imperfect hash tables; there are no key collisions



References:
http://geeksforgeeks.org/ [online] accessed 8 November 2014
http://www.idevelopment.info/data/Programming/data_structures/overview Data_Structures_Algorithms_Introduction.shtml&nbsp; [online] accessed 9 November 2014
http://www.webopedia.com/TERM/D/data_structure.html [online] accessed 9November 2014
Steven S. Skiena http://www3.cs.stonybrook.edu/~skiena/214/lectures/lect1/lect1.html [online] 9 November 2014

 


	              
	              Communication in its basic form can be considered as the exchange or flow of information and ideas from one person to another in an effort to convey understanding, delivery instructions, etc. Effective communication only occurs if the receiver understands the exact information or idea that the sender intended to transmit (www.nwlink.com, 2013). 
Let's consider the operation of a manufacturing organizations and the communication requirements of such an entity. In order for products to be consistently produce; clear, efficient ongoing&nbsp; communication must take place throughout all the production stages, ensuring that all personnel involves understands the requirements. It is the chain of understanding from the top of the organization right down to the bottom and all supporting entities (suppliers, labour&nbsp; and other stakeholders) which will determine that the output product meets the requirements.
In this discussion we'll analyse the challenges that may occur in the first phase of the system development life cycle when communication between different individuals is necessary in order for requirement analysis to be defined and the software requirement specification documented. The reasons for breakdown in communication within these kind of development teams will examined and resolutions recommended.
In computer science, the software development cycle's initial stage which is requirement analysis is very critical to the development and communication plays a major role in its success. Just as in the manufacturing example above, communication in this phase will determine the output product of a comprehensive software requirement specification document. During the this phase, compiling and analyzing the needs of the software user will require extensive negotiation with all stakeholders taking into account significant input from all users, legal and financial interest, governing and regulatory bodies (If necessary) and any other entity or individual that would aid in the target objective being met. Several rounds of meetings may be required in various forms in order to ensure that requirements are captures just as intended by the stakeholders, formal communication channels are established and no ambiguity is recorded in the specification document. One of the techniques frequently used in this phase is rapid prototyping in which a prototype program is built with the desire software functionality. This tool allows users and other stakeholders to interact with a functional sample and provide the necessary feedback and capability modifications required.(Brookshear, 2012, p.304 and Virginia Tech, n.d.)
Even with such detail approach to communication in the requirement analysis phase of software development, there are still several reasons communication can breakdown. Three such reasons are examined below and include: Personality conflicts, Smothering and stress.
Personality conflicts: different personalities may come into play and words, terms are not received the way they were intended and this leads to misunderstandings. Culture and experience defining personality traits may allow place strain on communication as different perceptions will bring about different views to the specifications.
Smothering: "We take it for granted that the impulse to send useful information is automatic. Not true! Too often we believe that certain information has no value to others or they are already aware of the facts" (www.nwlink.com, 2013).
Stress: "People do not see things the same way when under stress. What we see and believe at a given moment is influenced by our psychological frames of references, our beliefs, values, knowledge, experiences and goals" (www.nwlink.com, 2013).
Breakdown in communication can be devastating to the requirement phase and the overall software development cycle. Therefore solutions to alleviate such costly interruptions are critical. The use of a tested and proven methods for software development such as the agile methods will ensure that specific timely processes are adhere to and an iterative approach to user requirements is maintained.
Methods such as agile also ensure that effective communication is maintained throughout the phase, utilizing various channels. A software development team without business knowledge most likely will not able to deliver the product which is valuable to customers, thus causing customers to lose faith in the development team. Hence, it is important for developers to have basic understanding in business communication. (Leau, Y., Loo,W., Tham, W. and Tan, S., 2012)
In this discussion we analysed the communication challenges that may occur in the requirement phase of the software development life cycle, we Identified three reasons communications can breakdown during this phase and recommended solutions that could mitigate against challenges and avoid communication breakdowns.
&nbsp;
Reference:
Brookshear, J. G. Computer Science: An Overview XML Vital Source ebook for Laureate Education, 11th Edition. Pearson Learning Solutions.
&nbsp;
www.nwlink.com (2013) Communication and Leadership [Online]. Available from: http://www.nwlink.com/~donclark/leader/leadcom.html (Accessed: 9 November 2014).
Virginia Tech (n.d.) Software life cycle models [Online]. Available from: http://courses.cs.vt.edu/csonline/SE/Lessons/LifeCycle/index.html (Accessed: 7 November 2014).
Lecture Notes (2014) Software engineering and data Structures [Online]. Available from: https://elearning.uol.ohecampus.com/bbcswebdav/institution/UKL1/201480_SEPTEMBER/MS_CKIT/CKIT_501/readings/UKL1_CKIT_501_Week10_LectureNotes.pdf (Accessed: 9 November 2014).
Leau, Y., Loo,W., Tham, W. and Tan, S. (2012) Software Development Life Cycle AGILE vs Traditional Approaches [Online]. Available from: http://www.ipcsit.com/vol37/030-ICINT2012-I2069.pdf (Accessed 9&nbsp; November 2014).

	              
	              
Week 10 – Data Structures

I. Introduction
Data structure is responsible for the organization of data in the appropriate manner so that the relevant actions can be performed on them. This paper aims to briefly examine these data structures, it benefits and disadvantages.
II. Common types of data structures
Below is a list of the common types of data structures in use today.
 A. Lists, Stacks, and Queues
 In list structures, the items are arranged in a particular structure or sequence. The top of the list is usually referred to as the head and the bottom is usually referred to as the tail, Brookshear, (2011)
 Stacks and queues are special types of lists. With a stack items can be inserted and removed from the top only, as such it uses the last-in, first-out (LIFO) method of storing and retrieving data. 
With a queue items are items are inserted at the top of the queue or the head but they are removed from the bottom or tail of the queue. This method uses the First-in, First-out (FIFO) methodology.
 B. Arrays – are a list of information or objects that may be stored in rows and columns. They are two types of arrays homogeneous and heterogeneous arrays. Homogeneous arrays are the data entities are the same type while with heterogeneous arrays that data may not be the same type. Arrays are usually represented using rows and columns and as such a particular array may be represented as Array [8, 3] which may mean row 8 column 3. 
 
 C. Trees or Hierarchies
 Trees utilizes a hierarchal structure of storing data, which may be similar to that of an organizational structure. 

III. Merits and weaknesses Common types of data structures
 A. Lists – modifying, inserting and deleting can be fast. However there may be extra memory overhead involved and random access may be slow.

 B. Tree – while this structure may be difficult to implement, items may usually be sorted and it may have quick searches, inserts and deletions.
 C. Arrays – iteration and indexing may be fast. However, there may be a waste of space and adding element in the middle of the array may be a bit slow. Arrays may also have slow search and deletes.
 

IV. Data structure best suited for a phone contact list.
I believe that data structure best suited for a phone book contact list would be that of an array. This is because the array structure would not have any particular retrieval types, that is FIFO or LIFO structure. Additionally searching and retrieval of information in the array will be easy. The use of a tree structure in this case may be unnecessary and difficult to implement.


References:

Brookshear, J.G. (2011)&nbsp;Computer science: An overview.&nbsp;11th ed. Pages 340-360,  Boston: Pearson Education / Addison-Wesley.
Harel, D. &amp; Feldman, Y.A. (2004)&nbsp;Algorithmics: the spirit of computing.&nbsp;3rd ed. Pages 3-17, New York: Addison Wesley.




	              
	              Week 10 – Data StructuresI. IntroductionData structure is responsible for the organization of data in the appropriate manner so that the relevant actions can be performed on them. This paper aims to briefly examine these data structures, it benefits and disadvantages.II. Common types of data structuresBelow is a list of the common types of data structures in use today. A. Lists, Stacks, and Queues In list structures, the items are arranged in a particular structure or sequence. The top of the list is usually referred to as the head and the bottom is usually referred to as the tail, Brookshear, (2011) Stacks and queues are special types of lists. With a stack items can be inserted and removed from the top only, as such it uses the last-in, first-out (LIFO) method of storing and retrieving data.&nbsp;With a queue items are items are inserted at the top of the queue or the head but they are removed from the bottom or tail of the queue. This method uses the First-in, First-out (FIFO) methodology. B. Arrays – are a list of information or objects that may be stored in rows and columns. They are two types of arrays homogeneous and heterogeneous arrays. &nbsp;Homogeneous arrays are &nbsp;the data entities are the same type while with heterogeneous arrays that data may not be the same type. Arrays are usually represented using rows and columns and as such a particular array may be represented as Array [8, 3] which may mean row 8 column 3.&nbsp;  C. Trees or Hierarchies Trees utilizes a hierarchal structure of storing data, which may be similar to that of an organizational structure.&nbsp;III. Merits and weaknesses Common types of data structures &nbsp; &nbsp; &nbsp; &nbsp; A. Lists – modifying, inserting and deleting can be fast. However there may be extra memory overhead involved and random access may be slow. B. Tree – while this structure may be difficult to implement, items may usually be sorted and it may have quick searches, inserts and deletions. C. &nbsp;Arrays – iteration and indexing may be fast. However, there may be a waste of space &nbsp;and adding element in the middle of the array may be a bit slow. Arrays may also have slow search and deletes. IV. Data structure best suited for a phone contact list.I believe that data structure best suited for a phone book contact list would be that of an array. This is because the array structure would not have any particular retrieval types, that is FIFO or LIFO structure. Additionally searching and retrieval of information in the array will be easy. The use of a tree structure in this case may be unnecessary and difficult to implement.&nbsp;References:Brookshear, J.G. (2011) Computer science: An overview. 11th ed. Pages 340-360, &nbsp;Boston: Pearson Education / Addison-Wesley.Harel, D. &amp; Feldman, Y.A. (2004) Algorithmics: the spirit of computing. 3rd ed. Pages 3-17, New York: Addison Wesley.
	              
	              Data structures give us the ability to find, retrieve and present bits of data quickly. This is a critical component of software development and as such the most suitable structures must be used when developing programs.
In this discussion we will examine the merits and weaknesses of the common data structures and identify which is most suitable for a phone's contact list.
Five common types of data structures will be analysed and they include: arrays, list, queues, stack and trees. 
Array: two types of arrays exist; homogeneous and heterogeneous. "A homogeneous array is a block of data whose entries are of the same type"(Brookshear, 2012, p.342) and in heterogeneous entries of&nbsp; different types are stored.
Merits:

Used to represent multiple data items of the same type.
It can be used to implement other data structures

&nbsp;
Weaknesses:

It must be known in advance how many elements are to be stored (www.xpode.com, n.d.).
Arrays is static structure of fixed size. Memory allocated cannot be changed.
Insertion and deletion are very difficult and time consuming.

&nbsp;
List: Two types of list exist, ordered list and linked list. In ordered list the entries are sequentially arranged, while in the linked list the entries are unarranged in various memory cells and are connected via a memory pointer (www.c4learn.com, n.d.).
Merits:

Easy to make and remove entries
Memory usage is efficient

&nbsp;
Weaknesses:

Time consuming 
Wastage of memory as extra storage is required for the pointer

&nbsp;
Queues: entries are made at the tail and exit a the top of a list, in the order they entered subscribing to the FIFO structure of first-in, first-out. 
Merits:

Quick and efficient to add and remove entries 
Fast and flexible, requiring no inter-process communication

&nbsp;
Weaknesses:

Not suitable for large queues
Queue is not readily searchable
Adding and removing entries from the middle of the queue is complex

&nbsp;
Stack: entries to this list are arranged from the bottom to the top, one way in, one way out via the top of the list. A last-in, first-out structure defines this list.
Merits:

Easy to get started

&nbsp;
Weaknesses:

Inflexible
Lack of scalability

&nbsp;
Trees: the entries of this structure has a hierarchical design and has root node from which branches extend towards a leaf node. 
Merits:

Simple to implement
Small memory footprint
Very fast

&nbsp;
Weaknesses:

High overhead
Large waste of unused links

&nbsp;
For the phone's contact list I would use a heterogeneous array. This array allows blocks of data items that are of different types to be stored as components. "A heterogeneous array would be the block of data relating to a single employee, the components of which might be the employee’s name (of type character), age (of type integer), and skill rating (of type real)" (Brooksheer, 2012, p.342). This example illustrates the ideal components for establishing the phone's contacts.
In this discussion we analyse the merits and weaknesses of five data structures to ascertain which would be ideal for creating a phones contact list. We able to identify the most suitable structure and state why it would be ideal.
&nbsp;
Reference:
Brookshear, J. G. Computer Science: An Overview XML Vital Source ebook for Laureate Education, 11th Edition. Pearson Learning Solutions.
&nbsp;
www.xpode.com (n.d.) What are Advantages and Disadvantages of arrays? [Online]. Available from: http://www.xpode.com/ShowArticle.aspx?ArticleId=502 (Accessed: 9 November 2014).
&nbsp;
www.c4learn.com (n.d.) Link list disadvantages [Online]. Available from: http://www.c4learn.com/data-structure/linked-list-disadvantages/ (Accessed: 9 November 2014).
Virginia Tech (n.d.) Multidimensional arrays [Online]. Available from: http://courses.cs.vt.edu/~csonline/DataStructures/Lessons/2DArrays/index.html, (Accessed: 9 November 2014).
Virginia Tech (n.d.) Ordered list: The implementation view [Online]. Available from: http://courses.cs.vt.edu/csonline/DataStructures/Lessons/OrderedListImplementation View/index.html, (Accessed: 9 November 2014).
Virginia Tech (n.d.) Queues: The abstract view [Online]. Available from: http://courses.cs.vt.edu/~csonline/DataStructures/Lessons/QueuesAbstractView/index.html, (Accessed: 9 November 2014).
Lecture Notes (2014) Software engineering and data Structures [Online]. Available from: https://elearning.uol.ohecampus.com/bbcswebdav/institution/UKL1/201480_SEPTEMBER/MS_CKIT/CKIT_501/readings/UKL1_CKIT_501_Week10_LectureNotes.pdf (Accessed: 9 November 2014).

	              
	              Sparsity corresponds to systems which are loosely coupled (as explained by Terry below)&nbsp;Sparse matrix algorithms will show up in applications ranging from models of the physical world to web search and graph clustering. Using them efficiently involves techniques from linear algebra, graph algorithms, and computer &nbsp;architecture. Sparse matrix algorithms combine two languages that are often quite different, those of numerical computation and of graph theory.&nbsp;

References
URL http://en.wikipedia.org/wiki/Sparse_matrix
URL http://www.cs.ucsb.edu/~gilbert/cs219/cs219Spr2013/
	              
	              From experience 
Agile development has a less likelihood of miscommunication because of the following
Team is much smaller ~3-6 people, they are usually in one location (one room). They communicate verbally. There is a daily meeting (15 Min) and everyone share progress and challenges for ~3 min. You are a team player and can't hide. Your contributions make a huge impact to the 30 day deliverable. Thus allowing for free communications between the team members.
There is also little to no interruption from outside, ie distractions. The product owner is the only person in dialog with the outside. Keeping the communications flow within the scrum team only.

	              
	              Hi Bram,

It's funny you mentioned the Jungian scheme and the colours.
I work for the NHS and over the past year we have had something called "managment means" days where managers from all over the country come together usually 20-30 at a time and go through categorising themselves in this way as well as role play and all sorts.
What I find ammusing is the fact for the last 6 months all you get in the office is managers asking other managers what colour they are. !
To the normal memeber of staff it sounds weird but the funny part is them explaining the traits and why they are that colour.
ta
Chris



	              
	              Hi Anthony,

One of the fundamental tenets of any agile software methodology is the importance of communication between the various people involved in software development. Furthermore agile methods put a large premium on improving communication through face-to-face communication. As the agile manifesto states "The most efficient and effective method of conveying information to and within a development team is face-to-face conversation."



Use wikis to contain common information



I assume we've all played around with various ways to hold common information; my favorite so far is wiki. Wikis work well because they are simple to use, can be worked with any browser, and are simple to set up. Any common information can be put there, design guidelines, and build instructions, notes on progress - anything that needs to be written down for reference by the team. I found it's very useful to use the change notification capability. Many wikis have that ability, so that page changes trigger notifications through email or an RSS feed.

Wikis are by nature unstructured, and this lack of structure is part of the benefit.



Use Regular Builds to Get Feedback on Functionality



When people consider the requirements gathering in agile methods, they often fail to see the importance of the feedback loop. Often the requirements process is seen as analysts providing requirements, which developers go off and implement. At some later point somebody checks to see if the developers have implemented what they were asked for. On an agile project, the close proximity between customer and developer allows the customer to monitor progress much more frequently, which allows them to spot misunderstandings more quickly. Furthermore a partially developed system can also educate the customer, for often there's a difference between what's asked for and what's needed.

Best Regards, Babatunde

References

Margaret Rouse Agile Manifesto [online] Available from: http://searchcio.techtarget.com/definition/Agile-Manifesto (Accessed 9 November 2014)

T Chau, F. Maurer, Knowledge Sharing in Agile Software Teams Lecture Notes in Computer Science Volume 3075, 2004, pp 173-183

S. Burgess March 18, 2014 An Agile Approach, Part 2 [online] Available from: http://www.thecoelement.com/an-agile-approach-part-2/ (Accessed 9 November 2014)
	              
	              Hi Ramin,
I feel that I need to say something here, because I don't know which way is "below":-) So just to make sure your post is surrounded by my posts.
I think some commercial source code analysis tool can generate the sparse matrix for code coupling, showing how coupled the code is by its density.
br, Terry
	              
	              Hi Anthony&nbsp;
I have worked in Agile environments, from a service perspective. I have found them to be really dynamic methods for delivering software into live production environment, at very regular intervals, in which there was strong levels of communications on progress, next steps and issue resolution.&nbsp;
One of the good functions within this methodology was the daily stand up meetings, with all the key stakeholders. Everyone was crystal clear on what was happening, however I have felt it was critically important to have a strong PM who is particularly effective at managing communications and relationships, along with having a reasonably good technical understanding.
In my more recent roles, the PM extended these calls to include a service representative to attend regularly i.e. weekly. If there was a service related issue, my team was engaged immediately, and then attended the call over the next couple of subsequent days.
One of the key benefits of this dynamic methodology, again from a service perspective, was the ability to reduce the cycle time for fixes to be fully delivered into live during regular scheduled implementation and change windows.
I have to say however, I hadn’t really been overly concerned about which methodology was in play, as irrespective of the method, I have still always looked to instill a proactive and collaborative culture within my team to drive the engagement into the delivery programmes and to take specific responsibilities for having a thorough understanding of what was being designed and delivered from a service perspective, as well as defining our requirements and acceptance criteria.
It’ll be interesting to see the other responses from class colleagues who sit more on the software development side.
Best Wishes, Craig

	              
	              Hi Chris
Have you ever had one of those 'multi coloured' days! :-)
Best Wishes, Craig

	              
	              ...so long?... :-)
	              
	              Ah, ok, thanks Terry.
That is something I hadnt been able to interpret / figure out, from my reading. I appreciate your response, something I will read further on.
Best Wishes, Craig
	              
	              Hi Everyone,
&nbsp;
How much influence does the culture within an organization (and beyond it) have on the likelihood of miscommunications and communications breakdown during software development projects or systems analysis phases?&nbsp;
&nbsp;
Which types of organizational culture do you think would help minimize the incidence of such miscommunications?
&nbsp;
Anthony

	              
	               I.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Introduction 'Software Engineering', field of 'engineering' whose aim is the design, manufacturing and maintenance of complex software, safe and quality systems It is also an art of producing a complex collective system, embodied in a set of design documents, programs and test sets, often with multiple versions. Motivation  Responding to the software crisis emerged in the 70s (awareness that the software cost exceeded the cost of materials) Meet the growing size and complexity of systems Dealing with time getting shorter, Manage big teams, with multiple skills  Development cycle •&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Pre-study: Defining the scope of the project and development of cases •&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Vision: Glossary, Determination of stakeholders and users, Determination of needs, functional requirements and non-functional, design constraints •&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Elaboration: Project planning, specification characteristics, fundamentals of architecture •&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Architecture: Software Architecture Document, Different views of stakeholder, a candidate architecture, design and behavior of system components •&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Construction: Construction Product •&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Transition: Preparing the product for users II.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Reasons communications can breakdown The communication management project is to determine who needs what information, when and in what form. Fred Brooks notes in his book "The mythical man-month" if there are n employees on a project: there are (n-1) / 2 communication needs. While all projects share the common goal of establishing an effective communication project, the information needs and communication practices vary considerably and Communications planning can be formalized or informal, somewhat or very detailed, depending on the nature of the project. In practice there are several types of problems that can cause the breakdown of communication between each other,  Frequency of meetings: in fact the one hand when you have frequencies too long it can affect communication and has caused a lack of information, and secondly when the frequencies are too short, this can reduce the time of work as they say "Work" or "Talk". Technologies and communications devices: Obtaining effective communication during the project also depends on the choice of tools and communication technology used for a project. When communication tools are not suitable or do not match the skill level of different players, it can also cause a breakdown in communication.  &nbsp;  III.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Recommendations and solutions For effective project communication, must consider  The relationship of responsibility between stakeholders and organization in charge of the project The disciplines, services and specialties involved in the project The geographical location and the geographic mobility of project stakeholders The need for external information  In projects involving many stakeholders, services and actors, it is recommended to setup a communication plan early in the beginning of the project to facilitate communication between different stakeholders. The communication plan is a document that outlines:  The methods used to collect and maintain different types of information. The procedures should also specify the procedures for collecting and disseminating updates and corrections to previously released documents. Recipients of the information depending on the nature of the information (status reports, data, schedule, technical documentation ...), the methods used to distribute various types of information and providers of such information. A description of the information to be disseminated: the format, content, level of detail, conventions and definitions to be used. The transmission schedules that specify when each type of information is issued Methods for accessing information between scheduled communications A method of updating and redefining the communication plan during the project  About technologies and communication devices, Achieving effective communication during the project also depends on the choice of tools and materials for a project. To do this, several points should be noted: •&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; The urgency of the need for information: the success of the project is depending to the regular update of information at all times, or regular written reports are enough? •&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; The technology available: the technologies of communication already in place are enough, or new ways of communication should be put in place (Intranet, teleconferencing software installations ...) •&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; The skill levels of project stakeholders: The proposed communication systems are compatible with experience or skill level of the participants. If this is not the case, training in communication tools are expected. •&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; The project duration: For long projects, it must be asked whether the current technology is not likely to change and does not require updating during the project. IV.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Conclusion Communication an essential element to the success of a project, Essentially during the systems analysis phase of software development. But it can become overwhelming because of the number of contacts to be taken into account. Manage communication involves both achieve effective communication plan for each class of stakeholders and create a good seal between the layers to avoid confusion. It is this double action you can expect to achieve an overall control of the project communication. V.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Reference J. G. (2011) Computer science: An overview.&nbsp;11th ed. Boston: Pearson Education / Addison-Wesley., Section 7. Lecture Notes, (2014).&nbsp; Week 6: Computer networks [Online]. Available from: https://elearning.uol.ohecampus.com/bbcswebdav/institution/UKL1/201480_SEPTEMBER/MS_CKIT/CKIT_501/readings/UKL1_CKIT_501_Week10_LectureNotes.pdf. (Accessed: 09 November 2014) Wikibooks Inc. (2014) Introduction to Software Engineering [Online]: Wikibooks Inc. Available from: http://en.wikibooks.org/wiki/Introduction_to_Software_Engineering/Process/Life_Cycle (Accessed: 09 November 2014) &nbsp; 
	              
	              Hi Everyone,
&nbsp;
In your opinion, which would be the best programming language for coding up applications using complex list-based data structures?&nbsp; What are your reasons for the choice you have made?
&nbsp;
Anthony

	              
	              Dear Dr. Ayoola,
My choice is Python. (That's pretty obvious, isn't it:-)
Python is especially good at 2 type of applications, one is string processing, the other is list processing. And string processing is mostly also list processing.
My reasons are:

Python has very good collection data types including tuple, list, set and dict.
Python has powerful list comprehansion that can easily express complicated list manipulations.
If that's not enough, there's still the "Scientific Python" library Scipy, which provides more powerful collection data types and algorithms on them.
Python has Lisp style funcitonal programming support.
The generator feature and yield keyword makes Python more expressive in list processing.

Lisp, as the "LISt Processing" language, was designed for list processing. Programming lanuage like Python and many others are influenced by it. But Lisp is not a popular programming language for making "applications". And it's Polish prefix syntax makes it a bit hard to read, IMHO. I've also used Perl before I started programming Python. Perl, like Python, is also heavily influenced by Lisp. But Python is a lot easier and more understandable than Perl.
br, Terry
Reference:
Python (programming language). (2014, November 6). In&nbsp;Wikipedia, The Free Encyclopedia. Retrieved 14:47, November 10, 2014, from&nbsp;http://en.wikipedia.org/w/index.php?title=Python_(programming_language)&amp;oldid=632671479
Lisp (programming language). (2014, October 27). In&nbsp;Wikipedia, The Free Encyclopedia. Retrieved 14:48, November 10, 2014, from&nbsp;http://en.wikipedia.org/w/index.php?title=Lisp_(programming_language)&amp;oldid=631351138
	              
	              Dear Dr. Ayoola,
This is a very interesting question. I won't assume the culture and communication have a one-way influence.
So, does the culture within an organization influence the communication? Or, the way of communication defines the culture of an organization?
Craig Larman coined the&nbsp;Larman's Laws Of Organizational Behavior. It goes like this:


1. Organizations are implicitly optimized to avoid changing the status quo middle- and first-level manager and “specialist” positions &amp; power structures.&nbsp;

2. As a corollary to (1), any change initiative will be reduced to redefining or overloading the new terminology to mean basically the same as status quo.&nbsp;

3. As a corollary to (1), any change initiative will be derided as “purist”, “theoretical”, and “needing pragmatic customization for local concerns” -- which deflects from addressing weaknesses and manager/specialist status quo.&nbsp;

4. Culture follows structure.&nbsp;
i.e., if you want to really change culture, you have to start with changing structure, because culture does not really change otherwise. and that's why deep systems of thought such as&nbsp;organizational learning&nbsp;are not very sticky or impactful by themselves, and why systems such as scrum (that have a strong focus on structural change at the start) tend to more quickly impact culture. i discovered that john seddon also observed this: "Attempting to change an organization’s culture is a folly, it always fails. Peoples’ behavior (the culture) is a product of the system; when you change the system peoples’ behavior changes."

This is of course more of a tone-in-cheek "Law", or rather, observation. But I think it's very insightful. I believe a flat structure with no hierarchy and encouraging self-leading (rather than PM leading) will help to minimized the miscommunication. Therefore, the culture following this kind of structure is a healthy culture.
br, Terry
References:
http://www.craiglarman.com/wiki/index.php?title=Larman's_Laws_of_Organizational_Behavior
	              
	              Hi Anthony

Thats a really hard one to say and i'm sure we will all probably give different answers to it.
What comes to my mind first is the size or application needs, EG:

Speed
physical DB size
client server or web based
etc

With these first thoughts i'm sure different languages would be recommended depending on the answers.
My initial reaction was about Java though.
From 1.5 onwards Java has what is called "the collection framework" which has some very handy data structures such as the ArrayList, LinkedList, Vector, Stack, HashSet, HashMap and Hashtable which can handle complex data structures. The collection framework is a collection of objects which helps to adopt some good design principles so you can concentrate on the front end rather than the data. Granted this doesnt sound like the thing someone would look at when you mention complexity because you'd imediately think of the need to delve right into the data structures and specifically handle them but i've been told by our Java team where I work that it handles complex list based data structures very well without the need for complex handling.
The Java Collection Framework package (java.util) contains: , (Chuan, 2012)

&nbsp;&nbsp;&nbsp; A set of interfaces,
&nbsp;&nbsp;&nbsp; Implementation classes, and
&nbsp;&nbsp;&nbsp; Algorithms (such as sorting and searching).

So it has already within it a packaged solution of sorts.
References
Chuan (2012) 'The collection framework' [http://www.ntu.edu.sg/home/ehchua/programming/java/J5c_Collection.html] (accessed: 10/11/2014)

	              
	              Thanks for the list, it's good. I especially liked the text " A great ScrumMaster can handle&nbsp;one&nbsp;team at a time", though I'm not sure my boss agrees:-)
Only problem with the list is it's a review list, while the "rules" are supposed to be at startup/proactive. And I agree that the word "rules" can be to strict/formal. We just need to kind off agree on a collaboration mindset.
Br. Bo
	              
	              Hi Anthony

I recommend Python, as it's based on ABC, which was invented for the purpose of teaching programming.

Theoretically, you can stick with Python, as Python can do almost anything. It's a good vehicle to teach object-oriented programming and (most) algorithms. You can run Python in interactive mode like a command line to get a feel for how it works, or run whole scripts at once. You can run your scripts interpreted on the fly, or compile them into binaries. There are thousands of modules to extend the functionality. You can make a graphical calculator like the one bundled with Windows, or you can make an IRC client, or anything else. 

XKCD describes Python's power a little better:
 

You can move to C# or Java after that, though they don't offer much that Python doesn't already have. The benefit of these is that they use C-style syntax, which many languages use. You don't need to worry about memory management yet, but you can get used to having a bit more freedom and less handholding from the language interpreter. Python enforces whitespace and indenting, which is nice most of the time but not always. C# and Java let you manage your own whitespace while remaining strongly-typed.

Regards, Babatunde

Reference:

XYCD Python [online] Available from: http://xkcd.com/353/ (Accessed 10 November 2014)

Wikipedia, The Free Encyclopedia. Python (programming language) [online] Retrieved 18:00, (10 November 2014) from:&nbsp;http://en.wikipedia.org/w/index.php?title=Python_(programming_language)&amp;oldid=632671479

Codecademy Python- learn to program in python [online] Available from: http://www.codecademy.com/en/tracks/python (Accessed 10 November 2014)
	              
	              Hi Anthony

The culture within an organisation has a great deal of influence on many aspects across the entire organisation.

I would make the specific observation about ‘entrepreneurial’ type organisations who tend to have a flatter structure – I’ve engaged with many of these and quite frankly, they’re just brilliant with such positive go get can do will share attitudes. Certainly the ones I’ve worked with anyway…

I believe there are many organisational cultures that have their place, however I’ve found that organisations that thrive in teaming and collaborative working, along with united or common goals are excellent cultures for effective communications. I believe this is complimentary with the discipline of software development projects or systems analysis.

I also believe that culture is in many ways influenced by the employees themselves. Leaders and managers have a key role to play in creating the right opportunities and environment, but it is the employees who can embrace and influence those that make it work and bring it to life.

Furthermore, and to build on the critical role that employees play, it is also critically important that individuals are singled out when it comes to performance management – no one wants to work in a team carrying substandard performers – equally, individuals need to be recognised for achievement, in both performing their roles very well, in addition to when performance is over and above their standard terms of reference.
&nbsp;
Best Wishes, Craig

	              
	              Hi Augusto
First, your IDQ is very well written with good examples. And I can’t help but commenting on several areas.
Domain knowledge I agree, and especially on your text “most importantly, Analysts or….”. I have seen this several times.
Org. barriers I have never understood the policy regarding “only managers can interact with external customers”. I have rarely experienced this, and when I have, I have only seen bad outcomes; the “floor people” must be heard and active.
Personality types Interesting point about the “introvert” developers. I have notices this, and we have actually discussed this, and they know it. The key is to be aware of this and then work from there, i.e. find ways to have them communicating in some way, e.g. through a project manager as interpreter/translator.
I also believe that teams should be mixed. But sometimes you are “stuck” with the team, and I these situations, I think it’s important to be aware of the differences, accept them, and know how to work them.
By the way, such tests can be cheating, or rather, people can cheat with them. What I mean is that I have talked to a few people who “voted” according to how they wanted the result to be, not how they really are.
Underestimating training I agree, and actually I don’t remember when it’s not underestimating. It could be interesting to see some numbers estimating/explaining who much is unnecessary spend because of this.
Admitting knowledge limitations One of the worst examples from my experience is when a very knowledgeable person, someone you trust, says something very convincingly, and you later find out it was not true.
Bw. Bo
PS: My last check showed me being ESTJ, on another day I was ENTJ. I try to mix these, starting with ENTJ then later moving into ESTJ.
	              
	              Hi Terry,
Very interesting post. Specifically the handoff. I have seen cases in which a terrible handoff created quite some havoc. This is especially critical in global environments where you often have architects, engineers and programmers in different time zones, countries and continents and even in the same city they could be located in different offices. This is further complicated in the actual development phase when programmers actually hand off code on a day to day basis.
As research has showed, the length and quality of the handoff has a direct impact on the whole project. Also handoffs are difficult to manage due to team’s heterogeneous skills and cultures; good communication management is a key to enhance the quality of the handoffs (Kroll et al. 2014).
Best Regards,
Augusto
&nbsp;
References
Kroll, J., Audy, J.L.N., Richardson, I. &amp; Fernandez, J. (2014) 'Handoffs management in follow-the-sun software projects: A case study', Proceedings of the 47th Annual Hawaii International Conference on System Sciences, HICSS 2014, Hawaii, 2014. Proceedings of the Annual Hawaii International Conference on System Sciences. doi: 10.1109/HICSS.2014.49.
&nbsp;

	              
	              Hi Terry/Craig.
Agreed, it’s a binary tree. However this is not a bad idea, you can certainly have any kind of tree structure there. But if you think about it in terms of a search algorithm (that is, to search a contact), the best scenario for speed search is using a binary tree. Say that we have a tree with all the letters of the alphabet split into two A-&gt;M ; N-&gt;Z (to have a balanced tree), we know that a name in the contact list is not likely to change much, so we can pretty much:

Group all our contacts in a particular group of names (for example everyone whose name starts with M) in a data structure (assume here a simple Array, named ArrayNames-A).
Then Index our Contacts groups by assigning them under the appropriate tree node (thus creating a binary tree with nodes only having one child). This will give us something like:

&nbsp;




ArrayNames-A


B


NIL




ArrayNames-B


C


NIL




ArrayNames-C


D


NIL




…


…


…




&nbsp;

At this point when we type B in our search, the search function will traverse the binary tree right to the B pointer, and display the names inside ArrayNames-B.

As a simple search function, in theory this should work and it should be super-fast. In fact I suspect that most trees can be “normalized” to become binary trees. Even if that means creating trees inside trees, which may not be a bad idea, depending on what you need to do.
Do I make sense?
Best Regards,
Augusto

	              
	              I actually realized that the table needed some clarification and that I forgot to add the reference to it, re-posting. My apologies.
----
Hi Terry/Craig.
Agreed, it’s a binary tree. However this is not a bad idea, you can certainly have any kind of tree structure there. But if you think about it in terms of a search algorithm (that is, to search a contact), the best scenario for speed search is using a binary tree. Say that we have a tree with all the letters of the alphabet split into two A-&gt;M ; N-&gt;Z (to have a balanced tree), we know that a name in the contact list is not likely to change much, so we can pretty much:

Group all our contacts in a particular group of names (for example everyone whose name starts with M) in a data structure (assume here a simple Array, named ArrayNames-A).
Then Index our Contacts groups by assigning them under the appropriate tree node (thus creating a binary tree with nodes only having one child). This will give us something like:

&nbsp;




Cells containing the data


Left child pointer


Right child pointer




ArrayNames-A


B


NIL




ArrayNames-B


C


NIL




ArrayNames-C


D


NIL




…


…


…




As per Fig 8.15 on (Bookshear 2011)

At this point when we type B in our search, the search function will traverse the binary tree right to the B pointer, and display the names inside ArrayNames-B.

As a simple search function, in theory this should work and it should be super-fast. In fact I suspect that most trees can be “normalized” to become binary trees. Even if that means creating trees inside trees, which may not be a bad idea, depending on what you need to do.
Do I make sense?
Best Regards,
Augusto 
References
Bookshear, J. G. (2011) '8.3 Implementing Data Structures (Figure 8.15)' in Computer Science: An Overview, 11th edn, Addison-Wesley, pp. 358.
&nbsp;

	              
	              Hello Chris,
I am not a manager but I must say I am like your managers having had the course. Instead of asking peoples colours I must admit my approach would be interrupting conversations saying: You must bet THAT colour!&nbsp;
Being a septic towards Human Resource efforts categorising&nbsp;people I was surprised how accurate and useful description could be made from a 24 questions long questionnaire. Of course I did not agree with the colour they gave me (RED!) but reading everybody's reports and trying to understand reactions on communications was really helpfull. It changed the team and improved our communication. At least for 2 months until everything was forgotten and we were back to normal.s
Listening is a large part of a conversation and that means trying to understand people, where they are coming from and what there intension's are. This should be emphasized more in education that for some reason concentrates more on debating which is one directional.
Greetings from Bram


	              
	              Hi Anthony,
My choices are Python and .Net-C#. Python is good for scientific calculations, and is a great option for research-based applications, as it supports high-precision data processing. C# is good for rapid application development&nbsp;and hence the perfect choice of programming languages&nbsp;when we are coding for financial and accounting software. In my opinion, we cannot pick just one language as the best one for programming, as it all depends on usability and maintainability.&nbsp;
Best regards,Kharavela
	              
	              What colour are you Craig? Yellow?

Greetings from Bram
	              
	              Hello Craig and Terry,

I totally agree with you guys and hope you do not think I post this saying we should people like recourses. As said I am a sceptic towards HR methods categorising people. The fact I was surprised of is how this method could help people by adressing communication and other peoples thoughts without making it personal.
Having said that the theories does say your will never change which I do not believe. I think I would have been another colour 15 years ago.
Greetings from Bram
	              
	              Hi Augusto
Like your other IDQ, this one is also really good and pedagogically explained with text, and especially with the figures.
I agree with your conclusion on mixing with OOP objects. I also think it can be more complex fist thought, e.g. if we also add extra functionality like pictures, GIS information, favourites, groups, job, and other data about the person.
Br. Bo
	              
	              Hi Craig,

I haven't as far as taking part in the session to decide which colour you fall into. I think I was promoted a little to late last year to get on the list. Either that or they missed me out on purpose..:)
But certainly from a pepersonal view I can attest to having a very multi coloured management/personality type. I tend to change my colour like a chameleon depending on my mood ...
Chris

	              
	              ...perhaps on the weekend :-)
Best Wishes, Craig
	              
	              Hi Anthony
I have never heard about this before. But as I understand it, it is used in different optimization situations where you have a lot of numbers, and more precisely many zeros, and you want to optimize the memory usage, then sparse will optimize by saving only the values which have numbers &lt;&gt; zero, still knowing were the zeros should be, e.g. in a spreadsheet. (Amunategui, 2014) 
Also researching the web I find it’s being used by IBM regarding CPU and memory. Further it’s much used in any area which has a lot of data, e.g. mathematical and vector based.

References
Amunategui, Manuel. (2014). “Sparse Matrix and GLMNET: Machine Learning with R”. [Online] Available at: https://www.youtube.com/watch?v=Ysh2gs8VKvQ. (Accessed 10-11-2014). 
&nbsp;
Br. Bo
	              
	              Sorry, I forgot the IBM link:&nbsp;
IBM. http://researcher.watson.ibm.com/researcher/view_group.php?id=1426. 
	              
	              Absolutely Augusto! .... I think... :-) &nbsp;
It is sometimes a challenge to interpret somethings when you are in unfamiliar territory. I was just a little confused with you jumping from M, then to B ...however, just so I am following, if you had typed C, the search would traverse to the pointer C and display the names in ArrayNames-C
Also, this was one of the reasons I suggested a tree, in that my research suggested it was easily searchable and needed to be balanced. Thank you for helping to fine tune the thinking here.
Sorry if this is obvious, just wanted to clarify - this is a good example of missing the practical side of learning, and being able to demonstrate the scenario.
Best Wishes, 'Mr Sunshine Yellow'&nbsp; sorry, thought it was still weekend, Monday's I am 'Mr Earth Green'
	              
	              COMMUNICATION
Introduction: 
Communication is very important for effective software development. The end users using the software and the programmers must communicate effectively for the development cycle to be complete.
Reasons for communication breakdown:

&nbsp;Group diversity: The developers of the software within the group should have coherent communication. Team members may not know how to transfer ideas to other team members in the group due to their background diversity. Some of the examples of&nbsp; group diversity include:

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; A- Experience
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; B- Skills
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; C- Personal biase
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; D- Environment growing up
Experience: Some team member have a lot of knowledge on a particular area but because of their background of not being exposed to expressing themselves properly either in school or at home will affect how they communicate or transfer ideas to others.
Skills: This has to do with ability to do something very well. It has to do with the team members expertise which some have a better skill-set than others. Developing software with team members from different backgrounds and with diverse skills could slow down communication during the system analysis phase.
Personal biases: These are personal differences from team members of the software development team. These differences are as a result of team members coming from different backgrounds and not being able to tolerate certain behaviors, lifestyle, mode of communication etc.
Environmental growing up: Environmental backgrounds affect team member behaviors differently.
&nbsp;

Lack of established communication goals: This is a major problem associated with the developing a software. Vocal team members tend not to establish specific goals early and less vocal team members never make any contributions at all. Discussions between team members in the analysis phase could deviate from topic to topic without any sense of purpose or direction. However, discussions in which applications are brainstormed and design follow this same course. The team leader needs to set communication goals as part of software application requirements. e.g inform, engage, listen, persuade, advocate, rally.

Solutions for alleviating communication breakdown are:

Stifled communication: 

This involves each member of the team contributing his or her own idea to the development of the software application. Communication should be democratized and contributions should be gotten from every member of the team whether quiet, aggressive or introverts. This is a good way of balancing your contributions toward the project. All of the ideas may not be valid but there will be something to gain from it. Also, this will give the stifled group a sense of belonging in the final product.

Spend time talking about the application:

The team leader should ensure he or she creates forums that every member of the team can quickly relate to or communicate ideas. Some example of such forums include: instant messaging groups, texting groups, emails, group chats, group forums etc. this will be effective in discussing urgent interest/ideas to the team.

Initial involvement for every member of the team: 

Participation and contribution of ideas should be by everyone in the team. Each member should take responsibility for his or her contribution towards the project.
Conclusion:
Effective communication is a necessity in developing a software application. This is achieved by getting everyone involved through to the final phase of the project.
&nbsp;
References:
Agile modeling (n.d) communication on agile software teams [online] Available from:https://www.agilemodeling.com/essays/communication.htm.(Accessed 9 November, 2014)
Coughlan, J. &amp; macredie, R. (n.d) Effective communication in requirements elicitations: A comparison of methodologies [online] Available from:https://www.core.kmi.open.ac.uk/download/pdf/333154.pdf.(Accessed 9 November, 2014)
Mueller, J. (2012) 10 reasons development teams don’t communicate[online] Available from:https://www.blog.smartbear.com/management/10-reasons-development-teams-dont-communicate. (Accessed 9 November, 2014)

	              
	              hello Augusto,
very informative post you got here.
I like your work and also noticed we both had a similarity in a particular point you mentioned in your last paragraph-"personality traits play a role in any team". This is similar to a point i mentioned (personal Bias) which affects teams
Regards, Martins
	              
	              hello Ramin,&nbsp;
I have not heard of spars matrix but will read up your references to check it out.

Regards&nbsp;
martins
	              
	              Hi Augusto,
Thanks for your reading:-)
I think the best we can do with handoff is to avoid it. Improving the quality of handoff is actaully going to the wrong direction.&nbsp;
br, Terry
	              
	              Hi Augusto,
Yep, you are right. Any tree can be mormalized to a binary tree, where the left branch for a node is the first child, and the right branch for a node is the next sibling.
br, Terry
	              
	              Hi Bram,
No, I didn't mean your post implid treating people like resource. I like yours and Craig's posts.
I just added something to see the reaction from colleagues:-)
br, Terry
	              
	              Hi Bo,
I like your post, it higlighted some of the areas of breakdown that I didn't even think of. As I learn I agree that a well developed SRS is paramount to this phase and the communication breakdown you highlighted can cause gaps in the SRS.
I also like the Agile method you mentioned "scrum" and the cheklist Terry mentioned, that seems to be a good tool for ensuring everything is in place. Thank you for the insight.

Regards,
Ricardo&nbsp;
	              
	              Dr. Anthony,
I agree with with Craig 100%, he has mentioned some of the same sentiments i have in relations tothe projects I have worked on. The principles of the agile methods stand out in many of the project I have been apart of. The frequent meetings, constant updates and iterations to the project.&nbsp;
Indeed the key benefit is the dynamic nature of the agile methods, which allows communication and the outcomes to direct the development of the project. Another key benefit to the method is that of protoype and rapid prototyping to give functional samples of the program to be developed.

Regards,
Ricardo
	              
	              Hi Bo, Terry.
On this issue of the SRS document being dynamic, that would work fine for an in-house self controlled project, but for most vendor contracted work, it becomes tricky to try and add new specifications to the original especially because of time and budget constraints. In as much as the waterfall model may not be the most appropriate, it lends well for these vendor contracted projects because the development is based on agreed milestones documented at each stage which are signed off
I am currently dealing with such a situation where we would like to add some additional functionality to the software which we feel should not be material enough to warrant being viewed as a major change, but the vendor because their interest is in getting additional payment keeps pointing out that the new requirements was not part of the original specifications neither is it a new functionality.


V&amp;V - Verification &amp; Validation
References
http://courses.cs.vt.edu/csonline/SE/Lessons/Waterfall/index.html


	              
	              Meant missing functionality not new functionality in my statement above

Joseph
	              
	              Hi Augusto. I believe on the issue of Personality types, the shoe is also on the other foot as far as the 'client' is concerned. An analyst I believe should be able to ask the right questions in order to elicit required and informative responses. However, if you are dealing with a respondent who is not expressive and only gives broad highlights of what is required, a lot can be missed out that will only come out later when it's too late. Another element in personality types is the fact that those with dominant personalities may influence the way in which a matter is broached even though they may not necessarily be bringing out the best approach in defining what's needed. In 'The Art of Critical Decision Making' , Prof Michael A. Roberta gives an account of the element of 'Group think' and the fact that often times, some leaders conduct themselves in ways that do not encourage candid dialogue and accommodate dissenting views on a subject from others.&nbsp; Joseph References Prof Roberta, M. 'The Art of Critical Decision Making', The Great Courses, Available at&nbsp; http://www.thegreatcourses.com/courses/art-of-critical-decision-making.html, Accessed 11/11/2014     Joseph
	              
	              
Dr. Anthony,
Culture can be considered as the assumptions of the world or should be. This assumption is often shared among a particular set or group of individuals and as such it often determines their thoughts, feelings, perceptions and behavior [1]. 

Apart from the cultural differences that may exist amongst individuals raised in different environments and more wildly different parts of the world. There lies cultural differences amongst persons educated differently, that is persons of different professional backgrounds. Engineers maybe thought to deal with matters one way while a marketing personal may be thought to deal with the matter a different way. A simple example is that a marketing person maybe focused on the appearance of the product while an engineer may be thought to focus on its functionality. The language that these individuals use may also have different meanings, additionally some of these said executives may tend to focus their direction on their experiences in the field and this simple difference in experience may cause one person to lead in one direction and the other in the opposite.

There are basically three cultures of Management:
1) Operational cultures – this varies between persons of different operational groups or operating industries. The culture is often based on human interactions and working in teams with open lines of communication. Persons in this culture know that products do not always work as designed or expected to and as such their may be unpredictable operation errors or flaws.
2) The Engineering culture – these individuals posses the engineer and technical knowledge and may stem across varying industries as in operational cultures. These individuals may be very optimistic and place a lot on energy on perfecting what they do.
3) Executive cultures – these individuals may hold similar views and opinions of executives around the world and maybe be focused on the finances and the management aspect of the company. They may also be more focused on controlling.

Thus we can see issues the following possible issues arising:
1) Cost arising as Engineers may be more focused on developing better and safer products while managers may not be willing to invest in this product.

2) There may be issues of management and controlling as executives may want to implement more control over operators.

3) Power differences may also make one party believe that they are better than the other and as such their opinions need to be adhered to.

4) There may be differences in organization goals and technical goals of the software. For example it may be difficult to develop the system to meet some of the organization goals.

5) There may also arise challenges in meeting deadlines.

There needs to be more understanding of cultural differences among stakeholders and engineers in the software analysis phase. Thus there can be a greater understanding of the differences that exist amongst individuals. There also need to be more communication among individuals so that requirements and specifications can be clearly understand. No one individual have all the answers or can solve the problem, there needs to be greater collaboration and involvement of individuals in the process.



References:
Schein, E. H. (1996) Three Cultures of Management: The Key to Organizational Learning [Online]. Available at: 
http://sloanreview.mit.edu/article/three-cultures-of-management-the-key-to-organizational-learning/ (Accessed on November 10, 2014)

Sommerville, I. and Rodden. T. (1995) Human, Social and Organisational Influences on the Software Process [Online]. Available at: 
https://www-systems.cs.st-andrews.ac.uk/STSE-Handbook/Papers/HumanSocialandOrganisationalFactorsintheSoftwareProcess-Sommerville.pdf (Accessed on November 10, 2014)



	              
	              Dr. Anthony,
Culture can be considered as the assumptions of the world or should be. This assumption is often shared among a particular set or group of individuals and as such it often determines their thoughts, feelings, perceptions and behavior [1].&nbsp;Apart from the cultural differences that may exist amongst individuals raised in different environments and more wildly different parts of the world. There lies cultural differences amongst persons educated differently, that is persons of different professional backgrounds. Engineers maybe thought to deal with matters one way while a marketing personal may be thought to deal with the matter a different way. A simple example is that a marketing person maybe focused on the appearance of the product while an engineer may be thought to focus on its functionality. The language that these individuals use may also have different meanings, additionally some of these said executives may tend to focus their direction on their experiences in the field and this simple difference in experience may cause one person to lead in one direction and the other in the opposite.There are basically three cultures of Management:1) Operational cultures – this varies between persons of different operational groups or operating industries. The culture is often based on human interactions and working in teams with open lines of communication. Persons in this culture know that products do not always work as designed or expected to and as such their may be unpredictable operation errors or flaws.2) The Engineering culture – these individuals posses the engineer and technical knowledge and may stem across varying industries as in operational cultures. These individuals may be very optimistic and place a lot on energy on perfecting what they do.3) Executive cultures – these individuals may hold similar views and opinions of executives around the world and maybe be focused on the finances and the management aspect of the company. They may also be more focused on controlling.Thus we can see issues the following possible issues arising.1) Cost arising as Engineers may be more focused on developing better and safer products while managers may not be willing to invest in this product.2) There may be issues of management and controlling as executives may want to implement more control over operators.3) Power differences may also make one party believe that they are better than the other and as such their opinions need to be adhered to.4) There may be differences in organization goals and technical goals of the software. For example it may be difficult to develop the system to meet some of the organization goals.5) There may also arise challenges in meeting deadlines.There needs to be more understanding of cultural differences among stakeholders and engineers in the software analysis phase. Thus there can be a greater understanding of the differences that exist amongst individuals. There also need to be more communication among individuals so that requirements and specifications can be clearly understand. No one individual have all the answers or can solve the problem, there needs to be greater collaboration and involvement of individuals in the process.&nbsp;References:Schein, E. H. (1996) Three Cultures of Management: The Key to Organizational Learning [Online]. Available at:&nbsp;http://sloanreview.mit.edu/article/three-cultures-of-management-the-key-to-organizational-learning/ (Accessed on November 10, 2014)Sommerville, I. and Rodden. T.&nbsp;(1995) Human, Social and&nbsp;Organisational Influences on the&nbsp;Software Process&nbsp;[Online]. Available at:&nbsp;https://www-systems.cs.st-andrews.ac.uk/STSE-Handbook/Papers/HumanSocialandOrganisationalFactorsintheSoftwareProcess-Sommerville.pdf (Accessed on November 10, 2014)
	              
	              Hi Babatunde,
I like your post very much, learning a few new things from reading it. I didn't know about the dynamic arrays &nbsp;before your post. Only read about homogeneous and heterogeneous from the course text. Doing some additional reading to gain a better understanding and its application.

Regards,
Ricardo
	              
	              Hi Bo,
I agree with Terry. great post. &nbsp;As a person that works in Agile the rules of engagement is a reality. It doesnt have to be scary though,for us we use it for even the silliest things, for instance we might agree on a few words we cant say during the sprint along with more serious rules like we will communicate face to face about say bugs before we post them on Jira. That way we remember the fun things together with the important things.
kind regards,
Belinda
	              
	              Hi Dr Anthony,

An agile environment is all about facilitating communication and encouraging interactions between team members. I think the scrum ceremonies bridge the communication gap e.g.:
1. daily 15 minute standups- everybody gets a chance to speak on what they did yesterday, what they will do today and if they have any impedimentspreventing them from fulfilling their tasks - this means the feedback loop is much shorter and everybody is aware of what is going on and help can be rendered if needed thus faster product delivery
2. Planning 1 and 2 - teams decide what they will do and how they will do it. When the meeting is over everybody knows what the sprint deliverables are and commit to what they can handle
3. Back log grooming- teams get to see in advance of what is coming in the coming sprints and raise alarm bells if any -there is constant communication
4. Retro-teams get to reflect on each sprint and work on becoming better. I've found this to be evry helpful and it definitely&nbsp;improves relationships between team members.
5. sprint review-teams get to demo what they have done to stakeholders-this bridges the communication gap and facilitates direct communication with stakeholder where everybody can see what they really have in hand in terms of the product and informed decisions can be &nbsp;made.
Best regards,
Belinda
&nbsp;
	              
	              Hi Terry,

Thank you.

I dont know why it came out like that :-(&nbsp;
	              
	              Hi Anthony,

Every organization has a culture.&nbsp; Culture is defined by the way people communicate, interact, how information is shared and how decisions are made.

I believe organizational culture is something that is created by the leadership of the organization and can become ingrained into the core fabric of the way things are communicated. Certainly, positive work cultures can influence productivity and a healthy work environment; however a culture that is less than positive can have the opposite effect and can actually stifle worker productivity and have an impact on the bottom line.

Good&nbsp;communication&nbsp;and how information is shared and exchanged within the organization is a key characteristic of culture. Effective communication is what keeps internal processes running smoothly and helps to create positive relations with people both inside and outside the organization.

I’ve always said that wars are fought because of cultural communication misunderstandings.&nbsp; This is a sad truth because it need not be that way.

When there are communication breakdowns within an organization, it can lead to conflict in the workplace which is often caused from lack of communication or inaccurate information.

Often these breakdowns are caused from ineffective communication channels within an organization.&nbsp; When information is not shared in a structured way, it results in those who need the information to fill in the blanks.

Organizational leadership needs to be cognizant of what information needs to be shared, when it should be shared and what process should be used to share information.&nbsp;When employees don’t have all the information, they tend to fill in the blanks and often are not correct in their assumptions. &nbsp;Being proactive in communicating minimizes the productivity gaps when employees are trying to figure it out.

Best Regards, Babatunde
	              
	              
I am going with LISP
The name LISP derives from "LISt Processing". Linked lists are one of Lisp language's major data structures, and Lisp source code is itself made up of lists. As a result, Lisp programs can manipulate source code as a data structure, giving rise to the macro systems that allow programmers to create new syntax or new domain-specific languages embedded in Lisp

Reference
URL http://en.wikipedia.org/wiki/Lisp_(programming_language)

	              
	              Anthony
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Culture is the basis of communication style. Look at the extreme e.g. Ray Dalio, CEO of BridgeWater Associates. They have &nbsp;a total honest and challenging your manager culture and then you have the other extreme e.g. old well established brokerage houses (like Goldman)&nbsp;where one follows the set hierarchy guidelines and don't deviate from them.
The more open "flat" organization structure organization the more likelihood of open and honest communication between your development team.
robin

Reference
URL http://www.bwater.com/Uploads/FileManager/Principles/Bridgewater-Associates-Ray-Dalio-Principles.pdf

	              
	              Hi Anthony

I have not heard of sparse matrix, but reading about it, I could chip in something for our use.

For software applications that use sparse matrix, the Sun Performance Library has two software packages namely SPSOVLE and SuperLU mainly written in FORTRAN, which can be used to factor and solve sparse linear systems of equations. 

SPSOLVE is a collection of routines that solve symmetric, structurally symmetric, and unsymmetric coefficient matrices, using one of the several ordering methods, including a user-specified ordering.

The previous releases of SPSOLVE were referred to as the sparse solver package. This package contains interfaces for FORTRAN 77, but Fortran 95 and C interfaces weren’t currently provided. So in other to use SPSOLVE routines from Fortran 95, use the FORTRAN 77 interfaces. To call SPSOLVE from C, append an underscore to the routine name (dgssin_(), dgssor_(), and so on), pass arguments by reference, and use one-based array indexing.

On the other hand, the SuperLU package in the Sun Performance Library is the sequential version (version 3.0) of the public domain application that solves general unsymmetric sparse systems. While it is sequential, SuperLU does make use of several levels 2 and level 3 BLAS routines that are place parallel to one another. SuperLU is written in C; therefore, array indexing must be zero-based regardless of whether its routines are being called from Fortran-based SPSOLVE or a C driver program.

Best Regards, Babatunde

Reference:

Sun Microsystems Sparse Computation [online] Available from: https://docs.oracle.com/cd/E19205-01/819-5268/plug_sparse.html (Accessed 10 November 2014)&nbsp; 
	              
	              Hello Craig,
The Insight method does take into account in your free time you can have a different dominant colour.&nbsp;
But I am quite sure that was not what you were touching upon ...
Must admit I can be yellow, red, blue and green in the weekend.
Greetings from Bram
	              
	              A sparse matrix is one with most of its entries equal to 0.0 (zero) or one with enough zeros to take advantage of. The opposite of sparse is dense, meaning that all of the entries of the matrix are explicitly stored, even if some of them are exactly equal to zero.
Regards, Babatunde
	              
	              Hi Joseph,
I couldn't agree more. You are absolutely on the money here.
This is one of the key challenges to navigate through when working with strategic partners and vendors and using the waterfall method - which, and as you referred, tends to be the most common approach for contractually driven programmes.
If I may, I'd like to share my perspective on this.
It is also important that this is accepted as a 3-way 'issue' - You, the IT provider, your IT vendor, and your customer, a business representative. I'll come on to this shortly.
I have found that as part of my 3-year planning process, and more specifically the 1-year budget refinement, that it is important to establish some form of commercial flexibility in my overall budgets to allow for such changes.
Some of these changes are 'IT' specific, which as we know is a dynamic environment and can change massively very quickly, therefore sometimes somethings cannot be accounted for in the requirements or design phases of large projects. As such, I have tended to 'build' some commercial buffer in my budgets but also, and probably more critically, developed strong collaborative relationships with vendors and customers that enable 'discussions' to take place to be able to flex previous agreements, within reason. This is typically a balanced approach and based on the implications of the change itself. Anything more, the wider programme steering board needs to consider the broader impacts of not doing the change, and make a decision together on way forward.&nbsp;
If the change is 'Business' driven, then again this requires similar considerations relative to impact, and discussions with the vendor, but ultimately I would not cover that change commercially. I would expect the business to step up for that small change budget. Again, anything more, the wider programme board needs to be engaged as above. &nbsp;(BTW, I have funded business changes in the past, but this was in the wider interests of the business and IT, and only because I had some funding available, and the business didn't - but, this is NOT the norm).
Essentially, in my experience it all comes down to relationships and management of those relationships. This, as with the theme this week, is critically dependent on effective communications and collaboration. The relationships are, in the broadest sense, 3-way and each 'party' should be proactive in planning for such changes, even though they dont know what those changes are at the time of planning.&nbsp;
Lastly, whilst we recognise that we're all in business to profit, I've found that by developing and leveraging great relationships (strong, but mutually beneficial), you may find that some vendors won't actually charge for small changes in the interests of the longer term relationship and can easily accommodate the work for the types of changes that we're talking about. Larger changes are a different story, and require much more oversight and governance at steering level.
Just wanted to share my perspective, not intending to teach you to 'suck eggs'
Hope all makes sense,&nbsp;
Best Wishes, Craig



	              
	              Hi Joseph,
I don't think you are alone in this case:-)
I think the V&amp;V (verification &amp; validation) in the waterfall model are not useful or making much sense to most of the stakeholders. I would recommand a different V&amp;V:
Visible &amp; Valuable
Visible: It should be visible from external (the user's) point of viewValuable: It has value for the customer
Base on this definition, none of the V&amp;Vs in waterfall model you just showed satisfied the new V&amp;V criteria, probably except the very last V&amp;V. If both the vendor and the customer focus on continuously delivering these 2 Vs rather than verifying and validating interim steps, the cost of change could be lower for both of them.
I choose not to comment on you &amp; your vendor's situation, because lack of details. If you are looking for new way of contracting, I would like to recommend the "agile contract primer":
http://www.agilecontracts.org
br, Terry
	              
	              Hi Belinda,
Some years ago I was told grooming is a bad word in English. So I quit using that word since then.
br, Terry
	              
	              Belinda,
You are showing exactly a good example for something you agreed. What's scary is a rule given by the others (especially when it doesn't make much sense). Read "the rules", how much of it would you agree? :-)
br, Terry
	              
	              Hi Craig,
Have your come across a book called Implementing beyond Budgeting by&nbsp;Bjarte Bogsnes?
I like your experience sharing very much. It's really great. I would like to add something. I don't remember which industry Joseph is from. But for some companies, it's essentailly wrong to let a vendor to develop their software system, for example a bank. A bank nowadays is mostly just a software company. Making software is one of their core competence and competitiveness. It doesn't make sense for them to give their work to any other vendors.
br, Terry
	              
	              Hi Anthony,  The modeling of sparse&nbsp;matrix is a representation that stores only the nonzero coefficients thereof. This gives the following advantages:  Saving space by filling rate of the matrix, Knowledge, starting from a non-zero coefficient gives we obtained the next non-zero coefficient according to the line and / or the column very efficiently. This reflects an immediate gain in computational operations on it.  Disadvantage, we will certainly have a loss of efficiency when accessing a value in the matrix. The example of such matrices are array manipulated in the analysis of textual data containing as many columns as words and as many rows as documents, the cells giving the number of times that the word on the column was found in the document associated with the line. BR,
	              
	              Hi Everyone,
&nbsp;
Resistance to change is often cited as a “primary reason” for software systems development miscommunication and subsequent failure. This resistance usually stems from key stakeholders, who typically have not bought into the new scheme or who have an axe to grind with project management.
&nbsp;
Do you have any practical suggestions for effective management of change and related stakeholder communication, within organizations experiencing significant change as a result of developing new IT systems? &nbsp;Some of you have dealt briefly with this in a few posts &nbsp;- perhaps others in class have additional insight or fresh ideas for managing these aspects?
&nbsp;&nbsp;
Anthony

	              
	              Hi Robin,
I was involved with a Master's program in Applied Computing several years ago, that taught Prolog and Lisp as sample declarative languages. &nbsp;Lisp was indeed a language noted for flexible manipulation of list data structures. &nbsp;I wonder how much usage is made of Lisp these days?
Anthony
	              
	              Hello Class,
Delving further into characteristics of the sparse matrix data structure ...... &nbsp;
There are a number of common storage formats used for sparse matrices, such as Compressed Sparse Row Format (CSR), Coordinate Format (COO), Diagonal Format (DIA) and Block Sparse Row format (BSR).&nbsp;&nbsp; Which of these are the most efficient and/or easiest to implement?
&nbsp;
Anthony

	              
	              Hi terry,

Vendor developed support solutions don’t have a monopoly on solid feature sets or good support, but they may be a faster and easier choice especially for simple needs.

br, Babatunde
	              
	              Hi Joseph

I recently had an opportunity to participate in an Open Space session where we explored how organizations that are mainly guided by Waterfall methodologies, unwittingly also employed agile practices.

I observed that most projects that were considered "high priority" projects for an organization typically had the luxury of being situated into a team room. As much as the team initially resisted this set-up, they quickly found that the ability to collaborate and bounce ideas off of each other was quite beneficial to their success versus being bound to the walls of their cubicles on various areas of the floor. The team was able to brainstorm ideas and quickly rule them in or out by conducting mini-proof of concepts that allowed them to understand the solution in more detail and determine if it was a logical path to follow. I also learned that the communication with neighboring groups improved; therefore, strengthening their cross-functional relationships.

br, Babatunde 
	              
	              Hi Terry
I guess banking sectors are different around the world. It's very normal here in Scandinavia for banks to let vendors develop systems. Of course this depends on the bank size. To my knowledge there are, e.g. in the Danish kingdom, only 3-5 banks that have their own software development. All the others establish associations, so-called data-centres, where they join forces in IT-systems development, primarily the core-systems and in some cases also IT-infrastructure. There are 3 of these data-centres in Denmark with customers in Denmark, Faroe Islands, Greenland, Norway and Sweden, and to my knowledge they aim towards getting more customers. The 3 companies are:

SDC, http://www.sdc.dk/web/EN/About/historie.asp 
BEC, http://www.bec.dk/uk/about-bec.aspx 
BankData, http://www.bankdata.dk/da/om/aarsrapport

Br Bo

	              
	              Hi Babatunde
In my experience, people how sit in the same physical "team room" consequently get better results. I have tried different kinds of collaboration, but none beat the physical approach. And yes, this normally just happens when you have "high priority" projects - or at least where I've worked.
Br. Bo
	              
	              Hi Joseph, Craig, Terry and all
I will start by quoting a few sentences, from Craig, which I like very much:

“..buffer in my budgets”
&nbsp;“….flex previous agreements, within reason.”
“…it all comes down to relationships..”

I agree, the SRS must not be a dynamic “gift-shop” (I hope you know what I mean), changes must be controlled and agreed upon. What I mean in my writing above, is, that if the SRS is “closed/final” by principle, then it could destroy a system idea. I’ve worked in a bank for many years both with small and big system changes/developments (100+ mill. DKK where four Faroese banks (unfortunately for the Faroese community) converted to a Danish data centre) were the developers are external suppliers. In the biggest projects we had pre-agreed rules for handling changes, but still, it also came down to daily relationships and understanding. I you manage to build a good relationship with your supplier, then it have been my experience that it’s possible to make changes to the SRS. Yes, sometimes you have to pay extra for it, but sometimes the supplier will just fix it in the interest of good future relationships.
Br. Bo
	              
	              Hi Kharavela
Nice post, and I agree on your conclusion on the iterative process. I sure it's bound to happen.
By the way, and a bit outside of the original discussion; Several systems have the field "middle name". What are your thoughts on this? My own thoughts are that I don't like it, and if I meet a system with this field, I don't write in it. There are at least two reasons: 1) Even more systems don't have this field. 2) If you have different systems connected to a core system, you probably will need some home-made conversion/concatenation procedures to handle this.
Br. Bo
	              
	              Hi Terry,

As with most words it's all down to the context it is delivered in.
There is nothing wrong with the word 'grooming' if you are talking about yourself such as "I have a regimented grooming routine" or "I enjoy grooming myself" , it just means doing such things as cutting your hair or clipping your nails. In case you didn't know.
Where it might be a bad or inappropriate word is if you said something like "I enjoy grooming on the Internet" . or if you mention it about someone else eg " I've been grooming someone"
Grooming in that context is not good. It tends to get used to mean when a man or woman is befriending a child with the aim to sexual contact or pedophilia ...
But yeah, better safe than sorry and if your not sure just stick away from using the word.

ta
Chris


	              
	              Hello Anthony,
This type of resistance is typical when implementing an entirely new system/process in an organization. In my experience, the only way that I have dealt with such situations, is by explaining to the stakeholders the relevant details and advantages of the new system/process being introduced and, of course, the effective cost-saving that this new system/process will definitely bring about in their current operations. Additionally, a thorough demo/presentation of the new process and the detailed impact analysis has helped us many a time in getting the required buy-in from our clients.
Thanks,Kharavela
	              
	              Hello Bo,
Thanks, yes I fully agree with you on the “middle name” field. This has become obsolete in the most of the systems. I also agree with your point on homemade mechanisms to handle the name fields either in query or in the code. For example; SELECT UM_FNAME + ‘ ‘ + UM_MNAME + ‘ ‘ + UM_LNAME FROM USER_MASTER or SELECT UM_FNAME + ‘ ‘ + UM_LNAME FROM USER_MASTER or even with reference to Contact classclass Contacts{
 public string FirstName{ get;set;} public string LastName{get;set;}
// read only attribute :-) public string Name {get { return FirstName + ‘ ‘ + LastName;} public string DoorNo{get;set;} public string AreaStreet{get;set;} public string State{get;set;} public string Country{get;set;} public string Zipcode{get;set;} public int CountryCode{get;set;} public int PhoneNumber{get;set;} public char Status{get;set;}
}
But, I feel “mid name” may be necessary in Govt. applications such as Passport details, SSN or medical documents, where they need to capture your complete information along with biometrics.
Wouldn’t you agree?Thanks,Kharavela
 
	              
	              Hi Craig and others,
In order for a tree to be easily search-able it must be balanced. It think this is not the case for a contact list. Most of the the time for example the q is not filled at all and there are a lot of entries for the t.&nbsp;
Therefore I concluded that the linked list would be most optimal. It is better search-able but performs probably less when new entires or deletes are preformed. But from an end user perspective the shifting process has less impact as it happens only when a change has been made. Because a change is most of the time an unique instance (except when one needs to import a full new database manually) this operation is active as the user stopped interacting with the software. If searching takes a lot of time that has very much impact as an end users expects to get fast feedback on his quiry. -
Therfore a tree is less effective than a linked list.
Greetings from Bram&nbsp;
	              
	              hi Kharavela and Dr. Ayoola,
I also have the same experience as kharavela, when it comes to change, peoples resistance is always very stiff and this can affect positive development. In my office I had gotten stiff resistance from colleques and even superiors from changing I.T items as simple as laptops and anti-virus softwares. At some point after explaining to them why the new item is better and they still dont buy it, I have to even bring in my boss to explain things better.
However, my take on how to bring about change in organisations will be to make out time and explain things better to their understanding.
regards, martins&nbsp;
	              
	              hi babatunde,
The detail information of sparse matrix in your work was very explicit. It is more detailed than i read about and it has helped me in understanding it better.
Best Regards,
martins
	              
	              Hi Terry,
Thanks for your reply. Your welcome - I think the aspect of sharing knowledge is a key benefit to this programme. However, I honestly feel that I am benefiting more from your (and other colleagues) insight, rather than you from mine. Collaborative relationships are important to me, one of my core values.
In terms of your response, there isn't really that kind of restriction in the UK, not that I'm aware of. I've worked for a couple of banks here and both of these had various sourcing models with a combination of full in-house, co-sourcing and full outsourcing. They tend to keep the IP and architectural design (both infrastructure and software) in house, but typically looked to strategic partners for actual software development. There was two key reasons for this. The first is primarily related to resource planning and headcount, in that it was extremely expensive to keep programmers on board if there was no investment or demand portfolio that required that skillset. Banks in the UK (I guess globally too!) have needed to reign in their costs, so this was a method to reduce their headocunt and costs in line with industry benchmarks, and to then demonstrate better governance practice, in what is a highly scrutinised industry. The second was to be able to hold more accountability on their commercial partnerships, and drive value through contractual delivery.&nbsp;
In addition to these two reasons, there is also the view that I subscribe to in that many businesses invest massively on non core business activities. I know you mentioned this in your reply, around core competences, and differentiation, however I share a different view. What I mean by this is that software development is not their core business, it is Banking. Yes, software development provide capability for the bank to deliver compelling propositions to their customers, but the industry trend is to look to businesses whose core business is IT and is software development, and to leverage that capability and their broader economies of scale and transformative reach, that is too often not possible in house. You can still build capability and differentiation through partnerships. I think we also see this approach with trends for cloud computing, in that many large enterprises are now more confident about putting their services 'in the cloud' and leveraging an OPEX model. This also enables the ability to scale rapidly, without CAPITAL investment.&nbsp;
I am a proponent of this type of operating model, although I understand that there are situations when in-house does work best and I am aware of the arguments to support this. Its very much a case of building the right strategy, approach and investment plan, to deliver capability before you actually need it.
Thanks for your reply Terry&nbsp;
Best Wishes, Craig
	              
	              Hi Bo,
Thanks for your reply. I think we're on the same page - I have touched on some similar themes in my response to Terry.
Yes, relationships matter significantly.
Best Wishes, Craig
	              
	              Hi Dr. Ayoola,
I have worked with Agile, specifically Scrum. For software development (DevOps in my case) it is very good for communications. Standup meetings and sprint planning were the features that most contributed to the improvement of the communication. In turn, I found that the retrospectives, were not that great for communications as there was always little to be said. But perhaps that was just specific to our organization.
On the other hand, I also live through an attempted implementation of Scrum in IT Operations. That did not go well. There were challenges that we never manage to manage, but I found out that daily meetings and planning were causing over communication. We ended up making a mess (literally), and turned to Kanban for IT Operations, which is part of Lean software development. Kanban was better suited for us.
Best Regards,
Augusto

	              
	              Hi Bo, Hi Babatunde,
Great insights there, thank you.&nbsp;
Best Wishes, Craig.
	              
	              Hi Chris,
Thanks for the explaination:-) So many interesting uses of the same word.
br, Terry
	              
	              Hi Anthony,
We have many agile development teams where I work. We have adopted it over the last few years due to many reasons.
As the general agile principles state from Wikipedia (2014):


&nbsp;&nbsp;&nbsp; Customer satisfaction by rapid delivery of useful software
&nbsp;&nbsp;&nbsp; Welcome changing requirements, even late in development
&nbsp;&nbsp;&nbsp; Working software is delivered frequently (weeks rather than months)
&nbsp;&nbsp;&nbsp; Close, daily cooperation between business people and developers
&nbsp;&nbsp;&nbsp; Projects are built around motivated individuals, who should be trusted
&nbsp;&nbsp;&nbsp; Face-to-face conversation is the best form of communication (co-location)
&nbsp;&nbsp;&nbsp; Working software is the principal measure of progress
&nbsp;&nbsp;&nbsp; Sustainable development, able to maintain a constant pace
&nbsp;&nbsp;&nbsp; Continuous attention to technical excellence and good design
&nbsp;&nbsp;&nbsp; Simplicity—the art of maximizing the amount of work not done—is essential
&nbsp;&nbsp;&nbsp; Self-organizing teams
&nbsp;&nbsp;&nbsp; Regular adaptation to changing circumstances


Generally we embraced many of these principles but they are as stated and not a hard and fast rule.&nbsp;
Point 1. is a given really, of course you want to produce rapid useful software, isn't that the aim for any development to please the customer.
Point 2. Changing requirements is a good one. Anyone in software development knows things change and at the worst times possible. An example being is that during a recent development of a new Intranet system I was involved in, at the nth hour literally 2 weeks before going live just as we were going into the system testing phase the business changed it's mind on all of the logos and colour scheme because of an update to the overall business corporate image. We took this on and adapted this in 2 days with good negotiation.
Point 3. again yes to be solutioned quicker rather than later
Point 4. and point 6 we would have daily meetings (scrum) as well as working it what we called a 'WAR ROOM' where all project staff would all work together.
Point 5. This was easy as I don't know anyone who goes into an agile environment who isnt motivated. It's such a close group you can't help but be that way even if you were not.
Points 7 - 10 are a natural president to this way of working, Working software at progressing steps, a constant pace under tight deadlines with a focus on good design.
Points 11-12 i've already alluded to with regards to adaptability to change and the way agile teams are closer in a working environment.

These all in their own way lead to less miscommunication because you have to talk to rapidly develop anything. Because you're all in the same room and you have daily project catchup meetings it's an ever developing momentum of work.

I've found that some structured approaches have still worked for certain projects but I do like and where I work we do embrace more agile development teams .

Agile is good if done correctly!

References
Wikipedia (2014) 'Agile software development' [http://en.wikipedia.org/wiki/Agile_software_development] (accessed: 11/11/2014)
	              
	              Hi Bram,
Funny you say about that but people where I work do the same...
If a discussion is going a certain way on occasion I have heard people say ..." oh you must be a Blue because you're too cautious" .. lol&nbsp; It tends to leave the people who don't know about the subject a bit dumbfoundedas to what is going on.

I completely agree with your last point. Listening is VITAL. I could'nt underestimate this enough. You really have to take on board what people are saying and really try to put yourself into their frame of mind. I always try to take thoughts away and reflect on it if I can so it's like you said to stop it from being one directional.
thanks
Chris


	              
	              Hi Bram
Thanks for the reply.
I started to think about your reply, not as to challenge your view on a linked list, but more around a 'balanced tree'.&nbsp;I was wondering, do the letters need to be in any particular order for a binary tree?
For example, could you order the letters that are less common in a particular way, but still maintain a balanced tree?
Ive done a little research, but can't figure out whether it would work?
There is something called the 'Huffman Tree', which is some form of optimal prefix code algorithm or minimum redundancy code based on estimated probability or frequency of occurence - I wonder whether this method or philosophy could be adopted in a tree, which would potentially improve its balance, and thus make it more optimal?
Best Regards, Craig

	              
	              Hi Craig,
It would be a huge waste if we work with you and didn't learn a lot from you:-) You are actually demonstrating and practicing many of my believings. One example is your core value, collaborative relationship. Especially, after seeing so much focus on "contract negotiation" between banks and their vendors.
I agree with you that for bank in and from UK the do have their core software development in-house. And banking was prabably their most important differentiation. But I think maybe there will be more and more "non-core" software becomes core to a bank. We've switched from HSBC to ANZ Internet Bank recently, because as a small company we cannot afford the bad usability of HSBC's (and most banks') Internet banking. (this doesn't mean ANZ is good, we only started our journey with it recently.)
Another thing that's frustrating is banks tend to optimize for cost, as you said. It leaves the vendor no choice but cheating. And very often the bank ended up producing something useless for themselves or ended up in maintenance hells. One common scene is (not sure if you've seen this before) a bank hired a vendor to developed the "non-core" software, but the bank ended up in-source the key engineers (or the entire team) from the vendor because of the project delay and heavy maintenance burden.
br, Terry
	              
	              Hello Dr. Anthony,Change is a must. All humans, animals, plants and all other things living or not are victims of change. From the time we enter this world we are changing, from being in our mothers womb, from a baby to a adolescent to a teenager and to an adult. We see change surrounding us daily in all manners and forms. But what is prime is how we adopt to change.&nbsp;At the organizational level we are often greeted by stakeholders who may want to hold on to the old way or their way of doing things. Or as we have learn they may believe that based on their experience, knowledge and background that their way is the best or the better option. This may often be because of self-opinionated characteristics that exist among these individuals. As such getting them to break away from these old habits may often become a challenge for project leaders. Below lies a few tips on change management:1) Make meetings with stakeholders involving change more effective, so that there is a clear and better understanding of the changes involved and its benefits. Listen keenly and try to address effectively the concerns that they might have.2) Try to have a better understanding of the concept of resistance to change. What are the stakeholders really resisting? Is it the technical change itself or is it the social implications that comes with the technological change?3) Build better relationships with stakeholders. Try to establish and build a better relationship with stakeholders involved; develop interpersonal skill and ways of maintaining and managing relationships, also try to increase feedback from the clients.4) You can use testimonials from other executives or stakeholders who may have had similar experiences or were resistant to change but have succumb to change.5) Broaden the interest of stakeholders. Try to get them to see the bigger picture or to focus their attention on a wider aspect of things. Perhaps an example of this might be that of letting them know how similar industries in a particular environment operate, it could be in the came country or in other countries.6) Encourage more participation among stakeholders.7) Training of stakeholders involved. At some stage it may be necessary to train those involved in the relevant areas.Like the 'there is no silver built' theory there may be no one solution to the problem, there may be a series of solutions. What is important is that stakeholders and project leaders or Engineers need to work together and try to establish common grounds amongst one another. Overcoming or managing change can take some time.References:Lawrence, P. R., How to Deal With Resistance to Change [Online]. Available at: https://hbr.org/1969/01/how-to-deal-with-resistance-to-change/ar/1(Accessed on November 11, 2014)Frederick, P. B. Jr., No silver bullet [Online]. Available at:http://people.eecs.ku.edu/~saiedian/Teaching/Sp08/816/Papers/Background-Papers/no-silver-bullet.pdf (Accessed on November 11, 2014)&nbsp;
	              
	              Hi Terry.
I like the way you have analysed the details for the phone contact list through different scenarios and emulating all the basic phone functions.&nbsp;The use of elimination to identify the appropriate data structure helps narrow down the choices.
I came across a site that talked of the use of multi-index containers&nbsp;( Containers with built in indexes that allow different sorting and access semantics ) for efficient phone directories. Is this something you have come across?
Joseph
References
Quora, Available at http://www.quora.com/What-data-structures-are-used-in-phone-book-How-do-the-search-functionality-work-efficiently-in-it, Accessed 12/11/2014
Boost Multi-Index Container Libraries, (n,d), Available at http://www.boost.org/doc/libs/1_57_0/libs/multi_index/doc/index.html, Accessed12/11/2014
	              
	              Complex list-based data structures includes link-lists, unbalanced binary search trees, binomial heap, graphs, etc. From the little research I was able to do it seems that a language that supports multiple programming paradigm such as Python and C# are some of the preferred languages for coding complex lists. Lisp language is another such language, "&nbsp;link lists are one of Lisp language's major data structures and Lisp source code&nbsp;is itself made up of lists. As a result, Lisp programs can manipulate source code as a data structure, giving rise to the macro&nbsp;systems that allow programmers to create new syntax or new domain-specific languages&nbsp;embedded in Lisp" (en.wikipedia.org, n.d.).
These languages support automatic memory management, a dynamic type system, large and comprehensive library.
&nbsp;
References:
en.wikipedia.org (n.d.) Lisp (programming language) [Online]. Available from: http://en.wikipedia.org/wiki/Lisp_(programming_language) (Accessed: 11 November 2014).
en.wikipedia.org (n.d.) Object-oriented programming [Online]. Available from: http://en.wikipedia.org/wiki/Object-oriented_programming (Accessed: 11 November 2014).

	              
	              Hi Dr. Anthony,
The Coordinate Format (COO) is the most intuitive storage format for a sparse matrix of those listed in the question. According to (Stanimirovic, I. and Tasic, M., 2009) a list of coordinates are stored with the associated nonzero values. No specific structure is required of the matrix and its very flexible. Utility subroutines can be used to transform from COO to the other storage formats. This makes COO the easiest to implement and the most efficient of the four examined.

References:

www.alglib.net (n.d.) Sparse matrices [Online]. Available from: http://www.alglib.net/matrixops/sparse.php (Accessed: 11 November 2014).
Stanimirovic, I. and Tasic, M. (2009) Performance Comparison of Storage Formats for Sparse Matrices [Online]. Available from: http://facta.junis.ni.ac.rs/mai/mai24/fumi-24_39_51.pdf (Accessed: 11 November 2014).

Regards,
Ricardo
	              
	              Hi Bo, Hi Craig

Thanks for your reply,

One last observation that I made was that most of the work produced by these teams was immediately being tested by the team itself or by dedicated testers. So, the solution was continuously being improved and proven out.

I realize that the items discussed may not fully satisfy a complete agile development shop. However, I believe they all are tenets, or rather the foundation, of an organization that is willing to be Agile in some shape or form. This is the beginning, of an organization that is willing to accept change to make the entire organization successful.

br, Babatunde
	              
	              Hi Colleagues,
I just wanted to weigh in on the discussion and first of all say that you all teach me new things every week. The sharing of expertise and industry experience by all the participants in the discussion forum, especially the super active colleagues, provide much knowledge in the discussions that take place. Please do not under estimate the value this means of collaboration offers.
In regards to banking industry and most corporations in the Caribbean islands subscribe to the sourcing models Craig describe above. From my knowledge most of banking systems in place are establish systems from North America, where software development companies have established their products as proven systems to handle the banking requirements we may have in the Islands. Most of these vendor based products are modular and can be customize to provide some amount of uniqueness, but the core systems are still the same. Internal IT departments focus on the customization and security framework of the software. This approach to core business applications allow organizations to avoid costly software development that would've added further burden to the small economies by increasing the cost of which services are offered.&nbsp;
I know for sure that a few organizations are slowly shifting the level of investment in technology from CAPEX to more OPEX by using cloud based services. This adds better resilience and scalability for their operations.
Regards,
Ricardo
	              
	              Hi Dr. Anthony,
As most of my colleagues has stated a flat organization structure with open communication and strong interactive leadership will definately be ideal for avoiding miscommunication and communication breakdown during software development projects. In a flat structure, well the one I've worked with, each unit of the organization and to a further extent the employees are empowered and motivated for development. Based on the four common type of organization cultures as described by (www.artsfwd.org, 2013), the market "oriented culture are result oriented, with a focus on competition, achievement and getting the job done". As their is no right or wrong culture, every organization culture is shape by the social environment, the background of the employees and the leadership of the organization.
Miscommunications will never totally be avoided but in a closely flat market culture these challenges can be addressed quickly from the constant communication taking place.

Reference:
www.artsfwd.org (2013) 4 Types of Organization Culture [Online]. Available from: http://artsfwd.org/4-types-org-culture/ (Accessed: 11 November 2014).

	              
	              Hi Tanisha,


Nice read, I think the reasons whereby the above cultures of management will not align very well is a central sameness in a way of regarding situations. In my experience, preconceptions amongst the cultures grow over time as each group views their personal worth to the overall organization. Engineering team believes that without their brain trust, there would be no product. Sales team thinks it is bringing in all the money... Executives think that their management savvy is responsible for the company’s growth.

These built in partiality that prevents objective consideration of an issue or situation only grow in strength over time, and it calls for a very strong management team at least in each group who will direct all groups toward a focused learning program that benefits the entire organization in line with the direction that the senior executive team want the organization pointed towards.


br, Babatunde
	              
	              Hello Class,
&nbsp;
As our usual way of rounding up the week's activity, please provide a short summary of the key 'Computer Structures' lessons learnt in week 10, from your perspective.
&nbsp;
Anthony

	              
	              Hi Anthony,


Resistance to change is not a “Problem” in organizations … it is a key feature of human behavior. And I would suggest that if you are planning making a change …either personally or in business … and you DON’T meet resistance, the step you are planning is not big enough.

Why?

….When you propose a change in a work routine you are asking people to do something different for the promise of future gain. You are asking them to change a habitual behavior.

Even if the results they are currently getting are less than optimal … the current systems are a “known evil” and your changes plunge them into an “unknown future”. Fear and resistance are normal in this situation. 

The question is…How do leaders enroll people in Change?

In my suggestion, it is through getting out in front of your people and holding the Vision of What’s In It for Them.

- Voluminous and unflawed communication.

- If there is a change to be made the Leader shows they have made it first.

- Lead by focusing on the desired results and asking questions so your people are immediately involved in designing the change…..And if you don’t meet resistance … pick a wider change topic.


Best Regards, Babatunde
	              
	              Hi Martins, 
I am glad it helped you.
br, Babatunde
	              
	              Hi Anthony, 

I’d say Lisp is still in use, but you have to know where to look. You don't always see it, but Lisp is all around you. People who use LISP don't tend to shout too loudly about it but there are a handful of examples of a few high-profile startups having used it to great effect over the last 20 years.

An example is Yahoo! Store which includes a WYSIWYG editor for editing your online store through a standard web browser. The editor is written in Common Lisp.

Br, Babatunde

References

LispWorks Common Lisp - Myths and Legends [online] Available from: http://www.lispworks.com/products/myths_and_legends.html (Accessed 12 November 2014)


Wikipedia The Free Encyclopedia Clojure [online] Available from: http://en.wikipedia.org/wiki/Clojure (Accessed 12 November 2014)
	              
	              Hi Babatunde
I am an advaocate of best practice however, I've also come to realise that tweeking best practice processes and methodologies is an effective way of modeling the way you work. Quite frankly, some things just arent required in some cases.
In my opinion, its important to define your business processes around your outcome needs, and not be constrained to a particular methodology just because it is deemed to be best practice. It just may not be appropriate or suitable in certain businesses.
Many businesses go on to develop their own Programme Delivery Management Frameworks, that take the best or most appropriate uses of best practice that are suitable for their needs, and bespoke the model to accommodate other aspects more suited to their business.
Best Wishes, Craig


	              
	              Hi Bo and Craig,
It will be interesting to see new players like Alibaba to takeover banks in some years. All they need is the permission of doing the banking business.
What do you think?
br, Terry
	              
	              Hi Anthony,
This week was really tough. I thought the DQ on communications was a good way to understand everyone perspectives on potential breakdowns, particularly from a software engineering perspective, given my own is related to service and operations. 
The data structure DQ was really interesting, albeit tough to figure out. I am struggling without the practical side of learning, and seeing how this actually works in practice – I am a visual learner, so ‘demonstrations’ and ‘prototyping’ are good examples of what works particularly well for me, especially when we get into the real technical detail, which for me, this is. 
I found the Design Patterns tough. Not in relation to the concept, I get that – in that they serve to provide practical solutions to recurring problems – my challenge was the comparison between design patterns in other fields, and those within software engineering. I could identify and interpret design patterns in other fields, but not knowing too much about any design patterns in software engineering, meant it was difficult to then demonstrate a comparison between both i.e. compare an apple with another fruit, but not knowing what the other fruit actually are. Hope that makes sense.
Tough week – but I am well up for the challenge!
Best Wishes, Craig

	              
	              Hi Joseph,
Some people said "if you have a C++ problem and you use Boost to solve it, now you have 2 problems."
The Boost.MultiIndex container is highly optimized for performance (by using template meta-programming). In the case of a phone contact list, I don't think the performance is an issue as long as using the right solution.
It seems a MultiIndex container with ordered indice can solve at least all the requirement I've listed. But I don't think it's worthe it to own the complexity.
br, Terry
	              
	              Dear Dr. Ayoola,
This is an extremely busy week for me. A product I've been developing for a couple of months had the first official release. We do continuous delivery. So "releasing" actually just means "announcing" for us. As you may expect, there're several issues after the release and we have to deal with some of them asap. Besides that, I've organized 2 BBQ parties with 30 people this week. Many friends came to Singapore from every corner in the world this week for the Agile Singapore conference. And that's still not all I did!
The learning part has been quite interesting and fruitful.
To finish the first DQ initial post, I bought the Bridging The Communication Gap pdf book and did a brief reading. Also revisited some books I've read before by the same and different authors. I've learned the personality colours from other colleagues.
The phone contact list DQ, as I mentioned in my post, has been a good exercise for me to find solution by first clarifying the requirement. In this case, where there's no one to confirm the requirement, I've made my assumptions explicit. I practiced implementing a trie using Python. And it surprised me that I created a full functioning trie with about 20 lines of code and manage to put all my code in the post.
The sparse matrix question got me study the topic and playing with Scipy for one whole evening. It is quite challenging for me. But my understanding is I'm going to need it quite a lot for my studies later (big data).
Question: does anybody know the nice lady in the two pictures of one of my post demonstrating colorizing?
br, Terry
	              
	              Hi Anthony
I have worked with the Agile software techniques but not strictly by the rule book. When I used to work with an IT firm, the norm was to have daily touch base with the rest of the developers working on various components of the application, end of day checking in of code and ongoing testing by a different team who would also be in the loop as far as completed modules are concerned.
This approach for sure helped in being able to quickly identify issues and establish any needed workarounds and points of clarification. However I'd say that for the most part, the customer would not be part of the day to day briefs and only came in to confirm milestones achieved based on the agreed SRS signed off earlier at project start. Today I see for sure that using the agile approach would work very well but with a clear way to limit scope creep on the project
Joseph

	              
	              Hi Augusto.
I am still trying to come to terms with point number 2 of changing requirements. That to me can be a major challenge and I am interested in knowing how as the client, you would structure that agreement with the vendor to allow for that flexibility. Most projects i have done, time and budget are major &nbsp;constraints and vendors would only be too quick to point out any scope creep that shows up that was not part of the original specifications.
So does the initial contract explicitly provide for this sort of variation without necessarily impacting budget. I imagine if I was a vendor to agree to such a requirement would mean that the client is paying a premium. From what I know about Agile, most of the requirements are not necessarily defined upfront with biinding detail.
Joseph

	              
	              hi Ramin,
I enjoyed reading your work this week.
I was specifically reading about TRIE and where it would be best used. I didn't think to use it for this example of a contacts list as I understood it to be more complicated than it actually was. But reading how you explained it I can understand how you came to your selection.
Do you think people adopt this method because of the way it uses search capabilities and the matching string capabilities or is it know to be more / less stable or quicker than other solutions?
ta
Chris



	              
	              Hi Belinda,
I am not sure I would wholly agree with you on the point about domain knowledge being absolutely essential and here's why.&nbsp;
A contract programmer could in his working realm work on a number of diverse projects covering different domains ranging from financial accounting systems, medical management, pharmaceuticals, engineering type to much more complicated instrument control systems for manufacturing concerns. They are of course not subject matters in this area but are still able to hack it due to the ability to conceptualize and break down a requirement into its programmable component pieces.
So domain knowledge would be desirable but because of the diversity of projects that a programmer undertakes, the ability to conceptualize and break down an idea into it's parts and know what data structures and technologies would be appropriate for the problem would have a better impact.
	              
	              Hi Anthony,

Much like all of my other student colleagues I did not immediately recognize the term "sparse matrix data structure" and certainly can't recall any time I've come across it in my work to date.
Looking into it a little further to try to answer the question the characteristics could be defined as:

a matrix in which most of the elements are zero.
if most of the elements are nonzero, then the matrix is considered dense
are loosely coupled
use specialized algorithms and data structures
typically stored as a two-dimensional array

I believe Google Earth and Google Maps use something called SuiteSparse which is a sparse matrix software suite. I'm not sure directly if it can potentially use none sparse built software without knowing the full ins and out of how Google Maps works but i'd imagine it could adopt other data structures.

References
Wikipedia, A (2014) 'Sparse Matrix' [http://en.wikipedia.org/wiki/Sparse_matrix] (accessed: 11/11/2014)
Wikipedia, B (2014) 'List of numerical libraries' [http://en.wikipedia.org/wiki/List_of_numerical_libraries] (accessed: 11/11/2014)
	              
	              Hi Augusto. I am still trying to come to terms with point number 2 of changing requirements. That to me can be a major challenge and I am interested in knowing how as the client, you would structure that agreement with the vendor to allow for that flexibility. Most projects i have done, time and budget are major &nbsp;constraints and vendors would only be too quick to point out any scope creep that shows up that was not part of the original specifications. So does the initial contract explicitly provide for this sort of variation without necessarily impacting budget. I imagine if I was a vendor to agree to such a requirement would mean that the client is paying a premium. From what I know about Agile, most of the requirements are not necessarily defined upfront with biinding detail. Joseph  
	              
	              Hi Everyone

I'll be honest and say I found it hard this week. Not from learning perspective but from a personal perspective.
I had to first get though a few days of sickness which didn't start the week very well but managed to spend the weekend cramming and got the 2 DQ's done in time.
Then I've had a weird week where I've just felt mentally exhausted.&nbsp; I've booked a weeks holiday for the end of this month to coincide with the final week of this module, I think I need to switch off for a few days.
I did however really enjoy doing the assignment on Communications and enjoyed reading everyone else's reports. There can't be enough discussions around this field as I feel it's one of the key topics to any projects success.
The Individual assignment was a little tough, not from the subject of Design Patterns but more comparing them to other professional fields. The concepts were fine but understanding how they fit in with other areas was like chalk and cheese.
Overall a challenging week
Now i'm off to celebrate with the wife by going to the cinema and watching this new "Interstellar" movie.
Ta
Chris


	              
	              Hi Augusto,

I liked your work this week especially when you referred to "personality types".
I also agree with this aspect as sometimes it can be overlooked and just expected of people to be like the machines they are working on.
What I'd add to it is the understanding of human life and psychology. We tend to forget during a project that the members of staff around us have families and lives outside of work and this can also be a factor in communication breakdowns within work. It may not be a lack of technical skills or missunderstanding as such but more of an underlying personal or social issue. You're mind can be somewhere else other than on work. I know from my own experience that sometimes in meetings I've left thinking I can't remember anything that was said because I sat there thinking about my home life and didnt realise it.
I agree with the summary of having differing personality types in theory would be great and on large projects where many departments and teams are involved it's more likely to achieve this. i'd like to see a study of the success factors for software projects correlated against that teams personality types.
EG 60% of successful projects had multiple personality types present while 80% of all failed projects had little or no diverse personality types.
( stats made up for examples) but it would be interesting to see if any studies have been done on this aspect.

ta
Chris

	              
	              Hi Anthony,     Sparse matrices can be used in arithmetic operations: they support addition, subtraction, multiplication, division, and matrix power.      bsr_matrix - Block Sparse Row matrix coo_matrix - A sparse matrix in COOrdinate format. csr_matrix - Compressed Sparse Row matrix dia_matrix - Sparse matrix with DIAgonal storage       Compressed Sparse Row Format (CSR)      Advantages of the CSR format   Efficient arithmetic operations CSR + CSR, CSR * CSR, etc. efficient row slicing fast matrix vector products Disadvantages of the CSR format slow column slicing operations (consider CSC) changes to the sparsity structure are expensive (consider LIL or DOK)    Coordinate Format (COO)    Advantages of the COO format   facilitates fast conversion among sparse formats permits duplicate entries very fast conversion to and from CSR/CSC formats Disadvantages of the COO format does not directly support:      arithmetic operations slicing      Intended Usage COO is a fast format for constructing sparse matrices Once a matrix has been constructed, convert to CSR or CSC format for fast arithmetic and matrix vector operations By default when converting to CSR or CSC format, duplicate (i,j) entries will be summed together. This facilitates efficient construction of finite element matrices.      Diagonal Format (DIA)   The Block Compressed Row (BSR) format is very similar to the Compressed Sparse Row (CSR) format. BSR is appropriate for sparse matrices with dense sub matrices. Block matrices often arise in vector-valued finite element discretizations. In such cases, BSR is considerably more efficient than CSR for many sparse arithmetic operations.   Conclusion   For the most efficient, I’d choose the Coordinate Format (COO). Since it facilitates fast conversion among other sparce formats and it permits duplicate entries which others do not.       Reference:   Compressed Sparse Row Format (CSR) [online] Available from: http://students.mimuw.edu.pl/~pbechler/scipy_doc/generated/scipy.sparse.csr_matrix.html#scipy.sparse.csr_matrix (Accessed 12 November 2014)    Coordinate Format (COO) [online] Available from: http://students.mimuw.edu.pl/~pbechler/scipy_doc/generated/scipy.sparse.coo_matrix.html#scipy.sparse.coo_matrix (Accessed 12 November 2014)   Diagonal Format (DIA) [online] Available from: http://students.mimuw.edu.pl/~pbechler/scipy_doc/generated/scipy.sparse.dia_matrix.html#scipy.sparse.dia_matrix (Accessed 12 November 2014)   Block Sparse Row format (BSR) [online] Available from: http://students.mimuw.edu.pl/~pbechler/scipy_doc/generated/scipy.sparse.bsr_matrix.html (Accessed 12 November 2014)
	              
	              Hi Anthony,

Resistance to chance is a massive thing where I work at the NHS.
Literally for decades certain members of staff have really got stuck in their ways regarding some practices and a large percentage are fearful of change.
I can't believe I'm going to say this but where I have found this helps is actually through using outsourcing or consultants and 3rd parties to do certain developments. This is a major contentious thing as we all in the NHS really don't want outsourcing of projects but it does help when it comes to this situation.
Reason being is that for some reason senior management tend to be easier and more agreeable to change with outsiders rather then inside knowledge.
Communication tends to be better managed because they are directly paying for a resource rather than relying on internally managed resource.
The only thing I can reason it to acting this way is because of internal hierarchies. Much like a family the parents are always right even when they are wrong.
I find in business if a junior or middle manager has opinions and ideas communicated to key stakeholders they tend to look down on them( not always but often).
Whereas outsiders tend to get listened too even if its the same idea.
So my resolution is that if you have a culture at work for this happening then consider outsourcing certain pieces of work and then bring the results in house to support going forward.
ta
Chris


	              
	              Hi Craig,

Thanks for your reply… I’m interested in hearing more about your observations, and also from those of us who are currently in organizations that think agile is fragile and may be challenged with slowly introducing Agile concepts into resisting organization, so we can expand the thinking into a collaborative effort in defining if Agile truly is visible.

Thanks Craig

br, Babatunde
	              
	              Hi Terry
Yes, I think the banking market is primed for disruption. Over the last 10 years, here in the UK we have seen the emergence of 'traditional' supermarkets (Tesco, Sainsburys, Marks &amp; Spencer etc) move into new markets such as banking. One of the attractions for consumers is moving away from the 'traditional' banking experience, which has (quite rightly!) faced critical reviews over the same period.
Consumers seem comfortable because these new emerging and disruptive players are already well known and trusted brands. I don't know much about Alibaba, apart from the fact they're gigantic in the e-commerce space and predominately operate in China. Given they operate through online digital channels, I can only assume 'payments' is a critical aspect to their business, so very familiar with the transfer of monies.
I assume they're well respected and have a great brand (given their size, and growth over the last 10 years or so) - I seem to recall their IPO launch recently, and perhaps they even have the scale to challenge Apple in terms of company value over the next 10 years?&nbsp;
Unless there are some genuine monopoly issues, I see no immediate reason why they can't disrupt the market.
"Alibababank" or Bankalibaba" - not too sure which one I prefer :-)
Best Wishes, Craig
	              
	              Hi
I must admit, that I'm kind of scared of companies, like Alibaba, take over. I think it's because of my general "fear" of everything being owned and controlled by few - there are pros and cons.
But yes, it will be interesting. Perhaps this will also make the banks better when they, if they can, take up the competition.
Br. Bo
	              
	              Hi again
I'm not sure about that. The government and social systems that I have seen have actually just had two fields, the just need a certain length, and people who register into the systems have to know the rules. Examplex of how my we do:
Fname(Paul Taylor), Ename(Johnson)
Fname(Paul-Taylor), Ename(Johnson)
Fname(Paul), Ename(Taylor-Johnson)
Though I'm not sure what to do with this example:&nbsp;
Hubert Blaine Wolfeschlegelsteinhausenbergerdorff.&nbsp;
http://en.wikipedia.org/wiki/Wolfe+585,_Senior

Br. Bo
	              
	              Hi Anthony
In a bullet list, this is what I learned about this week:

The importance of good communications in the systems analysis phase, but also all other phases.
Also the importance of areas like quality assurance and documentation, which often is forgotten.
The common data structures, but also briefly about the scarce matrix data structure.
Design patterns, i.e. reusable ways of solving problems is in all professions.

But I must admit, especially the lack of programming knowledge still disrupts me, so it has been especially hard regarding data structures and design patterns.
And as usual is interesting to follow discussions and hear about experiences from around the world and different branches and methods&nbsp;(e.g. waterfall and agile).

Best wishes
Bo W. Mogensen

	              
	              Hi Dr Anthony,
The pressure was at an all time high for me this week. I learnt about:
1. data structures- which i found a bit tough
2. communication problems &nbsp;in the systems analysi phase- it was great to get to the root of some of the communication issues we experience in our projects and with all the input from my colleagues I definitely learnt a lot and read about some practical, effective solutions.
3. design patterns- the theory seemed straigh forward until i had to find a profession and apply the SE patterns to that profession
4. the importance of quality assurance- as someone who is in QA, i loved this and I hope that QA can one day garner some recognition and importance in the African software develpoment landscape.
A big thank you to my colleagues for their input and feedback on my posts.
kind regards
Belinda
	              
	              Hi Terry,

It is sad to treat people as resources and can be alienating:
"But I think referring to a person as a resource is problematic in two ways:

People are more than their monetary value. (See “R-E-S-P-E-C-T, find out what it means to IT, say TechRepublic members.”)
IT workers are fiercely independent. They do understand their monetary value, but view it as their value, not yours. You may find they’re perfectly capable of reallocating their own resource—i.e., themselves—right out of your company and over to a competitor.

http://www.techrepublic.com/article/management-lingo-must-go-because-people-are-not-resources/&nbsp;
Best regards,Belinda
	              
	              Thanks Cris and Belinda,
Great replies to different instances of this tread.
Leaves me nothing to add.
Greetings from Bram
	              
	              Hello Craig,
Mr. Hufman makes interesting code architectures...
As you have shown there are ways to balance a tree but as a side effect they introduce complexity. Another example would be to introduce variables for the first nodes to avoid the alphabetical disbalance from the beginning. But as a result you will have to re-balance the tree every time after a delete er new entry. With this you would probably gain more speed in search-ability but loose the andvantage of being able to perform quick changes.
Greetings from Bram
	              
	              hi babatunde,&nbsp;
yeah it did.
br, martins
	              
	              hi Terry,&nbsp;
If the phone contact sounds simple how is it complex? in what way- i dont get can you explain?

br. martins
	              
	              hi everyone,
It will surely be bankAlibaba! funny! The post is interesting and i bet it will be a very good competition for those guys who own the banking sector because companies like Alibaba and co will not leave any stone unturn if they have to go into banking. Like in Nigeria we have smaller online e-commerce giants like 'jumai' and 'konga' these two will kick out anyone on their way if they have to get into banking.

br. martins&nbsp;
	              
	              Dear All,
As Ben Weatherall’s opinion in his writing “Overcoming Resistance to Change by SCM”, the SCM (Software Configuration Management) can be one of the benefits modus to deal with this issue. That SCM defines a mechanism to deal with different technical difficulties of a project plan. In a software organization, effective implementation of software configuration management can improve productivity by increased coordination among the programmers in a team. SCM helps to eliminate the confusion often caused by miscommunication among team members. The SCM system controls the basic components such as software objects, program code, test data, test output, design documents, and user manuals.  
The SCM system has the following advantages: 

Reduced redundant work.
Effective management of simultaneous updates.
Avoids configuration-related problems.
Facilitates team coordination.
Helps in building management; managing tools used in builds.
Defect tracking: It ensures that every defect has traceability back to its source.

&nbsp;
Reference
AccuRev (2009); Excerpts from "Overcoming Resistance to Change by SCM" (By Ben Weatherall); Available on: (http://accurev.com/blog/2009/11/17/overcoming-resistance-to-change-by-scm/) (Accessed on: 12-Nov-2014)
BR,
KingTan Yu

	              
	              Dear All,
Organizational culture is something that is created by the leadership of the organization and can become ingrained into the core fabric of the way things are communicated and business is done on a day-to-day basis.Positive work cultures can influence productivity and a healthy work environment; however a culture that is less than positive can have the opposite effect and can actually stifle worker productivity and have an impact on the bottom line.Culture When there are communication breakdowns within an organization, it can lead to conflict in the workplace.&nbsp; This conflict is often caused from lack of communication or distorted or inaccurate information.Often these breakdowns are caused from ineffective communication channels within an organization.&nbsp; When information is not shared, or not shared in a structured way, it results in those who need the information to fill in the blanks.Organizational leadership needs to be cognizant of what information needs to be shared, when it should be shared and what process should be used to share information. When employees don’t have all the information, they tend to fill in the blanks and often are not correct in their assumptions. Being proactive in communication minimizes the productivity gaps when employees are trying to figure it out.
Reference
Eight to Late (n.d.); Patterns of miscommunication in organisations; Availabe on: (http://eight2late.wordpress.com/2013/11/01/patterns-of-miscommunication-in-organisations/) (Accessed on: 12-Nov-2014)
BR,
KingTan Yu
	              
	              Hi Craig,
That's my point. Companies like Alibaba look at themselves as software &amp; Internet companies. In the long term view, they have huge advantage over traditional banks. Although they probably still don't know anything about banking more than transfering a lot of monies.
Alibaba did outsource their software development in its earlier years, but in an extremely collaborative way. I cannot imagine they oursource their software development today, because they are one of the best software companies now.
br, Terry
	              
	              Hi Bo,
I don't like to see that either. I want to see more diversity.
But that's not my point. My point is software companies are taking over banks.
br, Terry
	              
	              Dear All,
&nbsp;
There are seven available sparse matrix types:

csc_matrix: Compressed Sparse Column format
csr_matrix: Compressed Sparse Row format
bsr_matrix: Block Sparse Row format
lil_matrix: List of Lists format
dok_matrix: Dictionary of Keys format
coo_matrix: COOrdinate format (aka IJV, triplet format)
dia_matrix: DIAgonal format

To construct a matrix efficiently, use either dok_matrix or lil_matrix. The lil_matrix class supports basic slicing and fancy indexing with a similar syntax to NumPy arrays. The COO format may also be used to efficiently construct matrices.
To perform manipulations such as multiplication or inversion, first convert the matrix to either CSC or CSR format. The lil_matrix format is row-based, so conversion to CSR is efficient, whereas conversion to CSC is less so.
All conversions among the CSR, CSC, and COO formats are efficient, linear-time operations. The CSR format is especially suitable for fast matrix vector products.
CSR column indices are not necessarily sorted. Likewise for CSC row indices. Use the .sorted_indices() and .sort_indices() methods when sorted indices are required (e.g. when passing data to other libraries).
BR,
KingTan Yu
References
Jack Dongarra (1995); Survey of Sparse Matrix Storage Formats; Available on :(http://netlib.org/linalg/html_templates/node90.html) (Accessed on: 12-Nov-2014)
&nbsp;
&nbsp;
Sparse matrices (2014); Sparse matrices; Available on: (http://www.alglib.net/matrixops/sparse.php) (Accessed on: 12-Nov-2014)
&nbsp;

	              
	              Hi Belinda,
Thanks for the shared article:-)
br, Terry
	              
	              Hi Martins,
What phone are you using? Just play with the contact list and think about how to implement each feature:-)
The different ways of indexing (including partial prefix indexing) make it not a trivial problem. Of course that's only the behavior of the software and can be implemented in different ways. But a right data structure will may the algorithm to implement behavior much easier.
But that's only my "speculation". I'm always wrong in predicting software design. Perhaps it turns out to be very easy. We don't know as long as we don't really implement it.
br, Terry
	              
	              Yeah! Software is eating the world:-)
	              
	              Hi Dr Ayoola,
I am not surprised that resistance is the primary reason for miscommunication and subsequent failure. I do not think that this is only related to software development, this can easily be applied for everything!
In my experience I have seen two key components for this resistance: first is that the things that need to be changed often do not lose importance of management does not change their importance or priority. I have heard things like: “we must improve handling tickets [this way] this to improve our services, but we must focus on keeping the current tickets flowing”. This implies that the change is as important as the thing to be change, it’s a conflict that people don’t easily resolve. Second, is that changes often lack the clarity and detail needed for workers to understand the change. I have heard things such as this: “we decided to remove Active Directory because we want to reduce our financial commitment with Microsoft”. This is at best confusing to a Windows engineer because Active Directory is actually free (included with any copy of windows server). It seems I am not the only one seeing this as Marks (2010) point out too.
I think to improve resistance to change we need to improve the communication (chicken / egg kind of issue here), we need to communicate better the change. But regarding my two points above: first, management must reduce the support given by themselves and workers to the “old things”, and focus on the “new things”. Second, management (or stakeholders) must articulate in detail the plans and the reasons why the change is needed and why is being done (they are not necessarily the same thing).
Best Regards,
Augusto
&nbsp;
References
Marks, M.L. (2010) 'In With the New',&nbsp;The Wall Street Journal&nbsp;May 10, pp. 12 November 2014.

	              
	              Hi Christopher,
I am not surprised that change is more easily managed via outsourcing or external contractors. If people inside do not want to change, external factors are the only ones that really can make things change. I have seen that in many companies, private or public. The culture of the company plays an important role, if must have a very open culture to be able to have people that are comfortable with change, and even in such cultures you will always find the team or person that resist more than others.
Best Regards,
Augusto
	              
	              I can't recall her name! I have seen her in some photshop tutorials for imaging processing, and I know she was a playboy girl. But that's all I can remember right now.
	              
	              
Hi Dr Ayoola,
All in all, I think this was the toughest week, the design patterns and the data structures required a lot of time from my part to research and understand it properly. The discussions were very interesting! But I have to be honest: this week I have been a little absent in my mind as one direct college (and friend) suddenly passed away so I have been having difficulties focusing and concentrating (plus extra workload a top of it).
I spent quite some time reading the e-book Design Patterns: Elements of Reusable Object-Oriented Software, I first found a sample in google scholar and I just could not help but read more and more. It is really interesting and it was a foundational element for my HIA. That helped me understand the design patterns and realize how little I know in that matter. I even use some of those designs for my little programming without knowing!
It was a good challenge, and it was rewarding at the end (as usual).
Best Regards,
Augusto.
Ps: aww nobody noticed my LOST references :P


	              
	              Hi Anthony.
This week one of the key lessons learnt was the benefit of communication and the agile approach. Part of the gap I realised that I was experiencing in a current project was the inadequate communication early in the project to key contributors. After the requirements specification was drawn out, a round table face to face discussion with key contributors should have been done, instead I sent out an email which was apparently not read and the result was that after a few months into development with the product undergoing testing, a number of missing functionality were pointed out by my project contributors (who did not read the email.)
So in communications, face time is important.
The topic on data structures was also quite informative although tough to really get the right understanding of the most appropriate solution for the phone book. However, it opened up further understanding on the basic data structures and their use.
Joseph
	              
	              Thanks Babatunde,
I agree with your points, it calls for a lot of growth over time as change does not come over night. I think training and exposure is the key as well. All Stakeholders and Engineers need to be exposed to the appropriate levels of training, as growth can be achieved through knowledge and experience. Thus, they can be in a better position to function, interact and deal with conflicts and differences that may exist amongst one another. Exposure too can help if those involved can see how others in similar industries operate. But once again like you said, all this takes growth and time.
Tanisha
	              
	              Hello Dr. Anthony,I believe that Java may be a good suitable application for coding applications using complex list-based data structures. Java allows the easy creation of linked list and in addition to this Java comes predefined link-list structures that are reusable. Java Object-Oriented structure is also an added advantage for this type of data structure.Tanisha
	              
	              Hi Chris
Yes Trie an efficient information retrieval data structure. Using trie, search complexities can be brought to optimal limit (key length). However&nbsp;is downside is its&nbsp;larger storage requirements.
robin
	              
	              Hi Anthony
&nbsp;&nbsp;&nbsp; I used Lisp myself in the 80's to do my own thesis! I believe it is now managed by &nbsp;http://clojure.org/&nbsp; Use of Lisp continues in specific areas od computer science such as game development and AI. Also example provided by Babatunde (Yahoo store).
robin
	              
	              Hard week for me, a lot of catch up. Traveled to Asia for work, time zones challenged big time...
Learned different prospective of importance of communications, Data structures deep dive and design patterns

robin
	              
	              Hi Anthony

In a bullet list, this is what I learned about this week:


communication and the phases of software development
means to alleviate communication breakdowns during system analysis phase of software development and other areas
Miscommunications resulting from resistance to change
Communications and Agile Software Development Techniques
data structures and its different types
Commercial Software Applications that use Sparse Matrix Data Structures


Though this week was though but I survived and I enjoyed discussions and shared knowledge with my colleagues

Best wishes, Babatunde
	              
	              Hi Dr. Anthony,
&nbsp;
I have not been exposed to agile software development techniques, because I have not worked in the field of software engineering and like indicated only recently I was appointed a programing job. However, in my field thus far I was not given any opportunity to develop any software or place on any software development projects.
&nbsp;
However I have learnt that one of the key benefits of agile software development is that it places lots of focus on communicating with stakeholders as prototypes are developed and constantly tested and improved upon. With agile software development, a simple prototype is developed, tested and presented to stakeholders for feedback. Based on feedback and improvement on the prototype is done, presented again to stakeholders for feedback and the process may continue in a similar manner. It is during this time and even before there is constant communication. Therefore because of the emphasis on communication and testing involved in this technique, it is undoubtedly so that this method will reduce the likelihood of miscommunication among stakeholders during the software development process.
&nbsp;
Communication is also increased among developers as they are working in closely netted environments, sharing ideas more freely.
&nbsp;
Tanisha
&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Brookshear, J.G. (2011)&nbsp;Computer science: An overview.&nbsp;11th ed. Pages 308. &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Boston: Pearson Education / Addison-Wesley.
&nbsp;

	              
	              Hi Anthony
Its a great question.
In my experience, and some of my colleagues have alluded to this also, change is perhaps seen as a concern because of uncertainty relative to the change itself.
In my opinion, the levels of resistence become higher in certain situations when employees fear for their jobs.
One of the key attributes to any successful business, is the ability to keep driving change, and this comes back to everone needing to embrace such cultures whereby change is the norm. One of the ways to do this is to build a company whereby everyone is bought into the same common goals, through effective communications across the entire organisation, at all levels, and recognise that change creates success, opportunity and longevity.
However, and unfortunately, there are of course going to be some casualties along the way. The key for employees who want to be working in very successful environments is to not stand still themselves, and in addition to any coaching, leadership or direction provided, to also take personal responsibility for grasping every and any oppportunity that is presented to them.&nbsp;
Everyone has it within their own sphere of capability to define their own future, and to 'carpe diem'.
Best Wishes, Craig


	              
	              Hi Babatunde
Sure, I'll touch on a couple of examples;
- My last organsation developed a combination of delivery frameworks which took the bits that were most appropriate from methods such as Agile (dynamic, flexibility, and very regular face to face communications) and Waterfall (clearer structure of output, higher level of documentation quality, vendor engagement). It was about extending the better bits from two specific methods. In addition, only some parts of Prince 2 methodologies where adopted, depending on the size of the investment portfolio plan. Smaller stuff tended to be much more flexible and dynamic, larger stuff tended to adopt a more structured approach with the ability the change. The key here was to have flexibility and not have a method dicate the best way to deliver.
- Another example is stuff I have done myself in the employment of service processes such as ITIL. I've used them to guide the creation of business processes within my organisation, but in some cases enhanced them (do more) or put them aside (not done at all).
Best Wishes, Craig
&nbsp;
	              
	              Thanks Bram
Best Wishes, Craig
	              
	              Hi Terry
Just wondering if this is a 'trick' question - is the image completely computer generated?
Best Wishes, Craig
	              
	              Hi Craig,

Let say it's not a trick question at all and completely computer science related.

Use the others' reply as clue...

br, Terry
	              
	              Hi Craig,
Thanks for the new phrase.
br, Terry
	              
	              Hi Anthony
I've never encountered these before.
In addition to most of my colleagues comments about sparse matrix data structures,&nbsp;I understand that some very large sparse matrices are unfeasible for data manipulations using standard dense matrix algorithms, and that specialised algorithms are needed for taking advantage of the sparse nature of the data structure.
I have to be honest here Anthony, without doing extensive reading on this, alot of it is going straight over my head - which as it happens, is actually beginning to feel like a sparse matrix data structure :-)
Best Wishes, Craig
	              
	              The week’s discussions dealt with reasons for miscommunications and the breakdown of communications during systems/software development.&nbsp; We talked about the relevance of Agile software development techniques, such as Scrum and XP, particularly for minimizing miscommunications.&nbsp;
The discussions also dealt with influence of organizational culture, and resistance to change, on the quality of communications between stakeholders, particularly for large projects.
&nbsp;
Good discussions!
&nbsp;
&nbsp;Anthony

	              
	              The thread featured discussions on key data structures, and considered how these were used commercially, as well as the best languages for coding such structures.&nbsp;
As a useful exemplar we considered Sparse matrices, &nbsp;and compared these with list data structures, as well as having discussions of the relative merits of common storage formats used for sparse matrices, such as Compressed Sparse Row Format (CSR), Coordinate Format (COO), Diagonal Format (DIA) and Block Sparse Row format (BSR).
The posts also reviewed data structures that faciltated searches.
&nbsp;
Anthony

	              
	              Hello Anthony,
It was a refreshing week spanning over data structures, OS and design patterns. It felt good learning new and enhanced concepts than the ones I had learnt a decade ago. Data structures, even today, is one of my favourite subjects. Even after all these years, the one thing that hasn't changed is the importance of communication in the entire lifespan of software development and the impact of communication breaks. Even going forward, the means of communication may change but I'm sure that it&nbsp;will continue playing the vital role.Thanks,Kharavela
	              
	              Thanks for the summary and comments, Craig. 
Yes, I understand your sentiments about the challenge posed by some of the work, but I think you handled this very well. &nbsp;Some topics, suchas Design Patterns, will show up in future modules, with more focussed examples and greater detail.
Regards,Anthony
	              
	              Thanks for the comments, Terry. 
I applaud you on being able to deal with all the commitments you listed, whilst still being able to complete the module assignments on time - well done!&nbsp;Regards,Anthony
	              